- en: Monitoring, Logging, and Metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will focus on the operational side of running a large-scale
    distributed system on Kubernetes, as well as on how to design the system and what
    to take into account to ensure top-notch operational posture. That being said,
    things will always go south and you must be ready to detect, troubleshoot, and
    respond as soon as possible. The operational best practices that Kubernetes provides
    out of the box include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Self-healing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Auto scaling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Resource management
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, the cluster administrator and the developers must understand how these
    capabilities work, configure, and interact in order to understand them properly.
    There is always a balancing act between high availability, robustness, performance,
    security, and cost. It's also important to realize that all of these factors and
    the relationships between them change over time and must be revisited and evaluated
    regularly.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is where monitoring comes in. Monitoring is all about understanding what''s
    going on with your system. There are several sources of information that are relevant
    for different purposes:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Logging**: You explicitly log relevant information in your application code
    (and libraries you use may log too).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Metrics**: Collect detailed information about your system such as CPU, memory,
    disk usage, disk I/O, network, and custom application metrics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tracing**: Attach an ID to follow a request across multiple microservices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, we will see how Go-kit, Kubernetes, and the ecosystem enable
    and support all the relevant use cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics are covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Self-healing with Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Autoscaling a Kubernetes cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Provisioning resources with Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting performance right
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logging
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Collecting metrics on Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alerting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distributed tracing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will install several components into the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Prometheus**: A metrics and alerting solution'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fluentd**: A central logging agent'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Jaeger**: A distributed tracing system'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The code is split between two Git repositories:'
  prefs: []
  type: TYPE_NORMAL
- en: You can find the code samples here: [https://github.com/PacktPublishing/Hands-On-Microservices-with-Kubernetes/tree/master/Chapter12](https://github.com/PacktPublishing/Hands-On-Microservices-with-Kubernetes/tree/master/Chapter12)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can find the updated Delinkcious application here: [https://github.com/the-gigi/delinkcious/releases/tag/v0.10](https://github.com/the-gigi/delinkcious/releases/tag/v0.10)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Self-healing with Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Self-healing is a very important property of large-scale systems made up of
    a myriad of physical and virtual components. Microservice-based systems running
    on large Kubernetes clusters are a prime example. Components can fail in multiple
    ways. The premise of self-healing is that the overall system will not fail and
    will be able to automatically heal itself, even if this causes it to operate in
    a reduced capacity temporarily.
  prefs: []
  type: TYPE_NORMAL
- en: 'The building blocks of such reliable systems are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Redundancy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Observability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Auto-recovery
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The basic premise is that every component might fail – machines crash, disks
    get corrupted, network connections drop, configuration may get out of sync, new
    software releases have bugs, third-party services have outages, and so on. Redundancy
    means there are no **single point of failures** (**SPOFs**). You can run multiple
    replicas of many components, like nodes and pods, write data to multiple data
    stores, and deploy your system in multiple data centers, availability zones, or
    regions. You are even able to deploy your system on multiple cloud platforms (especially
    if you use Kubernetes). There is a limit to redundancy, of course. Total redundancy
    is very expansive. For example, running a complete redundant system on both AWS
    and GKE is probably a luxury that very few companies can afford or even need.
  prefs: []
  type: TYPE_NORMAL
- en: Observability is the ability to detect when things go wrong. You must monitor
    your system and understand the signals you observe in order to detect abnormal
    situations. This is the first step before remediation and recovery can take place.
  prefs: []
  type: TYPE_NORMAL
- en: The automated part of auto healing and recovery is not needed in theory. You
    could have a team of operators watching a dashboard all day and take corrective
    action when they identify a problem. In practice, this approach doesn't scale.
    Humans are slow to respond, interpret, and act – not to mention that they are
    much more error-prone. That being said, most automated solutions start with manual
    processes that get automated later as the cost of repeated manual intervention
    becomes clear. If some issues happen only once in a blue moon, then it may be
    OK to address those with manual intervention.
  prefs: []
  type: TYPE_NORMAL
- en: Let's discuss several failure modes and see how Kubernetes helps with all the
    pillars of self-healing.
  prefs: []
  type: TYPE_NORMAL
- en: Container failures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes runs containers inside pods. If a container dies for whatever reason,
    Kubernetes will detect it and restart it right away by default. The behavior of
    Kubernetes can be controlled by the `restartPolicy` file of the pod spec. The
    possible values are `Always` (default), `OnFailure`, and `Never`. Note that the
    restart policy applies to all the containers in the pod. There is no way to specify
    a restart policy per container. This seems a little short-sighted as you may have
    multiple containers in a pod that require a different restart policy.
  prefs: []
  type: TYPE_NORMAL
- en: 'If a container keeps failing, it will enter a `CrashOff`. Let''s see this in
    action by introducing an intentional error to our API gateway:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'After performing a tilt up, we can see that the API gateway enters a `CrashLoopBackOff`
    state. This means that it keeps failing and Kubernetes keep restarting it. The
    backoff part is the delay between restart attempts. Kubernetes uses an exponential
    backoff delay starting at 10 seconds and doubling every time, up to a maximum
    delay of 5 minutes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/f2ee91c7-ecb9-417d-8dc5-6c168a71c5a5.png)'
  prefs: []
  type: TYPE_IMG
- en: Crash loop backoff
  prefs: []
  type: TYPE_NORMAL
- en: This approach is very useful because if the failure was transient, then Kubernetes
    would self-heal by restarting the container until the transient issue went away.
    However, if the problem were to persist, then the container status and the error
    logs are around and provide observability that can be used by higher-level recovery
    processes or as a last resort by a human operator or developer.
  prefs: []
  type: TYPE_NORMAL
- en: Node failure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When a node fails, all the pods on the node will become unavailable and Kubernetes
    will schedule them to run on other nodes in the cluster. Assuming you design your
    system with redundancy in place and the failed node is not a SPOF, the system
    should recover automatically. If the cluster has just a few nodes, then the loss
    of a node can be significant to the cluster's ability to handle traffic.
  prefs: []
  type: TYPE_NORMAL
- en: Systemic failures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Sometimes, systemic failures take place. Some of these are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Total networking failure (entire cluster is unreachable)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data center outage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Availability zone outage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Region outage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cloud provider outage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In these situations, you may not have redundancy by design (the cost-benefit
    ratio is not economical). The system will be down. Users will experience an outage.
    The important thing is not to lose or corrupt any data and be able to come back
    online as soon as the root cause is addressed. However, if it is important for
    your organization to stay online at all costs, Kubernetes will have options for
    you. The operative word is *will*, as in the future. The work on this is conducted
    under a project called federation v2, (v1 was deprecated as it suffered from too
    many problems.)
  prefs: []
  type: TYPE_NORMAL
- en: You will be able to bring up a complete Kubernetes cluster or even a set of
    clusters in a different data center, a different availability zone, a different
    region, or even a different cloud provider. You will be able to run, manage, and
    treat those physically distributed clusters as a single logical cluster and, hopefully,
    fail over between those clusters seamlessly.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to implement this kind of cluster-level redundancy, you may consider
    building it using the gardener ([https://gardener.cloud/](https://gardener.cloud/))
    project.
  prefs: []
  type: TYPE_NORMAL
- en: Autoscaling a Kubernetes cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Autoscaling is all about adapting your system to demand. This can mean adding
    more replicas to a deployment, expanding the capacity of existing nodes, or adding
    new nodes. While scaling your cluster up or down is not a failure, it follows
    the same pattern as self-healing. You can consider a cluster that is misaligned
    with demand as unhealthy. If the cluster is underprovisioned, then requests are
    not handled or wait too long, which can lead to timeouts or just poor performance.
    If the cluster is overprovisioned, then you're paying for resources you don't
    need. In both cases, you can consider the cluster as unhealthy, even if the pods
    and services themselves are up and running.
  prefs: []
  type: TYPE_NORMAL
- en: 'Just like with self-healing, you first need to detect that you need to scale
    your cluster, and then you can take the correct action. There are several ways
    to scale your cluster capacity: you can add more pods, you can add new nodes,
    and you can increase the capacity of existing nodes. Let''s review them in detail.'
  prefs: []
  type: TYPE_NORMAL
- en: Horizontal pod autoscaling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The horizontal pod autoscaler is a controller that is designed to adjust the
    number of pods in a deployment to match the load on those pods. The decision of
    whether a deployment should be scaled up (add pods) or down (remove pods) is based
    on metrics. Out of the box, the horizontal pod autoscaler supports CPU utilization,
    but custom metrics can be added too. The cool thing about the horizontal autoscaler
    is that it sits on top of the standard Kubernetes deployment and just adjusts
    its replica count. The deployment itself and the pods are blissfully unaware that
    they are being scaled:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/5adba37c-b6ff-470b-af58-1f0d64729baf.png)'
  prefs: []
  type: TYPE_IMG
- en: Horizontal pod autoscaler
  prefs: []
  type: TYPE_NORMAL
- en: The preceding diagram illustrates how the horizontal autoscaler works.
  prefs: []
  type: TYPE_NORMAL
- en: Using the horizontal pod autoscaler
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can use kubectl for autoscaling. Since the autoscaler relies on Heapster
    and the metrics server, we need to enable them using the `minikube addons` command.
    We have already enabled Heapster, so this should be good enough:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We must also specify a CPU request in the pod spec of the deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: As you may recall, a resource request is what Kubernetes promises it can provide
    to the container if it is ever scheduled. This way, the horizontal pod autoscaler
    can ensure that it will start a new pod only if it can provide this requested
    minimum of CPU to the new pod.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s introduce some code that will cause the social graph manager to waste
    a lot of CPU:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we are scaling the social graph manager between 1 and 5 pods based on
    CPU utilization of 50%:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'After running a tilt up and deploying the CPU wasting code, the CPU utilization
    increased, and more and more pods were created up to the maximum of five. Here
    is a screenshot of the Kubernetes dashboard that shows the CPU, the pods, and
    the horizontal pod autoscaler:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/81876839-787b-4c2a-a890-1802c792179d.png)'
  prefs: []
  type: TYPE_IMG
- en: Hp dashboard
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s review the horizontal pod autoscaler itself:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the current load is `138%` of CPU utilization, which means that
    more than one CPU core is needed, which is greater than the 50%. Therefore, the
    social graph manager will keep running five pods (the maximum that's allowed).
  prefs: []
  type: TYPE_NORMAL
- en: The horizontal pod autoscaler is a universal mechanism that has been part of
    Kubernetes for a long time. It depends on internal components only for collecting
    metrics. We've demonstrated the default CPU-based autoscaling here, but it can
    be configured to work based on multiple custom metrics, too. Now is a good time
    to look at some other autoscaling methods.
  prefs: []
  type: TYPE_NORMAL
- en: Cluster autoscaling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Pod autoscaling is a gift to developers and operators – there's no need for
    them to manually scale services up and down or write their own half-based autoscaling
    scripts. Kubernetes provides a robust solution that is well-designed, well-implemented,
    and battle tested. However, that leaves the question of cluster capacity. If Kubernetes
    tries to add more pods to your cluster, but the cluster is running at maximum
    capacity, then the pod autoscaler will fail. On the other hand, if you over-provision
    your cluster just in case the pod autoscaler needs to add a few more pods, then
    you're wasting money.
  prefs: []
  type: TYPE_NORMAL
- en: Enter the `auto-scaler` cluster ([https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler](https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler)).
  prefs: []
  type: TYPE_NORMAL
- en: It is a Kubernetes project that has been generally available since Kubernetes
    1.8\. It works with GCP, AWS, Azure, AliCloud, and BaiduCloud. If GKE, EKS, and
    AKS give you a managed control plane (they take care of managing Kubernetes itself),
    then the cluster autoscaler gives you a managed data plane. It will add or remove
    nodes from your cluster based on your needs and your configuration.
  prefs: []
  type: TYPE_NORMAL
- en: The trigger for adjusting the size of the cluster is when Kubernetes can't schedule
    pods due to insufficient resources. This works really well with the horizontal
    pod autoscaler. Together, the combination gives you a truly elastic Kubernetes
    cluster that can grow and shrink automatically (within bounds) to match the current
    load.
  prefs: []
  type: TYPE_NORMAL
- en: The cluster autoscaler is essentially very simple. It doesn't care why pods
    can't be scheduled. It will add nodes to the cluster as long as pods can't be
    scheduled. It will remove empty nodes or nodes that their pods can be rescheduled
    on other nodes. That being said, it is not a completely mindless mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is aware of several Kubernetes concepts and takes them into account when
    deciding to grow or shrink the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: PodDisruptionBudgets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overall resource constraints
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Affinity and anti-affinity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pod priorities and preemption
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For example, if pods with best effort priority can''t be scheduled, the cluster
    autoscaler will not grow the cluster. In particular, it will not remove nodes
    that have one or more of these properties:'
  prefs: []
  type: TYPE_NORMAL
- en: Use local storage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Annotated with `"cluster-autoscaler.kubernetes.io/scale-down-disabled": "true"`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Host pods annotated `"cluster-autoscaler.kubernetes.io/safe-to-evict": "false"`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Host nodes with restrictive `PodDisruptionBudget`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The total time for adding a node is typically less than 5 minutes. The cluster
    autoscaler scans for unscheduled pods every ten seconds and immediately provisions
    a new node if necessary. However, the cloud provider takes 3-4 minutes to provide
    and attach the node to the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s move on to another form of autoscaling: vertical pod autoscaling.'
  prefs: []
  type: TYPE_NORMAL
- en: Vertical pod autoscaling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The vertical pod autoscaler is currently (Kubernetes 1.15) in its Beta stages.
    It takes on a different task related to autoscaling – fine-tuning your CPU and
    memory requests. Consider a pod that doesn't really do much and needs 100 MiB
    of memory, but it currently requests 500 MiB. First of all, it's a net waste of
    400 MiB of memory that is always allocated to the pod and is never used. However,
    the impact can be much greater. Because the pod is chunkier, it can prevent other
    pods from getting scheduled alongside it.
  prefs: []
  type: TYPE_NORMAL
- en: The vertical autoscaler addresses this problem by monitoring the actual CPU
    and memory usage of pods and adjusting their requests automatically. It also requires
    that you install the metrics server.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is very cool. The vertical pod autoscaler works in several modes:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Initial**: Assigns resource requests when the pod is created'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Auto**: Assigns resource requests when the pod is created and also updates
    them during the pod''s lifetime'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Recreate**: Similar to Auto, the pod always restarts when its resource requests
    need to be updated'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**updatedOff**: Doesn''t modify the resource requests, but recommendations
    can be viewed'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'At the moment, Auto works just like Recreate and restarts the pods on every
    change. In the future, it will use an in-place update. Let''s take the vertical
    autoscaler for a spin. The installation is pretty rough and requires cloning the
    Git repository and running a shell script (that runs many other shell scripts):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'It installs a service, two CRDs, and three pods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s create a VPA configuration file for the link manager deployment. We''ll
    set the mode to `Off` so that it only recommends on proper values of CPU and memory
    requests, but doesn''t actually set them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We can create it and examine the recommendations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: I don't recommend letting the vertical pod autoscaler loose on your system at
    this point. It is still in flux and has some serious limitations. The biggest
    one is that it can't run side by side with the horizontal pod autoscaler.
  prefs: []
  type: TYPE_NORMAL
- en: An interesting approach, if you want to utilize it to fine-tune your resource
    requests, is to run it for a while on a test cluster that mimics your production
    cluster, turn off the horizontal pod autoscaler, and see how well it does.
  prefs: []
  type: TYPE_NORMAL
- en: Provisioning resources with Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Provisioning resources has traditionally been an operator or system administrator
    job. However, with the DevOps approach, developers are often tasked with self-provisioning.
    If the organization has a traditional IT department, they are often more concerned
    with what permissions developers should have for provisioning and what global
    limits should they set. In this section, we will look at the problem of resource
    provisioning from both viewpoints.
  prefs: []
  type: TYPE_NORMAL
- en: What resources should you provision?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It's important to distinguish between Kubernetes resources and the underlying
    infrastructure resources they depend on. For Kubernetes resources, the Kubernetes
    API is the way to go. How you interact with the API is up to you, but I recommend
    that you generate YAML files and run them through `kubectl create` or `kubectl
    apply` as part of your CI/CD pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Commands like `kubectl run` and `kubectl scale` are useful for interactive exploration
    of your cluster and running ad hoc tasks, but they go against the grain of declarative
    infrastructure as code.
  prefs: []
  type: TYPE_NORMAL
- en: You could also directly hit the REST endpoints of the Kubernetes API or use
    a client library if you have a very complex CI/CD workflow that you implement
    using some higher-level programming language like Python. Even there, you can
    consider just invoking `kubectl`.
  prefs: []
  type: TYPE_NORMAL
- en: Let's move on to the infrastructure layer that your cluster is running on. The
    primary resources are compute, memory, and storage. Nodes combine compute, memory,
    and local storage. Shared storage is provisioned separately. In the cloud, you
    may use pre-provisioned cloud storage. This means that your primary concern is
    to provision nodes and external storage for your cluster. But that's not all.
    You also need to connect all these nodes via a networking layer and consider permissions.
    The networking in a Kubernetes cluster is taken care of most of the time by a
    CNI provider. The famous flat networking model where each pod gets its own IP
    is one of the best features of Kubernetes and simplifies so many things for developers.
  prefs: []
  type: TYPE_NORMAL
- en: Permissions and access are usually handled by **role-based access control**
    (**RBAC**) on Kubernetes, as we discussed at length in [Chapter 6](f7718dfe-8c96-495b-9089-36b9bbced4c8.xhtml),
    *Securing Microservices with Kubernetes*.
  prefs: []
  type: TYPE_NORMAL
- en: It's very important to impose reasonable quotas and limits on resources given
    that we strive for automatic provisioning.
  prefs: []
  type: TYPE_NORMAL
- en: Defining container limits
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'On Kubernetes, we can define limits on CPU and memory per container. These
    ensure that the container will not use more than the limit. It serves two primary
    purposes:'
  prefs: []
  type: TYPE_NORMAL
- en: Prevents containers and pods on the same node from cannibalizing each other
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Helps Kubernetes schedule pods in the most efficient way by knowing the maximum
    amount of resources a pod will use
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We've looked at limits from a security lens in [Chapter 6](f7718dfe-8c96-495b-9089-36b9bbced4c8.xhtml),
    *Securing Microservices on Kubernetes*. The emphasis was on controlling the blast
    radius. If a container is compromised, it can utilize more than the limit of resources
    configured for it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of setting CPU and memory limits for the `user-manager`
    service. It follows the best practice of setting both resource limits and resource
    requests to the same values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Setting container limits is very useful, but it doesn't help with the problem
    of runaway allocation of many pods or other resources. This is where resource
    quotas come in.
  prefs: []
  type: TYPE_NORMAL
- en: Specifying resource quotas
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Kubernetes lets you specify quotas per namespace. There are different types
    of quotas you can set, for example, CPU, memory, and counts of various objects,
    including persistent volume claims. Let''s set some quotas for the default namespace
    of Delinkcious:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the command to apply to `quota`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can check the resource quota objects for the actual usage and compare
    it to the quota to see how close we are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Obviously, this resource quota is far beyond the current utilization of the
    cluster. That's fine. It doesn't allocate or reserve any resources. It just means
    that the quota is not very restricting.
  prefs: []
  type: TYPE_NORMAL
- en: There are many more nuances and options for resource quotas. There are scopes
    that apply the resource quota for resources with certain conditions or states
    (`Terminating`, `NotTerminating`, `BestEffort`, and `NotBestEffort`). There are
    resources that are quota-specific to certain priority classes. The gist is that
    you can get pretty granular and provide resource quota policies to control the
    resource allocation in your cluster, even in the face of mistakes in configuration
    or attacks.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, we have got our bases covered with resource quotas and can move
    on to actually provisioning resources. There are several ways to do this, and
    we may want to employ some, if not all of them, for complicated systems.
  prefs: []
  type: TYPE_NORMAL
- en: Manual provisioning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Manual provisioning sounds like an anti-pattern, but in practice it is useful
    in several situations; for example, if you are managing on-premises cluster where
    you physically have to provision servers, wire them together, and install storage
    too. Another common use case is during development when you want to develop your
    automated provisioning, but you have an interactive experiment (probably not in
    production). However, even in production, if you discover some misconfiguration
    or another issue, you may need to respond by manually provisioning some resources
    to address the crisis.
  prefs: []
  type: TYPE_NORMAL
- en: Utilizing autoscaling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: On the cloud, it is highly recommended to use the autoscaling solutions we discussed
    earlier. The horizontal pod autoscaler is a no-brainer. The cluster autoscaler
    is great too, if your cluster deals with a very dynamic workload and you don't
    want to overprovision on a regular basis. The vertical autoscaler is probably
    best for fine-tuning your resource requests at this point.
  prefs: []
  type: TYPE_NORMAL
- en: Rolling your own automated provisioning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you have more sophisticated needs, you can always roll your own. Kubernetes
    encourages both running your own controllers that can watch for different events
    and respond by provisioning some resources or even running some tools locally,
    or as part of your CI/CD pipeline, that check the state of the cluster and make
    some provisioning decisions.
  prefs: []
  type: TYPE_NORMAL
- en: Once your cluster is properly provisioned, you should start thinking about performance.
    Performance is interesting because there are so many trade-offs you need to take
    into account.
  prefs: []
  type: TYPE_NORMAL
- en: Getting performance right
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Performance is important for many reasons, which we will delve into soon. It
    is very important to understand when is the right time to try and improve performance.
    My guiding principle is: make it work, make it right, make it fast. That is, first,
    just get the system to do whatever it needs to do, however slow and clunky. Then,
    clean up the architecture and the code. Now, you are ready to take on performance
    and consider refactoring, changes, and many other factors that can impact performance.'
  prefs: []
  type: TYPE_NORMAL
- en: But there is a preliminary step for performance improvements, and that's profiling
    and benchmarking. Trying to improve performance without measuring what you try
    to improve is just like trying to make your code work correctly without writing
    any tests. Not only is it futile, but, even if you actually got lucky and improved
    the performance, how would you know without measurements?
  prefs: []
  type: TYPE_NORMAL
- en: Let's understand something about performance. It makes everything complicated.
    However, it is often a necessary evil. Improving performance is important when
    it affects the user experience or cost. To make things worse, improving the user
    experience often comes at a cost. Finding the sweet spot is difficult. Unfortunately,
    the sweet spot doesn't stay put. Your system evolves, the number of users grow,
    technologies change, and the costs of your resources change. For example, a small
    social media startup has no business building its own data centers, but a social
    media giant like Facebook now designs their own custom servers to squeeze a little
    bit more performance and save costs. Scale changes a lot.
  prefs: []
  type: TYPE_NORMAL
- en: The bottom line is, that in order to make those decisions, you must understand
    how your system works and be able to measure every component and the impact on
    the performance of changes that have been made to your system.
  prefs: []
  type: TYPE_NORMAL
- en: Performance and user experience
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The user experience is all about perceived performance. How fast do I see pretty
    pictures on my screen after I click a button? Obviously, you can improve the real
    performance of your system, buy faster hardware, run things in parallel, improve
    your algorithms, upgrade your dependencies to newer and more performant versions,
    and so on. But, very often, it is more about smarter architecture and doing less
    work by adding caches, providing approximate results, and pushing work to the
    client. Then, there are methods like pre-fetching, where you try to do work before
    it is needed in order to anticipate the user's needs.
  prefs: []
  type: TYPE_NORMAL
- en: User experience decisions can significantly impact performance. Consider a chat
    program where the clients constantly poll the server every second for every keystroke
    versus just checking once every minute for new messages. That's a different user
    experience with a 60x performance price tag.
  prefs: []
  type: TYPE_NORMAL
- en: Performance and high availability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the worst routine things that can happen on a system is a timeout. A
    timeout means that the user will not get an answer on time. A timeout means that
    you did a lot of work that is now wasted. You may have retry logic and the user
    will eventually get their answer, but performance will take a hit. When your system
    and all its components are highly available (as well as not overloaded), you can
    minimize the occurrence of timeouts. If your system is very redundant, you can
    even send the same request multiple times to different backends and, whenever
    one of them responds, you have the answer.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, a highly available and redundant system sometimes requires
    syncing with all the shards/backends (or at least a quorum) to make sure you have
    the latest, most up-to-date answer. Of course, inserting or updating data is also
    more complicated and often takes longer on a highly available system. If the redundancy
    is across multiple availability zones, regions, or continents, it can add orders
    of magnitude to the response time.
  prefs: []
  type: TYPE_NORMAL
- en: Performance and cost
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Performance and cost have a very interesting relationship. There are many ways
    to improve performance. Some of them reduce costs, like optimizing your code,
    compressing the data you send, or pushing computation to the client. However,
    other ways to improve performance increase the cost, like running on stronger
    hardware, replicating your data to multiple locations close to your client, and
    pre-fetching unrequested data.
  prefs: []
  type: TYPE_NORMAL
- en: In the end, this is a business decision. Even win-win performance improvements
    are not always like improving your algorithms is, at the high-priority. For example,
    you can invest a lot of time in coming up with an algorithm that runs 10x faster
    than the previous algorithm. But the computation time might be negligible in the
    overall time to process a request because it's dominated by access to the database,
    serializing the data, and sending it to the client. In this case, you just wasted
    time that could have been used for developing something more useful, you potentially
    destabilized your code and introduced bugs, and made your code harder to understand.
    Again, good metrics and profiling will help you identify the hot spots in your
    system that are worth improvement performance-wise and cost-wise.
  prefs: []
  type: TYPE_NORMAL
- en: Performance and security
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Performance and security are generally at odds. Security typically pushes toward
    encryption across the board, outside and inside the cluster. There are strong
    authentication and authorization methods, may be necessary, but has performance
    overhead. However, security sometimes indirectly helps performance by advocating
    to cut unnecessary features and reduce the surface area of the system. This spartan
    approach that produces tighter systems allows you to focus on a smaller target
    to improve performance. Typically, secure systems don't just add arbitrary features
    that can hurt performance without careful consideration.
  prefs: []
  type: TYPE_NORMAL
- en: Later, we will explore how to collect and use metrics with Kubernetes, but first
    let's take a look at logging, which is another pillar of monitoring your system.
  prefs: []
  type: TYPE_NORMAL
- en: Logging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Logging is the ability to record messages during the operation of your system.
    Log messages are typically structured and timestamped. They are often indispensable
    when trying to diagnose problems and troubleshoot your system. They are also critical
    when doing post-mortems and discovering root causes after the fact. In a large-scale
    distributed system, there will be many components that log messages. Collecting,
    organizing, and sifting through them is a non-trivial task. But first, let's consider
    what information is useful to log.
  prefs: []
  type: TYPE_NORMAL
- en: What should you log?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is the million dollar question. A simplistic approach is to log *everything*.
    You can never have too much data, and it's difficult to predict what data you'll
    need when trying to figure out what's wrong with your system. However, what does
    everything mean exactly? You can obviously go too far. For example, you can log
    every call to every little function in your code, including all the parameters,
    as well as the current state, or log the payload of every network call. Sometimes,
    there are security and regulatory restrictions that prevent you from logging certain
    data, like **protected health information** (**PHI**) and **personally identifiable
    information** (**PII**). You'll need to understand your system well enough to
    decide what kind of information is relevant for you. A good starting point is
    logging any incoming requests and interactions between your microservices and
    between your microservices and third-party services.
  prefs: []
  type: TYPE_NORMAL
- en: Logging versus error reporting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Errors are a special kind of information. There are errors that your code can
    handle (for example, with retries or using some alternative). However, there are
    also errors that must be handled as soon as possible or the system will suffer
    partial or complete outage. But even errors that are not urgent sometimes require
    that you record a lot of information. You could log errors just like any other
    information, but it is often worthwhile to record errors to a dedicated error
    reporting service like Rollbar or Sentry. One of the crucial pieces of information
    with errors is a stack trace that includes the state (local variables) of each
    frame in the stack. For a production system, I recommend that you use a dedicated
    error reporting service, in addition to just logging.
  prefs: []
  type: TYPE_NORMAL
- en: The quest for the perfect Go logging interface
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Delinkcious is primarily implemented with Go, so let''s talk about logging
    in Go. There is a standard library Logger, which is a struct and not an interface.
    It is configurable, and you can pass an `io.Writer` object when you create it.
    However, the methods of the `Logger` struct are rigid and don''t support log levels
    or structured logging. Also, the fact that there is just one output writer may
    be a limitation in some cases. Here is the specification for the standard Logger:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'If you need those capabilities, you need to use another library that sits on
    top of the standard library `Logger`. There are several packages that provide
    various flavors:'
  prefs: []
  type: TYPE_NORMAL
- en: '`glog`: [https://godoc.org/github.com/golang/glog](https://godoc.org/github.com/golang/glog)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logrus`: [https://github.com/Sirupsen/logrus](https://github.com/Sirupsen/logrus)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`loggo`: [https://godoc.org/github.com/juju/loggo](https://godoc.org/github.com/juju/loggo)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`log15`: [https://github.com/inconshreveable/log15](https://github.com/inconshreveable/log15)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They address the interface, comparability, and playability in different ways.
    However, we're using Go-kit, which has its own take on logging.
  prefs: []
  type: TYPE_NORMAL
- en: Logging with Go-kit
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Go-kit has the simplest interface ever. There is just one method, `Log()`,
    that accepts a list of keys and values that can be of any type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The basic idea here is that Go-kit has no opinions about how you log your messages.
    Do you always add a timestamp? Do you have logging levels? What levels? The answers
    to all these questions are up to you. You get a totally generic interface and
    you decide what key-values you want to log.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up a logger with Go-kit
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'OK. The interface is generic, but we need an actual logger object to work with.
    Go-kit supports several writers and logger objects that generate familiar log
    formats like JSON, logfmt, or logrus out of the box. Let''s set up a logger with
    JSON formatter and a sync writer. A sync writer is safe to use from multiple Go
    routines, and a JSON formatter formats the key values into a JSON string. In addition,
    we can add some default fields, such as the service name, which is where the log
    message is coming from in the source code and the current timestamp. Since we
    may want to use the same logger specification from multiple services, let''s put
    it in a package all the services can use. One last thing is to add also a `Fatal()`
    function that will forward to the standard `log.Fatal()` function. This allows
    code that currently uses `Fatal()` to continue working without changes. Here is
    the Delinkcious log package that contains a factory function for the logger and
    the `Fatal()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The writer just writes to the standard error stream, which will be captured
    and sent to the container logs on Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: To see our logger in action, let's attach it to our link service.
  prefs: []
  type: TYPE_NORMAL
- en: Using a logging middleware
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's think about where we want to instantiate our logger and then where we
    want to use it and log messages. This is important because we need to make sure
    that the logger is available to all the places in the code that need to log messages.
    A trivial approach is to just add a logger parameter to all our interfaces and
    propagate the logger in this way. However, this is very disruptive and will violate
    our clean object model. Logging is really an implementation and operational detail.
    Ideally, it should not appear in our object model types or interfaces. Also, it
    is a Go-kit type and, so far, we've managed to keep our object model and even
    our domain packages totally oblivious to the fact that they are wrapped by Go-kit.
    The Delinkcious services under SVC are the only part of the code that is Go-kit
    aware.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s try to keep it this way. Go-kit provides the middleware concept, which
    allows us to chain multiple middleware components in a loosely coupled way. All
    the middleware components for a service implement the service interface, and a
    little shim allows Go-kit to call them one after the other. Let''s begin with
    the shim, which is just a function type that accepts a `LinkManager` interface
    and returns a `LinkManager` interface:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The `logging_middleware.go` file has a factory function called `newLoggingMiddlware()`
    that takes a logger object and returns a function that matches `linkManagerMiddleware`.
    That function, in turn, instantiates the `loggingMiddelware` struct, passing it
    the next component in the chain and the logger:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'This may be very confusing, but the basic idea is having the ability to chain
    arbitrary middleware components that do some work and let the rest of the computation
    go on. The reason that we have all these layers of indirection is that Go-kit
    doesn''t know anything about our types and interfaces, so we have to assist by
    writing this boilerplate code. As I mentioned earlier, all of it can, and should
    be, auto-generated. Let''s examine the `loggingMiddleware` struct and its methods.
    The struct itself has a `linkManager` interface, which is the next component in
    the chain and the logger object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'As a `LinkManager` middleware component, it must implement the `LinkManager`
    interface methods. Here is the implementation of `GetLinks()`. It uses the logger
    to log a few values, specifically, the method name, that is, `GetLinks`, the request
    object, the result, and the duration. Then, it calls the `GetLinks()` method on
    the next component in the chain:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'For simplicity, the other methods just call the next component in the chain
    doing anything:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The middleware chain concept is very powerful. The middleware can preprocess
    inputs before passing them on to the next component, it can short circuit and
    return immediately without calling the next component, or it can postprocess the
    result coming from the next component.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see the log output from the link service when running our smoke test.
    It looks a bit messy for humans, but all the necessary information is there, clearly
    marked and ready for large-scale analysis if needed. It''s easy to grep and it''s
    easy to use tools like `jq` to dig deeper:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Thanks to Go-kit, we have a strong and flexible logging mechanism in place.
    However, manually fetching logs with `kubectl logs` doesn't scale. For real-world
    systems, we need centralized log management.
  prefs: []
  type: TYPE_NORMAL
- en: Centralized logging with Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In Kubernetes, containers write to the standard output and standard error streams.
    Kubernetes makes those logs available (for example, via `kubectl logs`). You can
    even get logs of the previous run of a container if it crashed by using `kubectl
    logs -p`, but, if the pod is rescheduled, then its containers and their logs disappear.
    If the node itself crashes, you'll lose the logs too. Even when all the logs are
    available for a cluster with a lot of services, it is a non-trivial task to sift
    through the container logs and try to make sense of the state of the system. Enter
    centralized logging. The idea is to have a log agent running, either as a side
    container in each pod, or as daemon set on each node, listen to all the logs,
    and ship them in real time to a centralized location where they can be aggregated,
    filtered, and sorted. Of course, you can explicitly log from your containers directly
    to the centralized logging service too.
  prefs: []
  type: TYPE_NORMAL
- en: The simplest and most robust approach in my opinion is the demon set. The cluster
    admin makes sure that a log agent is installed on each node and that's it. There's
    no need to change your pod spec to inject side containers, no need to depend on
    special libraries to communicate with remote logging services. Your code writes
    to standard output and standard error, and you're done. Most other services you
    may use, like web servers and databases, can be configured to write to the standard
    output and standard error, too.
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the most popular log agents on Kubernetes is Fluentd ([https://www.fluentd.org](https://www.fluentd.org)).
    It is also a CNCF graduated project. You should use Fluentd unless you have a
    very good reason to use another logging agent. Here is a diagram that illustrates
    how Fluentd fits into Kubernetes as a DaemonSet that is deployed into each node,
    pulls all the logs of all the pods, and sends them to a centralized log management
    system:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/79606ae7-f9ee-4d71-8fab-e1ed57d8b274.png)'
  prefs: []
  type: TYPE_IMG
- en: Fluentd
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s talk about the log management system. In the open source world, the
    ELK stack – ElasticSearch, LogStash, and Kibana – are a very popular combination.
    ElasticSearch stores the logs and provides various ways to slice and dice them.
    LogStash is the log ingest pipeline and Kibana is a powerful visualization solution.
    Fluentd can replace LogStash as the logging agent, and you get the EFK stack –
    ElasticSearch. Fluentd and Kibana work very well on Kubernetes. There''s also
    Helm charts and the GitHub repository, which can install EFK on your Kubernetes
    cluster in one click. However, you should also consider out of cluster logging
    service. As we discussed previously, logs are very helpful for troubleshooting
    and post mortems. If your cluster is in trouble, you might not be able to access
    your logs at the time you need them the most. Fluentd can integrate with a plethora
    of data outputs. Check the full list here: [https://www.fluentd.org/dataoutputs](https://www.fluentd.org/dataoutputs).
    We''ve got logging covered, so now it''s time to talk about metrics.'
  prefs: []
  type: TYPE_NORMAL
- en: Collecting metrics on Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Metrics are a key component that enables many interesting use cases like self-healing,
    autoscaling, and alerting. Kubernetes, as a distributed platform, has a very strong
    offering around metrics, with a powerful yet generic and flexible metrics API.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes always had support for metrics via cAdvisor (integrated into kube-proxy)
    and Heapster ([https://github.com/kubernetes-retired/heapster](https://github.com/kubernetes-retired/heapster)).
    However, cAdvisor was removed in Kubernetes 1.12 and Heapster was removed in Kubernetes
    1.13\. You can still install them (like we did earlier on minikube using the Heapster
    add-on), but they aren't part of Kubernetes and aren't recommended  anymore. The
    new way to do metrics on Kubernetes is by using the metrics API and the metrics
    server ([https://github.com/kubernetes-incubator/metrics-server](https://github.com/kubernetes-incubator/metrics-server)).
  prefs: []
  type: TYPE_NORMAL
- en: Introducing the Kubernetes metrics API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Kubernetes metrics API is very generic. It supports node and pod metrics,
    as well as custom metrics. A metric has a usage field, a timestamp, and a window
    (the time range the metric was collected in). Here is the API definition for node
    metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The usage field type is `ResourceList`, but it''s actually a map of a resource
    name to a quantity:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'There are two other metrics-related APIs: the external metrics API and the
    custom metrics API. They are designed for extending the Kubernetes metrics with
    arbitrary custom metrics or metrics coming from outside Kubernetes, such as cloud
    providers monitoring. You can annotate those additional metrics and use them for
    autoscaling.'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the Kubernetes metrics server
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Kubernetes metric server is a modern replacement for Heapster and cAdvisor.
    It implements the metrics API and provides the nodes and pods metrics. Those metrics
    are used by the various autoscalers and by the Kubernetes scheduler itself when
    dealing with best effort scenarios. Depending on your Kubernetes distribution,
    the metrics server may or may not be installed. If you need to install it, you
    can use helm. For example, on AWS EKS, you have to install the metrics server
    yourself using the following command (you can choose any namespace):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Typically, you don''t interact with the metrics server directly. You can access
    the metrics using the `kubectl get --raw` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'In addition, you can use the very useful `kubectl` command, that is, `kubectl
    top`, which gives you a quick overview of the performance of your nodes or pods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Note that as of Kubernetes 1.15 (current version at the time of writing) the
    Kubernetes dashboard doesn't integrate with the metrics server for performance
    yet. It still requires Heapster. I'm sure you will be able to work with the metrics-server
    soon.
  prefs: []
  type: TYPE_NORMAL
- en: 'The metrics-server is the standard Kubernetes metrics solution for CPU and
    memory, but, if you want to go further and consider custom metrics, there is one
    obvious choice: Prometheus. Unlike most things Kubernetes, where you have a plethora
    of options in the metrics arena, Prometheus stands head and shoulders above all
    the other free and open source options.'
  prefs: []
  type: TYPE_NORMAL
- en: Using Prometheus
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Prometheus ([https://prometheus.io/](https://prometheus.io/)) is an open source
    and CNCF graduated project (the second after Kubernetes itself). It is the de
    facto standard metrics collection solution for Kubernetes. It has an impressive
    set of features, a large installation base on Kubernetes, and an active community.
    Some of the prominent features are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: A generic multi-dimensional data model where each metric is modeled as a time
    series of key-value pairs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A powerful query language, called PromQL, that lets you generate reports, graphs,
    and tables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A built-in alerting engine where alerts are defined and triggered by PromQL
    queries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Powerful visualization – Grafana, console template language, and more
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Many integrations with other infrastructure components beyond Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s look at the following references:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Monitoring your Kubernetes Deployments with Prometheus**:[ https://supergiant.io/blog/monitoring-your-kubernetes-deployments-with-prometheus/](https://supergiant.io/blog/monitoring-your-kubernetes-deployments-with-prometheus/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Configure Kubernetes Autoscaling With Custom Metrics**: [https://docs.bitnami.com/kubernetes/how-to/configure-autoscaling-custom-metrics/](https://docs.bitnami.com/kubernetes/how-to/configure-autoscaling-custom-metrics/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying Prometheus into the cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Prometheus is a large project with many capabilities, options, and integrations.
    Deploying it and managing it is not a trivial task. There a couple of projects
    that can help. The Prometheus operator ([https://github.com/coreos/prometheus-operator](https://github.com/coreos/prometheus-operator))
    provides a way to deeply configure Prometheus using Kubernetes resources.
  prefs: []
  type: TYPE_NORMAL
- en: 'The operator concept ([https://coreos.com/blog/introducing-operators.html](https://coreos.com/blog/introducing-operators.html))
    was introduced in 2016 by CoreOS (who was acquired by RedHat, who was acquired
    by IBM). A Kubernetes operator is a controller that is responsible for managing
    stateful applications inside a cluster using Kubernetes CRDs. Operators, in practice,
    extend the Kubernetes API to provide a seamless experience when managing foreign
    components like Prometheus. Actually, the Prometheus operator was the first operator
    (along with the Etcd operator):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/462b7964-4c5c-4de6-8951-9e6a3a0bd822.png)'
  prefs: []
  type: TYPE_IMG
- en: Prometheus operator
  prefs: []
  type: TYPE_NORMAL
- en: 'The kube-promethus ([https://github.com/coreos/kube-prometheus](https://github.com/coreos/kube-prometheus))
    project is built on top of the Prometheus operator and adds the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Grafana visualization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A highly available Prometheus cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A highly available `Alertmanager` cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An adapter for the Kubernetes Metrics APIs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kernel and OS metrics via the Prometheus node exporter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Various metrics on the state of Kubernetes objects via `kube-state-metrics`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Prometheus operator brings the ability to launch a Prometheus instance into
    a Kubernetes namespace, configure it, and target services via labels to the table.
  prefs: []
  type: TYPE_NORMAL
- en: 'For now, we''ll just use helm to deploy a full-fledged installation of Prometheus:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The Prometheus server can be accessed via port `80` on the following DNS name
    from within your cluster: `prometheus-server.default.svc.cluster.local`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Get the Prometheus server URL by running the following commands in the same
    shell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The Prometheus `alertmanager` can be accessed via port `80` on the following
    DNS name from within your cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Get the `Alertmanager` URL by running the following commands in the same shell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The Prometheus `pushgateway` can be accessed via port 9091 on the following
    DNS name from within your cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Get the `PushGateway` URL by running the following commands in the same shell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s see what services were installed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Everything seems in order. Let''s follow the instructions and check out the
    Prometheus web UI:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now browse to `localhost:9090` and do some checks. Let''s check the
    number of goroutines in the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/a6db6584-7a81-464e-9125-3a8597f6ea86.png)'
  prefs: []
  type: TYPE_IMG
- en: Prometheus web UI
  prefs: []
  type: TYPE_NORMAL
- en: 'The number of metrics that have been collected by Prometheus is mind-numbing.
    There are hundreds of different built-in metrics. Check out how small the scroll
    thumb on the right is when opening the metric selection dropdown:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/479deab5-2693-4282-9023-bb58d35be2be.png)'
  prefs: []
  type: TYPE_IMG
- en: Prometheus dropdown
  prefs: []
  type: TYPE_NORMAL
- en: There are way more metrics than you will ever need, but each one of them can
    be important for some specific troubleshooting task.
  prefs: []
  type: TYPE_NORMAL
- en: Recording custom metrics from Delinkcious
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'OK: Prometheus is installed and collecting standard metrics automatically,
    but we want to record our own custom metrics too. Prometheus works in pull mode.
    A service that want to provide metrics needs to expose a `/metrics` endpoint (it
    is possible to push metrics to Prometheus too, using its Push Gateway). Let''s
    utilize the middleware concept of Go-kit and add a metrics middleware that''s
    similar to the logging middleware. We will take advantage of the Go client library
    provided by Prometheus.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The client library provides several primitives like counter, summary, histogram,
    and gauge. For the purpose of understanding how to record metrics from a Go service,
    we will instrument each endpoint of the link service to record the number of requests
    (a counter), as well as a summary of all requests (a summary). Let''s start by
    providing factory functions in a separate library called pkg/metrics. The library
    provides a convenient wrapper around the Prometheus Go client. Go-kit has its
    own abstraction layer on top of the Prometheus Go client, but it doesn''t provide
    a lot of value, unless you plan to switch to another metrics provider like `statsd`.
    This is unlikely for Delinkcious and probably for your system too. The service
    name, metric name, and help string will be used to construct the fully qualified
    metric name later:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to construct the middleware. It should look very familiar,
    as it is almost identical to the logging middleware. The `newMetricsMiddleware()`
    function creates a counter and a summary metrics for each endpoint and returns
    it as the generic `linkManagerMiddleware` function we defined earlier (a function
    that accepts the next middleware and returns itself to assemble a chain of components
    that all implement the `om.LinkManager` interface):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The `metricsMiddleware` struct stores the next middleware and two maps. One
    map is a mapping of method names to Prometheus counters, while the other map is
    a mapping of method names to Prometheus summaries. They are used by the `LinkManager`
    interface methods to record metrics separately for each method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The middleware methods use the pattern of performing an action, which, in this
    case, is recording metrics and then calling the next component. Here is the `GetLinks()`
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'The actual metric recording is done by the `recordMetrics()` method, which
    takes the method name (`GetLinks` here) and beginning time. It is deferred until
    the end of the `GetLinks()` method, which allows it to calculate the duration
    of the `GetLinks()` method itself. It uses the counter and summary from the maps
    that match the method name:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, we have our metrics middleware ready to go, but we still need
    to hook it up to the middleware chain and expose it as the `/metrics` endpoint.
    Since we''ve done all the preliminary work, these are just two lines in the link
    service''s `Run()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Now, we can query the `/metrics` endpoint and see our metrics being returned.
    Let's run the smoke test three times and check the metrics of the `GetLinks()`
    and `AddLink()` methods. As expected, the `AddLink()` method was called once per
    smoke test (three times total) and the `GetLinks()` method was called three times
    per test and nine times total. We can also see the help strings.
  prefs: []
  type: TYPE_NORMAL
- en: 'The summary quantiles are very useful when dealing with large datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Custom metrics are great. However, beyond looking at a lot of numbers and graphs
    and histograms and admiring your handiwork, the real value of metrics is to inform
    an automated system or you about changes in the state of the system. That's where
    alerting comes in.
  prefs: []
  type: TYPE_NORMAL
- en: Alerting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Alerting is super important for critical systems. You can plan and build resiliency
    features as much as you want, but you will never build a failproof system. The
    right mindset for building robust and reliable systems is to try to minimize failures,
    but also acknowledge that failures will happen. When failures do happen, you need
    quick detection and have to alert the right people so that they can investigate
    and address the problem. Note that I said explicitly *alerting people*. If your
    system has self-healing capabilities, then you may be interested in viewing a
    report of the issues that the system was able to rectify itself. I don't consider
    those failures, because the system is designed to handle them. For example, containers
    can crash as much as they want; the kubelet will keep restarting them. A container
    crash is not considered a failure from a Kubernetes point of view. If your application
    running inside the container is not designed to handle such crashes and restarts,
    then you may want to configure an alert for this case, but that's your decision.
  prefs: []
  type: TYPE_NORMAL
- en: The main point I want to raise is that failure is a big word. Many things that
    could be considered failures are processes running out of memory, a server crashing,
    a corrupted disk, an intermittent or prolonged network outage, and a data center
    going offline. However, if you design for it and put mitigating measures in place,
    they are not failures of the system. The system will keep running as designed,
    possibly in a reduced capacity, but still running. If those incidents happen a
    lot and degrade to the total throughput of the system or the user experience in
    a significant way, you may want to investigate the root causes and address them.
    This is all part of defining **service-level objectives** (**SLOs**) and **service-level
    agreements** (**SLAs**). As long as you operate within your SLAs, the system is
    not failing, even if multiple components are failing and even if a service doesn't
    meet its SLO.
  prefs: []
  type: TYPE_NORMAL
- en: Embracing component failure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Embracing failure means recognizing that components will fail all the time in
    a large system. This is not an unusual situation. You want to minimize component
    failures because each failure has various costs, even if the system as a whole
    continues to work. But it will happen. Most component failures can be handled
    either automatically or without urgency by having redundancy in place. However,
    systems evolve all the time and most systems are not in the perfect position where
    every component failure has mitigation in place for each type of failure. As a
    result, theoretically preventable component failures might become system failures.
    For example, if you write your logs to a local disk and you don't rotate your
    log files, then, eventually, you'll run out of disk space (very common failure),
    and, if the server using this disk is running some critical component with no
    redundancy, then you've got a system failure on your hands.
  prefs: []
  type: TYPE_NORMAL
- en: Grudgingly accepting system failure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So, system failures will happen. Even the largest cloud providers have outages
    from time to time. There are different levels of system failures, ranging from
    temporary short failure of a non-critical subsystem, through to total outage of
    the entire system for a prolonged time, all the way to massive data loss. An extreme
    example is when malicious attackers target a company and all its backups, which
    can put it out of business. This is more related to security, but it's good to
    understand the full spectrum of system failures.
  prefs: []
  type: TYPE_NORMAL
- en: Common approaches for dealing with system failures are redundancy, backups,
    and compartmentalization. These are solid approaches, but are expensive, and,
    as we mentioned earlier, will not prevent all failures. The next step after minimizing
    the likelihood and impact of system failures is to plan for quick disaster recovery.
  prefs: []
  type: TYPE_NORMAL
- en: Taking human factors into account
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, we're strictly in the domain of people responding to an actual incident.
    Some critical systems may have 24/7 live monitoring by people watching the system
    state diligently and ready to act. Most companies will have alerts based on various
    triggers. Note that, even if you have live 24/7 monitoring for complex systems,
    you still need to surface alerts to the people monitoring the system because,
    for such systems, there is typically a huge amount of data and information that
    describe the current state.
  prefs: []
  type: TYPE_NORMAL
- en: Let's look at several aspects of a reasonable alerting plan that work well for
    people.
  prefs: []
  type: TYPE_NORMAL
- en: Warnings versus alerts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's consider our out of disk space situation again. This is a case were the
    state gets worse over time. The disk space is gradually reduced as more and more
    data is logged to the log files. If you've got nothing in place, you will discover
    that you've ran out of disk space when an application starts to issue strange
    errors, often downstream from the actual failure, and you'll have to trace it
    back to the source. I've been there and done that; it's no fun. A better approach
    is to check the disk space regularly and raise an alert when it exceeds a certain
    threshold (for example, 95%). But why wait until the situation becomes critical?
    In such gradually worsening situations, it is much better to detect the problem
    early (for example, 75%) and issue a warning through some mechanism. This will
    give the system operator ample time to respond without causing an unnecessary
    crisis.
  prefs: []
  type: TYPE_NORMAL
- en: Considering severity levels
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This brings us to alert severity levels. Different severity levels deserve
    different responses. Different organizations may define their own levels. For
    example, PagerDuty has a 1-5 scale that follows the DEFCON ladder. I personally
    prefer two levels for alerts: *wake me up at 3 AM* and *it can wait to the morning*.
    I like to think of severity levels in practical terms. What kind of response or
    follow up do you perform for each severity level? If you always do the same thing
    for severity levels 3 -5, then what''s the benefits of classifying them as 3,
    4, and 5 as opposed to just grouping them together into a single low-priority
    severity level?'
  prefs: []
  type: TYPE_NORMAL
- en: Your situation may be different, so make sure that you consider all stakeholders.
    Production incidents are not fun.
  prefs: []
  type: TYPE_NORMAL
- en: Determining alert channels
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Alert channels are tightly coupled to severity levels. Let''s consider the
    following options:'
  prefs: []
  type: TYPE_NORMAL
- en: Wake-up call to on-call engineer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instant message to a public channel
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Email
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Often, the same incident will be broadcasted to multiple channels. Obviously,
    the wake-up call is the most intrusive, the instant message (for example, slack)
    may pop up as a notification, but someone has to be around and look at it. The
    email is often more informative in nature. It is common to combine multiple channels.
    For example, the on-call engineer gets the wake up call, the team incident channel
    gets a message, and the group manager gets and email.
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning noisy alerts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Noisy alerts are a problem. If there are too many alerts – especially low-priority
    ones – then there are two major problems:'
  prefs: []
  type: TYPE_NORMAL
- en: It is distracting to all the people that get notified (especially to the poor
    engineer being woke up in the middle of the night).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It might lead to people ignoring alerts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You don't want to miss an important alert because of a lot of noisy low-priority
    alerts. Fine-tuning your alerts is an art and an ongoing process.
  prefs: []
  type: TYPE_NORMAL
- en: I recommend reading and adopting *My Philosophy on Alerting* ([https://docs.google.com/document/d/199PqyG3UsyXlwieHaqbGiWVa8eMWi8zzAn0YfcApr8Q/edit](https://docs.google.com/document/d/199PqyG3UsyXlwieHaqbGiWVa8eMWi8zzAn0YfcApr8Q/edit))
    by Rob Ewaschuk (ex-Google site reliability engineer).
  prefs: []
  type: TYPE_NORMAL
- en: Utilizing the Prometheus alert manager
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Alerts naturally feed off metrics. Prometheus, in addition to being a fantastic
    metrics collector, also provides an alert manager. We''ve already installed it
    as part of the overall Prometheus installation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: We're not going to configure any alerts because I don't want to be on call for
    Delinkcious.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Alert manager has a conceptual model that includes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Groupings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integrations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inhibition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Silences
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Grouping deals with consolidating multiple signals into a single notification.
    For example, if many of your services use AWS S3 and it suffers an outage, then
    a lot of services might trigger alerts. But with grouping, you can configure the
    alert manager to send just one notification.
  prefs: []
  type: TYPE_NORMAL
- en: Integrations are notification targets. The alert manager supports many targets
    out of the box like email, PagerDuty, Slack, HipChat, PushOver, OpsGenie, VictoOps,
    and WeChat. For all other integrations, the recommendation is to use the generic
    HTTP webhook integration.
  prefs: []
  type: TYPE_NORMAL
- en: Inhibition is an interesting concept where you can skip sending notifications
    for alerts if other alerts are already firing. This is another way on top of grouping
    to avoid sending multiple notifications for the same high-level problem.
  prefs: []
  type: TYPE_NORMAL
- en: Silences are just a mechanism to temporarily mute some alerts. This is useful
    if your alerting rules are not neatly configured with grouping and inhibitions,
    or even if some valid alerts keep firing, but you are already handling the situation
    and you don't need more notifications at the moment. You can configure silences
    in the web UI.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring alerts in Prometheus
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can raise alerts by configuring rules in the Prometheus server configuration
    file. Those alerts are handled by the alert manager which decides, based on its
    configuration, what to do about them. Here is an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: The rule has an expression, which, if true, triggers the alert. There is a period
    of time (1 minute here) where the condition must be true, so that you can avoid
    triggering one-off anomalies (if you so choose). There is severity associated
    with the alert and some annotations.
  prefs: []
  type: TYPE_NORMAL
- en: With metrics and alerts covered, let's move on and see what to do when an alert
    fires and we get notified of a problem.
  prefs: []
  type: TYPE_NORMAL
- en: Distributed tracing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The notifications that alert you that something is wrong can be as vague as
    *Something is wrong with the website*. Well, that''s not very useful for troubleshooting,
    detecting the root cause, and fixing it. This is especially true for microservice-based
    architectures where every user request can be handled by a large number of microservices
    and each component might fail in interesting ways. There are several ways to try
    and narrow down the scope:'
  prefs: []
  type: TYPE_NORMAL
- en: Look at recent deployments and configuration changes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Check whether any of your third-party dependencies suffered an outage.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consider similar issues if the root cause hasn't been fixed yet.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you're lucky, you can just diagnose the problem right away. However, when
    debugging large-scale distributed systems, you don't really want to rely on luck.
    It's much better to have a methodical approach in place. Enter distributed tracing.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the Jaeger ([https://www.jaegertracing.io/](https://www.jaegertracing.io/))
    distributed tracing system. It is yet another CNCF project that started as an
    Uber open source project. The problems Jaeger can help with are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Distributed transaction monitoring
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performance and latency optimization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Root cause analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Service dependency analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distributed context propagation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Before we can use Jaeger, we need to install it into the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Jaeger
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The best way to install Jaeger is using the Jaeger-operator, so let''s install
    the operator first:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the operator has been installed, we can create a Jaeger instance using
    the following manifest:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'This is a simple in-memory instance. You can also create instances that are
    backed up by Elasticsearch and Cassandra:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/84c05323-cd15-47b8-8331-24d78c6fc841.png)Jaeger UI'
  prefs: []
  type: TYPE_NORMAL
- en: Jaeger has a very slick web UI that lets you drill down and explore distributed
    workflows.
  prefs: []
  type: TYPE_NORMAL
- en: Integrating tracing into your services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are several steps here, but the gist of it is that you can think of tracing
    as another form of middleware. The core abstraction is a span. A request spans
    multiple microservices, and you record those spans and associate logs with them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the tracing middleware, which is similar to the logging middleware,
    except that it starts a span for the `GetLinks()` method instead of logging. As
    usual, there is the factory function that returns a `linkManagerMiddleware` function
    that calls the next middleware in the chain. The factory function accepts a tracer,
    which can start and finish a span:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s add the following function to create a Jaeger tracer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, the `Run()` function creates a new tracer and a tracing middleware that
    it hooks up to the chain of middlewares:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'After running the smoke test, we can search the logs for reports of spans.
    We expect three spans since the smoke test calls `GetLinks()` three times:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: There is much more to tracing and Jaeger. This is barely starting to scratch
    the surface. I encourage you to read more on it, experiment with it, and integrate
    it into your systems.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered a large number of topics, including self-healing,
    autoscaling, logging, metrics, and distributed tracing. Monitoring a distributed
    system is tough. Just installing and configuring the various monitoring services
    like Fluentd, Prometheus, and Jaeger is a non-trivial project. Managing the interactions
    between them and how your services support logging, instrumentation, and tracing
    adds another level of complexity. We've seen how Go-kit, with its middleware concept,
    makes it somewhat easier to add those operational concerns in a decoupled way
    from the core business logic. Once you have all the monitoring for those systems
    in place, there's a new set of challenges to take into account – how do you gain
    insights from all the data? How can you integrate it into your alerting and incident
    response process? How can you continuously improve your understanding of the system
    and improve your processes? These are all hard questions that you'll have to answer
    for yourself, but you may find some guidance in the *Further reading* section
    that follows.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will look at the exciting world of service meshes and
    Istio. Service meshes are a true innovation that can really offload many operational
    concerns from the services and let them focus on their core domain. However, a
    service mesh like Istio has a large surface area and there is a significant learning
    curve to overcome. Do the benefits of the service mesh compensate for the added
    complexity? We'll find out soon.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Refer to the following links to find out more about what was covered in this
    chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Kubernetes federation**: [https://github.com/kubernetes-sigs/federation-v2](https://github.com/kubernetes-sigs/federation-v2)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kubernetes autoscaler**: [https://github.com/kubernetes/autoscaler](https://github.com/kubernetes/autoscaler)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The hunt for a logger interface**: [https://go-talks.appspot.com/github.com/ChrisHines/talks/structured-logging/structured-logging.slide#1](https://go-talks.appspot.com/github.com/ChrisHines/talks/structured-logging/structured-logging.slide#1)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gradener**: [https://gardener.cloud](https://gardener.cloud)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prometheus**: [https://prometheus.io/docs/introduction/overview/](https://prometheus.io/docs/introduction/overview/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fluentd**: [https://www.fluentd.org/](https://www.fluentd.org/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cluster-level logging**: [https://kubernetes.io/docs/concepts/cluster-administration/logging/#cluster-level-logging-architectures](https://kubernetes.io/docs/concepts/cluster-administration/logging/#cluster-level-logging-architectures)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Monitoring best practices**: [https://docs.google.com/document/d/199PqyG3UsyXlwieHaqbGiWVa8eMWi8zzAn0YfcApr8Q/edit#](https://docs.google.com/document/d/199PqyG3UsyXlwieHaqbGiWVa8eMWi8zzAn0YfcApr8Q/edit#)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Jaeger**: [https://github.com/jaegertracing/jaeger](https://github.com/jaegertracing/jaeger)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
