- en: Trident Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapters, we covered the architecture of Storm, its topology,
    bolts, spouts, tuples, and so on. In this chapter, we are covering Trident, which
    is a high-level abstraction over Storm.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are covering the following points in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Trident
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding Trident's data model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Writing Trident functions, filters, and projections
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Trident repartitioning operations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Trident aggregators
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When to use Trident
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Trident introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Trident is a high-level abstraction built on top of Storm. Trident supports
    stateful stream processing, while pure Storm is a stateless processing framework.
    The main advantage of using Trident is that it guarantees that every message entered
    into the topology is processed only once, which would be difficult to achieve
    with vanilla Storm. The concept of Trident is similar to high-level batch processing
    tools, such as Cascading and Pig, developed over Hadoop. To achieve exactly-once
    processing, Trident processes the input stream in small batches. We will cover
    this in more detail in the [Chapter 5](part0102.html#318PC0-6149dd15e07b443593cc93f2eb31ee7b),
    *Trident Topology and Uses*, *Trident state* section.
  prefs: []
  type: TYPE_NORMAL
- en: In the first three chapters, we learned that, in Storm's topology, the spout
    is the source of tuples. A tuple is a unit of data that can be processed by a
    Storm application, and a bolt is the processing powerhouse where we write the
    transformation logic. But in the Trident topology, the bolt is replaced with the
    higher level semantics of functions, aggregates, filters, and states.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Trident's data model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Trident tuple is the data model of a Trident topology. The Trident tuple
    is the basic unit of data that can be processed by a Trident topology. Each tuple
    consists of a predefined list of fields. The value of each field can be a byte,
    char, integer, long, float, double, Boolean, or byte array. During the construction
    of a topology, operations are performed on a tuple, which will either add new
    fields to the tuple or replace the tuple with a new set of fields.
  prefs: []
  type: TYPE_NORMAL
- en: Each of the fields in a tuple can be accessed by name, `(getValueByField(String))`,
    or its positional index, `(getValue(int))`, in the tuple. The Trident tuple also
    provides convenience methods, such as `getIntegerByField(String)`, which saves
    you from typecasting the objects.
  prefs: []
  type: TYPE_NORMAL
- en: Writing Trident functions, filters, and projections
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section covers the definition of Trident functions, filters, and projections.
    Trident functions, filters, and projections are used to modify/filter the input
    tuples based on certain criteria. This section also covers how we can write Trident
    functions, filters, and projections.
  prefs: []
  type: TYPE_NORMAL
- en: Trident function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Trident functions contain logic to modify the original tuple. A Trident function
    gets a set of fields of the tuple as input and emits one or more tuples as output.
    The fields of the output tuples are merged with the fields of the input tuple
    to form the complete tuple, which will pass to the next action in the topology.
    If the Trident function emits no tuples corresponding to the input tuple, then
    that tuple is removed from the stream.
  prefs: []
  type: TYPE_NORMAL
- en: We can write a custom Trident function by extending the `storm.trident.operation.BaseFunction`
    class and implementing the `execute(TridentTuple tuple, TridentCollector collector)`
    method.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s write the sample Trident function, which will return the new field called
    `sum`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Suppose we get `dummyStream` as input, which contains four fields, `a`, `b`,
    `c`, `d`, and only fields `a` and `b` are passed as input fields to the `SumFunction`
    function. The `SumFunction` class emits new a field, `sum`. The `sum` field emitted
    by the `execute` method of the `SumFunction` class is merged with the input tuple
    to form the complete tuple. Hence, the total number of fields in the output tuple
    is `5 (a, b, c, d, sum)`. Here is a sample piece of code that shows how we can
    pass the input fields and the name of the new field to the Trident function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The following diagram shows the input tuples, `SumFunction`, and the output
    tuples. The output tuples contain five fields, `a`, `b`, `c`, `d`, and `sum`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00026.gif)'
  prefs: []
  type: TYPE_IMG
- en: Trident filter
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A Trident filter gets a set of fields as input and returns either true or false
    depending on whether a certain condition is satisfied or not. If true is returned,
    then the tuple is kept in the output stream; otherwise, the tuple is removed from
    the stream.
  prefs: []
  type: TYPE_NORMAL
- en: We can write a custom Trident filter by extending the `storm.trident.operation.BaseFilter`
    class and implementing the `isKeep(TridentTuple tuple)` method.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s write a sample Trident filter that will check whether the sum of the
    input fields is even or odd. If the sum is even, then the Trident filter emits
    true; otherwise it emits false:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Suppose we get `dummyStream` as input, which contains four fields, `a`, `b`,
    `c`, `d`, and only fields `a` and `b` are passed as input fields to the `CheckEvenSumFilter`
    filter. The `execute` method of the `CheckEvenSumFilter` class will emit only
    those tuples whose sum of `a` and `b` is even. Here is a sample piece of code
    that shows how we can define the input fields for a Trident filter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The following diagram shows the input tuples, `CheckEvenSumFilter`, and output
    tuples. `outputStream` contains only those tuples whose sum of fields `a` and
    `b` is even:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00027.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Trident projection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A Trident projection keeps only those fields in the stream that are specified
    in the projection operation. Suppose an input stream contains three fields, `x`,
    `y`, and `z`, and we are passing field `x` to the projection operation, then the
    output tuples will contain a single field, `x`. Here is the piece of code that
    shows how we can use the projection operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The following diagram shows the Trident projection:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00028.gif)'
  prefs: []
  type: TYPE_IMG
- en: Trident repartitioning operations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By performing repartitioning operations, a user can partition tuples across
    multiple tasks. The repartitioning operation doesn't make any changes to the content
    of the tuples. Also, the tuples will only pass over the network for the repartitioning
    operation. Here are the different types of repartitioning operation.
  prefs: []
  type: TYPE_NORMAL
- en: Utilizing shuffle operation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This repartitioning operation partitions the tuples in a uniform, random way
    across multiple tasks. This repartitioning operation is generally used when we
    want to distribute the processing load uniformly across the tasks. The following
    diagram shows how the input tuples are repartitioned using the `shuffle` operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00029.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'Here is a piece of code that shows how we can use the `shuffle` operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Utilizing partitionBy operation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This repartitioning operation enables you to partition the stream on the basis
    of the fields in the tuples. For example, if you want all the tweets from a particular
    user to go to the same target partition, then you can partition the tweet stream
    by applying `partitionBy` to the `username` field in the following manner:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The `partitionBy` operation applies the following formula to decide the target
    partition:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Target Partition = hash(fields) % (number of target partition)*'
  prefs: []
  type: TYPE_NORMAL
- en: As the preceding formula shows, the `partitionBy` operation calculates the hash
    of the input fields to decide the target partition. Hence, it does not guarantee
    that all the tasks will get tuples to process. For example, if you have applied
    a `partitionBy` to a field, say `X`, with only two possible values, `A` and `B`,
    and created two tasks for the `MyFilter` filter, then it might be possible that
    hash (`A`) % 2 and hash (`B`) % 2 are equal, which will result in all the tuples
    being routed to a single task and the other tuples being completely idle.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows how the input tuples are repartitioned using the
    `partitionBy` operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00030.gif)'
  prefs: []
  type: TYPE_IMG
- en: As the preceding diagram shows, **Partition 0** and **Partition 2** contain
    a set of tuples, but **Partition 1** is empty.
  prefs: []
  type: TYPE_NORMAL
- en: Utilizing global operation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This repartitioning operation routes all the tuples to the same partition.
    Hence, the same target partition is selected for all the batches in the stream.
    Here is a diagram that shows how the tuples are repartitioned using the `global`
    operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00031.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'Here is a piece of code that shows how we can use the `global` operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Utilizing broadcast operation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `broadcast` operation is a special repartitioning operation that does not
    partition the tuples, but replicates them to all partitions. Here is a diagram
    that shows how the tuples are sent over the network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00032.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'Here is a piece of code that shows how we can use the `broadcast` operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Utilizing batchGlobal operation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This repartitioning operation sends all the tuples belonging to one batch into
    the same partition. The other batches of the same stream may go to a different
    partition. As the name suggests, this repartition is global at the batch level.
    Here is a diagram that shows how the tuples are repartitioned using the `batchGlobal`
    operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00033.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'Here is a piece of code that shows how we can use the `batchGlobal` operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Utilizing partition operation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If none of the preceding repartitioning fits your use case, you can define your
    own custom repartition function by implementing the `org.apche.storm.grouping.CustomStreamGrouping`
    interface.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a sample custom repartition that partitions the stream on the basis
    of the value of the `country` field:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The `CountryRepartition` class implements the `org.apache.storm.grouping.CustomStreamGrouping`
    interface. The `chooseTasks()` method contains the repartitioning logic to identify
    the next task in the topology for the input tuple. The `prepare()` method is called
    at the start and performs the initialization activity.
  prefs: []
  type: TYPE_NORMAL
- en: Trident aggregator
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Trident aggregator is used to perform the aggregation operation on the
    input batch, partition, or input stream. For example, if a user wants to count
    the number of tuples present in each batch, then we can use the count aggregator
    to count the number of tuples in each batch. The output of the aggregator completely
    replaces the value of the input tuple. There are three types of aggregator available
    in Trident:'
  prefs: []
  type: TYPE_NORMAL
- en: '`partitionAggregate`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`aggregate`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`persistenceAggregate`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's understand each type of aggregator in detail.
  prefs: []
  type: TYPE_NORMAL
- en: partitionAggregate
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As the name suggests, the `partitionAggregate` works on each partition instead
    of the whole batch. The output of `partitionAggregate` completely replaces the
    input tuple. Also, the output of `partitionAggregate` contains a single-field
    tuple. Here is a piece of code that shows how we can use `partitionAggregate`
    :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'For example, we get an input stream containing the fields `x` and `y` and we
    apply a `partitionAggregate` function to each partition; the output tuples contain
    a single field called `count`. The `count` field represent the number of tuples
    presents in the input partition:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00034.gif)'
  prefs: []
  type: TYPE_IMG
- en: aggregate
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `aggregate` works on each batch. During the aggregate process, the tuples
    are first repartitioned using the global operation to combine all the partitions
    of the same batch into a single partition and then the aggregation function is
    run on each batch. Here is a piece of code that shows how we can use `aggregate`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'There are three types of aggregator interface available in Trident:'
  prefs: []
  type: TYPE_NORMAL
- en: '`ReducerAggregator`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Aggregator`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CombinerAggregator`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These three aggregator interfaces can also be used with `partitionAggregate`.
  prefs: []
  type: TYPE_NORMAL
- en: ReducerAggregator
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `ReducerAggregator` first runs the global repartitioning operation on the
    input stream to combine all the partitions of the same batch into a single partition,
    and then runs the aggregation function on each batch. The `ReducerAggregator<T>`
    interface contains the following methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '`init()`: This method returns the initial value'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Reduce(T curr, TridentTuple tuple)`: This method iterates over the input tuples
    and emits a single tuple with a single value'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This example shows how we can implement `Sum` using the `ReducerAggregator`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Aggregator
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `Aggregator` first runs the global repartitioning operation on the input
    stream to combine all the partitions of the same batch into a single partition,
    and then runs the aggregation function on each batch. By definition, the `Aggregator`
    looks very similar to the `ReduceAggregator`. The `BaseAggregator<State>` contains
    the following methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '`init(Object batchId, TridentCollector collector)`: The `init()` method is
    called before starting the processing of a batch. This method returns the `State`
    object, which will be used to save the state of the batch. This object is used
    by the `aggregate()` and `complete()` methods.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`aggregate (State s, TridentTuple tuple, TridentCollector collector)`: This
    method iterates over each tuple of a given batch. This method updates the state
    in the `State` object after processing each tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`complete(State state, TridentCollector tridentCollector)`: This method is
    called at the end, if all the tuples of a given batch are processed. This method
    returns a single tuple corresponding to each batch.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is an example that shows how we can implement a sum using the `BaseAggregator`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: CombinerAggregator
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `CombinerAggregator` first runs the `partitionAggregate` on each partition,
    then runs the global repartitioning operation to combine all the partitions of
    the same batch into a single partition, and then reruns the `aggregator` on the
    final partition to emit the desired output. The network transfer here is less
    compared to the other two aggregators. Hence, the overall performance of the `CombinerAggregator`
    is better than the `Aggregator` and `ReduceAggregator`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `CombinerAggregator<T>` interface contains the following methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '`init()`: This method runs on each input tuple to retrieve the fields'' value
    from the tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`combine(T val1, T val2)`: This method combines the values of the tuples. This
    method emits a single tuple with a single field as the output.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`zero()`: This method returns zero if the input partition contains no tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This example shows how we can implement `Sum` using `CombinerAggregator`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: persistentAggregate
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `persistentAggregate` works on all the tuples across all the batches in
    a stream and persists the aggregate result into the source of state (memory, Memcached,
    Cassandra, or some other database). Here is some code that shows how we can use
    the `persistentAggregate`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: We will discuss in more detail in the [Chapter 5](part0102.html#318PC0-6149dd15e07b443593cc93f2eb31ee7b),
    *Trident Topology and Uses*, *Trident state* section.
  prefs: []
  type: TYPE_NORMAL
- en: Aggregator chaining
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Trident provides a feature to apply multiple aggregators to the same input
    stream, and this process is called **aggregator chaining**. Here is a piece of
    code that shows how we can use aggregator chaining:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: We have applied the `Average()` and `Sum()` aggregators to each partition. The
    output of `chainedAgg()` contains a single tuple corresponding to each input partition.
    The output tuple contains two fields, `sum` and `average`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows how aggregator chaining works:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00035.gif)'
  prefs: []
  type: TYPE_IMG
- en: Utilizing the groupBy operation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `groupBy` operation doesn''t involve any repartitioning. The `groupBy`
    operation converts the input stream into a grouped stream. The main function of
    the `groupBy` operation is to modify the behavior of the subsequent aggregate
    function. The following diagram shows how the `groupBy` operation groups the tuples
    of a single partition:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00036.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'The behavior of `groupBy` is dependent on a position where it is used. The
    following behavior is possible:'
  prefs: []
  type: TYPE_NORMAL
- en: If the `groupBy` operation is used before a `partitionAggregate`, then the `partitionAggregate`
    will run the `aggregate` on each group created within the partition.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the `groupBy` operation is used before an `aggregate`, the tuples of the
    same batch are first repartitioned into a single partition, then `groupBy` is
    applied to each single partition, and at the end it will perform the `aggregate`
    operation on each group.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When to use Trident
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is very easy to achieve exactly-once processing using the Trident topology,
    and Trident was designed for this purpose. It would be difficult to achieve exactly-once
    processing with vanilla Storm, so Trident will be useful when we need exactly-once
    processing.
  prefs: []
  type: TYPE_NORMAL
- en: Trident is not fit for all use cases, especially for high-performance use cases,
    because Trident adds complexity to Storm and manages the state.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we mainly concentrated on Trident high-level abstraction over
    Storm and learned about the Trident filter, function, aggregator, and repartitioning
    operations.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will cover non-transactional topology, Trident topology,
    and Trident topology using a distributed RPC.
  prefs: []
  type: TYPE_NORMAL
