- en: Scraping Using pyquery – a Python Library
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Starting from this chapter, we will be exploring scraping-related tools and
    techniques, as we will also be deploying some scraping code. Features related
    to web exploration, Python libraries, element identification, and traversing are
    the major concepts we have learned about so far.
  prefs: []
  type: TYPE_NORMAL
- en: Web scraping is often a challenging and long process that requires an understanding
    of how the website is performing. A basic ability to understand and identify the
    backends or tools that are used to build a website will assist in any scraping
    task. This is also related to a process known as reverse engineering. For more
    information on such tools, please refer to [Chapter 3](9e1ad029-726f-4ed3-897a-c68bcd61f71e.xhtml),
    *Using LXML, XPath, and CSS Selectors*, and the *using web browser developer tools
    for accessing web content* section. In addition to this, identifying the tools
    for traversing and manipulating elements such as HTML tags is also required, and
    `pyquery` is one of them.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous chapters, we explored XPath, CSS Selectors, and LXML. In this
    chapter, we will look into using `pyquery`, which has a jQuery-like ability that
    seems to be more efficient and, hence, easier to deal with when it comes to web
    scraping procedures.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, you will learn about the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to `pyquery`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring `pyquery` (major methods and attributes)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using `pyquery` for web scraping
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A web browser (Google Chrome or Mozilla Firefox) is required for this chapter.
    We will be using the following Python libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '`pyquery`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`urllib`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`requests`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If these libraries don't exist in your current Python setup, refer to [Chapter
    2](b9919ebf-2d5c-4721-aa76-5c1378262473.xhtml), *Python and the Web – Using urllib
    and Requests*, and the *Setting things up* section, for installation and setup
    help.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code files for this chapter are available in this book''s GitHub repository:
    [https://github.com/PacktPublishing/Hands-On-Web-Scraping-with-Python/tree/master/Chapter04](https://github.com/PacktPublishing/Hands-On-Web-Scraping-with-Python/tree/master/Chapter04).'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to pyquery
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`pyquery` is a jQuery-like library for Python that uses the `lxml` library.
    This provides an easy and interactive environment for dealing with markup elements
    in terms of manipulation and traversal purposes.'
  prefs: []
  type: TYPE_NORMAL
- en: '`pyquery` expressions are also similar to `jquery`, and users with `jquery`
    knowledge will find it more convenient to use in Python.'
  prefs: []
  type: TYPE_NORMAL
- en: The `pyquery` Python library, as its name suggests, enhances `query` writing
    procedures related to elements found in XML and HTML. `pyquery` shortens element
    processing and provides a more insightful scripting approach that is fit for scraping
    and DOM-based traversal and manipulation tasks.
  prefs: []
  type: TYPE_NORMAL
- en: '`pyquery` expressions use CSS selectors to perform queries, alongside additional
    features that it implements. For example, the following expression is used by
    `pyquery`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The following expression is used by `cssselect`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: jQuery (write less, do more) is one of the most admired JavaScript libraries
    and is small, quick, and has lots of features that support DOM/HTML/CSS, and more.
    Web document-based traversing, manipulation, event handling, animation, AJAX,
    and more are some of its main features. Please visit [https://jquery.com/](https://jquery.com/)
    for more information.For more information on `pyquery` and its documentation,
    please visit [https://pythonhosted.org/pyquery/](https://pythonhosted.org/pyquery/)
    or [https://github.com/gawel/pyquery/](https://github.com/gawel/pyquery/).
  prefs: []
  type: TYPE_NORMAL
- en: Exploring pyquery
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we move on and explore `pyquery` and its features, let''s start by installing
    it by using `pip`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: For more information on using `pip` and library installation, please refer to
    the *Setting things up* section in [Chapter 2](b9919ebf-2d5c-4721-aa76-5c1378262473.xhtml),
    *Python and the Web – Using urllib and Requests*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following libraries are installed on a successful installation of `pyquery`
    using `pip`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`cssselect-1.0.3`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lxml-4.3.1`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pyquery-1.4.0`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`>>>` in the code represents the use of the Python IDE; it accepts the code
    or instructions and displays the output on the next line.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the installation is completed and successful, we can use `pyquery`, as
    shown in the following code, to confirm the setup. We can explore the properties
    it contains by using the `dir()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we will explore certain features from `pyquery` that are relevant to scraping
    concepts. For this purpose, we will be using a page source available from [https://www.python.org](https://www.python.org)
    that has been saved locally as `test.html` to provide real-world usability:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/3ab8aac9-7322-44af-8a01-d3fd6723b621.png)'
  prefs: []
  type: TYPE_IMG
- en: Page source obtained from https://www.python.orgIn Google Chrome, you can right-click
    on the web page and choose the View page source menu option or press *Ctrl* +
    *U* to obtain the page source.
  prefs: []
  type: TYPE_NORMAL
- en: Obtaining the page source or HTML code only is not enough, though, as we need
    to load this content into the library to gain more tools to explore with. We'll
    be doing this in the upcoming section.
  prefs: []
  type: TYPE_NORMAL
- en: While testing or following the code, you might find or require changes to be
    done on the `pyquery` code expressions in order to obtain the real output. Page
    sources that are obtained now might be updated or changed. You are suggested to
    obtain the latest page source from the source URL ([https://www.python.org](https://www.python.org)).
  prefs: []
  type: TYPE_NORMAL
- en: Loading documents
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In most cases, a document''s content is obtained by using `requests` or `urllib`
    and is provided to `pyquery` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '`pyquery` can also load URLs using the Python library, `urllib` (default),
    or requests. It also supports requests-based parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The `pq` object we obtained from the preceding code is being parsed using the
    XML parser (default) that''s available from `lxml`, which can also be updated
    with the extra `parser` argument being passed to it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Normally, HTML code from a page source or other sources, such as files, is
    provided as a string to `pyquery` for further processing, as shown in the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: With the `PyQuery` object or `pq` that was received from the document or URL
    that was loaded, we can proceed and explore the features that are available from
    `pyquery`.
  prefs: []
  type: TYPE_NORMAL
- en: Element traversing, attributes, and pseudo-classes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`pyquery` has a large set of attributes and methods that can be deployed to
    obtain the desired content. In the following examples, we''ll identify the implementation
    from the code that''s found in this section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The following are a few of their functions, along with a description, that
    can be seen in the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '`find()`: Searches the provided element or evaluates the query expression build
    using CSS selectors'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`text()`: Returns the element content as a string'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attr()`: Identifies the attribute and returns its content'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`html()`: Returns the HTML content of the evaluated expression'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `class` and `id` CSS attributes are represented with `.` and `#`, respectively,
    and are prefixed to the attribute's value. For example, `<a class="main" id="mainLink">`
    will be identified as `a.main` and `a#mainLink`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code, we are listing all the identified `<ul>` elements with
    the `class` attribute and the `menu` value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The expression was passed to a PyQuery object, which generated a list of evaluated
    elements. These elements are iterated for their exact values or their content.
  prefs: []
  type: TYPE_NORMAL
- en: 'PyQuery also contains pseudo classes or `:pseudo element`, and are used for
    indexing and obtaining predefined expression results. `:pseudo element` can also
    be appended to an existing selector query. The following code implements some
    of the pseudo elements that are common while traversing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s go over the pseudo elements that were used in the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '`:first`: Returns the first occurrence of an element from the content provided'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`:last`: Returns the last occurrence of an element from the content provided'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s look at a general implementation of a few more `:pseudo element` to
    list the HTML elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The following are the `:pseudo element` that we used in the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '`:header`: Returns the header elements (*h1, h2,..., h5, h6*) found in the
    page.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`:input`: Returns all the input elements. Large numbers of HTML `<form>`-based
    pseudo elements exist. Please refer to [https://pythonhosted.org/pyquery/](https://pythonhosted.org/pyquery/)
    for more information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`:empty`: Returns all the elements that don''t have any child element.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`:odd`: Returns elements indexed as odd numbers. They can be used with other
    `:pseudo element` as `:empty:odd`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`:even`: Similar to `:odd`, but returns evenly indexed elements.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following code demonstrates an expression for traversing, `:pseudo element`,
    and element attributes together:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The following are a few more `:pseudo element`. We can use these to address
    the `index` of the elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '`:eq`: Selects the particular index number; evaluates to `equals to`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`:lt`: Evaluates to `less than` for the provided index number. For example, `page(''a:lt(2)'')`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`:gt`: Evaluates to `greater than` for the provided index numbers. For example, `page(''a:gt(0)'')`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Apart from the general features that are used to identify the index and find
    elements, `:pseudo element` can also be used to search the element with the provided
    text, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The following list describe simple definitions of `:contains` and `eq()`, as
    used in the previous code:'
  prefs: []
  type: TYPE_NORMAL
- en: '`:contains`: Matches all elements that contain the provided text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eq()`: Returns the element that was found for a particular index number. Evaluates
    as `equals to` and is similar to `:eq`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pyquery` has a few functions that return a Boolean answer, which is quite
    effective in circumstances where you need to search for an element with attributes
    and also confirm the attribute''s value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The following are the functions that were used in the previous code, along
    with their definitions:'
  prefs: []
  type: TYPE_NORMAL
- en: '`is_()`: Accepts a selector as an argument and returns `True` if the selector
    matches elements, otherwise, it returns `False`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`has_class()`: Returns `True` if the selector matches the class that''s provided.
    It is useful for identifying elements with the `class` attribute.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have used a few important functions and tools with `pyquery` that enhance
    element identification and traversal-related properties. In the next section,
    we will learn about and demonstrate iteration.
  prefs: []
  type: TYPE_NORMAL
- en: Iterating
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will be demonstrating the iterating (perform repeatedly)
    facility that's available with `pyquery`. It's effective and easy to process in
    many situations.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code, we are searching for the `name` and `property` attributes
    that are found in the `<meta>` tags that contain the word `Python.org`. We are
    also using Python''s `List Comprehension` technique to demonstrate the one-line
    coding feature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see in the preceding code, we are using the `items()` function in
    a loop with the element meta to iterate for the provided option. An expression
    resulting in iterable objects can be explored using `items()`. Results that return
    `None` are excluded from the list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, the `pyquery` object collects the names and links that
    are available from the social and web development section. These can be found
    under Use Python for... in the following screenshot. The object is iterated using
    the Python list comprehension technique:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/2fa561d2-c2e5-446b-9e2a-0fbc729fdb29.png)'
  prefs: []
  type: TYPE_IMG
- en: Upcoming events to be extracted using pyquery
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code, we will be exploring a few more details that were retrieved
    from the `upcomingevents` iteration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '`eventsList` contains extracted details from Upcoming Events, as shown in the
    preceding screenshot. The output from `eventsList` is provided here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: DevTools can be used to identify a CSS selector for the particular section and
    can be further processed with the looping facility. For more information regarding
    the CSS Selector, please refer to [Chapter 3](9e1ad029-726f-4ed3-897a-c68bcd61f71e.xhtml),
    *Using LXML, XPath, and CSS Selectors*, *and the* *XPath and CSS selectors using
    DevTools* section.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code illustrates a few more examples of the `pyquery` iterating
    process via the use of `find()` and `items()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: For more information on features, attributes, and methods from `pyquery`, please
    refer to the [https://pythonhosted.org/pyquery/index.html](https://pythonhosted.org/pyquery/index.html).
  prefs: []
  type: TYPE_NORMAL
- en: Web scraping using pyquery
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we learned about using some important features that
    are available from `pyquery` and traversing or identifying elements using those
    features. In this section, we will be using most of these features from `pyquery`
    and we will be using them to scrape data from the web by providing examples with
    various use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Example 1 – scraping data science announcements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this example, we will be scraping announcements-related details that are
    found within the data science category from [https://developer.ibm.com/announcements/category/data-science/](https://developer.ibm.com/announcements/category/data-science/).
  prefs: []
  type: TYPE_NORMAL
- en: The same URL from [https://developer.ibm.com/](https://developer.ibm.com/) has
    also been used to collect data using `lxml.cssselect` under *Example 3*, in the
    *Web scraping using LXML* section from [Chapter 3](9e1ad029-726f-4ed3-897a-c68bcd61f71e.xhtml),
    *Using LXML, XPath, and CSS Selectors*. It is suggested that you explore both
    examples and compare the features that were used.
  prefs: []
  type: TYPE_NORMAL
- en: 'To begin with, let''s import `pyquery` and `requests`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Create `dataSet` so that you have an empty list to collect data that we will
    find from various pages, along with the libraries to be used. We have declared
    `read_url()`, which will be used to read the provided URL and return a `PyQuery`
    object. In this example, we will be using `sourceUrl`, that is, [https://developer.ibm.com/announcements/](https://developer.ibm.com/announcements/):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The information to be collected can be retrieved from [https://developer.ibm.com/announcements/category/data-science/?fa=date:DESC&fb=](https://developer.ibm.com/announcements/category/data-science/?fa=date:DESC&fb=)
    or obtained using `sourceUrl+"category/data-science/?fa=date:DESC&fb="`. Here,
    we will be looping through `pageUrls`.
  prefs: []
  type: TYPE_NORMAL
- en: '`pageUrls` results in the following page URLs. These were obtained by using
    list comprehension and `range()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://developer.ibm.com/announcements/category/data-science/page/1?fa=date:DESC&fb=](https://developer.ibm.com/announcements/category/data-science/page/1?fa=date:DESC&fb=)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://developer.ibm.com/announcements/category/data-science/page/2?fa=date:DESC&fb=](https://developer.ibm.com/announcements/category/data-science/page/2?fa=date:DESC&fb=)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As shown in the following code, `pageUrls` generates a list of page-based URLs
    that can be processed further via the use of the `get_details()` function. This
    is used to retrieve articles:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see from the preceding code, the following URLs were listed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://developer.ibm.com/announcements/category/data-science/page/1?fa=date:DESC&fb=](https://developer.ibm.com/announcements/category/data-science/page/1?fa=date:DESC&fb=)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://developer.ibm.com/announcements/category/data-science/page/2?fa=date:DESC&fb=](https://developer.ibm.com/announcements/category/data-science/page/2?fa=date:DESC&fb=)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The URLs from `pageUrls` are iterated and passed to `get_details()` for further
    processing, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The page URL that's passed to `get_details()` is read by `read_url()` and `response`
    from a `PyQuery` object is obtained. Information that contains blocks are identified
    as articles using CSS selectors. Since there's more than one `articles` iteration
    available, we use `items()`. Individual data elements are then processed with
    the help of cleaning, replacing, and merging activities before they are appended
    to the main dataset, which in this case is `dataSet`. PyQuery expressions can
    also be shortened via the use of `articlebody`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, the `remove()` `PyQuery` (manipulation) method is used to remove `.ibm--card__date`,
    which is found inside `<h5>`, in order to obtain `atype`. The `atype` content
    would also contain additional `.ibm--card__date` details if used without removing
    with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The final output that''s obtained from the preceding code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Example 2 – scraping information from nested links
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this example, we will be scraping details for quotes found in books from
    [http://quotes.toscrape.com/tag/books/](http://quotes.toscrape.com/tag/books/).
    Each individual quote contains certain information, plus a link to the author''s
    detail page, which will also be processed so that we can obtain information regarding
    the author:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/baf63b98-caa9-42bd-9bb1-2a98431ed559.png)'
  prefs: []
  type: TYPE_IMG
- en: Main page from http://quotes.toscrape.com/tag/books/
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code, the elements in `keys` will be used as keys for output
    and will contain the Python dictionary. Basically, we will be collecting data
    for elements in keys:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '`read_url()` from the preceding code is also updated and is different in comparison
    to the libraries we used in the *Example 1 – scraping data science announcements*
    section. In this example, it returns the PyQuery object for the provided URL:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: There is an additional iteration being done with `dataSet` for certain values
    from the `info` dictionary, which is found inside `dataSet`.
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in the following code, `get_details()` uses a `while` loop for pagination
    purposes, and is controlled by the `nextPage` value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '`:has()` returns the element that matches the selector that''s passed to it.
    In this example, we are confirming whether the `pager` class has an `<li>` element
    with the `next` class, that is, `ul.pager:has(''li.next'')`. If the expression
    is `true`, then a page link exists for another page, and `else` terminates the
    loop.'
  prefs: []
  type: TYPE_NORMAL
- en: '`quotes` that are obtained are iterated using `items()` to obtain `title`, `author`, `tags`,
    and `authorLink`. The `authorLink` URL is further processed using the `read_url()`
    function in order to obtain author-related, specific information from the `.author-born-date`
    and `.author-born-location` classes for `born_date` and `born_location`, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The elements classes we used in the preceding code can be found in Page Source,
    as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/f47c0fb5-1f76-4c9e-913f-744b7e77353d.png)'
  prefs: []
  type: TYPE_IMG
- en: Inner page with author details
  prefs: []
  type: TYPE_NORMAL
- en: The `zip()` Python function is used with *keys* and quotes fields, which is
    appended to `dataSet` as a Python Dictionary.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output for the preceding code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'An additional loop was run for the obtained `dataSet`, which results in a string,
    as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Example 3 – extracting AHL Playoff results
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this example, we will be extracting data from **American Hockey League** (**AHL**)
    Playoff results, which are available from [http://www.flyershistory.com/cgi-bin/ml-poffs.cgi](http://www.flyershistory.com/cgi-bin/ml-poffs.cgi):
    [](http://www.flyershistory.com/cgi-bin/ml-poffs.cgi)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/d25987e0-887b-486b-91a4-ef3741168acb.png)'
  prefs: []
  type: TYPE_IMG
- en: AHL Playoff results
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding URL contains the Playoff results for the AHL. This page presents
    information about the results in tabular format. The portion of the page source
    that shows relevant information is shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/18529f0b-9c17-4f1c-a590-e6395d50908c.png)'
  prefs: []
  type: TYPE_IMG
- en: Page source from http://www.flyershistory.com/cgi-bin/ml-poffs.cgiThe preceding
    screenshot contains the top and bottom part of the tabular information from the
    source URL and presents two different formats of `<tr>` that are available in
    the page source. The number of `<td>` that are available in `<tr>` have different,
    extra information.
  prefs: []
  type: TYPE_NORMAL
- en: With the source format analyzed, it's also necessary to point out that `<td>`
    containing the desired values has no attributes that can be used to identify particular
    table cells. In this case, we can target the position of `<td>` or cell with data
    by using CSS selectors, that is, *pseudo selectors* such as `td:eq(0)` or `td:eq(1)`.
  prefs: []
  type: TYPE_NORMAL
- en: For more information on CSS selectors, please visit [Chapter 3](9e1ad029-726f-4ed3-897a-c68bcd61f71e.xhtml),
    *Using LXML, XPath, and CSS Selectors*, the *Introduction to XPath and CSS selector*
    section, in the *CSS Selectors* and *Pseudo Selectors* sub-section.
  prefs: []
  type: TYPE_NORMAL
- en: Since we will be using `pyquery` for this example, we will use the `eq()` method,
    which accepts the index and returns the element. For example, we could use `tr.find('td').eq(1).text()`
    for the chosen PyQuery object, `tr`, search for the element `td`, that is, `<td>`,
    with the index equal to `1`, and return the text of the element.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we are interested in collecting data for the columns that are listed
    in `keys`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s import the code with `pyquery` and `re`. Regex will be used to
    separate the date that was obtained from the page source:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, `read_url()` accepts one argument, that is, the link to the page, and
    returns the PyQuery object of the page source or `pageSource`. PyQuery automatically
    returns the page source for the provided URL. The page source can also be obtained
    by using other libraries, such as `urllib`, `urllib3`, `requests`, and LXML, and
    passed to create a PyQuery object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '`tableRows` is a PyQuery object that will be used to traverse `<tr>` that exists
    inside `<table>`, which is located after `<h1>`. It contains the `AHL Playoff
    Results` text, which is obtained by using the `find()` function. As we can see
    in the following output, a total of `463` `<tr>` elements exist, but the actual
    number of records that were obtained might be lower, in terms of the number of
    available `<td>` with the actual data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s do some more processing. Each `<tr>` or `tr` element is an item of `tableRows`
    and is traversed with the help of the `items()` method to find the exact `<td>`
    or `td` by using their index and retrieving the data it contains:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'So far, the desired data from the targeted `<td>` has been collected and also
    formatted in the case of `year`. Regex has also been applied in the code and used
    with `dates` and `game_status`. Finally, the collected objects are appended as
    a list to `dataSet`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The output regarding the total record count and `dataSet` is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Example 4 – collecting URLs from sitemap.xml
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this example, we will be extracting URLs that have been found for blogs in
    the `sitemap.xml` file from [https://webscraping.com/sitemap.xml](https://webscraping.com/sitemap.xml).
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding examples, we used HTML content, but PyQuery can also be used
    to traverse XML file content. By default, `pyquery` uses an LXML-based `xml` parser,
    which can be provided while creating a PyQuery object. We will be using both `lxml.html`
    and `xml` in the file's content.
  prefs: []
  type: TYPE_NORMAL
- en: For more information on `pyquery` and `parser`, please visit the *Exploring
    pyquery* section of this chapter. For information regarding the site map, please
    visit [Chapter 1](af7787bb-7fcf-4101-8680-9bad14bf22e1.xhtml), *Web Scraping Fundamentals*,
    the *Data finding techniques* *(seeking data from the web)* section, in the *Sitemaps*
    subsection.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows the content that''s available in the `sitemap.xml`
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/59bb93de-f744-4d15-acf0-5cc90a0e90f1.png)'
  prefs: []
  type: TYPE_IMG
- en: sitemap.xml file from https://webscraping.com
  prefs: []
  type: TYPE_NORMAL
- en: To begin with, let's import `pyquery` and read the file's content as `xmlFile`*:*
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Case 1 – using the HTML parser
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here, we will be using the `lxml.html` parser to parse `xmlFile` by passing
    an argument parser, `parser=''html''`, to PyQuery:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Using PyQuery''s `urlHTML` object allows us to check the count and the child
    elements that were obtained from the data, as shown in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, `urlHTML.children()` contains the required elements to look for
    the URL. We can process this data with the `items()` method, which traverses through
    each element that's obtained. Let's create `dataSet` (Python `list()`) that will
    be appended with the URLs that are extracted.
  prefs: []
  type: TYPE_NORMAL
- en: 'Element-based iteration can be performed with `urlHTML.children().find(''loc:contains("blog")'').items()`
    by using a selector that contains the `blog` string:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we will receive the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Case 2 – using the XML parser
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this case, we will be processing XML content with the PyQuery `urlXML` object,
    which uses `parser=''xml''`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code returns the length of the children''s count, that is, `137`
    total URLs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'As shown in the following code, the first and inner children elements return
    the required URL content we are willing to extract:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s proceed with the child elements by using a selector similar to the one
    we used in the *Case 1 – using the HTML parser* section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we have received no output in `dataSet`, and it looks like the selector
    isn''t working like it did in *Case 1 – using the HTML parser*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s verify this case by using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: The node that we received belongs to [https://www.sitemaps.org/schemas/sitemap/0.9](https://www.sitemaps.org/schemas/sitemap/0.9).
    Without removing the namespace selectors, it will not work.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `remove_namespace()` function can be used on a PyQuery object and processed
    for its final output, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'We receive the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: The PyQuery `remove_namespace()` and `xhtml_to_html()` methods remove the namespaces
    from XML and XHTML, respectively. Use of these two methods allows us to work with
    elements that use HTML-related properties.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also process the same content with a different approach; that is, by
    using a regular expression and obtaining the output as required. Let''s proceed
    with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'The PyQuery `children()` object method returns all the child nodes, and `text()`
    will extract the text content, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'As shown in the preceding output, all the links from the child nodes are returned
    as a single string:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, `re.split()` is used to split the string of URLs received with the space
    character, `\s`. This returns a total of `139` elements. Finally, `blogXML` is
    filtered using `re.findall()`, which finds the `blog` string in the `blogXML`
    elements and results in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: In this section, we have used a few scraping techniques to extract the desired
    content from files and websites. Content identification and the requirement to
    scrape is pretty dynamic and is also based on the structure of the website. With
    libraries such as `pyquery`, we can obtain and deploy the necessary tools and
    techniques for scraping in an effective and efficient manner.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`pyquery` seems to be more efficient in dealing with CSS selectors and provides
    a lot of features related to LXML. Simple and readable code is always in demand,
    and `pyquery` provides these features for scraping purposes. In this chapter,
    we explored various cases that you may encounter while performing scraping tasks
    and successfully managed to get the desired outcome.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will be exploring a few more libraries related to web
    scraping.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'PyQuery complete API: [https://pyquery.readthedocs.io/en/latest/api.html](https://pyquery.readthedocs.io/en/latest/api.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'pyquery: a jquery-like library for Python: [https://pythonhosted.org/pyquery/](https://pythonhosted.org/pyquery/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'CSS Selector Reference: [https://www.w3schools.com/cssref/css_selectors.asp](https://www.w3schools.com/cssref/css_selectors.asp)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'CSS Pseudo Class and Elements: [https://www.w3schools.com/css/css_pseudo_elements.asp](https://www.w3schools.com/css/css_pseudo_elements.asp)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'CSS information: [http://www.css3.info/](http://www.css3.info/) and [https://developer.mozilla.org/en-US/docs/Web/CSS](https://developer.mozilla.org/en-US/docs/Web/CSS)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sitemaps: [https://www.sitemaps.org/](https://www.sitemaps.org/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: XML*:* [https://www.w3schools.com/xml/](https://www.w3schools.com/xml/) and
    [https://www.w3.org/XML/](https://www.w3.org/XML/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
