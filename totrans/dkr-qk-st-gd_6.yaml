- en: Docker Networking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will learn about Docker networking. We will dive deep into
    Docker networking, learning how containers can be isolated, how they can communicate
    with each other, and how they can communicate with the outside world. We will
    explore the local network drivers Docker provides in the out-of-the-box installation.
    Then, we will examine the use of remote network drivers with an example deployment
    of the Weave driver. After that, we will learn how to create Docker networks.
    We will round out the discussion with a look at the free services that we get
    with our Docker networks.
  prefs: []
  type: TYPE_NORMAL
- en: '"Approximately 97% of all shipping containers are manufactured in China. It
    is far easier to produce the container close to the shipment than to re-position
    containers around the world."                                                 
                              – [https://www.billiebox.co.uk/](https://www.billiebox.co.uk/)'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: What is a Docker network?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What built-in (also known as **local**) Docker networks are all about
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What about third-party (also known as **remote**) Docker networks?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to create Docker networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The free service discovery and load balancing features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The right Docker network driver to use for your needs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You will be pulling Docker images from Docker's public repo, and installing
    network drivers from Weave, so basic internet access is required to execute the
    examples within this chapter. Also, we will be using the `jq software` package,
    so if you haven't installed it yet, please see the instructions on how to do so—they
    can be found in *The container inspect command* section of [Chapter 2](e66034ed-dcc0-48a8-a2ec-9466669e6649.xhtml),
    *Learning Docker Commands*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code files of this chapter can be found on GitHub:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Docker-Quick-Start-Guide/tree/master/Chapter06](https://github.com/PacktPublishing/Docker-Quick-Start-Guide/tree/master/Chapter06)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Check out the following video to see the code in action: [http://bit.ly/2FJ2iBK](http://bit.ly/2FJ2iBK)'
  prefs: []
  type: TYPE_NORMAL
- en: What is a Docker network?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you already know, a network is a linkage system that allows computers and
    other hardware devices to communicate. A Docker network is the same thing. It
    is a linkage system that allows Docker containers to communicate with each other
    on the same Docker host, or with containers, computers, and hardware outside of
    the container's host, including containers running on other Docker hosts.
  prefs: []
  type: TYPE_NORMAL
- en: If you are familiar with the cloud computing analogy of pets versus cattle,
    you understand the necessity of being able to manage resources at scale. Docker
    networks allow you to do just that. They abstract away most of the complexity
    of networking, delivering easy-to-understand, easy-to-document, and easy-to-use
    networks for your containerized apps. The Docker network is based on a standard,
    created by Docker, called the **Container Network Model** (**CNM**). There is
    a competing networking standard, created by CoreOS, called the **Container Network
    Interface** (**CNI**). The CNI standard has been adopted by several projects,
    most notably Kubernetes, and arguments can be made to support its use. However,
    in this chapter, we will focus our attention on the CNM standard from Docker.
  prefs: []
  type: TYPE_NORMAL
- en: 'The CNM has been implemented by the libnetwork project, and you can learn more
    about that project by following the link in the references for this section. The
    CNM implementation, written in Go, is made up of three constructs: the sandbox,
    the endpoint, and the network. The sandbox is a network namespace. Each container
    has its own sandbox. It holds the configuration of the container''s network stack.
    This includes its routing tables, interfaces, and DNS settings for IP and MAC
    addresses. The sandbox also contains the network endpoints for the container.
    Next, the endpoints are what join the sandbox to networks. Endpoints are essentially
    network interfaces, such as **eth0**. A container''s sandbox may have more than
    one endpoint, but each endpoint will connect to only a single network. Finally, a
    network is a collection of connected endpoints, which allow communication between
    connections. Every network has a name, an address space, an ID, and a network
    type.'
  prefs: []
  type: TYPE_NORMAL
- en: Libnetwork is a pluggable architecture that allows network drivers to implement
    the specifics for the components we just described. Each network type has its
    own network driver. Docker provides built-in drivers. These default, or local,
    drivers include the bridge driver and the overlay driver. In addition to the built-in
    drivers, libnetwork supports third-party-created drivers. These drivers are referred
    to as remote drivers. Some examples of remote drivers include Calico, Contiv,
    and Weave.
  prefs: []
  type: TYPE_NORMAL
- en: 'You now know a little about what a Docker network is, and after reading these
    details, you might be thinking, where''s the *easy* that he talked about? Hang
    in there. now we are going to start discussing how easy it is for you to create
    and use Docker networks. As with Docker volume, the network commands represent their own
    management category. As you would expect, the top-level management command for network
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The subcommands available in the network management group include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Let's now take a look at the built-in or local network drivers.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Check out the following links for more information:'
  prefs: []
  type: TYPE_NORMAL
- en: Pets versus cattle talk slide-deck: [https://www.slideshare.net/randybias/architectures-for-open-and-scalable-clouds](https://www.slideshare.net/randybias/architectures-for-open-and-scalable-clouds)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Libnetwork project: [https://github.com/docker/libnetwork](https://github.com/docker/libnetwork)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Libnetwork design: [https://github.com/docker/libnetwork/blob/master/docs/design.md](https://github.com/docker/libnetwork/blob/master/docs/design.md)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calico network driver: [https://www.projectcalico.org/](https://www.projectcalico.org/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contiv network driver: [http://contiv.github.io/](http://contiv.github.io/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Weave network driver: [https://www.weave.works/docs/net/latest/overview/](https://www.weave.works/docs/net/latest/overview/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Built-in (local) Docker networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The out-of-the-box install of Docker includes a few built-in network drivers.
    These are also known as local drivers. The two most commonly used drivers are
    the bridge network driver and the overlay network driver. Other built-in drivers
    include none, host, and MACVLAN. Also, without your creating networks, your fresh
    install will have a few networks pre-created and ready to use. Using the `network
    ls` command, we can easily see the list of pre-created networks available in the
    fresh installation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/75a654de-335d-4082-bf45-f0a8b6a60b6f.png)'
  prefs: []
  type: TYPE_IMG
- en: In this list, you will notice that each network has its unique ID, a name, a
    driver used to create it (and that controls it), and a network scope. Don't confuse
    a scope of local with the category of driver, which is also local. The local category
    is used to differentiate the driver's origin from third-party drivers that have
    a category of remote. A scope value of local indicates that the limit of communication
    for the network is bound to within the local Docker host. To clarify, if two Docker
    hosts, H1 and H2, both contain a network that has the scope of local, containers
    on H1 will never be able to communicate directly with containers on H2, even if
    they use the same driver and the networks have the same name. The other scope
    value is swarm, which we'll talk more about shortly.
  prefs: []
  type: TYPE_NORMAL
- en: The pre-created networks that are found in all deployments of Docker are special
    in that they cannot be removed. It is not necessary to attach containers to any
    of them, but attempts to remove them with the `docker network rm` command will
    always result in an error.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are three built-in network drivers that have a scope of local: bridge,
    host, and none. The host network driver leverages the networking stack of the
    Docker host, essentially bypassing the networking of Docker. All containers on
    the host network are able to communicate with each other through the host''s interfaces.
    A significant limitation to using the host network driver is that each port can
    only be used by a single container. That is, for example, you cannot run two nginx
    containers that are both bound to port `80`. As you may have guessed because the
    host driver leverages the network of the host it is running on, each Docker host
    can only have one network using the host driver:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/8878cc4b-db35-4ae2-9bd1-2bef6653af31.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Next up, is the null or none network. Using the null network driver creates
    a network that when a container is connected to it provides a full network stack
    but does not configure any interfaces within the container. This renders the container
    completely isolated. This driver is provided mainly for backward-compatibility
    purposes, and like the host driver, only one network of the null type can be created
    on a Docker host:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/ae095a5c-2365-445b-84b0-f4930d6f947e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The third network driver with a scope of local is the bridge driver. Bridge
    networks are the most common type. Any containers attached to the same bridge
    network are able to communicate with one another. A Docker host can have more
    than one network created with the bridge driver. However, containers attached
    to one bridge network are unable to communicate with containers on a different
    bridge network, even if the networks are on the same Docker host. Note that there
    are slight feature differences between the built-in bridge network and any user-created
    bridge networks. It is best practice to create your own bridge networks and utilize
    them instead of the using the built-in bridge network.  Here is an example of
    running a container using a bridge network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/e929dbdf-7012-41dc-8577-789de4f1c1ae.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In addition to the drivers that create networks with local scope, there are
    built-in network drivers that create networks with swarm scope. Such networks
    will span all the hosts in a swarm and allow containers attached to them to communicate
    in spite of running on different Docker hosts. As you probably have surmised,
    use of networks that have swarm scope requires Docker swarm mode. In fact, when
    you initialize a Docker host into swarm mode, a special new network is created
    for you that has swarm scope. This swarm scope network is named *ingress* and
    is created using the built-in overlay driver. This network is vital to the load
    balancing feature of swarm mode that saw used in the *Accessing container applications
    in a swarm* section of [Chapter 5](f1681897-580b-44fb-9e43-4aed37e67529.xhtml),
    *Docker Swarm*. There''s also a new bridge network created in the `swarm init`,
    named docker_gwbridge. This network is used by swarm to communicate outward, kind
    of like a default gateway.  Here are the default built-in networks found in a
    new Docker swarm:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/73d4326f-1d8f-409a-906d-603de525d80c.png)'
  prefs: []
  type: TYPE_IMG
- en: Using the overlay driver allows you to create networks that span Docker hosts.
    These are layer 2 networks. There is a lot of network plumbing that gets laid
    down behind the scenes when you create an overlay network. Each host in the swarm
    gets a network sandbox with a network stack. Within that sandbox, a bridge is
    created and named br0\. Then, a VXLAN tunnel endpoint is created and attached
    to bridge br0\. Once all of the swarm hosts have the tunnel endpoint created,
    a VXLAN tunnel is created that connects all of the endpoints together. This tunnel
    is actually what we see as the overlay network. When containers are attached to
    the overlay network, they get an IP address assigned from the overlay's subnet,
    and all communications between containers on that network are carried out via
    the overlay. Of course, behind the scenes that communication traffic is passing
    through the VXLAN endpoints, going across the Docker hosts network, and any routers
    connecting the host to the networks of the other Docker hosts. But, you never
    have to worry about all the behind-the-scenes stuff. Just create an overlay network,
    attach your containers to it, and you're golden.
  prefs: []
  type: TYPE_NORMAL
- en: The next local network driver that we're going to discuss is called MACVLAN.
    This driver creates networks that allow containers to each have their own IP and
    MAC addresses, and to be attached to a non-Docker network. What that means is
    that in addition to the container-to-container communication you get with bridge
    and overlay networks, with MACVLAN networks you also are able to connect with
    VLANs, VMs, and other physical servers. Said another way, the MACVLAN driver allows
    you to get your containers onto existing networks and VLANs. A MACVLAN network
    has to be created on each Docker host where you will run containers that need
    to connect to your existing networks. What's more, you will need a different MACVLAN
    network created for each VLAN you want containers to connect to. While using MACVLAN
    networks sounds like the way to go, there are two important challenges to using
    it. First, you have to be very careful about the subnet ranges you assign to the
    MACVLAN network. Containers will be assigned IPs from your range without any consideration
    of the IPs in use elsewhere. If you have a DHCP system handing out IPs that overlap
    with the range you gave to the MACVLAN driver, it can easily cause duplicate IP
    scenarios. The second challenge is that MACVLAN networks require your network
    cards to be configured in promiscuous mode. This is usually frowned upon in on-premise
    networks but is pretty much forbidden in cloud-provider networks such as AWS and
    Azure, so the MACVLAN driver will have very limited use cases.
  prefs: []
  type: TYPE_NORMAL
- en: There is a lot of information covered in this section on local or built-in network
    drivers. Don't despair! They are much easier to create and use than this wealth
    of information seems to indicate. We will go into creating and using info shortly
    in the *Creating Docker networks* section, but next, let's have a quick discussion
    about remote (also known as third-party) network drivers.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Check out these links for more information:'
  prefs: []
  type: TYPE_NORMAL
- en: Excellent, in-depth Docker article for Docker networking: [https://success.docker.com/article/networking](https://success.docker.com/article/networking)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Networking with Overlay Networks: [https://docs.docker.com/network/network-tutorial-overlay/](https://docs.docker.com/network/network-tutorial-overlay/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using MACVLAN networks: [https://docs.docker.com/v17.12/network/macvlan/](https://docs.docker.com/v17.12/network/macvlan/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Third-party (remote) network drivers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As mentioned previously in the *What is a Docker network*? section, in addition
    to the built-in, or local, network drivers provided by Docker, the CNM supports
    community- and vendor-created network drivers. Some examples of these third-party
    drivers include Contiv, Weave, Kuryr, and Calico. One of the benefits of using
    one of these third-party drivers is that they fully support deployment in cloud-hosted
    environments, such as AWS. In order to use these drivers, they need to be installed
    in a separate installation step for each of your Docker hosts. Each of the third-party
    network drivers brings their own set of features to the table. Here is the summary
    description of these drivers as shared by Docker in the reference architecture
    document:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/4b8d5ba1-8d15-41fe-9a21-fe4577e95705.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Although each of these third-party drivers has its own unique installation,
    setup, and execution methods, the general steps are similar. First, you download
    the driver, then you handle any configuration setup, and finally you run the driver.
    These remote drivers typically do not require swarm mode and can be used with
    or without it. As an example, let''s take a deep-dive into using the weave driver.
    To install the weave network driver, issue the following commands on each Docker
    host:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding steps need to be completed on each Docker host that will be used
    to run containers that will communicate with each other over the weave network.
    The launch command can provide the hostname or IP address of the first Docker
    host, which was set up and already running the weave network, to peer with it
    so that their containers can communicate. For example, if you have set up `node01`
    with the weave network when you start up weave on `node02`, you would use the
    following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, you can connect new (Docker host) peers using the connect command,
    executing it from the first host configured. To add `node02` (after it has weave
    installed and running), use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: You can utilize the weave network driver without enabling swarm mode on your
    hosts. Once weave has been installed and started, and the peers (other Docker
    hosts) have been connected, your containers will automatically utilize the weave
    network and be able to communicate with each other regardless of whether they
    are on the same Docker host or different ones.
  prefs: []
  type: TYPE_NORMAL
- en: 'The weave network shows up in your network list just like any of your other
    networks:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/77ef436f-4d57-4313-9ec0-9a109434c4f8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s test out our shiny new network. First, make sure that you have installed
    the weave driver on all the hosts you want to be connected by following the steps
    described previously. Make sure that you either use the launch command with `node01`
    as a parameter, or from `node01` you use the connect command for each of the additional
    nodes you are configuring. For this example, my lab servers are named ubuntu-node01
    and ubuntu-`node02`. Let''s start with `node02`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note the following, on `ubuntu-node01`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'And, note the following, on `ubuntu-node02`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, back on `ubuntu-node01`, note the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![](Images/c4bd83df-3b2a-4931-9dbd-bcd67e7bb982.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s launch a container on each node. Make sure we name them for easy
    identification, starting with `ubuntu-node01`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![](Images/3a1ca418-01e8-4774-96ac-cf34e7d948f3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, launch a container on `ubuntu-node02`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![](Images/6e796094-aece-4402-96d3-fcb4854893d2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Excellent. Now, we have containers running on both nodes. Let''s see whether
    they can communicate. Since we are on `node02`, we will check there first:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![](Images/69fb11da-ad53-4ec5-8b6a-f4ba031e70e4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Yeah! That worked. Let''s try going the other way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '![](Images/5ed6a092-d5db-46d9-bc71-2157d4bb58e1.png)'
  prefs: []
  type: TYPE_IMG
- en: Perfect! We have bi-directional communication. Did you notice anything else?
    We have name resolution for our app containers (we didn't have to ping by IP only).
    Pretty nice, right?
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Check out these links for more information:'
  prefs: []
  type: TYPE_NORMAL
- en: Installing and using the weave network driver: [https://www.weave.works/docs/net/latest/overview/](https://www.weave.works/docs/net/latest/overview/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Weaveworks weave github repo: [https://github.com/weaveworks/weave](https://github.com/weaveworks/weave)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating Docker networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'OK, you now know a lot about both the local and the remote network drivers,
    and you have seen how several of them are created for you when you install Docker
    and/or initialize swarm mode (or install a remote driver). But, what if you want
    to create your own networks using some of these drivers? It is really pretty simple.
    Let''s take a look. The built-in help for the `network create` command looks like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Examining this, we see there are essentially two parts of this command we need
    to handle, the OPTIONS followed by the NETWORK name to make the network we wish
    to create. What are our options? Well, there are quite a lot, but let's pick out
    a few to get you going quickly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Probably the most important option is the `--driver` option. This is how we
    tell Docker which of the pluggable network drivers to use when creating this network.
    As you have seen, the choice of driver determines the network characteristics.
    The value you supply to the driver option will be like the ones shown in the DRIVER
    column of the output from the `docker network ls` command. Some of the possible
    values are bridge, overlay, and macvlan. Remember that you cannot create additional
    host or null networks as they are limited to one per Docker host. So far, what
    might this look like? Here is an example of creating a new overlay network, using
    mostly defaults for options:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'That works just fine. You can run new services and attach them to your new
    network. But what else might we want to control in our network? Well, how about
    the IP space? Yep, and Docker provides options for controlling the IP settings
    for our networks. This is done using the `--subnet`, `--gateway`, and `--ip-range`
    optional parameters. So, let''s take a look at creating a new network using this
    options. See [Chapter 2](e66034ed-dcc0-48a8-a2ec-9466669e6649.xhtml), *Learning
    Docker Commands*, for how to install jq if you have not done so already:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Executing the preceding code in my lab looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/bbd502b4-374d-406d-915c-b2af371914b3.png)'
  prefs: []
  type: TYPE_IMG
- en: Looking over this example, we see that we created a new overlay network using
    specific IP parameters for the subnet, the IP range, and the gateway. Then, we
    validated that the network was created with the requested options. Next, we created
    a service using our new network. Then, we found the container ID for a container
    belonging to the service and used it to inspect the network settings for the container.
    We can see that the container was run using an IP address (in this case, `172.30.0.7`)
    from the IP range we configured our network with. Looks like we made it!
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned, there are many other options available when creating Docker networks,
    and I will leave it as an exercise for you to discover them with the `docker network
    create --help` command, and to try some of them out to see what they do.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can find the documentation for the `network create` command at [https://docs.docker.com/engine/reference/commandline/network_create/](https://docs.docker.com/engine/reference/commandline/network_create/).
  prefs: []
  type: TYPE_NORMAL
- en: Free networking features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are two networking features or services that you get for free with your
    Docker swarm networks. The first is Service Discovery, and the second is load
    balancing. When you create Docker services, you get these features automatically. We
    experienced these features in this chapter and in [Chapter 5](f1681897-580b-44fb-9e43-4aed37e67529.xhtml),
    *Docker Swarm*, but didn't really refer to them by name. So, let's call them out
    here.
  prefs: []
  type: TYPE_NORMAL
- en: 'First up is Service Discovery. When you create a service, it gets a unique
    name. That name gets registered with the swarm DNS. And, every service uses the
    swarm DNS for name resolution. Here is an example for you. We are going to leverage
    the `specifics-over` overlay network we created earlier in the creating Docker
    networks section. We''ll create two services (`tester1` and `tester2`) attached
    to that network, then we will connect to a container in the `tester1` services
    and ping the `tester2` service by name. Check it out:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is what the preceding commands look like when executed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/d89ec999-415d-4163-832d-414748894ff4.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that I typed the first part of the service name (`tester1`) and used command-line
    completion by hitting *Tab* to fill in the container name for the exec command.
    But, as you can see, I was able to reference the `tester2` service by name from
    within a `tester1` container.
  prefs: []
  type: TYPE_NORMAL
- en: For free!
  prefs: []
  type: TYPE_NORMAL
- en: The second free feature we get is Load balancing. This powerful feature is pretty
    easy to understand. It allows traffic intended for a service to be sent to any
    host in a swarm regardless of whether that host is running a replica of the service.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine a scenario where you have a six-node swarm cluster, and a service that
    has only one replica deployed. You can send traffic to that service via any host
    in the swarm and know that it will arrive at the service's one container no matter
    which host the container is actually running on. In fact, you can direct traffic
    to all hosts in the swarm using a load balancer, say in a round-robin model, and
    each time traffic is sent to the load balancer, that traffic will get delivered
    to the app container without fail.
  prefs: []
  type: TYPE_NORMAL
- en: Pretty handy, right? Again, for free!
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Want to have a go at service discovery? Then check out [https://training.play-with-docker.com/swarm-service-discovery/](https://training.play-with-docker.com/swarm-service-discovery/).
  prefs: []
  type: TYPE_NORMAL
- en: You can read about swarm service load balancing at [https://docs.docker.com/engine/swarm/key-concepts/#load-balancing](https://docs.docker.com/engine/swarm/key-concepts/#load-balancing).
  prefs: []
  type: TYPE_NORMAL
- en: Which Docker network driver should I use?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The short answer to that question is the right one for the job. That means there
    is no single network driver that is the right fit for every situation. If you're
    doing work on your laptop, running with swarm inactive, and you just need your
    containers to be able to communicate with each other, the simple bridge mode driver
    is ideal.
  prefs: []
  type: TYPE_NORMAL
- en: If you have multiple nodes and just need container-to-container traffic, the
    overlay driver is the right one to use. This one works well in AWS, if you are
    within the container-to-container realm. If you need container-to-VM or container-to-physical-server
    communication (and can tolerate promiscuous mode), the MACVLAN driver is the way
    to go. Or, if you have a more complex requirement, one of the many remote drivers
    might be just what the doctor ordered.
  prefs: []
  type: TYPE_NORMAL
- en: I've found that for most multi-host scenarios, the overlay driver will get the
    job done, so I would recommend that you enable swarm mode, and give the overlay
    driver a try before you ramp up to any of the other multi-host options.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'How do you feel about Docker networking now? Docker has taken a complex technology,
    networking, and made it easy to understand and use. Most of the crazy, difficult
    setup stuff is literally handled with a single `swarm init` command. Let''s review:
    you learned about the network design that Docker created, called the container
    network model, or CNM. Then,  you learned how the libnetwork project turned that
    model into a pluggable architecture. After that, you found out that Docker created
    a powerful set of drivers to plug into the libnetwork architecture to enable a
    variety of network options for most of your container communication needs. Since
    the architecture is so pluggable, others have created even more network drivers
    that solve any edge cases that the Docker drivers don''t handle. Docker networking
    has really come into its own.'
  prefs: []
  type: TYPE_NORMAL
- en: I hope you are ready for more, because in [Chapter 7](1a206f3a-faf8-43cb-9413-d1e451bd2a35.xhtml),
    *Docker Stacks*, we are going to dive into Docker stacks. This is where all of
    the information you have learned so far really comes together into a symphony
    of brilliance. Take a deep breath and turn the page!
  prefs: []
  type: TYPE_NORMAL
