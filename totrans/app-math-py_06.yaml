- en: Working with Data and Statistics
  prefs: []
  type: TYPE_NORMAL
- en: One of the most attractive features of Python for people who need to analyze
    data is the huge ecosystem of data manipulation and analysis packages, as well
    as the active community of data scientists working with Python. Python is easy
    to use, while also offering very powerful, fast libraries, which enables even
    relatively novice programmers to quickly and easily process vast sets of data.
    At the heart of many data science packages and tools is the pandas library. Pandas
    provides two data container types that build on top of NumPy arrays and have good
    support for labels (other than simple integers). They also make working with large
    sets of data extremely easy.
  prefs: []
  type: TYPE_NORMAL
- en: Statistics is the systematic study of data using mathematical—specifically,
    probability—theory. There are two aspects to statistics. The first is to find
    numerical values that describe a set of data, including characteristics such as
    the center (mean or median) and spread (standard deviation or variance) of the
    data. The second aspect of statistics is inference, describing a much larger set
    of data (a population) using a relatively small sample dataset.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will see how to leverage Python and pandas to work with
    large sets of data and perform statistical tests.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter contains the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Creating Series and DataFrame objects
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loading and storing data from a DataFrame
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Manipulating data in DataFrames
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Plotting data from a DataFrame
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting descriptive statistics from a DataFrame
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding a population using sampling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing hypotheses using t-tests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing hypotheses using ANOVA
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing hypotheses for non-parametric data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating interactive plots with Bokeh
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For this chapter, we will mostly make use of the pandas library for data manipulation,
    which provides R like data structures, such as `Series` and `DataFrame` objects,
    for storing, organizing, and manipulating data. We will also use the Bokeh data
    visualization library in the final recipe of this chapter. These libraries can
    be installed using your favorite package manager, such as pip:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: We will also make use of the NumPy and SciPy packages.
  prefs: []
  type: TYPE_NORMAL
- en: The code for this chapter can be found in the `Chapter 06` folder of the GitHub
    repository at [https://github.com/PacktPublishing/Applying-Math-with-Python/tree/master/Chapter%2006](https://github.com/PacktPublishing/Applying-Math-with-Python/tree/master/Chapter%2006).
  prefs: []
  type: TYPE_NORMAL
- en: 'Check out the following video to see the Code in Action: [https://bit.ly/2OQs6NX](https://bit.ly/2OQs6NX).'
  prefs: []
  type: TYPE_NORMAL
- en: Creating Series and DataFrame objects
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most data handling in Python is done using the pandas library, which builds
    on NumPy to provide R-like structures for holding data. These structures allow
    the easy indexing of rows and columns, using strings or other Python objects besides
    just integers. Once data is loaded into a pandas `DataFrame` or `Series`, it can
    be easily manipulated, just as if it were in a spreadsheet. This makes Python
    when combined with pandas a powerful tool for processing and analyzing data.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will see how to create new pandas `Series` and `DataFrame`
    objects and access items from `Series` or `DataFrame`.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For this recipe, we will import the pandas library as pd using the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The NumPy package is `np`. We also create a (seeded) random number generator
    from NumPy, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following steps outline how to create a `Series` and `DataFrame` object
    that holds data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, create the random data that we will store in the `Series` and `DataFrame`
    objects:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, create a `Series` object that holds `diff_data`. We''ll print `Series`
    to produce a view of the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, create a `DataFrame` object with two columns:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Print the `DataFrame` object to produce a view of the data it holds:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The pandas package provides the `Series` and `DataFrame` classes, which mirror
    the function and capabilities of their R counterparts. `Series` is used to store
    one-dimensional data, such as time-series data, and `DataFrame` is used to store
    multidimensional data; you can think of a `DataFrame` object as a "spreadsheet."
  prefs: []
  type: TYPE_NORMAL
- en: What separates `Series` from a simple NumPy `ndarray` is the way that `Series`
    indexes its items. A NumPy array is indexed by integers, which is also the default
    for a `Series` object. However, `Series` can be indexed by any hashable Python
    object, including strings and `datetime` objects. This makes `Series` useful for
    storing time-series data. A `Series` can be created in a number of ways. In this
    recipe, we used a NumPy array, but any Python iterable, such as a list, can be
    used instead.
  prefs: []
  type: TYPE_NORMAL
- en: Each column in a `DataFrame` object is a series containing rows, just as in
    a traditional database or spreadsheet. In this recipe, the columns are given labels
    when the `DataFrame` object is constructed via the keys of the dictionary.
  prefs: []
  type: TYPE_NORMAL
- en: The `DataFrame` and `Series` objects create a summary of the data they contain
    when printed. This includes column names, the number of rows and columns, and
    the first and last five rows of the frame (series). This is useful for quickly
    obtaining an overview of the object and the spread of data it contains.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The individual rows (records) of a `Series` object can be accessed using the
    usual index notation by providing the corresponding index. We can also access
    the rows by their numerical position using the special `iloc` property object.
    This allows us to access the rows by their numerical (integer) index, such as
    with Python lists or NumPy arrays.
  prefs: []
  type: TYPE_NORMAL
- en: The columns in a `DataFrame` object can be accessed using the usual index notation,
    providing the name of the column. The result of this is a `Series` object that
    contains the data from the selected column. DataFrames also provides two properties
    that can be used to access data. The `loc` attribute provides access to individual
    rows by their index, whatever this object may be. The `iloc` attribute provides
    access to the rows by numerical index, just as for the `Series` object.
  prefs: []
  type: TYPE_NORMAL
- en: You can provide selection criteria to `loc`(or just using index notation for
    the object) to select data. This includes a single label, a list of labels, a
    slice of labels, or a Boolean array (of an appropriate size). The `iloc`selection
    method accepts similar criteria.
  prefs: []
  type: TYPE_NORMAL
- en: There are other ways to select data from a `Series`or `DataFrame`object beyond
    the simple methods we describe here. For example, we can use the `at`attribute
    to access a single value at a specified row (and column) in the object.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The pandas documentation contains a detailed description of the different ways
    to create and index a `DataFrame` or `Series` object, at [https://pandas.pydata.org/docs/user_guide/indexing.html](https://pandas.pydata.org/docs/user_guide/indexing.html).
  prefs: []
  type: TYPE_NORMAL
- en: Loading and storing data from a DataFrame
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is fairly unusual to create a `DataFrame` object from the raw data in a Python
    session. In practice, the data will often come from an external source, such as
    an existing spreadsheet or CSV file, database, or API endpoint. For this reason,
    pandas provides numerous utilities for loading and storing data to file. Out of
    the box, pandas supports loading and storing data from CSV, Excel (`xls` or `xlsx`),
    JSON, SQL, Parquet, and Google BigQuery. This makes it very easy to import your
    data into pandas and then manipulate and analyze this data using Python.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will see how to load and store data into a CSV file. The
    instructions will be similar for loading and storing data to other file formats.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For this recipe, we will need to import the pandas package under the `pd`alias
    and the NumPy library as `np`, and we create a default random number generator
    from NumPy using the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Follow these steps to store data to a file and then load the data back into
    Python:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we''ll create a sample `DataFrame` object using random data. We then
    print this `DataFrame` object so that we can compare it to the data that we will
    read later:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We will store the data in this `DataFrame` object into the `sample.csv` file
    using the `to_csv` method on the `DataFrame` object. We will use the `index=False`
    keyword argument so that the index is not stored in the CSV file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can use the `read_csv` routine from pandas to read the `sample.csv`
    file into a new `DataFrame` object. We will print this object to show the result:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The core of this recipe is the `read_csv` routine in pandas. This routine takes
    path- or file-like objects as an argument and reads the contents of the file as
    CSV data. We can customize the delimiter using the `sep` keyword argument, which
    is a comma (`,`) by default. There are also options to customize the column headers
    and customize the type of each column.
  prefs: []
  type: TYPE_NORMAL
- en: The `to_csv` method in a `DataFrame` or `Series` stores the contents into a
    CSV file. We used the `index` keyword argument here so that the indices are not
    printed into the file. This means that pandas will infer the index from the row
    number in the CSV file. This behavior is desirable if the data is indexed by integers,
    but this might not be the case if the data is indexed by times or dates, for example.
    We can also use this keyword argument to specify which column in the CSV file
    is the indexing column.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: See the pandas documentation for a list of supported file formats at[https://pandas.pydata.org/docs/reference/io.html](https://pandas.pydata.org/docs/reference/io.html)[.](https://pandas.pydata.org/docs/reference/io.html)
  prefs: []
  type: TYPE_NORMAL
- en: Manipulating data in DataFrames
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once we have data in a `DataFrame`, we often need to apply some simple transformations
    or filters to the data before we can perform any analysis. This could include,
    for example, filtering the rows that are missing data or applying a function to
    individual columns.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will see how to perform some basic manipulation of `DataFrame`
    objects to prepare the data for analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For this recipe, we will need the `pandas` package imported under the `pd`alias,
    the NumPy package imported under the `np`alias, and a default random number generator
    object from NumPy created using the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following steps illustrate how to perform some basic filtering and manipulations
    on a pandas `DataFrame`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will first create a sample `DataFrame` using random data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we have to generate a new column from an existing column. This new column
    will hold `True` if the corresponding entry of column `"one"` is greater than
    `0.5`, and `False` otherwise:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'We now have to create a new function that we will apply to our `DataFrame`.
    This function multiplies the row `"two"` value by the maximum of row `"one"` and
    `0.5` (there are more concise ways to write this function):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now apply the previously defined function to each row in the DataFrame
    to generate a new column. We will also print the updated DataFrame for comparison
    later:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we have to filter out the rows in the DataFrame that contain a **Not
    a Number** (**NaN**) value. We will print the resulting DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: New columns can be added to an existing `DataFrame` by simply assigning them
    to the new column index. However, some care needs to be taken here. In some situations,
    pandas will create a "view" to a `DataFrame` object rather than copying, and in
    this case, assigning to a new column might not have the desired effect. This is
    discussed in the pandas documentation ([https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy](https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy)).
  prefs: []
  type: TYPE_NORMAL
- en: Pandas `Series` objects (columns in a `DataFrame`) support the rich comparison
    operators, such as equality and less than or greater than (in this recipe, we
    used the greater than operator). These comparison operators return a `Series`
    containing Boolean values corresponding to the positions at which the comparison
    was true and false. This can, in turn, be used to index the original Series and
    get just the rows where the comparison was true. In this recipe, we have simply
    added this Series of Boolean values to the original `DataFrame`.
  prefs: []
  type: TYPE_NORMAL
- en: The `apply` method takes a function (or other callable function) and applies
    it to each column in the DataFrame. In this recipe, we insteadwanted to apply
    the function to each row, so we used the `axis=1`keyword argument to apply the
    function to each row in the DataFrame. In either case, the function is provided
    with a `Series`object indexed by the rows (columns). We have also applied a function
    to each row, which returned a value computed using the data from each row. In
    practice, this application would be quite slow if the DataFrame contains a large
    number of rows. If possible, you should operate on the columns as a whole, using
    functions designed to operate on NumPy arrays, for better efficiency. This is
    especially true for performing simple arithmetic on values in columns of a DataFrame.
    Just like NumPy arrays, `Series` objects implement standard arithmetic operations,
    which can greatly improve the operation time for large DataFrames.
  prefs: []
  type: TYPE_NORMAL
- en: In the final step of this recipe, we used the `dropna` method to quickly select
    only the rows from the DataFrames that do not contain a NaN value. Pandas uses
    NaN to represent missing data in a DataFrame, so this method selects the rows
    that don't contain a missing value. This method returns a view to the original
    `DataFrame` object, but it can also modify the original DataFrame by passing the
    `inplace=True` keyword argument. As used in this recipe, this is roughly equivalent
    to using the indexing notation to select rows using an indexing array containing
    Boolean values.
  prefs: []
  type: TYPE_NORMAL
- en: You should always be cautious when modifying original data directly since it
    might not be possible to return to this data to repeat your analysis later. If
    you do need to modify the data directly, you should make sure that it is either
    backed up or that the modifications do not remove data that you might later need.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Most pandas routines deal with missing data (NaN) in a sensible way. However,
    if you do need to remove or replace missing data in a DataFrame, then there are
    several ways to do this. In this recipe, we have used the `dropna` method to simply
    drop the rows from the DataFrames that are missing data. We could instead fill
    all the missing values with a specific value using the `fillna` method, or interpolate
    missing values using the surrounding values using the `interpolate` method.
  prefs: []
  type: TYPE_NORMAL
- en: More generally, we can use the `replace` method to replace specific (non-NaN)
    values with other values. This method can work with both numeric values or string
    values, including pattern-matching with regex.
  prefs: []
  type: TYPE_NORMAL
- en: The `DataFrame` class has many useful methods. We've only covered the very basic
    methods here, but there are two other methods that we should also mention. These
    are the `agg` method and the `merge` method.
  prefs: []
  type: TYPE_NORMAL
- en: The `agg` method aggregates the results of one or more operations over a given
    axis of the DataFrame. This allows us to quickly produce summary information for
    each column (or row) by applying an aggregating function. The output is a DataFrame
    that has the names of the functions applied as the rows, and the labels for the
    chosen axis (column labels, for instance) for the columns.
  prefs: []
  type: TYPE_NORMAL
- en: The `merge` method performs a SQL-like join over two DataFrames. This will produce
    a new DataFrame that contains the result of the join. There are various parameters
    that can be passed to the `how` keyword argument to specify the type of merge
    to be performed, with the default being `inner`. The name of the column or index
    over which to perform the join should be passed to either the `on` keyword argument—if
    both `DataFrame` objects contain the same key—or to `left_on` and `right_on`.
  prefs: []
  type: TYPE_NORMAL
- en: Plotting data from a DataFrame
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As with many mathematical problems, one of the first steps to find some way
    to visualize the problem and all the information is to formulate a strategy. For
    data-based problems, this usually means producing a plot of the data and visually
    inspecting it for trends, patterns, and the underlying structure. Since this is
    such a common operation, pandas provides a quick and simple interface for plotting
    data in various forms, using Matplotlib under the hood by default, directly from
    a `Series` or `DataFrame`.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will see how to plot data directly from a `DataFrame` or
    `Series` to understand the underlying trends and structure.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For this recipe, we will need the pandas library import as `pd`, the NumPy
    library import as `np`, the matplotlib `pyplot` module imported as `plt`, and
    a default random number generator instance created using the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Follow these steps to create a simple DataFrame using random data and produce
    plots of the data it contains:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a sample DataFrame using random data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we have to create a blank figure with two subplots ready for plotting:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'We have to plot the `walk`column as a standard line graph. This is done by
    using the `plot`method on the `Series` (column) object without additional arguments.
    We will force the plotting on `ax1`by passing the `ax=ax1`keyword argument:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we have to plot a histogram of the `diffs` column by passing the `kind="hist"`
    keyword argument to the `plot` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting plots are shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/32597d04-7669-4b87-a2cc-8ecb88435534.png)Figure 6.1 – Plot of the
    walk value and a histogram of differences from a DataFrame'
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `plot` method on a `Series` (or a `DataFrame`) is a quick way to plot the
    data it contains against the row index. The `kind` keyword argument is used to
    control the type of plot that is produced, with a line plot being the default.
    There are lots of options for the plotting type, including `bar` for a vertical
    bar chart, `barh` for a horizontal bar chart, `hist` for a histogram (also seen
    in this recipe), `box` for a box plot, and `scatter` for a scatter plot. There
    are several other keyword arguments to customize the plot that it produces. In
    this recipe, we also provided the `title` keyword argument to add a title to each
    subplot.
  prefs: []
  type: TYPE_NORMAL
- en: Since we wanted to put both plots on the same figure side by side using subplots
    that we had already created, we used the `ax` keyword argument to pass in the
    respective axes handles to the plotting routine. Even if you let the `plot` method
    construct its own figure, you may still need to use the `plt.show` routine in
    order to display the figure with certain settings.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can produce several common types of plots using the pandas interface. This
    includes, in addition to those mentioned in this recipe, scatter plots, bar plots
    (horizontal bars and vertical bars), area plots, pie charts, and box plots. The
    `plot` method also accepts various keyword arguments to customize the appearance
    of the plot.
  prefs: []
  type: TYPE_NORMAL
- en: Getting descriptive statistics from a DataFrame
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Descriptive statistics, or summary statistics, are simple values associated
    with a set of data, such as the mean, median, standard deviation, minimum, maximum,
    and quartile values. These values describe the location and spread of a dataset
    in various ways. The mean and median are measures of the center (location) of
    the data, and the other values measure the spread of the data from the mean and
    median. These statistics are vital in understanding a dataset and form the basis
    for many techniques for analysis.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will see how to generate descriptive statistics for each
    column in a DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For this recipe, we need the pandas package imported as `pd`, the NumPy package
    imported as `np`, the matplotlib `pyplot` module imported as `plt`, and a default
    random number generator created using the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following steps show how to generate descriptive statistics for each column
    in a DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will first create some sample data that we can analyze:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we plot histograms of the data so that we can understand the distribution
    of the data in the DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Pandas `DataFrame` objects have a method for getting several common descriptive
    statistics for each column. The `describe` method creates a new DataFrame, where
    the column headers are the same as from the original object and each row contains
    a different descriptive statistic:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'We also compute the *kurtosis* and add this to the new DataFrame we just obtained.
    We also print the descriptive statistics to the console to see what the values
    are:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we add vertical lines to the histograms to illustrate the value of
    the mean in each case:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting histograms are shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/46b543ec-8e94-4bb2-8246-3bb9db0e213c.png)Figure 6.2 – Histograms
    of three sets of data with their mean values indicated'
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `describe` method returns a DataFrame with rows for the following descriptive
    statistics of the data: the count, mean, standard deviation, minimum value, 25%
    quartile, median (50% quartile), 75% quartile, and maximum value. The count is
    fairly self-explanatory, as are the minimum and maximum values. The mean and the
    median are two different *averages* of the data, which roughly represent the central
    value of the data. The mean is defined in a familiar way as the sum of all values
    divided by the number of values. We can express this quantity using the following
    formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/c536ac7d-473a-45ef-9c61-437e89e568c5.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, the *x[i]* values represent the data values and *N* is the number (count)
    of values. Here, we also adopt the common notation of the bar to represent the
    mean value. The median is the "middle value" when all the data is sorted (taking
    an average of the two middle values if there are an odd number of values). The
    quartile values at 25% and 75% are similarly defined, but taking the value at
    25% or 75% of the way through the ordered values. You might also think of the
    minimum as the 0% quartile and the maximum as the 100% quartile.
  prefs: []
  type: TYPE_NORMAL
- en: '**Standard deviation** is a measure of the spread of the data from the mean
    and is related to another quantity that is frequently mentioned in statistics,
    the **variance***.* The variance is the square of the standard deviation and is
    defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/f014bad4-8c1c-4559-bd40-9607e5653105.png)'
  prefs: []
  type: TYPE_IMG
- en: You might also see *N –* 1 appear in the fraction here, which is a correction
    for **bias** when estimating population parameters from a sample. We will discuss
    population parameters and their estimation in the next recipe. Standard deviation,
    variance, the quartiles, and the maximum and minimum values describe the spread
    of the data. For example, if the maximum value is 5, the minimum value is 0, the
    25% quartile is 2, and the 75% quartile is 4, then this indicates that most (at
    least 50% of the values, in fact) of the data is concentrated between 2 and 4.
  prefs: []
  type: TYPE_NORMAL
- en: The *kurtosis* is a measure of how much the data is concentrated in the "tails"
    of the distribution (far from the mean). This is not as common as the other quantities
    we have discussed in this recipe, but it does appear in some analysis. We have
    included it here mostly as a demonstration of how to compute summary statistic
    values that do not appear in the DataFrame returned from the `describe` method
    using the appropriately named method—here, `kurtosis`. There are, of course, separate
    methods for computing the mean (`mean`), standard deviation (`std`), and the other
    quantities from the `describe` method.
  prefs: []
  type: TYPE_NORMAL
- en: When pandas computes the quantities described in this recipe, it will automatically
    ignore any "missing values" represented by NaN. This will also be reflected in
    the count reported in the descriptive statistics.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The third dataset that we included in our statistics illustrates the importance
    of looking at the data to make sure the values we have calculated make sense.
    Indeed, we compute the mean as approximately `2.9`, but looking at the histogram,
    it is clear that most of the data is relatively far from this value. We should
    always check whether the summary statistics that we calculate give an accurate
    summary of the data in our sample. Simply quoting the mean might give an inaccurate
    representation of the sample.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding a population using sampling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the central problems in statistics is to make estimations—and quantify
    how good these estimations are—of the distribution of an entire population given
    only a small (random) sample. A classic example is to estimate the average height
    of all the people in a country when measuring the height of a randomly selected
    sample of people. These kinds of problems are particularly interesting when the
    true population distribution, by which we usually mean the mean of the whole population,
    cannot feasibly be measured. In this case, we must rely on our knowledge of statistics
    and a (usually much smaller) randomly selected sample to estimate the true population
    mean and standard deviation, and also quantify how good our estimations are. It
    is the latter that is the source of confusion, misunderstanding, and misrepresentation
    of statistics in the wider world.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will see how to estimate the population mean and give a **confidence
    interval** for these estimates.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For this recipe, we need the pandas package import as `pd`, the `math` module
    from the Python standard library, and the SciPy `stats` module, imported using
    the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the following steps, we will give an estimation of the mean height of males
    in the United Kingdom, based on a randomly selected sample of 20 people:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We have to load our sample data into a pandas `Series`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will compute the sample mean and standard deviation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we will compute the **standard error***,* as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'We will compute the **critical values** for the confidence values we desire
    from the student *t* distribution:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can compute the 95% and 99% confidence intervals for the true population
    mean using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The key to parameter estimation is normal distribution, which we discussed in
    [Chapter 4](5da67d86-40e0-4cc5-9dd1-26b6d52369af.xhtml), *Working with Randomness
    and Probability*. If we find the critical value of *z* for which the probability
    that a standard, normally distributed random number lies below this value *z*
    is 97.5%, then the probability that such a number lies between the values of -*z*
    and *z* is 95% (2.5% in each tail). This critical value of *z* turns out to be
    1.96, rounded to 2 decimal places. That is, we can be 95% sure that the value
    of a standard normally distributed random number lies between *-z* and *z*. Similarly,
    the critical value of 99% confidence is 2.58 (rounded to 2 decimal places).
  prefs: []
  type: TYPE_NORMAL
- en: If our sample is "large," we could invoke the **central limit theorem**, which
    tells us that even if the population is not normally distributed itself, the means
    of random samples drawn from this population will be normally distributed with
    the same mean as the whole population. However, this is only valid assuming our
    samples are large. In this recipe, the sample is not large—it only has 20 values,
    which is certainly not large compared to the male population of the UK. This means
    that, rather than the normal distribution, we have to use a student *t* distribution
    with *N-*1 degrees of freedom to find our critical values, where *N* is the size
    of our sample. For this, we use the `stats.t.ppf` routine from the SciPy `stats`
    module.
  prefs: []
  type: TYPE_NORMAL
- en: The student *t* distribution is related to the normal distribution but has a
    parameter—the degree of freedom—that changes the shape of the distribution. As
    the number of degrees of freedom increases, the student *t* distribution will
    look more and more like a normal distribution. The point at which you consider
    the distributions to be sufficiently similar depends on your application and your
    data. A general rule of thumb says that a sample size of 30 is sufficient to invoke
    the central limit theorem and simply use the normal distribution, but it is by
    no means a good rule. You should be very careful when making deductions based
    on a sample, especially if the sample is very small compared to the total population.
    (Clearly, using a sample size of 20 would be pretty descriptive if the total population
    consists of 30 people, but not if the total population consists of 30 million
    people.)
  prefs: []
  type: TYPE_NORMAL
- en: Once we have the critical values, the confidence interval for the true population
    mean can be computed by multiplying the critical value by the standard error of
    the sample and adding and subtracting this from the sample mean. The standard
    error is an approximation of the spread of the distribution of sample means of
    a given sample size from the true population mean. This is why we use the standard
    error to give the confidence interval for our estimation of the population mean.
    When we multiply the standard error by the critical value taken from the student
    *t* distribution (in this case), we obtain an estimate of the maximum difference
    between the observed sample mean and the true population mean at the given confidence
    level.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, that means that we are 95% certain that the mean height of UK
    males lies between 168.7 cm and 175.6 cm, and we are 99% certain that the mean
    height of UK males lies between 167.4 cm and 176.9 cm. In fact, our sample was
    drawn from a population with a mean of 175.3 cm and a standard deviation of 7.2
    cm. This true mean (175.3 cm) does indeed lie within both of our confidence intervals,
    but only just.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There is a useful package called `uncertainties` for doing computations involving
    values with some uncertainty attached. See the *Accounting for uncertainty in
    calculations* recipe in [Chapter 10](169df36b-7160-4fe9-ab59-e20047fc4dc6.xhtml),
    *Miscellaneous Topics*.
  prefs: []
  type: TYPE_NORMAL
- en: Testing hypotheses using t-tests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the most common tasks in statistics is to test the validity of a hypothesis
    about the mean of a normally distributed population given that you have collected
    sample data from that population. For example, in quality control, we might wish
    to test that the thickness of a sheet produced at a mill is 2 mm. To test this,
    we would randomly select sample sheets and measure the thickness to obtain our
    sample data. Then, we can use a **t-test** to test our null hypothesis, *H[0]*,
    that the mean paper thickness is 2 mm, against the alternative hypothesis, *H[1]*,
    that the mean paper thickness is not 2 mm. We use the SciPy `stats` module to
    compute a *t* statistic*and a *p* value. If the *p* value is below 0.05, then
    we accept the null hypothesis with 5% significance (95% confidence). If the *p*
    value is larger than 0.05, then we must reject the null hypothesis in favor of
    our alternative hypothesis.*
  prefs: []
  type: TYPE_NORMAL
- en: '*In this recipe, we will see how to use a t-test to test whether the assumed
    population mean is valid given a sample.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For this recipe we will need the pandas package imported as `pd` and the SciPy
    `stats` module imported using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Follow these steps to use a t-test to test the validity of a proposed population
    mean given some sample data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will first load the data into a pandas `Series`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, set the hypothesized population mean and the significance level that we
    will be testing at:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, use the `ttest_1samp`routine from the SciPy `stats`module to generate
    the *t* statistic and the *p* value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, test whether the *p* value is smaller than the significance level
    we chose:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The *t* statistic is computed using the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/e9dc48d2-75d6-4d0c-a377-3b9a6ecbd9d1.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *μ[0]* is the hypothesized mean (from the null hypothesis), *x* bar is
    the sample mean, *s* is the sample standard deviation, and *N* is the size of
    the sample. The *t* statistic is an estimation of the difference between the observed
    sample mean and the hypothesized population mean, *μ[0]*, normalized by the standard
    error. Assuming the population is normally distributed, the *t* statistic will
    follow a *t* distributionwith *N*-1degrees of freedom. Looking at where the t
    statistic lies within in the corresponding student *t* distribution gives us an
    idea of how likely it is that the sample mean we observed came from the population
    with the hypothesized mean. This is given in the form of a *p* value.
  prefs: []
  type: TYPE_NORMAL
- en: The *p* value is the probability of observing a more extreme value than the
    sample mean we have observed, given the assumption that the population mean is
    equal to*μ[0]*. If the *p* value is smaller than the significance value we have
    chosen, then we cannot expect the true population mean to be the value,*μ[0]*,
    that we assumed. In this case, we have to accept the alternative hypothesis that
    the true population norm is not equal to*μ[0]*.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The test that we demonstrated in this recipe is the most basic use of a t-test.
    Here, we compared the sample mean to a hypothesized population mean to decide
    whether it was reasonable that the mean of the whole population is this hypothesized
    value. More generally, we can use t-tests to compare two independent populations
    given samples taken from each using a **2-sample t-test**, or compare the populations
    where data is paired (in some way) using a **paired t-test**. This makes the t-test
    an important tool for a statistician.
  prefs: []
  type: TYPE_NORMAL
- en: Significance and confidence are two concepts that occur frequently in statistics.
    A statistically significant result is one that has a high probability of being
    correct. In many contexts, we consider any result that has a probability of being
    wrong below a certain threshold (usually either 5% or 1%) to be statistically
    significant. Confidence is a quantification of how certain we are about a result.
    The confidence of a result is 1 minus the significance.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, the significance of a result is something that is often misused
    or misunderstood. To say that a result is statistically significant at 5% is to
    say that there is a 5% chance that we have wrongly accepted the null hypothesis.
    That is, if we repeated the same test on 20 other samples from the population,
    we would expect at least one of them to give the opposite result. That, however,
    is not to say that one of them is guaranteed to do so.
  prefs: []
  type: TYPE_NORMAL
- en: High significance indicates that we are more sure that the conclusion we have
    reached is correct, but it is certainly not a guarantee that this is indeed the
    case. In fact, the results found in this recipe are evidence for this; the sample
    that we used was in fact drawn from a population with a mean of `2.5` and a standard
    deviation of `0.35`. (Some rounding was applied to the sample after creating,
    which will have altered the distribution slightly.) This is not to say that our
    analysis is wrong, or that the conclusion we reached from our sample is not the
    right one.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to remember that t-tests are only valid when the underlying
    populations follow a normal distribution, or at least approximately do so. If
    this is not the case, then you might need to use a non-parametric test instead.
    We will discuss this in the *Testing hypotheses for non-parametric data* recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Testing hypotheses using ANOVA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Suppose we have designed an experiment that tests two new processes against
    the current process and we want to test whether the results of these new processes
    are different from the current process. In this case, we can use **Analysis of
    Variance***(**ANOVA**) to help us determine whether there are any differences
    between the mean values of the three sets of results (for this, we need to assume
    that each sample is drawn from a normal distribution with a common variance).*
  prefs: []
  type: TYPE_NORMAL
- en: '*In this recipe, we will see how to use ANOVA to compare multiple samples with
    one another.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For this recipe, we need the SciPy `stats` module. We will also need a default
    random number generator instance created using the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Follow these steps to perform a (oneway) ANOVA test to test for differences
    between three different processes:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will create some sample data, which we will analyze:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will set the significance level for our test:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we will use the `f_oneway`routine from the SciPy `stats`module to generate
    the F-statistic and the *p* value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we must test whether the *p* value is sufficiently small to see whether
    we should accept or reject our null hypothesis that all mean values are equal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: ANOVA is a powerful technique for comparing multiple samples against one another
    simultaneously. It works by comparing the variation in the samples relative to
    the overall variation. ANOVA is especially powerful when comparing three or more
    samples since no cumulative error is incurred from running multiple tests. Unfortunately,
    if ANOVA detects that not all the mean values are equal, then there is no way
    from the test information to determine which sample(s) are significantly different
    from the others. For this, you would need to use an extra test to find the differences.
  prefs: []
  type: TYPE_NORMAL
- en: The`f_oneway` SciPy `stats`package routine performs a one-way ANOVA test—the
    test statistic generated in ANOVA follows an F-distribution. Again, the *p* value
    is the crucial piece of information coming from the test. We accept the null hypothesis
    if the *p* value is less than our predefined significance level (in this recipe,
    5%) and reject the null hypothesis otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The ANOVA method is very flexible. The one-way ANOVA test that we presented
    here is the most simple case as there is only a single factor to test. A two-way
    ANOVA test can be used to test for differences over two different factors. This
    is useful in clinical trials of medicines, for example, where we test against
    a control but also measure the effects of gender (for instance) on the outcomes.
    Unfortunately, SciPy does not have a routine for performing two-way ANOVA in the
    `stats` module. You will need to use an alternative package, such as the `statsmodels`
    package. We will use this package in [Chapter 7](51dde93b-1346-4b79-af00-09004cc77864.xhtml),
    *Regression and Forecasting*.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned, ANOVA can only detect whether there are differences. It cannot
    detect where these differences occur if there are significant differences. For
    example, we can use Durnett's test to test whether the other sample mean values
    differ from a control sample, or Tukey's range test to test each group mean against
    every other group mean.
  prefs: []
  type: TYPE_NORMAL
- en: Testing hypotheses for non-parametric data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Both t-tests and ANOVA have a major drawback: the population that is being
    sampled must follow a normal distribution. In many applications, this is not too
    restrictive because many real-world population values follow a normal distribution,
    or some rules, such as the central limit theorem, allow us to analyze some related
    data. However, it is simply not true that all possible population values follow
    a normal distribution in any reasonable way. For these (thankfully, rare) cases,
    we need some alternative test statistics to use as replacements for t-tests and
    ANOVA.'
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will use a Wilcoxon rank-sum test and the Kruskal-Wallis
    test to test for differences between two (or more, in the latter case) populations.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For this recipe, we will need the pandas package imported as `pd`, the SciPy
    `stats` module, and a default random number generator instance created using the
    following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Follow these steps to compare the populations of two or more populations that
    are not normally distributed:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will generate some sample data to use in our analysis:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we set the significance level that we will use in this analysis:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we use the `stats.kruskal`routine to generate the test statistic and the
    *p* value for the null hypothesis that the populations have the same median value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'We will use a conditional statement to print a statement about the outcome
    of the test:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we use Wilcoxon rank-sum tests to obtain the *p* values for the comparisons
    between each pair of samples:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we use conditional statements to print out messages for those comparisons
    that indicate a significant difference:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We say that data is non-parametric if the population from which the data was
    sampled does not follow a distribution that can be described by a small number
    of parameters. This usually means that the population is not normally distributed
    but is broader than this. In this recipe, we sampled from uniform distributions,
    but this is still a more structured example than we would generally have when
    non-parametric tests are necessary. Non-parametric tests can and should be used
    in any situation where we are not sure about the underlying distribution. The
    cost of doing this is that the tests are slightly less powerful.
  prefs: []
  type: TYPE_NORMAL
- en: The first step of any (real) analysis should be to plot a histogram of the data
    and inspect the distribution visually. If you draw a random sample from a normally
    distributed population, you might also expect the sample to be normally distributed
    (we have seen this several times in this book). If your sample shows the characteristic
    bell curve of a normal distribution, then it is fairly likely that the population
    is itself normally distributed. You might also use a **kernel density estimation**
    plot to help determine the distribution. This is available on the pandas plotting
    interface as `kind="kde"`. If you still aren't sure whether the population is
    normal, you can apply a statistical test, such as D'Agostino's K-squared test
    or Pearson's Chi-squared test for normality. These two tests are combined into
    a single routine to test for normality called `normaltest` in the SciPy `stats`
    module, along with several other tests for normality.
  prefs: []
  type: TYPE_NORMAL
- en: The Wilcoxon rank-sum test—also called the Mann-Whitney U test—is a non-parametric
    replacement for a two-sample t-test. Unlike the t-test, the rank-sum test does
    not compare the sample mean values to quantify whether the populations have different
    distributions. Instead, it combines the data of the samples and ranks them in
    order of size. The test statistic is generated from the sum of the ranks from
    the sample with the fewest elements. From here, as usual, we generate a *p* value
    for the null hypothesis that the two populations have the same distribution.
  prefs: []
  type: TYPE_NORMAL
- en: The Kruskal-Wallis test is a non-parametric replacement for a one-way ANOVA
    test. Like the rank-sum test, it uses the ranking of the sample data to generate
    a test statistic and *p* values for the null hypothesis that all the populations
    have the same median value. As with one-way ANOVA, we can only detect whether
    all of the populations have the same median, and not where the differences lie.
    For this, we would have to use additional tests.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we used the Kruskal-Wallis test to determine whether there were
    any significant differences between the populations corresponding to our three
    samples. A difference was detected with a *p* value of `0.07`, which is not far
    from being significant at 5%. We then used rank-sum tests to determine where significant
    differences occur between the populations. Here, we found that sample A is significantly
    different from samples B and C, and samples B and C are not significantly different.
    This is hardly surprising given the way that these samples were generated.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, since we have used multiple tests in this recipe, our overall
    confidence in our conclusions is not as high as we might expect it to be. We performed
    four tests with 95% confidence, which means our overall confidence in our conclusion
    is only approximately 81%. This is because errors aggregate over multiple tests,
    reducing the overall confidence. To correct for this, we would have to adjust
    our significance threshold for each test, using the Bonferroni correction (or
    similar).
  prefs: []
  type: TYPE_NORMAL
- en: Creating interactive plots with Bokeh
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Test statistics and numerical reasoning are good for systematically analyzing
    sets of data. However, they don't really give us a good picture of the whole set
    of data like a plot would. Numerical values are definitive but can be difficult
    to understand, especially in statistics, whereas a plot instantly illustrates
    differences between sets of data and trends. For this reason, there is a large
    number of libraries for plotting data in ever more creative ways. One particularly
    interesting package for producing plots of data is Bokeh, which allows us to create
    interactive plots in the browser by leveraging JavaScript libraries.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will see how to use Bokeh to create an interactive plot that
    can be displayed in the browser.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For this recipe, we will need the pandas package imported as `pd`, the NumPy
    package imported as `np`, an instance of the default random number generator constructed
    with the following code, and the `plotting` module from Bokeh, which we have imported
    under the `bk`alias:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'These steps show how to create an interactive plot in the browser using Bokeh:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We first need to create some sample data to plot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we specify the output file where the HTML code for the plot will be stored
    by using the `output_file`routine:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we create a new figure and set the title and axes labels, and set the
    *x*-axis type to `datetime` so that our date index will be correctly displayed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'We add the data to the figure as a line:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we use either the `show`routine or the `save`routine to save or update
    the HTML in the specified output file. We use `show`here to cause the plot to
    open in the browser:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Bokeh plots are not static objects and are supposed to be interactive via the
    browser. The data as it will appear in the Bokeh plot has been recreated here,
    using `matplotlib` for comparison:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/dbe31bab-0892-4040-a72c-a2f9cd6c6900.png)Figure 6.3 – Plot of Time
    series data created using Matplotlib'
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Bokeh uses a JavaScript library to render a plot in a browser, using data provided
    by the Python backend. The advantage of this is that it can generate plots that
    a user can inspect for themselves. For instance, we can zoom in to see detail
    in the plot that might otherwise be hidden, or pan through the data in a natural
    way. The example given in this recipe is just a taster of what is possible using
    Bokeh.
  prefs: []
  type: TYPE_NORMAL
- en: The `figure` routine creates an object representing the plot, which we add elements
    to—such as a line through the data points—in the same way that we would add plots
    to a matplotlib `Axes` object. In this recipe, we created a simple HTML file that
    contains JavaScript code to render the data. This HTML code is dumped to the specified
    file whenever we save or, as is in the recipe, call the `show` routine. In practice,
    the smaller the *p* value, the more confident we can be that the hypothesized
    population mean is correct.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The capabilities of Bokeh go far beyond what is described here. Bokeh plots
    can be embedded in files such as Jupyter notebooks, which are also rendered in
    the browser, or into existing websites. If you are using a Jupyter notebook, you
    should use the `output_notebook` routine instead of the `output_file` routine
    to print the plot directly into the notebook. It has a wide array of different
    plotting styles, supports the sharing of data between plots (data can be selected
    in one plot and highlighted in the other(s), for example), and supports streaming
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are a large number of textbooks on statistics and statistical theory.
    The following book was used as reference for this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Mendenhall, W., Beaver, R., and Beaver, B., (2006), Introduction To Probability
    And Statistics, 12th ed., (Belmont, Calif.: Thomson Brooks/Cole)*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The pandas documentation ([https://pandas.pydata.org/docs/index.html](https://pandas.pydata.org/docs/index.html))
    and the following pandas book serve as good references for working with pandas:'
  prefs: []
  type: TYPE_NORMAL
- en: '*McKinney, W.,*(*2017*),*Python for Data Analysis, 2nd ed.,*(*Sebastopol: O''Reilly
    Media, Inc,* *US*)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The SciPy documentation ([https://docs.scipy.org/doc/scipy/reference/tutorial/stats.html](https://docs.scipy.org/doc/scipy/reference/tutorial/stats.html))
    also contains detailed information about the statistics module that was used several
    times in this chapter.**
  prefs: []
  type: TYPE_NORMAL
