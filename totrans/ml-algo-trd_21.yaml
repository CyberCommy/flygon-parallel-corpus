- en: '21'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '21'
- en: Generative Adversarial Networks for Synthetic Time-Series Data
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于合成时间序列数据的生成对抗网络
- en: 'Following the coverage of autoencoders in the previous chapter, this chapter
    introduces a second unsupervised deep learning technique: **generative adversarial
    networks** (**GANs**). As with autoencoders, GANs complement the methods for dimensionality
    reduction and clustering introduced in *Chapter 13*, *Data-Driven Risk Factors
    and Asset Allocation with Unsupervised Learning*.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中介绍了自动编码器后，本章介绍了第二种无监督深度学习技术：**生成对抗网络**（**GAN**）。与自动编码器一样，GAN补充了*第13章*中介绍的降维和聚类方法，即*基于数据的风险因子和无监督学习的资产配置*。
- en: '**GANs** were invented by Goodfellow et al. in 2014\. Yann LeCun has called
    GANs the "most exciting idea in AI in the last ten years." A **GAN** trains two
    neural networks, called the **generator** and **discriminator**, in a competitive
    setting. The generator aims to produce samples that the discriminator is unable
    to distinguish from a given class of training data. The result is a generative
    model capable of producing synthetic samples representative of a certain target distribution
    but artificially and, thus, inexpensively created.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**GAN**是由Goodfellow等人于2014年发明的。Yann LeCun称GAN为“过去十年人工智能中最激动人心的想法”。**GAN**在竞争环境中训练两个神经网络，称为**生成器**和**鉴别器**。生成器旨在生成鉴别器无法区分的样本，以模拟给定类别的训练数据。结果是一个能够生成代表某个目标分布的合成样本的生成模型，但是这些样本是人工制造的，因此成本低廉。'
- en: GANs have produced an avalanche of research and successful applications in many
    domains. While originally applied to images, Esteban, Hyland, and Rätsch (2017)
    applied GANs to the medical domain to generate **synthetic time-series data**.
    Experiments with financial data ensued (Koshiyama, Firoozye, and Treleaven 2019;
    Wiese et al. 2019; Zhou et al. 2018; Fu et al. 2019) to explore whether GANs can
    generate data that simulates alternative asset price trajectories to train supervised
    or reinforcement algorithms, or to backtest trading strategies. We will replicate
    the Time-Series GAN presented at the 2019 NeurIPS by Yoon, Jarrett, and van der
    Schaar (2019) to illustrate the approach and demonstrate the results.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: GAN在许多领域产生了大量的研究和成功的应用。虽然最初应用于图像，但Esteban，Hyland和Rätsch（2017）将GAN应用于医学领域，以生成**合成时间序列数据**。随后进行了与金融数据的实验（Koshiyama，Firoozye和Treleaven
    2019; Wiese等2019; Zhou等2018; Fu等2019），以探索GAN是否能够生成模拟替代资产价格轨迹的数据，以训练监督或强化算法，或者进行交易策略的回测。我们将复制Yoon，Jarrett和van
    der Schaar（2019）在2019年NeurIPS上提出的时间序列GAN，以说明该方法并展示结果。
- en: 'More specifically, in this chapter you will learn about the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，在本章中，您将了解以下内容：
- en: How GANs work, why they are useful, and how they can be applied to trading
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GAN的工作原理，它们为何有用，以及如何应用于交易
- en: Designing and training GANs using TensorFlow 2
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用TensorFlow 2设计和训练GAN
- en: Generating synthetic financial data to expand the inputs available for training
    ML models and backtesting
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成合成金融数据以扩展用于训练ML模型和回测的输入
- en: You can find the code samples for this chapter and links to additional resources
    in the corresponding directory of the GitHub repository. The notebooks include
    color versions of the images.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在GitHub存储库的相应目录中找到本章的代码示例和其他资源的链接。笔记本包括图像的彩色版本。
- en: Creating synthetic data with GANs
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用GAN创建合成数据
- en: This book mostly focuses on supervised learning algorithms that receive input
    data and predict an outcome, which we can compare to the ground truth to evaluate
    their performance. Such algorithms are also called **discriminative models** because
    they learn to differentiate between different output values.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本书主要关注接收输入数据并预测结果的监督学习算法，我们可以将其与基本事实进行比较，以评估其性能。这样的算法也被称为**鉴别模型**，因为它们学会区分不同的输出值。
- en: GANs are an instance of **generative models** like the variational autoencoder
    we encountered in the previous chapter. As described there, a generative model
    takes a training set with samples drawn from some distribution *p*[data] and learns
    to represent an estimate *p*[model] of that data-generating distribution.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: GAN是**生成模型**的一个实例，就像我们在上一章中遇到的变分自动编码器一样。如前所述，生成模型使用从某个分布*p*[data]中抽取的样本的训练集，并学习表示该数据生成分布的估计*p*[model]。
- en: As mentioned in the introduction, GANs are considered one of the most exciting
    recent machine learning innovations because they appear capable of generating
    high-quality samples that faithfully mimic a range of input data. This is very
    attractive given the absence or high cost of labeled data required for supervised
    learning.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在介绍中提到的，GAN被认为是最令人兴奋的最近的机器学习创新之一，因为它们似乎能够生成高质量的样本，忠实地模仿一系列输入数据。鉴于监督学习所需的标记数据的缺失或高成本，这一点非常吸引人。
- en: GANs have triggered a wave of research that initially focused on the generation
    of surprisingly realistic images. More recently, GAN instances have emerged that
    produce synthetic time series with significant potential for trading since the
    limited availability of historical market data is a key driver of the risk of
    backtest overfitting.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: GAN引发了一波研究热潮，最初集中在生成令人惊讶的逼真图像。最近，出现了产生合成时间序列的GAN实例，对于交易具有重要潜力，因为历史市场数据的有限可用性是回测过度拟合风险的关键驱动因素。
- en: In this section, we explain in more detail how generative models and adversarial
    training work and review various GAN architectures. In the next section, we will
    demonstrate how to design and train a GAN using TensorFlow 2\. In the last section,
    we will describe how to adapt a GAN so that it creates synthetic time-series data.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将更详细地解释生成模型和对抗训练的工作原理，并回顾各种GAN架构。在下一节中，我们将演示如何使用TensorFlow 2设计和训练GAN。在最后一节中，我们将描述如何调整GAN，以便它创建合成时间序列数据。
- en: Comparing generative and discriminative models
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 比较生成模型和鉴别模型
- en: 'Discriminative models learn how to differentiate among outcomes *y*, given
    input data *X*. In other words, they learn the probability of the outcome given
    the data: *p*(*y* | *X*). Generative models, on the other hand, learn the joint
    distribution of inputs and outcome *p*(*y*, *X*). While generative models can
    be used as discriminative models using Bayes'' rule to compute which class is
    most likely (see *Chapter 10*, *Bayesian ML – Dynamic Sharpe Ratios and Pairs
    Trading*), it often seems preferable to solve the prediction problem directly
    rather than by solving the more general generative challenge first (Ng and Jordan
    2002).'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: 'GANs have a generative objective: they produce complex outputs, such as realistic
    images, given simple inputs that can even be random numbers. They achieve this
    by modeling a probability distribution over the possible outputs. This probability
    distribution can have many dimensions, for example, one for each pixel in an image,
    each character or token in a document, or each value in a time series. As a result,
    the model can generate outputs that are very likely representative of the class
    of outputs.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: Richard Feynman's quote "**What I cannot create, I do not understand**" emphasizes
    that modeling generative distributions is an important step towards more general
    AI and resembles human learning, which succeeds using much fewer samples.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: Generative models have several **use cases** beyond their ability to generate
    additional samples from a given distribution. For example, they can be incorporated
    into model-based **reinforcement learning** (**RL**) algorithms (see the next
    chapter). Generative models can also be applied to time-series data to simulate
    alternative past or possible future trajectories that can be used for planning
    in RL or supervised learning more generally, including for the design of trading
    algorithms. Other use cases include semi-supervised learning where GANs facilitate
    feature matching to assign missing labels with much fewer training samples than
    current approaches.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: Adversarial training – a zero-sum game of trickery
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The key innovation of GANs is a new way of learning the data-generating probability
    distribution. The algorithm sets up a competitive, or adversarial game between
    two neural networks called the **generator** and the **discriminator**.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: The generator's goal is to convert random noise input into fake instances of
    a specific class of objects, such as images of faces or stock price time series.
    The discriminator, in turn, aims to differentiate the generator's deceptive output
    from a set of training data containing true samples of the target objects. The
    overall GAN objective is for both networks to get better at their respective tasks
    so that the generator produces outputs that a machine can no longer distinguish
    from the originals (at which point we don't need the discriminator, which is no
    longer necessary, and can discard it).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 21.1* illustrates adversarial training using a generic GAN architecture
    designed to generate images. We assume the generator uses a deep CNN architecture
    (such as the VGG16 example from *Chapter 18*, *CNNs for Financial Time Series
    and Satellite Images*) that is reversed just like the decoder part of the convolutional
    autoencoder we discussed in the previous chapter. The generator receives an input
    image with random pixel values and produces a *fake* output image that is passed
    on to the discriminator network, which uses a mirrored CNN architecture. The discriminator
    network also receives *real* samples that represent the target distribution and
    predicts the probability that the input is *real*, as opposed to *fake*. Learning
    takes place by backpropagating the gradients of the discriminator and generator
    losses to the respective network''s parameters:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15439_21_01.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
- en: 'Figure 21.1: GAN architecture'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: The recent GAN Lab is a great interactive tool inspired by TensorFlow Playground,
    which allows the user to design GANs and visualize various aspects of the learning
    process and performance over time (see resource links on GitHub).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的GAN Lab是一个受TensorFlow Playground启发的出色的交互式工具，允许用户设计GAN并可视化学习过程和性能随时间的各个方面（请参阅GitHub上的资源链接）。
- en: The rapid evolution of the GAN architecture zoo
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GAN架构动物园的快速演变
- en: Since the publication of the paper by Goodfellow et al. in 2014, GANs have attracted
    an enormous amount of interest and triggered a corresponding flurry of research.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 自2014年Goodfellow等人发表了该论文以来，GAN已经吸引了大量的兴趣，并引发了相应的研究热潮。
- en: The bulk of this work has refined the original architecture to adapt it to different
    domains and tasks, as well as expanding it to include additional information and
    create conditional GANs. Additional research has focused on improving methods
    for the challenging training process, which requires achieving a stable game-theoretic
    equilibrium between two networks, each of which can be tricky to train on its
    own.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这项工作的大部分工作是对原始架构进行改进，以使其适应不同的领域和任务，并将其扩展以包括额外的信息并创建条件GANs。额外的研究集中在改进具有挑战性的训练过程的方法，这需要在两个网络之间实现稳定的博弈均衡，每个网络本身都可能很难训练。
- en: The GAN landscape has become more diverse than we can cover here; see Creswell
    et al. (2018) and Pan et al. (2019) for recent surveys, and Odena (2019) for a
    list of open questions.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: GAN的发展已经变得比我们在这里可以涵盖的更加多样化；请参阅Creswell等人（2018）和Pan等人（2019）进行最新调查，以及Odena（2019）列出的一系列开放问题。
- en: Deep convolutional GANs for representation learning
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 用于表示学习的深度卷积GAN
- en: '**Deep convolutional GANs** (**DCGANs**) were motivated by the successful application
    of CNNs to supervised learning for grid-like data (Radford, Metz, and Chintala
    2016). The architecture pioneered the use of GANs for unsupervised learning by
    developing a feature extractor based on adversarial training. It is also easier
    to train and generates higher-quality images. It is now considered a baseline
    implementation, with numerous open source examples available (see references on
    GitHub).'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**深度卷积GANs**（**DCGANs**）受到CNN成功应用于网格数据的监督学习的启发（Radford，Metz和Chintala 2016）。该架构通过开发基于对抗训练的特征提取器，开创了使用GAN进行无监督学习的先河。它也更容易训练并生成更高质量的图像。现在它被认为是一个基准实现，有许多开源示例可用（请参阅GitHub上的参考资料）。'
- en: A DCGAN network takes uniformly distributed random numbers as input and outputs
    a color image with a resolution of 64×64 pixels. As the input changes incrementally,
    so do the generated images. The network consists of standard CNN components, including
    deconvolutional layers that reverse convolutional layers as in the convolutional
    autoencoder example in the previous chapter, or fully connected layers.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 一个DCGAN网络以均匀分布的随机数作为输入，并输出分辨率为64×64像素的彩色图像。随着输入的逐渐变化，生成的图像也会发生变化。网络由标准的CNN组件组成，包括反卷积层，这些层与上一章中的卷积自编码器示例中的卷积层相反，或者全连接层。
- en: The authors experimented exhaustively and made several recommendations, such
    as the use of batch normalization and ReLU activations in both networks. We will
    explore a TensorFlow implementation later in this chapter.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 作者进行了详尽的实验，并提出了一些建议，例如在两个网络中都使用批量归一化和ReLU激活。我们将在本章后面探讨一个TensorFlow实现。
- en: Conditional GANs for image-to-image translation
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 用于图像到图像转换的条件GANs
- en: '**Conditional GANs** (**cGANs**) introduce additional label information into
    the training process, resulting in better quality and some control over the output.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**条件GANs**（**cGANs**）将额外的标签信息引入训练过程中，从而提高了输出的质量并对输出结果进行了一定的控制。'
- en: cGANs alter the baseline architecture displayed previously in *Figure 21.1*
    by adding a third input to the discriminator that contains class labels. These
    labels, for example, could convey gender or hair color information when generating
    images.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: cGANs通过向鉴别器添加第三个输入来改变先前显示的基线架构*图21.1*，该输入包含类标签。例如，当生成图像时，这些标签可以传达性别或头发颜色信息。
- en: Extensions include the **generative adversarial what-where network** (**GAWWN**;
    Reed et al. 2016), which uses bounding box information not only to generate synthetic
    images but also to place objects at a given location.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展包括**生成对抗性what-where网络**（**GAWWN**；Reed等人2016），它不仅使用边界框信息生成合成图像，还将对象放置在给定位置。
- en: GAN applications to images and time-series data
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GAN应用于图像和时间序列数据
- en: Alongside a large variety of extensions and modifications of the original architecture,
    numerous applications to images, as well as sequential data like speech and music,
    have emerged. Image applications are particularly diverse, ranging from image
    blending and super-resolution to video generation and human pose identification.
    Furthermore, GANs have been used to improve supervised learning performance.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 除了原始架构的大量扩展和修改之外，还出现了许多应用于图像以及语音和音乐等序列数据的应用。图像应用特别多样，从图像混合和超分辨率到视频生成和人体姿势识别。此外，GAN已被用于提高监督学习的性能。
- en: We will look at a few salient examples and then take a closer look at applications
    to time-series data that may become particularly relevant to algorithmic trading
    and investment. See Alqahtani, Kavakli-Thorne, and Kumar (2019) for a recent survey
    and GitHub references for additional resources.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将看一些显著的例子，然后更详细地研究应用于时间序列数据的应用，这可能对算法交易和投资特别相关。有关最新调查和GitHub参考资料，请参阅Alqahtani，Kavakli-Thorne和Kumar（2019）。
- en: CycleGAN – unpaired image-to-image translation
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CycleGAN - 无配对图像到图像的转换
- en: Supervised image-to-image translation aims to learn a mapping between aligned
    input and output images. CycleGAN solves this task when paired images are not
    available and transforms images from one domain to match another.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 监督图像到图像的转换旨在学习对齐输入和输出图像之间的映射关系。当配对图像不可用并且需要将图像从一个域转换为另一个域时，CycleGAN解决了这个任务。
- en: Popular examples include the synthetic "painting" of horses as zebras and vice
    versa. It also includes the transfer of styles, by generating a realistic sample
    of an impressionistic print from an arbitrary landscape photo (Zhu et al. 2018).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 流行的例子包括将马的合成“绘画”成斑马，反之亦然。它还包括通过从任意风景照片生成印象派印刷的逼真样本来转移风格（Zhu等，2018年）。
- en: StackGAN – text-to-photo image synthesis
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: StackGAN-文本到照片图像合成
- en: One of the earlier applications of GANs to domain-transfer is the generation
    of images based on text. **Stacked GAN**, often shortened to **StackGAN**, uses
    a sentence as input and generates multiple images that match the description.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: GANs较早的一个应用是基于文本生成图像。**Stacked GAN**，通常缩写为**StackGAN**，使用一个句子作为输入，并生成与描述匹配的多个图像。
- en: The architecture operates in two stages, where the first stage yields a low-resolution
    sketch of shape and colors, and the second stage enhances the result to a high-resolution
    image with photorealistic details (Zhang et al. 2017).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 该架构分为两个阶段，第一阶段产生形状和颜色的低分辨率草图，第二阶段将结果增强到具有逼真细节的高分辨率图像（Zhang等，2017年）。
- en: SRGAN – photorealistic single image super-resolution
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: SRGAN-逼真的单图像超分辨率
- en: Super-resolution aims at producing higher-resolution photorealistic images from
    low-resolution input. GANs applied to this task have deep CNN architectures that
    use batch normalization, ReLU, and skip connection as encountered in ResNet (see
    *Chapter 18*, *CNNs for Financial Time Series and Satellite Images*) to produce
    impressive results that are already finding commercial applications (Ledig et
    al. 2017).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 超分辨率旨在从低分辨率输入产生更高分辨率的逼真图像。应用于此任务的GAN具有使用批量归一化、ReLU和跳跃连接的深度CNN架构，这些都在ResNet中遇到（参见*第18章*，*金融时间序列和卫星图像的CNNs*），以产生令人印象深刻的结果，已经找到商业应用（Ledig等，2017年）。
- en: Synthetic time series with recurrent conditional GANs
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 具有循环条件GAN的合成时间序列
- en: '**Recurrent GANs** (**RGANs**) and **recurrent conditional GANs** (**RCGANs**)
    are two model architectures that aim to synthesize realistic real-valued multivariate
    time series (Esteban, Hyland, and Rätsch 2017). The authors target applications
    in the medical domain, but the approach could be highly valuable to overcome the
    limitations of historical market data.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**Recurrent GANs**（**RGANs**）和**recurrent conditional GANs**（**RCGANs**）是两种旨在合成逼真的实值多变量时间序列的模型架构（Esteban，Hyland和Rätsch，2017年）。作者们针对医学领域的应用，但这种方法可能对克服历史市场数据的局限性非常有价值。'
- en: RGANs rely on **recurrent neural networks** (**RNNs**) for the generator and
    the discriminator. RCGANs add auxiliary information in the spirit of cGANs (see
    the previous *Conditional GANs for image-to-image translation* section).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: RGANs依赖于**循环神经网络**（**RNNs**）用于生成器和鉴别器。RCGANs添加辅助信息，符合cGANs的精神（参见前面的*图像到图像翻译的条件GANs*部分）。
- en: The authors succeed in generating visually and quantitatively compelling realistic
    samples. Furthermore, they evaluate the quality of the synthetic data, including
    synthetic labels, by using it to train a model with only minor degradation of
    the predictive performance on a real test set. The authors also demonstrate the
    successful application of RCGANs to an early warning system using a medical dataset
    of 17,000 patients from an intensive care unit. Hence, the authors illustrate
    that RCGANs are capable of generating time-series data useful for supervised training.
    We will apply this approach to financial market data this chapter in the *TimeGAN
    – adversarial training for synthetic financial data* section.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 作者成功地生成了视觉上和数量上令人信服的逼真样本。此外，他们通过使用合成数据来训练模型，包括合成标签，评估了合成数据的质量，只有轻微的降低了对真实测试集的预测性能。作者还展示了RCGANs成功应用于使用来自重症监护病房的1.7万名患者的医学数据的早期预警系统。因此，作者说明了RCGANs能够生成对监督训练有用的时间序列数据。我们将在本章的*TimeGAN-用于合成金融数据的对抗训练*部分中应用这种方法到金融市场数据。
- en: How to build a GAN using TensorFlow 2
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何使用TensorFlow 2构建GAN
- en: To illustrate the implementation of a GAN using Python, we will use the DCGAN
    example discussed earlier in this section to synthesize images from the Fashion-MNIST
    dataset that we first encountered in *Chapter 13*, *Data-Driven Risk Factors and
    Asset Allocation with Unsupervised Learning*.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明使用Python实现GAN，我们将使用本节早期讨论的DCGAN示例，从我们在*第13章*中首次遇到的Fashion-MNIST数据集中合成图像。
- en: See the notebook `deep_convolutional_generative_adversarial_network` for implementation
    details and references.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 有关实施细节和参考资料，请参阅笔记本`deep_convolutional_generative_adversarial_network`。
- en: Building the generator network
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建生成器网络
- en: 'Both generator and discriminator use a deep CNN architecture along the lines
    illustrated in *Figure 20.1*, but with fewer layers. The generator uses a fully
    connected input layer, followed by three convolutional layers, as defined in the
    following `build_generator()` function, which returns a Keras model instance:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器和鉴别器都使用深度CNN架构，沿着*图20.1*中所示的线路，但层数较少。生成器使用全连接输入层，然后是三个卷积层，如下所定义的`build_generator()`函数，它返回一个Keras模型实例：
- en: '[PRE0]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The generator accepts 100 one-dimensional random values as input, and it produces
    images that are 28 pixels wide and high and, thus, contain 784 data points.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器接受100个一维随机值作为输入，并产生宽高为28像素的图像，因此包含784个数据点。
- en: A call to the `.summary()` method of the model returned by this function shows
    that this network has over 2.3 million parameters (see the notebook for details,
    including a visualization of the generator output prior to training).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 通过调用此函数返回的模型的`.summary()`方法，可以看到该网络有超过230万个参数（有关详细信息，请参阅笔记本，包括在训练之前可视化生成器输出）。
- en: Creating the discriminator network
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建鉴别器网络
- en: 'The discriminator network uses two convolutional layers that translate the
    input received from the generator into a single output value. The model has around
    212,000 parameters:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴别器网络使用两个卷积层，将从生成器接收的输入转换为单个输出值。该模型有大约212,000个参数：
- en: '[PRE1]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '*Figure 21.2* depicts how the random input flows from the generator to the
    discriminator, as well as the input and output shapes of the various network components:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15439_21_02.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
- en: 'Figure 21.2: DCGAN TensorFlow 2 model architecture'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the adversarial training process
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we have built the generator and the discriminator models, we will
    design and execute the adversarial training process. To this end, we will define
    the following:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: The loss functions for both models that reflect their competitive interaction
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A single training step that runs the backpropagation algorithm
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The training loop that repeats the training step until the model performance
    meets our expectations
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining the generator and discriminator loss functions
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The generator loss reflects the discriminator's decision regarding the fake
    input. It will be low if the discriminator mistakes an image produced by the generator
    for a real image, and high otherwise; we will define the interaction between both
    models when we create the training step.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: 'The generator loss is measured by the binary cross-entropy loss function as
    follows:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The discriminator receives both real and fake images as input. It computes
    a loss for each and attempts to minimize the sum with the goal of accurately recognizing
    both types of inputs:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'To train both models, we assign each an Adam optimizer with a learning rate
    lower than the default:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The core – designing the training step
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Each training step implements one round of stochastic gradient descent using
    the Adam optimizer. It consists of five steps:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: Providing the minibatch inputs to each model
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Getting the models' outputs for the current weights
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Computing the loss given the models' objective and output
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Obtaining the gradients for the loss with respect to each model's weights
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Applying the gradients according to the optimizer's algorithm
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The function `train_step()` carries out these five steps. We use the `@tf.function`
    decorator to speed up execution by compiling it to a TensorFlow operation rather
    than relying on eager execution (see the TensorFlow documentation for details):'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Putting it together – the training loop
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The training loop is very straightforward to implement once we have the training
    step properly defined. It consists of a simple `for` loop, and during each iteration,
    we pass a new batch of real images to the training step. We also will sample some
    synthetic images and occasionally save the model weights.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that we track progress using the `tqdm` package, which shows the percentage
    complete during training:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Evaluating the results
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'After 100 epochs that only take a few minutes, the synthetic images created
    from random noise clearly begin to resemble the originals, as you can see in *Figure
    21.3* (see the notebook for the best visual quality):'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15439_21_03.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
- en: 'Figure 21.3: A sample of synthetic Fashion-MNIST images'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: The notebook also creates a dynamic GIF image that visualizes how the quality
    of the synthetic images improves during training.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand how to build and train a GAN using TensorFlow 2, we will
    move on to a more complex example that produces synthetic time series from stock
    price data.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: TimeGAN for synthetic financial data
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Generating synthetic time-series data poses specific challenges above and beyond
    those encountered when designing GANs for images. In addition to the distribution
    over variables at any given point, such as pixel values or the prices of numerous
    stocks, a generative model for time-series data should also learn the temporal
    dynamics that shape how one sequence of observations follows another. (Refer also
    to the discussion in *Chapter 9*, *Time-Series Models for Volatility Forecasts
    and Statistical Arbitrage*).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: Very recent and promising research by Yoon, Jarrett, and van der Schaar, presented
    at NeurIPS in December 2019, introduces a novel **time-series generative adversarial
    network** (**TimeGAN**) framework that aims to account for temporal correlations
    by combining supervised and unsupervised training. The model learns a time-series
    embedding space while optimizing both supervised and adversarial objectives, which
    encourage it to adhere to the dynamics observed while sampling from historical
    data during training. The authors test the model on various time series, including
    historical stock prices, and find that the quality of the synthetic data significantly
    outperforms that of available alternatives.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: Yoon，Jarrett和van der Schaar在2019年12月的NeurIPS上介绍了最新和有前途的研究，他们引入了一个新颖的时间序列生成对抗网络（TimeGAN）框架，旨在通过结合监督和无监督训练来考虑时间相关性。该模型在优化监督和对抗目标的同时学习时间序列嵌入空间，这些目标鼓励模型在训练期间从历史数据中采样时遵循观察到的动态。作者们在各种时间序列上测试了模型，包括历史股票价格，并发现合成数据的质量明显优于现有的替代品。
- en: In this section, we will outline how this sophisticated model works, highlight
    key implementation steps that build on the previous DCGAN example, and show how
    to evaluate the quality of the resulting time series. Please see the paper for
    additional information.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将概述这个复杂模型的工作原理，突出强调建立在先前DCGAN示例上的关键实施步骤，并展示如何评估生成的时间序列的质量。请参阅论文获取更多信息。
- en: Learning to generate data across features and time
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习跨特征和时间生成数据
- en: A successful generative model for time-series data needs to capture both the
    cross-sectional distribution of features at each point in time and the longitudinal
    relationships among these features over time. Expressed in the image context we
    just discussed, the model needs to learn not only what a realistic image looks
    like, but also how one image evolves from the previous as in a video.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 成功的时间序列数据生成模型需要捕捉每个时间点上特征的横截面分布以及这些特征随时间的纵向关系。用我们刚讨论过的图像上下文来表达，该模型不仅需要学习真实图像的外观，还需要学习一个图像如何从前一个图像演变而来，就像视频中一样。
- en: Combining adversarial and supervised training
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 结合对抗和监督训练
- en: As mentioned in the first section, prior attempts at generating time-series
    data, like RGANs and RCGANs, relied on RNNs (see *Chapter 19*, *RNNs for Multivariate
    Time Series and Sentiment Analysis*) in the roles of generator and discriminator.
    TimeGAN explicitly incorporates the autoregressive nature of time series by combining
    the **unsupervised adversarial loss** on both real and synthetic sequences familiar
    from the DCGAN example with a **stepwise supervised loss** with respect to the
    original data. The goal is to reward the model for learning the **distribution
    over transitions** from one point in time to the next that are present in the
    historical data.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在第一节中提到的，以前生成时间序列数据的尝试，如RGAN和RCGAN，依赖于RNNs（参见*第19章*，*用于多变量时间序列和情感分析的RNNs*）作为生成器和鉴别器的角色。TimeGAN通过将DCGAN示例中的**无监督对抗损失**与**关于原始数据的逐步监督损失**相结合，明确地结合了时间序列的自回归性质。其目标是奖励模型学习历史数据中存在的从一个时间点到下一个时间点的**转换分布**。
- en: Furthermore, TimeGAN includes an embedding network that maps the time-series
    features to a lower-dimensional latent space to reduce the complexity of the adversarial
    space. The motivation is to capture the drivers of temporal dynamics that often
    have lower dimensionality. (Refer also to the discussions of manifold learning
    in *Chapter 13*, *Data-Driven Risk Factors and Asset Allocation with Unsupervised
    Learning* and nonlinear dimensionality reduction in *Chapter 20*, *Autoencoders
    for Conditional Risk Factors and Asset Pricing*).
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，TimeGAN还包括一个嵌入网络，将时间序列特征映射到较低维度的潜在空间，以减少对抗空间的复杂性。动机是捕捉通常具有较低维度的时间动态的驱动因素。（还可以参考*第13章*中的流形学习讨论，*使用无监督学习进行数据驱动风险因素和资产配置*以及*第20章*中的非线性降维讨论，*用于条件风险因素和资产定价的自编码器*）。
- en: A key element of the TimeGAN architecture is that both the generator and the
    embedding (or autoencoder) network are responsible for minimizing the supervised
    loss that measures how well the model learns the dynamic relationship. As a result,
    the model learns a latent space conditioned on facilitating the generator's task
    to faithfully reproduce the temporal relationships observed in the historical
    data. In addition to time-series data, the model can also process static data
    that does not change or changes less frequently over time.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: TimeGAN架构的一个关键元素是生成器和嵌入（或自编码器）网络都负责最小化衡量模型学习动态关系的监督损失。因此，模型学习了一个潜在空间，条件是促进生成器忠实地再现历史数据中观察到的时间关系。除了时间序列数据，该模型还可以处理静态数据，即随时间不变或变化较少的数据。
- en: The four components of the TimeGAN architecture
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: TimeGAN架构的四个组件
- en: 'The TimeGAN architecture combines an adversarial network with an autoencoder
    and thus has four network components, as depicted in *Figure 21.4*:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: TimeGAN架构将对抗网络与自编码器结合在一起，因此具有四个网络组件，如*图21.4*所示：
- en: '**Autoencoder**: embedding and recovery networks'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**自编码器**：嵌入和恢复网络'
- en: '**Adversarial network**: sequence generator and sequence discriminator components'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**对抗网络**：序列生成器和序列鉴别器组件'
- en: The authors emphasize the **joint training** of the autoencoder and the adversarial
    networks by means of **three different loss functions**. The **reconstruction
    loss** optimizes the autoencoder, the **unsupervised loss** trains the adversarial
    net, and the **supervised loss** enforces the temporal dynamics. As a result of
    this key insight, the TimeGAN simultaneously learns to encode features, generate
    representations, and iterate across time. More specifically, the embedding network
    creates the latent space, the adversarial network operates within this space,
    and supervised loss synchronizes the latent dynamics of both real and synthetic
    data.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 作者强调了通过三种不同的损失函数对自动编码器和对抗网络进行联合训练的重要性。重构损失优化自动编码器，无监督损失训练对抗网络，监督损失强制时间动态。由于这一关键观点，TimeGAN同时学习编码特征、生成表示，并在时间上迭代。更具体地说，嵌入网络创建潜在空间，对抗网络在该空间内运行，监督损失同步真实和合成数据的潜在动态。
- en: '![](img/B15439_21_04.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_21_04.png)'
- en: 'Figure 21.4: The components of the TimeGAN architecture'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图21.4：TimeGAN架构的组件
- en: The **embedding and recovery** components of the autoencoder map the feature
    space into the latent space and vice versa. This facilitates the learning of the
    temporal dynamics by the adversarial network, which learns in a lower-dimensional
    space. The authors implement the embedding and recovery network using a stacked
    RNN and a feedforward network. However, these choices can be flexibly adapted
    to the task at hand as long as they are autoregressive and respect the temporal
    order of the data.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 自动编码器的嵌入和恢复组件将特征空间映射到潜在空间，反之亦然。这有助于通过对抗网络学习时间动态，后者在较低维空间中学习。作者使用堆叠的RNN和前馈网络实现了嵌入和恢复网络。然而，只要它们是自回归的并且尊重数据的时间顺序，这些选择可以灵活地适应手头的任务。
- en: The **generator and the discriminator** elements of the adversarial network
    differ from the DCGAN not only because they operate on sequential data but also
    because the synthetic features are generated in the latent space that the model
    learns simultaneously. The authors chose an RNN as the generator and a bidirectional
    RNN with a feedforward output layer for the discriminator.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗网络的生成器和鉴别器元素不仅因为它们在顺序数据上操作而与DCGAN不同，而且因为合成特征是在模型同时学习的潜在空间中生成的。作者选择了RNN作为生成器，选择了双向RNN和前馈输出层作为鉴别器。
- en: Joint training of an autoencoder and adversarial network
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自动编码器和对抗网络的联合训练
- en: 'The three loss functions displayed in *Figure 21.4* drive the joint optimization
    of the network elements just described while training on real and randomly generated
    time series. In more detail, they aim to accomplish the following:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图21.4中显示的三个损失函数驱动了对实际和随机生成的时间序列进行联合优化的网络元素的训练。更详细地说，它们旨在实现以下目标：
- en: The **reconstruction loss** is familiar from our discussion of autoencoders
    in *Chapter 20*, *Autoencoders for Conditional Risk Factors and Asset Pricing*;
    it compares how well the reconstruction of the encoded data resembles the original.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重构损失在我们在第20章“条件风险因素和资产定价”中讨论的自动编码器中是熟悉的；它比较了编码数据的重构与原始数据的相似程度。
- en: The **unsupervised loss** reflects the competitive interaction between the generator
    and the discriminator described in the DCGAN example; while the generator aims
    to minimize the probability that the discriminator classifies its output as fake,
    the discriminator aims to optimize the correct classification or real and fake
    inputs.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无监督损失反映了在DCGAN示例中描述的生成器和鉴别器之间的竞争性互动；生成器旨在最小化鉴别器将其输出分类为伪造的概率，而鉴别器旨在优化对真实和伪造输入的正确分类。
- en: The **supervised loss** captures how well the generator approximates the actual
    next time step in latent space when receiving encoded real data for the prior
    sequence.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监督损失捕捉了生成器在接收编码的真实数据进行先前序列的实际下一个时间步的潜在空间中的近似程度。
- en: 'Training takes place in **three phases**:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 训练分为三个阶段：
- en: Training the autoencoder on real time series to optimize reconstruction
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在真实时间序列上训练自动编码器以优化重构
- en: Optimizing the supervised loss using real time series to capture the temporal
    dynamics of the historical data
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 优化监督损失，使用真实时间序列捕捉历史数据的时间动态
- en: Jointly training the four components while minimizing all three loss functions
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 同时训练四个组件，同时最小化所有三个损失函数
- en: TimeGAN includes several **hyperparameters** used to weigh the components of
    composite loss functions; however, the authors find the network to be less sensitive
    to these settings than one might expect given the notorious difficulties of GAN
    training. In fact, they **do not discover significant challenges during training**
    and suggest that the embedding task serves to regularize adversarial learning
    because it reduces its dimensionality while the supervised loss constrains the
    stepwise dynamics of the generator.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: TimeGAN包括几个用于权衡复合损失函数组件的超参数；然而，作者发现网络对这些设置的敏感性要小于人们可能期望的，鉴于GAN训练的困难。事实上，他们在训练过程中并没有发现重大挑战，并建议嵌入任务有助于规范对抗学习，因为它降低了其维度，而监督损失约束了生成器的逐步动态。
- en: We now turn to the TimeGAN implementation using TensorFlow 2; see the paper
    for an in-depth explanation of the math and methodology of the approach.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将使用TensorFlow 2实现TimeGAN；请参阅论文以深入了解该方法的数学和方法论。
- en: Implementing TimeGAN using TensorFlow 2
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用TensorFlow 2实现TimeGAN
- en: 'In this section, we will implement the TimeGAN architecture just described.
    The authors provide sample code using TensorFlow 1 that we will port to TensorFlow
    2\. Building and training TimeGAN requires several steps:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将实现刚刚描述的TimeGAN架构。作者提供了使用TensorFlow 1的示例代码，我们将把它移植到TensorFlow 2。构建和训练TimeGAN需要几个步骤：
- en: Selecting and preparing real and random time series inputs
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择和准备真实和随机时间序列输入
- en: Creating the key TimeGAN model components
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建关键的TimeGAN模型组件
- en: Defining the various loss functions and training steps used during the three
    training phases
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Running the training loops and logging the results
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generating synthetic time series and evaluating the results
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We'll walk through the key items for each of these steps; please refer to the
    notebook `TimeGAN_TF2` for the code examples in this section (unless otherwise
    noted), as well as additional implementation details.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the real and random input series
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The authors demonstrate the applicability of TimeGAN to financial data using
    15 years of daily Google stock prices downloaded from Yahoo Finance with six features,
    namely open, high, low, close and adjusted close price, and volume. We'll instead
    use close to 20 years of adjusted close prices for six different tickers because
    it introduces somewhat higher variability. We will follow the original paper in
    targeting synthetic series with 24 time steps.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: Among the stocks with the longest history in the Quandl Wiki dataset are those
    displayed in normalized format, that is, starting at 1.0, in *Figure 21.5*. We
    retrieve the adjusted close from 2000-2017 and obtain over 4,000 observations.
    The correlation coefficient among the series ranges from 0.01 for GE and CAT to
    0.94 for DIS and KO.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15439_21_05.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
- en: 'Figure 21.5: The TimeGAN input—six real stock prices series'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: 'We scale each series to the range [0, 1] using scikit-learn''s `MinMaxScaler`
    class, which we will later use to rescale the synthetic data:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'In the next step, we create rolling windows containing overlapping sequences
    of 24 consecutive data points for the six series:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We then create a `tf.data.Dataset` instance from the list of `NumPy` arrays,
    ensure the data gets shuffled while training, and set a batch size of 128:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: We also need a random time-series generator that produces simulated data with
    24 observations on the six series for as long as the training continues.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: 'To this end, we will create a generator that draws the requisite data uniform
    at random and feeds the result into a second `tf.data.Datase`t instance. We set
    this dataset to produce batches of the desired size and to repeat the process
    for as long as necessary:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We'll now proceed to define and instantiate the TimeGAN model components.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: Creating the TimeGAN model components
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We'll now create the two autoencoder components and the two adversarial network
    elements, as well as the supervisor that encourages the generator to learn the
    temporal dynamic of the historical price series.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: 'We will follow the authors'' sample code in creating RNNs with three hidden
    layers, each with 24 GRU units, except for the supervisor, which uses only two
    hidden layers. The following `make_rnn` function automates the network creation:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The `autoencoder` consists of the `embedder` and the recovery networks that
    we instantiate here:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We then create the generator, the discriminator, and the supervisor like so:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We also define two generic loss functions, namely `MeanSquaredError` and `BinaryCrossEntropy`,
    which we will use later to create the various specific loss functions during the
    three phases:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Now it's time to start the training process.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: Training phase 1 – autoencoder with real data
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The autoencoder integrates the embedder and the recovery functions, as we saw
    in the previous chapter:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'It has 21,054 parameters. We will now instantiate the optimizer for this training
    phase and define the training step. It follows the pattern introduced with the
    DCGAN example, using `tf.GradientTape` to record the operations that generate
    the reconstruction loss. This allows us to rely on the automatic differentiation
    engine to obtain the gradients with respect to the trainable embedder and recovery
    network weights that drive `backpropagation`:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The reconstruction loss simply compares the autoencoder outputs with its inputs.
    We train for 10,000 steps in a little over one minute using this training loop
    that records the step loss for monitoring with TensorBoard:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Training phase 2 – supervised learning with real data
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We already created the supervisor model so we just need to instantiate the
    optimizer and define the train step as follows:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: In this case, the loss compares the output of the supervisor with the next timestep
    for the embedded sequence so that it learns the temporal dynamics of the historical
    price sequences; the training loop works similarly to the autoencoder example
    in the previous chapter.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: Training phase 3 – joint training with real and random data
  id: totrans-176
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The joint training involves all four network components, as well as the supervisor.
    It uses multiple loss functions and combinations of the base components to achieve
    the simultaneous learning of latent space embeddings, transition dynamics, and
    synthetic data generation.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: We will highlight a few salient examples; please see the notebook for the full
    implementation that includes some repetitive steps that we will omit here.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: 'To ensure that the generator faithfully reproduces the time series, TimeGAN
    includes a moment loss that penalizes when the mean and variance of the synthetic
    data deviate from the real version:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The end-to-end model that produces synthetic data involves the generator, supervisor,
    and recovery components. It is defined as follows and has close to 30,000 trainable
    parameters:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The joint training involves three optimizers for the autoencoder, the generator,
    and the discriminator:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The train step for the generator illustrates the use of four loss functions
    and corresponding combinations of network components to achieve the desired learning
    outlined at the beginning of this section:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Finally, the joint training loop pulls the various training steps together
    and builds on the learning from phase 1 and 2 to train the TimeGAN components
    on both real and random data. We run the loop for 10,000 iterations in under 40
    minutes:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Now we can finally generate synthetic time series!
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: Generating synthetic time series
  id: totrans-190
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To evaluate the `TimeGAN` results, we will generate synthetic time series by
    drawing random inputs and feeding them to the `synthetic_data` network just described
    in the preceding section. More specifically, we''ll create roughly as many artificial
    series with 24 observations on the six tickers as there are overlapping windows
    in the real dataset:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The result is 35 batches containing 128 samples, each with the dimensions 24×6,
    that we stack like so:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We can use the trained `MinMaxScaler` to revert the synthetic output to the
    scale of the input series:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '*Figure 21.6* displays samples of the six synthetic series and the corresponding
    real series. The synthetic data generally reflects a variation of behavior not
    unlike its real counterparts and, after rescaling, roughly (due to the random
    input) matches its range:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15439_21_06.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
- en: 'Figure 21.6: TimeGAN output—six synthetic prices series and their real counterparts'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: Now it's time to take a closer look at how to more thoroughly evaluate the quality
    of the synthetic data.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the quality of synthetic time-series data
  id: totrans-201
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The TimeGAN authors assess the quality of the generated data with respect to
    three practical criteria:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: '**Diversity**: The distribution of the synthetic samples should roughly match
    that of the real data.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fidelity**: The sample series should be indistinguishable from the real data.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Usefulness**: The synthetic data should be as useful as its real counterparts
    for solving a predictive task.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'They apply three methods to evaluate whether the synthetic data actually exhibits
    these characteristics:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: '**Visualization**: For a qualitative diversity assessment of diversity, we
    use dimensionality reduction—**principal component analysis** (**PCA**) and **t-SNE**
    (see *Chapter 13*, *Data-Driven Risk Factors and Asset Allocation with Unsupervised
    Learning*)—to visually inspect how closely the distribution of the synthetic samples
    resembles that of the original data.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Discriminative score**: For a quantitative assessment of fidelity, the test
    error of a time-series classifier, such as a two-layer LSTM (see *Chapter 18*,
    *CNNs for Financial Time Series and Satellite Images*), lets us evaluate whether
    real and synthetic time series can be differentiated or are, in fact, indistinguishable.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Predictive score**: For a quantitative measure of usefulness, we can compare
    the test errors of a sequence prediction model trained on, alternatively, real
    or synthetic data to predict the next time step for the real data.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We'll apply and discuss the results of each method in the following sections.
    See the notebook `evaluating_synthetic_data` for the code samples and additional
    details.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: Assessing diversity – visualization using PCA and t-SNE
  id: totrans-211
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To visualize the real and synthetic series with 24 time steps and six features,
    we will reduce their dimensionality so that we can plot them in two dimensions.
    To this end, we will sample 250 normalized sequences with six features each and
    reshape them to obtain data with the dimensionality 1,500×24 (showing only the
    steps for real data; see the notebook for the synthetic data):'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'PCA is a linear method that identifies a new basis with mutually orthogonal
    vectors that, successively, capture the directions of maximum variance in the
    data. We will compute the first two components using the real data and then project
    both real and synthetic samples onto the new coordinate system:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 't-SNE is a nonlinear manifold learning method for the visualization of high-dimensional
    data. It converts similarities between data points to joint probabilities and
    aims to minimize the Kullback-Leibler divergence between the joint probabilities
    of the low-dimensional embedding and the high-dimensional data (see *Chapter 13*,
    *Data-Driven Risk Factors and Asset Allocation with Unsupervised Learning*). We
    compute t-SNE for the combined real and synthetic data as follows:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '*Figure 21.7* displays the PCA and t-SNE results for a qualitative assessment
    of the similarity of the real and synthetic data distributions. Both methods reveal
    strikingly similar patterns and significant overlap, suggesting that the synthetic
    data captures important aspects of the real data characteristics.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15439_21_07.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
- en: 'Figure 21.7: 250 samples of real and synthetic data in two dimensions'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: Assessing fidelity – time-series classification performance
  id: totrans-221
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The visualization only provides a qualitative impression. For a quantitative
    assessment of the fidelity of the synthetic data, we will train a time-series
    classifier to distinguish between real and fake data and evaluate its performance
    on a held-out test set.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: 'More specifically, we will select the first 80 percent of the rolling sequences
    for training and the last 20 percent as a test set, as follows:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Then we will create a simple RNN with six units that receives mini batches
    of real and synthetic series with the shape 24×6 and uses a sigmoid activation.
    We will optimize it using binary cross-entropy loss and the Adam optimizer, while
    tracking the AUC and accuracy metrics:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The model has 259 trainable parameters. We will train it for 250 epochs on
    batches of 128 randomly selected samples and track the validation performance:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Once the training completes, evaluation of the test set yields a classification
    error of almost 56 percent on the balanced test set and a very low AUC of 0.15:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '*Figure 21.8* plots the accuracy and AUC performance metrics for both train
    and test data over the 250 training epochs:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15439_21_08.png)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
- en: 'Figure 21.8: Train and test performance of the time-series classifier over
    250 epochs'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: The plot shows that that model is not able to learn the difference between the
    real and synthetic data in a way that generalizes to the test set. This result
    suggests that the quality of the synthetic data meets the fidelity standard.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: Assessing usefulness – train on synthetic, test on real
  id: totrans-235
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Finally, we want to know how useful synthetic data is when it comes to solving
    a prediction problem. To this end, we will train a time-series prediction model
    alternatively on the synthetic and the real data to predict the next time step
    and compare the performance on a test set created from the real data.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: 'More specifically, we will select the first 23 time steps of each sequence
    as input, and the final time step as output. At the same time, we will split the
    real data into train and test sets using the same temporal split as in the previous
    classification example:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'We will select the complete synthetic data for training since abundance is
    one of the reasons we generated it in the first place:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'We will create a one-layer RNN with 12 GRU units that predicts the last time
    steps for the six stock price series and, thus, has six linear output units. The
    model uses the Adam optimizer to minimize the mean absolute error (MAE):'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'We will train the model twice using the synthetic and real data for training,
    respectively, and the real test set to evaluate the out-of-sample performance.
    Training on synthetic data works as follows; training on real data works analogously
    (see the notebook):'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '*Figure 21.9* plots the MAE on the train and test sets (on a log scale so we
    can spot the differences) for both models. It turns out that the MAE is slightly
    lower after training on the synthetic dataset:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15439_21_09.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
- en: 'Figure 21.9: Train and test performance of the time-series prediction model
    over 100 epochs'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: The result shows that synthetic training data may indeed be useful. On the specific
    predictive task of predicting the next daily stock price for six tickers, a simple
    model trained on synthetic TimeGAN data delivers equal or better performance than
    training on real data.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: Lessons learned and next steps
  id: totrans-249
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The perennial problem of overfitting that we encountered throughout this book
    implies that the ability to generate useful synthetic data would be quite valuable.
    The TimeGAN example justifies cautious optimism in this regard. At the same time,
    there are some **caveats**: we generated price data for a small number of assets
    at a daily frequency. In reality, we are probably interested in returns for a
    much larger number of assets, possibly at a higher frequency. The **cross-sectional
    and temporal dynamics** will certainly become more complex and may require adjustments
    to the TimeGAN architecture and training process.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: 'These limitations of the experiment, however promising, imply natural next
    steps: we need to expand the scope to higher-dimensional time series containing
    information other than prices and also need to test their usefulness in the context
    of more complex models, including for feature engineering. These are very early
    days for synthetic training data, but this example should equip you to pursue
    your own research agenda towards more realistic solutions.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-252
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we introduced GANs that learn a probability distribution over
    the input data and are thus capable of generating synthetic samples that are representative
    of the target data.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: While there are many practical applications for this very recent innovation,
    they could be particularly valuable for algorithmic trading if the success in
    generating time-series training data in the medical domain can be transferred
    to financial market data. We learned how to set up adversarial training using
    TensorFlow. We also explored TimeGAN, a recent example of such a model, tailored
    to generating synthetic time-series data.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we focus on reinforcement learning where we will build
    agents that interactively learn from their (market) environment.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
