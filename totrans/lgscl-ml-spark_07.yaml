- en: Chapter 7. Tuning Machine Learning Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Tuning an algorithm or a machine learning application is simply a process that
    one goes through in order to enable the algorithm to perform optimally (in terms
    of runtime and memory usage) when optimizing the parameters that impact the model.
    This chapter aims to guide the reader through model tuning. It will cover the
    main techniques used to optimize an ML algorithm''s performance. Techniques will
    be explained both from the MLlib and Spark ML perspective. In [Chapter 5](part0043_split_000.html#190862-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 5.  Supervised and Unsupervised Learning by Examples"), *Supervised and
    Unsupervised Learning by Examples* and [Chapter 6](part0049_split_000.html#1ENBI2-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 6.  Building Scalable Machine Learning Pipelines"), *Building Scalable
    Machine Learning Pipelines*, we described how to develop some complete machine
    learning applications and pipelines from data collection to model evaluation.
    In this chapter, we will try to reuse some of those applications to improve the
    performance by tuning several parameters such as Hyperparameter tuning, Grid search
    parameter tuning with MLlib and Spark ML, random search parameter tuning and cross-validation.
    The hypothesis ,which is also an important statistical test, will be discussed.
    In summary, the following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Details about machine learning model tuning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Typical challenges in model tuning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating machine learning models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Validation and evaluation techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parameter tuning for machine learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hypothesis testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Machine learning model selection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Details about machine learning model tuning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Fundamentally, one can argue that the ultimate goal of **machine learning**
    (**ML**) is to make a machine that can automatically build models from data without
    requiring tedious and time-consuming human involvement. You will see that one
    of the difficulties in ML is that learning algorithms are like decision trees,
    random forests, and clustering techniques that require you to set parameters before
    you use the models for practical purposes. Alternatively, you need to set some
    constraints on those parameters.
  prefs: []
  type: TYPE_NORMAL
- en: How you set those parameters depends on a set of factors and the specification.
    Your goal in this regard is usually to set those parameters to the optimal values
    that enable you to complete a learning task in the best possible way. Thus, tuning
    an algorithm or ML technique can be simply thought of as a process where one goes
    through a series of steps in which they optimize the parameters that impact the
    model's performance in order to enable the algorithm to perform in the best way.
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 3](part0031_split_000.html#TI1E2-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 3. Understanding the Problem by Understanding the Data"), *Understanding
    the Problem by Understanding the Data*, and [Chapter 5](part0043_split_000.html#190862-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 5.  Supervised and Unsupervised Learning by Examples"), *Supervised and
    Unsupervised Learning by Examples*, we discussed some techniques to choose the
    best algorithm based on your data and discuss the most widely used algorithms.
    To get the best result out of your model, you have to first define what the *best*
    actually is. We will discuss tuning in both abstract and concrete ways.
  prefs: []
  type: TYPE_NORMAL
- en: In the abstract sense of machine learning, tuning involves working with variables
    or based on parameters that have been identified to affect system performance
    as evaluated by some appropriate metric. Hence, the improved performance reveals
    which parameter settings are more favorable (that is, tuned) or less favorable
    (that is, un-tuned). In common sense terms, tuning is essentially selecting the
    best parameters for an algorithm to optimize its performance, given the working
    environment of hardware, specific workloads, and so on And tuning in machine learning
    is an automated process for doing this.
  prefs: []
  type: TYPE_NORMAL
- en: Well, let's get to the point and make the discussion more concrete through some
    examples. If you take an ML algorithm for clustering like KNN or K-Means, as a
    developer/data scientist/data engineer you need to specify the number of K in
    your model or centroids.
  prefs: []
  type: TYPE_NORMAL
- en: So the question is 'how can you do this?' Technically, there is no shortcut
    around the need to tune the model. Computationally a naïve approach would be to
    try with different values of K as a model and of course observing how it goes
    to inter and intra-group error as you vary the number of K in your model.
  prefs: []
  type: TYPE_NORMAL
- en: The second example could be by using the **Support Vector Machine** (**SVM**)
    for classification tasks. As you know, an SVM classification requires an initial
    learning phase in which the training data are used to adjust the classification
    parameters. This really denotes an initial parameter-tuning phase where you might
    try to tune the models in order to achieve high-quality results.
  prefs: []
  type: TYPE_NORMAL
- en: 'The third practical example suggest that there is no such thing as a perfect
    set of optimizations for all deployments of an Apache web server. A sysadmin learns
    from the data on the job, so to speak, and optimizes it''s own Apache web server
    configuration as appropriate for its specific environment. Now imagine an automated
    process for doing those three things, that is, a system that can learn from data
    on its own; the definition of machine learning. A system that tunes its own parameters
    in such a data-based fashion would be an instance of tuning in machine learning.
    Now, let us summarize the main points of why we evaluate the predictive performance
    of a model:'
  prefs: []
  type: TYPE_NORMAL
- en: We want to estimate the generalization error, the predictive performance of
    our model on future (unseen) data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We want to increase the predictive performance by tweaking the learning algorithm
    and selecting the best-performing model from a given hypothesis space.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We want to identify the machine learning algorithm that is best-suited for the
    problem at hand; thus, we want to compare different algorithms, selecting the
    best-performing one as well as the best-performing model from the algorithm's
    hypothesis space.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In a nutshell, there are four steps in the process of finding the best parameter
    set:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Define the parametric space**: First, we need to decide the exact parameter
    values we would like to consider for the algorithm.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Define the cross-validation settings**: Secondly we need to decide how to
    choose the optimal cross-validation folds for the data (to be discussed later
    in this chapter).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Define the metric**: Thirdly, we need to decide which metric to use for determining
    the best combination of parameters. For example, accuracy, root means squared
    error, precision, recall, or f-score, and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Train, evaluate and compare**: Fourthly, for each unique combination of the
    parameter values, cross-validation is carried out and based on the error metric
    defined by the user in the third step, the best-performing model can be chosen.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are many techniques and algorithms available for model tunings like hyperparameter
    optimization or model selection, hyperparameter tuning, grid search parameter
    tuning, random search parameter tuning and **Cross Validation** (**CV**). Unfortunately,
    the current implementation of Spark has developed only a few of them including
    Cross Validator and TrainValidationSplit.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, we will try to use these two hyperparameters to tune different models
    including Random Forest, Liner Regression, and Logistic Regression. Some applications
    from [Chapter 5](part0043_split_000.html#190862-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 5.  Supervised and Unsupervised Learning by Examples"), *Supervised and
    Unsupervised Learning by Examples* and [Chapter 6](part0049_split_000.html#1ENBI2-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 6.  Building Scalable Machine Learning Pipelines"), *Building Scalable
    Machine Learning Pipelines* will be re-used without providing many details again
    to make the model tuning easier.
  prefs: []
  type: TYPE_NORMAL
- en: Typical challenges in model tuning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After the following discussion, you might be thinking that this process is difficult,
    and you'd be right. In fact, because of the difficulty in determining what optimal
    model parameters are, often some more complex learning algorithms are used before
    experimenting effectively with simpler options with better-tuned parameters. As
    we've already discussed, machine learning involves a lot of experimentation. And
    the tuning of the internal knobs of a learning algorithm, commonly referred to
    as hyperparameters, are equally important from model building to prediction and
    before deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Technically, running a learning algorithm over a training dataset with different
    hyperparameter settings will result in different models, and of course different
    performance parameters. According to Oracle developers, it is not recommended
    to begin tuning without having first established clear objectives, since you cannot
    succeed if there is any definition of success.
  prefs: []
  type: TYPE_NORMAL
- en: Subsequently, we are typically interested in selecting the best-performing model
    from the training data set; we need to find a way to estimate their respective
    performances in order to rank them against each other. Going one step beyond mere
    algorithm fine-tuning, we are usually not only experimenting with the one single
    algorithm that we think would be the best solution under the given circumstances.
    More often than not, we want to compare different algorithms to each other, often
    in terms of predictive and computational performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Often there is a very basic question regarding parameter tuning using grid
    search and random search. Typically, some machine learning methods have parameters
    that need to be tuned using either of them. For example, according to Wei Ch.
    et al., (*A General Formulation for Support Vector Machine, proceedings of the
    9th International Conference on Neural Information Processing (ICONIP, 02), V-5,
    18-22 Nov. 2002*) the standard formulation of SVMs is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Typical challenges in model tuning](img/00086.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: The standard formula for SVM'
  prefs: []
  type: TYPE_NORMAL
- en: Now, suppose we need to tune model parameter C, and we need to do it with ease.
    It's clearly seen from the equation that, tuning C also involves other parameters
    like *xi*, *i* and *w*; where, the regularization parameter C > 0, is the norm
    of w. In the RHS it is the stabilizer and ![Typical challenges in model tuning](img/00100.jpeg)
    is the empirical loss term depending upon the target function f(xi). In the standard
    SVMs (either linear SVM or other variants), the regularized functionalities can
    be minimized further by solving the convex quadratic optimization problem. Once
    the problem is solved, it guarantees the unique global minimum solution to gain
    the best predictive performance. Therefore, the whole process is more or less
    an optimization problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, *Figure 2, The model tuning process, consideration, and workflow*
    shows the tuning process and its consideration as a workflow:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Typical challenges in model tuning](img/00061.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: The model tuning process, consideration, and workflow'
  prefs: []
  type: TYPE_NORMAL
- en: Now, if you are given the raw dataset, you will most probably do the pre-processing
    and split the dataset into training and test sets. Therefore, to tune hyperparameter
    C, you need to first split the training set into a validation training set and
    a validation test set.
  prefs: []
  type: TYPE_NORMAL
- en: After that, you might try tuning the parameters using the validation training
    set and validation test set. Then use the best parameters you've got and retrain
    the model on the complete training set. Now you can perform the testing on the
    test set as the final step.
  prefs: []
  type: TYPE_NORMAL
- en: Up to this point your approach seems to be okay, but which of the following
    two options do you think is better, on average?
  prefs: []
  type: TYPE_NORMAL
- en: Would it be better to use the final model from validation, which was trained
    on a validation training set for the final testing?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Or would it better to use the entire training set and retrain the model with
    the best parameters from the grid or random search? Although the parameters were
    not optimised for this set, we have final training data in this case.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Are you intending to go for option 1, because the parameters were already optimized
    on this training (that is, the validation training set) set? Or are you intending
    to go for option 2 because, although the parameters were not optimized for the
    training set, you have the final training data in this case? We suggest you go
    for option 2, but only if you trust your validation setup in option 2\. The reason
    is that you have performed the **Cross Validation** (**CV**) to identify the most
    general parameters setup or else model selection or whatever you're trying to
    optimize. These findings should be applied to the entire training set and tested
    once on the test set.
  prefs: []
  type: TYPE_NORMAL
- en: Well, suppose you went for option 2; now the second challenge is evolving. How
    do we estimate the performance of a machine learning model? Technically, you might
    argue that we should nourish the training data to our learning algorithm to learn
    the optimal model. Then we could predict the labels based on the test labels.
    Thirdly, we count the number of wrong predictions on the test dataset to compute
    the model's error rate.
  prefs: []
  type: TYPE_NORMAL
- en: 'That''s it? Not so fast my friend! Be contingent on our goal, unfortunately
    guesstimating the performance of that model is not that insignificant. Maybe we
    should address the previous question from another angle: why do we care about
    performance estimation at all? Well, ideally, the estimated performance of a model
    tells how well it performs on unobserved data - making predictions on future data
    is often the main problem we want to solve in applications of machine learning
    or the development of novel algorithms.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, there are several other challenges depending upon the data structure,
    problem type, problem domain and appropriate use cases that need to be addressed
    that you will come across when you start practicing a lot.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating machine learning models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will discuss how to evaluate a machine learning model because
    you should always evaluate a model to determine if it is ready to perform well
    consistently, predicting the target for new and future data. Obviously future
    data might have many unknown target values. Therefore, you need to check performance-related
    metrics such as the accuracy metric of the ML model on the data. In this regard,
    you need to provide a dataset containing scores generated from a trained model
    and then evaluate the model to compute a set of industry-standard evaluation metrics.
  prefs: []
  type: TYPE_NORMAL
- en: To evaluate a model appropriately, you need to present a sample of data that
    has been labeled with the target and this data will be used as the ground truth
    or facts dataset from the training data source. As we have already discussed,
    evaluating the predictive accuracy of an ML model with the same training dataset
    might not be useful.
  prefs: []
  type: TYPE_NORMAL
- en: The reason is that the model itself can remember the training data based on
    the rewards it receives instead of generalizing from it. Thus, when the ML model
    training has finished, you can get to know the target values to be predicted from
    the presented observations in the model. After that you can compare the predicted
    values that are returned by the ML model you have trained against the known target
    value.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, you might be interested in computing the summary metric that shows
    the performance metrics to indicate how well the predicted and true values match
    accuracy parameters such as precision, recall, weighted true positive, weighted
    true negative, lift, and so on. In this section, however, we will particularly
    discuss how we can evaluate regression, classification (that is, binary classification,
    multiclass classification) and clustering model in the first place.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating a regression model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Assume you that you are an online currency trader and you work on Forex or Fortrade.
    Right now you have two currency pairs in mind to buy or sell, for example, GBP/USD
    and USD/JPY pairs. If you look at these two pairs carefully, you'll see that USD
    is common in both pairs. Now, if you observe the historical prices of USD, GBP
    or JPY you can predict the future outcome of whether you should open the trade
    in buy or sell.
  prefs: []
  type: TYPE_NORMAL
- en: This kind of problem can be treated as the typical regression problem. Here,
    the target variable (price in this case) is a continuous numeric value changing
    over time, based on the market opening time. Therefore, for making predictions
    on the prices, based on the given feature values of a certain currency (that is,
    USD, GBP or JPY in this example), we can fit a simple linear regression or logistic
    regression model.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, the feature values could be the historical prices and some external
    factors that drift the value of a certain currency or currency pair. In this way,
    the trained build model can predict the price of a certain currency.
  prefs: []
  type: TYPE_NORMAL
- en: The regression models (that is, linear, logistic or generalized liner regression
    models) can be used for finding or calculating the score of the same dataset we
    trained, now that the predicted prices of all of the currencies or the historical
    prices of these three currencies are available. We can further evaluate the performance
    of the model by analyzing, on average, how much the predicted prices deviate compared
    to the actual prices. In this way, people can guess whether the predicted price
    will go up or down and can earn money from online currency websites such as Forex
    or Fortrade and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating a binary classification model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we have already discussed in the binary classification scenario, there are
    only two possible outcomes for the target variable. For example: {0, 1}, {spam,
    hap}, {B, N}, {false, true} and {negative, positive} and so on. Now assume that
    you are given a dataset comprising researchers around the world with demographic,
    socio-economic and employment variables and you would like to predict the Ph.D.
    scholarship amount (that is, salary level) as a binary variable with the values
    of, say, {<=1.5K$, >=4.5K$}.'
  prefs: []
  type: TYPE_NORMAL
- en: In this particular example, the negative class would represent the researcher
    whose salary or scholarship is less than or equal to $1,500 monthly. Consequently,
    the positive class, on the other hand, represents all other researchers whose
    salary is more than or equal to $4,500.
  prefs: []
  type: TYPE_NORMAL
- en: Now, from the problem scenario, it is clear that it is also a regression problem.
    As a result, you would train a model, score the data, and evaluate the results
    and see how much it deviates with actual labels. Therefore, in this type of problem,
    you would perform an experiment to evaluate the performance of a two-class (that
    is, binary class) logistic regression model, which is one of the most commonly
    used binary classifier in the area of ML.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating a multiclass classification model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In [Chapter 6](part0049_split_000.html#1ENBI2-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 6.  Building Scalable Machine Learning Pipelines"), *Building Scalable
    Machine Learning Pipelines*, we developed several applications and pipelines,
    and you might remember that we also developed a multiclass classification problem
    for the OCR dataset using logistic regression and showed the result using Multiclass
    Metrics. In that example, there were 26 classes for 26 characters (that is, from
    A to Z). We had to predict the class or label of a certain character and whether
    it really fell under the correct class or labels. This kind of regression problem
    can be resolved using the multiclass classification method.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, in this type of problem, you would perform an experiment to evaluate
    the performance of a multiclass (that is, more than two class) logistic regression
    model too.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating a clustering model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since the clustering models differ expressively from classification and regression
    models in many different aspects, if you evaluate a clustering model you will
    find a different set of statistics and performance related metrics for clustering
    models. The performance metrics that were returned in the clustering model evaluation
    technique, describe how many data points were assigned to each cluster, the amount
    of separation between clusters, and how tightly the data points are bunched within
    each cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, if you recall, in [Chapter 5](part0043_split_000.html#190862-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 5.  Supervised and Unsupervised Learning by Examples"), *Supervised and
    Unsupervised Learning by Examples*, you found a clustering problem that we discussed
    and resolved using Spark ML and MLlib in the *Unsupervised Learning with Spark:
    An example section*. In that particular example, we showed K-Means clustering
    of the neighborhood using the Saratoga NY Homes dataset, and showed an exploratory
    analysis based on price and lot size features for possible neighborhoods of houses
    located in the same area. This kind of problem can be resolved and evaluated using
    a clustering model. However, the current implementation of Spark does not yet
    provide any developed algorithm for model evaluation.'
  prefs: []
  type: TYPE_NORMAL
- en: Validation and evaluation techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are several widely used terms in machine learning application development
    that can be a bit tricky and confusing, so let''s talk through them and sort them
    out. These terms include model, target function, hypothesis, confusion matrix,
    model deployment, induction algorithm, classifier, learning algorithms, cross-validation,
    and parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Target function**: In reinforcement learning or predictive modeling, let''s
    say we focus on modeling an object. The ultimate target is to learn or approximate
    a specific and unknown but targeted function. The target function is denoted as
    *f(x) = y*; where *x* and *y* both are variable and *f(x)* is the true function
    that we want to model and it also signifies that we are trying to maximize or
    achieve the target value *y*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hypothesis**: A statistical hypothesis (don''t confuse this with the research
    hypothesis proposed by the researcher) is a generalized function that is testable
    on the basis of observing a process. The process is similar to the true function
    that is modeled through a set of random variables from the training dataset. The
    objective behind the hypothesis testing is that a hypothesis is proposed for measuring
    the statistical relationship between the two data sets for the example training
    set and test set, where, both the datasets have to be statistically significant
    from an idealized (remember not a randomized or normalized model) model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Learning algorithm**: As already stated, the ultimate goal of a machine learning
    application is to find or approximate the target function. In this continuous
    process, the learning algorithm is a set of instructions that models the target
    function using the training dataset. Technically, a learning algorithm often comes
    with a hypothesis space and formulates the final hypothesis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model**: A statistical model is a mathematical model that exemplifies a set
    of assumptions while generating sample and similar data from a larger population.
    Finally, the model often represents a significantly idealized form for the data-generating
    process. Also, in the machine learning area, the terms hypothesis and model are
    often used interchangeably.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Induction algorithm**: An induction algorithm takes input specific instances
    to produce a generalized model beyond these input instances.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model deployment**: Model deployment usually denotes applying an already
    built and developed model to the real data in order to make a prediction for an
    example.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cross-validation**: this is a method for estimating accuracy in terms of
    an error out of a machine learning model by dividing the data into K mutually
    exclusive subsets or folds of approximately equal size. The model then is trained
    and tested K times in iteration, each time on the available dataset excluding
    a fold and then tested on that fold.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Classifier**: A classifier is a special case of hypothesis or discrete-valued
    function that is used to assign the most categorical class labels to particular
    data points such as the label point.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Regressor**: A regressor is also a special case of hypothesis that does the
    mapping from unlabeled features to a value within a predefined metric space.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hyperparameters**: Hyperparameters are the tuning parameters of a machine
    learning algorithm to which a learning algorithm fits the training data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'According to Pedro D. et al., *A Few Useful Things to Know about Machine Learning*
    at [http://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf](http://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf),
    we also need a few more things outlined, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Representation**: a classifier or regressor must be represented in some formal
    language that a computer can process by creating proper hypothesis spaces.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Evaluation**: An evaluation function (that is, objective function or scoring
    function) is needed to distinguish a good vs bad classifier r regressor, which
    is used internally by the algorithm by which the model has been build or trained.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Optimization**: We also need to have a method for searching among the classifiers
    or regressor, aiming for the highest scoring one. Thus the optimization is the
    key to the efficiency of the learner that helps to determine the optimal parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In a nutshell, the key formula in learning in an ML algorithm is:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Learning = Representation + Evaluation + Optimization*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Consequently, to validate and evaluate the trained model you need to understand
    the above terms very clearly so that you can conceptualize the ML problem and
    the proper uses of the Spark ML and MLlib APIs.
  prefs: []
  type: TYPE_NORMAL
- en: Parameter tuning for machine learning models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will discuss tuning parameters and technique for machine
    learning models such as hyperparameter tuning, random search parameter tuning,
    and grid search parameter tuning and cross-validation.
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameter tuning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Hyperparameter tuning is a technique for choosing the right combination of parameters
    based on the performance of presented data. It is one of the fundamental requirements
    to obtain meaningful and accurate results from machine learning algorithms in
    practice. For example, suppose we have two hyperparameters to tune for a pipeline
    presented in *Figure 3*, a Spark ML pipeline model using a logistic regression
    estimator (dash lines only happen during pipeline fitting).
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see that we have put three candidate values for each. Therefore, there
    would be nine combinations in total. However, only four are shown in the diagram,
    namely **Tokenizer**, **HashingTF**, **Transformer** and **Logistic Regression**
    (**LR**). Now we want to find the one that will lead to the model with the best
    evaluation result eventually. As we have already discussed in [Chapter 6](part0049_split_000.html#1ENBI2-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 6.  Building Scalable Machine Learning Pipelines"), *Building Scalable
    Machine Learning Pipelines,* the fitted model consists of the tokenizer, the hashing
    TF feature extractor, and the fitted logistic regression model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Hyperparameter tuning](img/00148.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Spark ML pipeline model using logistic regression estimator (dash
    lines only happen during pipeline fitting)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 3* shows the typical workflow of the previously mentioned pipeline.
    The dash line however happens only during the pipeline fitting.'
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned earlier, the fitted pipeline model is a transformer. The transformer
    can be used for prediction, model validation, and model inspection. In addition,
    we also argued that one ill-fated distinguishing characteristic of the ML algorithms
    is that typically they have many hyperparameters that need to be tuned for better
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: For example, the degree of regularizations in these hyperparameters is distinctive
    from the model parameters optimized by the Spark MLlib. As a consequence, it is
    really hard to guess or measure the best combination of hyperparameters without
    expert knowledge of the data and the algorithm to use. Since the complex dataset
    is based on the ML problem type, the size of the pipeline and the number of hyperparameters
    may grow exponentially (or linearly), the hyperparameter tuning becomes cumbersome
    even for an ML expert, not to mention that the result of the tuning parameters
    may become unreliable
  prefs: []
  type: TYPE_NORMAL
- en: 'According to Spark API documentation provided at [http://spark.apache.org/docs/latest/ml-guide.html](http://spark.apache.org/docs/latest/ml-guide.html),
    a unique and uniform API is used for specifying Spark ML Estimators and Transformers.
    A `ParamMap` is a set of (parameter, value) pairs with a `Param` as a named parameter
    with self-contained documentation provided by Spark. Technically, there are two
    ways for passing the parameters to an algorithm as specified in the following
    options:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Setting parameters. For example, if an LR is an instance of `LogisticRegression`
    (that is, Estimator), you can call the `setMaxIter()` method as follows: `LR.setMaxIter(5)`.
    It essentially fits the model pointing the regression instance as follows: `LR.fit()`.
    In this particular example, there would be at most five iterations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second option involves passing a `ParamMaps` to `fit()` or `transform()`
    (refer *Figure 1* for details). In this circumstance, any parameters will be overridden
    by the `ParamMaps` previously specified via setter methods in the ML application-specific
    codes or algorithms.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'An excellent way to create a shortlist of well-performing algorithms for your
    dataset is to use the Caret package in R since tuning in Spark is not that robust.
    Caret is a package in R created and maintained by Max Kuhn from Pfizer. Development
    started in 2005 and was later made open source and uploaded to CRAN which is actually
    an acronym which stands for **Classification And Regression Training** (**CARET**).
    It was initially developed out of the need to run multiple different algorithms
    for a given problem. Interested readers can have a look at that package for theoretical
    and practical consideration by visiting: [http://topepo.github.io/caret/index.html](http://topepo.github.io/caret/index.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Grid search parameter tuning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Suppose you have selected your hyperparameters, and by applying tuning, you
    now also need to find the features. In this regard, a full grid search of the
    space of hyperparameters and features is computationally too intensive. Therefore,
    you need to perform a fold of the K-fold cross-validation instead of a full grid
    search:'
  prefs: []
  type: TYPE_NORMAL
- en: Tune the required hyperparameters using cross-validation on the training set
    of the fold, using all the available features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Select the required features using those hyperparameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Repeat the computation for each fold in K
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The final model is constructed on all the data using the N most prevalent features
    that were selected from each fold of CV
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The interesting thing is that the hyperparameters would also be tuned again
    using all the data in a cross-validation loop. Would there be a large downside
    from this method as compared to a full grid search? In essence, I am doing a line
    search in each dimension of free parameters (finding the best value in one dimension,
    holding that constant then finding the best in the next dimension), rather than
    every single combination of parameter settings.
  prefs: []
  type: TYPE_NORMAL
- en: The most important downside for searching along single parameters instead of
    optimizing them altogether is that you ignore interactions. It is quite common
    that, for instance, more than one parameter influences model complexity. In that
    case, you need to look at their interaction in order to successfully optimize
    the hyperparameters. Depending on how large your data set is and how many models
    you compare, optimization strategies that return the maximum observed performance
    may run into trouble (this is true for both grid search and your strategy).
  prefs: []
  type: TYPE_NORMAL
- en: 'The reason is that searching through a large number of performance estimates
    for the maximum skims the variance of the performance estimate: you may just end
    up with a model and training/test split combination that accidentally happens
    to look good. Even worse, you may get several perfect-looking combinations, and
    the optimization then cannot know which model to choose and thus becomes unstable.'
  prefs: []
  type: TYPE_NORMAL
- en: Random search parameter tuning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The default method for optimizing tuning parameters in the train is to use a
    grid search. This approach is usually effective but, in cases when there are many
    tuning parameters, it can be inefficient. There are a number of models where this
    can be beneficial in finding reasonable values of the tuning parameters in a relatively
    short time. However, there are some models where the efficiency in a small search
    field can cancel out other optimizations. Unfortunately, the current implementation
    in Spark for hyperparameter tuning does not provide any technique for random search
    tuning.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In contrast, for example, a number of models in CARET utilize the *sub-model
    trick* where M tuning parameter combinations are evaluated; potentially far fewer
    than M model fits are required. This approach is best leveraged when a simple
    grid search is used. For this reason, it may be inefficient to use a random search.
    Finally, many of the models wrapped by train have a small number of parameters.
    The average number of parameters is 1.7\. To use a random search, another option
    is available in **trainControl** called search. Possible values of this argument
    are `grid` and `random`. The built-in models contained in CARET contain code to
    generate random tuning parameter combinations. The total number of unique combinations
    is specified by the **tuneLength** option to train.
  prefs: []
  type: TYPE_NORMAL
- en: Cross-validation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Cross-validation (also called the **Rotation Estimation** (**RE**)) is a model
    validation technique for assessing the quality of the statistical analysis and
    results. The target is to make the model generalize towards an independent test
    set.
  prefs: []
  type: TYPE_NORMAL
- en: One of the perfect uses of the cross-validation technique is making a prediction
    from a machine learning model. Technically, it will help if you want to estimate
    how a predictive model will perform accurately in practice when you deploy it
    as an ML application.
  prefs: []
  type: TYPE_NORMAL
- en: During the cross-validation process, a model is usually trained with a dataset
    of a known type. Conversely, it is tested using a dataset of unknown type. In
    this regard, cross-validation help describes a dataset to test the model in the
    training phase using the validation set.
  prefs: []
  type: TYPE_NORMAL
- en: However, in order to minimize the flaws in the machine learning model such as
    overfitting and underfitting, the cross-validation technique provides insights
    into how the model will generalize to an independent set.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two types of cross-validation that can be typed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Exhaustive cross-validation**: This includes leave-p-out cross-validation
    and leave-one-out cross-validation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Non-exhaustive cross-validation**: This includes the K-fold cross-validation
    and repeated random sub-sampling validation cross-validation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detailed discussion of these will not be conducted in this book due to page
    limitation. Moreover, using Spark ML and Spark MLlib, readers will be able to
    perform the cross-validation following our examples in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Except for the time series data, in most of the cases, the researcher/data scientist/data
    engineer uses 10-fold cross-validation instead of testing on a validation set
    (where K = 10). This is the most widely used cross-validation technique across
    the use cases and problem type. Moreover, to reduce the variability, multiple
    iterations of cross-validation are performed using different partitions; finally,
    the validation results are averaged over the rounds across.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using cross-validation instead of conventional validation has two main advantages
    outlined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Firstly, if there is not enough data available to partition across the separate
    training and test sets, there's the chance of losing significant modelling or
    testing capability.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Secondly, the K-fold cross-validation Estimator has a lower variance than a
    single hold-out set Estimator. This low variance limits the variability and is
    again very important if the amount of available data is limited.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In these circumstances, a fair way to properly estimate the model prediction
    and related performance is to use cross-validation as a powerful general technique
    for model selection and validation.
  prefs: []
  type: TYPE_NORMAL
- en: 'A more technical example will be shown in the *Machine learning model selection*
    section. Let''s draw a concrete example to illustrate this. Suppose, we need to
    perform manual features and a parameter selection for the model tuning and, after
    that, perform a model evaluation with a 10-fold cross-validation on the entire
    dataset. What would be the best strategy? We would suggest you go for the strategy
    that provides an optimistic score as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Divide the dataset into training, say 80%, and testing 20% or whatever you chose
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the K-fold cross-validation on the training set to tune your model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Repeat the CV until you find your model optimized and therefore tuned
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now use your model to predict on the testing set to get an estimate of out-of-model
    errors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hypothesis testing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hypothesis testing is a statistical tool used to determine whether a result
    is statistically significant. Additionally, it can also be used to justify whether
    the result you received occurred by chance or whether it is an actual result.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this regard, moreover, according to Oracle developers at [https://docs.oracle.com/cd/A57673_01/DOC/server/doc/A48506/method.htm](https://docs.oracle.com/cd/A57673_01/DOC/server/doc/A48506/method.htm),
    a certain workflow would provide better performance tuning. The typical steps
    they have suggested are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Set clear goals for tuning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create minimum repeatable tests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Test the hypothesis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keep all records
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Avoid common errors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stop tuning when the objectives are achieved
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Usually, the observed value tobs of the test statistic T is first computed.
    After that, the probability, also called p-value, is calculated under the null
    hypothesis. Finally, if and only if the p-value is less than the significance
    level (the selected probability) threshold, the null hypothesis is rejected in
    favor of the alternative hypothesis.
  prefs: []
  type: TYPE_NORMAL
- en: 'To find out more, refer to the following publication: *R.A. Fisher et al.,
    Statistical Tables for Biological Agricultural and Medical Research, 6th ed.,
    Table IV, Oliver & Boyd, Ltd., Edinburgh*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are two rules of thumb (although these may vary for your case depending
    on data quality and types):'
  prefs: []
  type: TYPE_NORMAL
- en: If the p-value is p > 0.05, accept your hypothesis. Note that if a deviation
    is small enough that chance could be the acceptance level. A p-value of 0.6, for
    example, means that there is a 60% probability of any deviation from the expected
    result. However, this is within the range of an acceptable deviation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the p-value is p < 0.05, reject your hypothesis by concluding that some factors
    other than chance are operating for the deviation to be perfect. Similarly, a
    p-value of 0.01 means that there is only a 1% chance that this deviation is due
    to chance alone, which means that other factors must be involved, and these need
    to be addressed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, these two rules might not be applicable in every hypothesis testing.
    In the next subsection, we will show an example of hypothesis testing using Spark
    MLlib.
  prefs: []
  type: TYPE_NORMAL
- en: Hypothesis testing using ChiSqTestResult of Spark MLlib
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'According to the API documentation provided by Apache at [http://spark.apache.org/docs/latest/mllib-statistics.html#hypothesis-testing](http://spark.apache.org/docs/latest/mllib-statistics.html#hypothesis-testing),
    the current implementation of the Spark MLlib supports Pearson''s chi-squared
    (*χ2*) tests for goodness of fit and independence:'
  prefs: []
  type: TYPE_NORMAL
- en: The goodness of the fit, or if the independence test is being conducted, is
    determined by the input data types
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The goodness of the fit test requires an input of type vector (mostly dense
    vectors although it works for sparse vectors). On the other hand, the independence
    test requires a Matrix as an input format
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition to these, Spark MLlib also supports input type RDD [LabeledPoint]
    to enable feature selection via chi-square independence tests, especially for
    the SVM or regression based test where the statistical class provides the necessary
    methods to run Pearson's chi-squared tests.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, Spark MLlib provides a 1-sample, 2-sided implementation of the
    **Kolmogorov-Smirnov** (**KS**) test for equality of probability distributions.
    Spark MLlib provides online implementations of some tests to support use cases
    like A/B testing. These tests may be performed on a Spark Streaming DStream [(Boolean,
    Double)] where the first element of each tuple indicates the control group (false)
    or treatment group (true) and the second element is the value of an observation.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, due to brevity and page limitation, this two testing technique will
    not be discussed. The following example demonstrates how to run and interpret
    hypothesis tests through the `ChiSqTestResult`. In the example, we will show three
    tests: goodness of fit of the result on the dense vector created from the breast
    cancer diagnosis dataset, an independence test for a randomly created matrix and
    finally, an independence test on a contingency table from the cancer dataset itself.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1: Load required packages**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the code to load the required packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 2: Create a Spark session**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code helps us to create a Spark Session:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The implementation of the `UtilityForSparkSession` class is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 3: Perform the goodness of fit test**'
  prefs: []
  type: TYPE_NORMAL
- en: 'First we need to prepare a dense vector from a categorical dataset such as
    the Wisconsin Breast Cancer Diagnosis dataset. As we have already provided many
    examples on this dataset, we will not discuss the data exploration anymore in
    this section. The following line of code collects the vectors that we created
    using the `myVector()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Where, the implementation of the `myVector()` method goes as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s compute the goodness of the fit. Note, if a second vector to test
    is not supplied as a parameter, the test runs occur against a uniform distribution
    automatically:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s print the result of the goodness using the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Note the summary of the test includes the p-value, degrees of freedom, test
    statistic, the method used, and the null hypothesis. We got the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'There is a very strong presumption against a null hypothesis: the observed
    follows the same distribution as expected.'
  prefs: []
  type: TYPE_NORMAL
- en: Since the p-value is low enough to be insignificant, consequently we cannot
    accept the hypothesis based on the data.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 4: An independence test on the contingency matrix**'
  prefs: []
  type: TYPE_NORMAL
- en: 'First let''s create a contingency 4x3 matrix randomly. Here, the matrix appears
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s conduct Pearson''s independence test on the input contingency matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s evaluate the test result and give the summary of the test including
    the p-value and the degrees of freedom:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We got the following statistic summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 5: An independence test on the contingency table**'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s create a contingency table by means of RDDs from the cancer dataset
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We have constructed a contingency table from the raw (feature, label) pairs
    and used it to conduct the independence test. Now let''s conduct the test as `ChiSquaredTestResult`
    for every feature against the label:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s observe the test result against each column (that is, for each 30
    feature point) using the following code segment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: From this result, we can see that for some feature points (that is, column)
    we have a large p-value compared to the others. Readers are, therefore, advised
    to select the proper dataset and do the hypothesis test prior to applying the
    hyperparameter tuning. There is no concrete example in this regard since the result
    may vary against the datasets you have.
  prefs: []
  type: TYPE_NORMAL
- en: Hypothesis testing using the Kolmogorov–Smirnov test from Spark MLlib
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since Spark release 1.1.0, Spark also provides the facility of doing the hypothesis
    testing using for the real-time streaming data through the Kolmogorov-Smirnov
    test. Where, the probability of obtaining a test statistic result (at least as
    extreme as the one) that was actually observed. It actually assumes that the null
    hypothesis is always true.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For more details, interested readers should refer to the Java class (`JavaHypothesisTestingKolmogorovSmirnovTestExample.java`)
    in the Spark distribution under the following directory: `spark-2.0.0-bin-hadoop2.7\examples\src\main\java\org\apache\spark\examples\mllib`.'
  prefs: []
  type: TYPE_NORMAL
- en: Streaming significance testing of Spark MLlib
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Other than the Kolmogorov-Smirnov test, Spark also supports Streaming significance
    testing, which is an online implementation of the hypothesis testing like the
    A/B testing? These tests can be performed on a Spark streaming using DStream (more
    technical discussion on this topic will be carried out in [Chapter 9](part0073_split_000.html#25JP22-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 9.  Advanced Machine Learning with Streaming and Graph Data"), *Advanced
    Machine Learning with Streaming and Graph Data*).
  prefs: []
  type: TYPE_NORMAL
- en: 'The MLlib based streaming significance testing supports the following two parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**peacePeriod**: This is the number of initial data points from the stream
    to ignore. This is actually used to mitigate novelty effects and the quality of
    the streaming you will be receiving.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**windowSize**: This is the number of past batches over which to perform hypothesis
    testing. If you set its value to 0, it will perform cumulative processing using
    all the prior batches received and processed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interested readers should refer to the Spark API documentation at [http://spark.apache.org/docs/latest/mllib-statistics.html#hypothesis-testing](http://spark.apache.org/docs/latest/mllib-statistics.html#hypothesis-testing).
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning model selection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most machine learning algorithms are dependent on various parameters. When we
    train a model, we need to provide values for those parameters. The efficacy of
    the trained model is dependent on the model parameters that we choose. The process
    of finding out the optimal set of parameters is known as model selection.
  prefs: []
  type: TYPE_NORMAL
- en: Model selection via the cross-validation technique
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When performing machine learning using Python's scikit-learn library or R, you
    can often get a reasonably predictive performance by using out-of-the-box settings
    for your models. However, the payoff can be huge if you invest some time in tuning
    models to your specific problem and data set.
  prefs: []
  type: TYPE_NORMAL
- en: However, we also need to consider other issues like overfitting, cross-validation,
    and bias-variance trade-off. These ideas are central to doing a good job at optimizing
    the hyperparameters of algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will explore the concepts behind Hyperparameter optimization
    and demonstrate the process of tuning and training a logistic regression classifier
    for the famous Spam Filtering dataset. The goal is to tune and apply a logistic
    regression to these features in order to predict whether a given email/SMS is
    spam or not-spam:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Model selection via the cross-validation technique](img/00057.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Model selection via cross-validation'
  prefs: []
  type: TYPE_NORMAL
- en: Cross-validation and Spark
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Pipelines enable model selection by tuning an entire Pipeline at once, rather
    than tuning each element in the Pipeline unconnectedly. See the API documentation
    at [http://spark.apache.org/docs/latest/ml-guide.html](http://spark.apache.org/docs/latest/ml-guide.html).
  prefs: []
  type: TYPE_NORMAL
- en: The current implementation of Spark ML supports model selection using the `CrossValidator`
    class. It takes an Estimator, a set of `ParamMaps`, and an Evaluator. The model
    selection task begins with splitting the dataset (that is, splitting it into a
    set of folds) the folds of which are then used as separate training and test datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, with K=10 folds, the `CrossValidator` will generate 10 (training,
    test) dataset pairs. Each of these uses two thirds (2/3) of the data for the training,
    and the other third (1/3) for the testing. After that, the `CrossValidator` iterates
    through the set of `ParamMaps`. For each `ParamMap`, it trains the given Estimator
    and evaluates it using the available Evaluator. The Evaluator can be a related
    ML task for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '`RegressionEvaluator` for regression related problems'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`BinaryClassificationEvaluator` for binary data and its related problems'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MultiClassClassificationEvaluator` for a multiclass problem'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'When it comes to the best `ParamMap` selection, a default metric is used. Note
    that the `ParamMap` can be also be overridden by the `setMetric()` method in each
    of these evaluators. In contrast, when it comes to the best model selection:'
  prefs: []
  type: TYPE_NORMAL
- en: The `ParamMap` produces the best evaluation metric
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The evaluations metrics are then averaged over the K folds
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, the best model is selected
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once the best `ParamMap` and model are selected, the `CrossValidator` fits the
    Estimator using them for the entire dataset.
  prefs: []
  type: TYPE_NORMAL
- en: To get a clearer insight into the `CrossValidator` and to select from a grid
    of parameters, Spark uses the `ParamGridBuilder` utility to construct the parameter
    grid. For example, suppose the parameter grid has a value of 4 as the `hashingTF.numFeatures`
    and a value of 3 for the LR.`regParam`. Also, lets say that the `CrossValidator`
    uses 10 folds.
  prefs: []
  type: TYPE_NORMAL
- en: Once, these values are multiplying results to 120 (that is, 4*3 *10 = 120),
    this signifies that a significant amount of different models (that is, 120) are
    being trained. Therefore, using the `CrossValidator` sometimes can be very expensive.
    Nevertheless, it is also a well-established method for choosing associated performance
    and hyperparameters that are sounder statistically compared to the heuristic-based
    hand tuning.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Interested readers may refer to the following three books for more insight:'
  prefs: []
  type: TYPE_NORMAL
- en: Evan R. Sparks et al., *Automating Model Search for Large-Scale Machine Learning*,
    ACM, 978-1-4503-3651-2/15/08, [http://dx.doi.org/10.1145/2806777.2806945](http://dx.doi.org/10.1145/2806777.2806945).
  prefs: []
  type: TYPE_NORMAL
- en: Cawley, G. C. & Talbot, N. L on over-fitting in model selection and subsequent
    selection bias in performance evaluation, *The Journal of Machine Learning Research*,
    JMLR. org, 2010, 11, 2079-2107.
  prefs: []
  type: TYPE_NORMAL
- en: 'N. Japkowicz and M. Shah, *Evaluating learning algorithms: a classification
    perspective*, Cambridge University Press, 2011.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next sub-section, we will show how to perform cross-validation on a dataset
    for model selection using Spark ML API.
  prefs: []
  type: TYPE_NORMAL
- en: Cross-validation using Spark ML for SPAM filtering a dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this sub-section, we will show you how to perform cross-validation on the
    e-mail spam dataset for model selection. We will use logistic regression in the
    first place then we will move forward for other models. Finally, we will recommend
    the most suitable model for e-mail spam classification.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1: Import necessary packages/libraries/APIs**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the code to import necessary packages/libraries/APIs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 2: Initialize the necessary Spark environment**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code helps us to initialize the necessary Spark environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Here we set the application name as cross-validation, the master URL as `local[*]`,
    and the Spark session as the entry point of the program. Please set these parameters
    accordingly. Most importantly, set the warehouse directory as `E:/Exp/` and replace
    it with the appropriate path.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 3: Prepare a Dataset from the SMS spam dataset**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Take the e-mail spam data as your input, prepare a dataset out of the data,
    using it as raw text, and check if the data was read properly by calling the `show()`
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '![Cross-validation using Spark ML for SPAM filtering a dataset](img/00046.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: The top 20 rows'
  prefs: []
  type: TYPE_NORMAL
- en: To learn more about the data, please refer to the section *Pipeline - an example
    with Spark ML*, in [Chapter 4](part0038_split_000.html#147LC2-5afe140a04e845e0842b44be7971e11a
    "Chapter 4. Extracting Knowledge through Feature Engineering"), *Extracting Knowledge
    through Feature Engineering*, and the description for the dataset exploration.
    As you can see in *Figure 5*, The top 20 rows, there are only two labels - spam
    or ham (that is, it is a binary classification problem) associated along with
    the text values (that is, each row).
  prefs: []
  type: TYPE_NORMAL
- en: However, there is no numeric label or ID. Therefore, we need to prepare the
    dataset (training set) such that the data frame also contains the ID, and labels
    along with text (that is, value) so that we can prepare a test set and predict
    the corresponding labels using any classification algorithm (that is, logistic
    regression) and can decide if our model selection is appropriate.
  prefs: []
  type: TYPE_NORMAL
- en: However, for doing so we need to prepare the training dataset first. As you
    can see, the data frame shown above has only one column and, as mentioned previously,
    we do need to have three columns. If we prepare an RDD from the previous Dataset
    (that is, `df`), it would be easier for us to make that transformation.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 4: Create Java pairs of RDDs to store the rows and indices**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a Java pair of RDDs by converting (transforming) the DataFrame (that
    is, `df`) to Java RDD and by zipping the index:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 5: Create LabeledDocument RDDs**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create `LabeledDocument` Java RDDs by splitting the dataset based on two labels
    and converting the text label to a numeric label (that is, `1.0` if ham otherwise
    0.0). Note that the `LabeledDocument` is a user-defined class that is discussed
    in *step 6*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 6: Prepare the training dataset**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Prepare the training dataset from the `LabeledDocument` RDDs using the `createDataFrame()`
    method and by specifying the class. Finally, see the data frame structure using
    the `show()` method as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '![Cross-validation using Spark ML for SPAM filtering a dataset](img/00147.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: The newly created label and ID from the dataset; where ID is the
    number of row.'
  prefs: []
  type: TYPE_NORMAL
- en: 'From *Figure 6*, *The newly created label and ID from the dataset; where ID
    is the number of rows*, we can see that the new training dataset has three columns:
    ID, test, and label. It can actually be done by adding a new ID against each row
    (that is, each line) of the original document. Let''s create a class, called `Document`
    for our purpose, that should set a unique ID against each line of text. The structure
    of the class can be something like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s look at the structure of the class constructor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The setter and getter for the ID could be something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly the setter and getter methods for the text could be something like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Therefore, if we summarize, the `Document` class could be something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The structure of the `LabeledDocument` class, on the other hand, can be as
    follows and that can be extended from the Document class (to be discussed later):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s look at the structure of the class constructor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'However, we''re not done yet since we will be extending the `Document` class
    we need to inherit the constructor from the `Document` class using the `super()`
    method as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Now the setter method can be something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'And of course the getter method for the label could be something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Therefore, in a nutshell, the `LabelDocument` class is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 7: Configure an ML pipeline**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Configure an ML pipeline, which consists of three stages: `tokenizer`, `hashingTF`,
    and `lr`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 8: Construct a grid of parameters to search over**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Currently, Spark uses a `ParamGridBuilder` to construct a grid of parameters
    to search over. In this regard, suppose we have three values for `hashingTF.numFeatures`
    and two values for `lr.regParam`, this grid will have 3 x 2 = 6 parameter settings
    for the `CrossValidator` to choose from:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: We now treat the pipeline as an estimator, wrapping it in a `CrossValidator`
    instance. This will allow us to jointly choose parameters for all Pipeline stages.
    A `CrossValidator` requires an Estimator, a set of Estimator `ParamMaps`, and
    an Evaluator. Note that the evaluator here is a `BinaryClassificationEvaluator`
    and its default metric is `areaUnderROC`.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 9: Create a CrossValidator instance**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the code to create a `CrossValidator` instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 10: Run cross-validation**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the cross-validation and choose the best set of parameters. Just use the
    following code segments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Now your `CrossValidator` model is ready to perform the prediction. However,
    before that, we need a test set or validation set. Now let''s prepare a sample
    test set. Just create a dataset using the following code segment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s see the structure of the test set by calling the `show()` method
    in *Figure 7*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Cross-validation using Spark ML for SPAM filtering a dataset](img/00072.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: The test set'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 11: Create a dataset to collect the prediction parameters**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code illustrates how to create a dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 12: Display the prediction parameters for each text in the test set**'
  prefs: []
  type: TYPE_NORMAL
- en: 'With the help of the following code, we can display the prediction parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '![Cross-validation using Spark ML for SPAM filtering a dataset](img/00118.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Prediction against each text and ID'
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, if you compare the results shown in *Figure 6*, *The newly created
    label and ID from the dataset; where ID is the number of rows,* you'll find that
    the prediction accuracy increases from more sophisticated methods for measuring
    predictive accuracy that can be used to identify places where the error rate can
    be optimized depending on the costs of each type of error.
  prefs: []
  type: TYPE_NORMAL
- en: Model selection via training validation split
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'According to the API documentation provided by Spark at [http://spark.apache.org/docs/latest/ml-guide.html](http://spark.apache.org/docs/latest/ml-guide.html),
    Spark also offers a `TrainValidationSplit` for hyperparameter tuning along with
    the `CrossValidator`. The idea of the `TrainValidationSplit` is it only evaluates
    each combination of the parameters compared to cross-validation that iterates
    to k times. It is, therefore, computationally less expensive and produces the
    result more quickly. The results, however, will not be as reliable as the `CrossValidator`.
    There is an exception: if the training dataset is sufficiently large then it can
    also produce reliable results.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The theory behind the `TrainValidationSplit` is that it takes the following
    three as inputs:'
  prefs: []
  type: TYPE_NORMAL
- en: An Estimator
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A set of `ParamMap``s` provided in the `estimatorParamMaps` parameter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An Evaluator
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consequently, it begins the model selection by splitting the dataset into two
    parts using the `trainRatio` parameter. The `trainRatio` parameter, on the other
    hand, is used for separate training and test datasets.
  prefs: []
  type: TYPE_NORMAL
- en: For example, with *trainRatio = 0.75* (the default value is also 0.75), the
    `TrainValidationSplit` algorithm generates a training and testing pair. In that
    case, 75% of the total data is used for training the model. Consequently, the
    rest of the 25% is used as the validation set.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to the `CrossValidator`, the `TrainValidationSplit` also iterates through
    a set of ParamMaps as mentioned earlier. For each combination of the parameters,
    it trains the given Estimator in each iteration.
  prefs: []
  type: TYPE_NORMAL
- en: Consequently, the model is evaluated using the given Evaluator. After that,
    the best model is selected as the best option, since the `ParamMap` produces the
    best evaluation metric and thereby eases the model selection. The `TrainValidationSplit`
    finally fits the Estimator using the best available `ParamMap` and for the entire
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression–based model selection for an OCR dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this sub-section, we will show how to perform train validation split tuning
    for OCR data. The logistic regression will be used in the first place; then we
    will move forward for other models. Finally, we will recommend the most suitable
    parameters for the OCR data classification.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1: Import necessary packages/libraries/APIs:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 2: Initialize necessary Spark environment**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Here we set the application name as `TrainValidationSplit`, the master URL as
    `local[*]`, and the Spark Context is the entry point of the program. Please set
    these parameters accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 3: Prepare the OCR data as a libsvm format**'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you recall *Figure 19* in [Chapter 6](part0049_split_000.html#1ENBI2-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 6.  Building Scalable Machine Learning Pipelines"), *Building Scalable
    Machine Learning Pipelines,* you will remember the data as follows in *Figure
    9*, *A snapshot of the original OCR dataset as a Data Frame*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Linear regression–based model selection for an OCR dataset](img/00071.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: A snapshot of the original OCR dataset as a Data Frame'
  prefs: []
  type: TYPE_NORMAL
- en: However, the current implementation of the `TrainValidationSplitModel` API only
    works on datasets that are already in `libsvm` format.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Interested readers should refer to the following research article for more
    in depth knowledge: Chih-Chung Chang and Chih-Jen Lin, *LIBSVM - A Library for
    Support Vector Machines*. ACM Transactions on Intelligent Systems and Technology,
    2:27:1--27:27, 2011\. The software is available at [http://www.csie.ntu.edu.tw/~cjlin/libsvm](http://www.csie.ntu.edu.tw/~cjlin/libsvm).'
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, we do need to convert the dataset from the current tab separated
    OCR data to a `libsvm` format.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Readers should use the dataset provided with Packt packages or can convert the
    CSV/CSV file to the corresponding `libsvm` format. Interested readers can refer
    to our public script provided on GitHub at [https://github.com/rezacsedu/CSVtoLibSVMConverterinR](https://github.com/rezacsedu/CSVtoLibSVMConverterinR) that
    directly converts a CSV file to `libsvm` format. Just properly show the input
    and output file path and run the script on your RStudio.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 4: Prepare the OCR data set and also prepare the training and test set**'
  prefs: []
  type: TYPE_NORMAL
- en: 'We are assuming that readers already have downloaded the data or have converted
    the OCR data using our GitHub script or using their own script. Now, take the
    OCR `libsvm` format data as input and prepare the Dataset out of the data as raw
    texts and check if the data was read properly by calling the `show()` method as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '![Linear regression–based model selection for an OCR dataset](img/00150.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: The top 20 rows'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: To learn more about the data, please refer to the section *Pipeline - An Example
    with Spark ML*, in [Chapter 4](part0038_split_000.html#147LC2-5afe140a04e845e0842b44be7971e11a
    "Chapter 4. Extracting Knowledge through Feature Engineering"), *Extracting Knowledge
    through Feature Engineering*, and the description of dataset exploration. As you
    can see in *Figure 2*, *Spark ML pipeline model using logistic regression estimator
    (dash lines only happen during pipeline fitting)*, there are only two labels (spam
    or ham) associated along with the text values (that is, each row). However, there
    is no numeric label or ID.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, we need to prepare the dataset (training set) so that the Dataset
    also contains the ID, and labels along with text (that is, value) so that we can
    prepare a test set and predict their corresponding labels for any classification
    algorithm (that is, logistic regression) and can decide if our model selection
    is appropriate.
  prefs: []
  type: TYPE_NORMAL
- en: However, in order to do so we first need to prepare the training dataset. As
    you can see, the data frame shown above has only one column and as previously
    mentioned we do need to have three columns. If we prepare an RDD from the above
    Dataset (that is, `df`) it would be easier for us to make that transformation.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 5: Configure an ML pipeline using linear regression**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 6: Construct a grid of parameters to search over**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Currently, Spark uses a `ParamGridBuilder` to construct a grid of parameters
    to search over. In this regard, then, with three values for `hashingTF.numFeatures`
    and two values for `lr.regParam`, this grid will have 3 x 2 = 6 parameter settings
    for the `CrossValidator` to choose from:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: We now treat the pipeline as an Estimator, wrapping it in a `CrossValidator`
    instance. This will allow us to jointly choose parameters for all pipeline stages.
    As already discussed, a `CrossValidator` requires an Estimator, a set of Estimator
    `ParamMaps`, and an Evaluator.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note that the evaluator here is a `BinaryClassificationEvaluator` and its default
    metric is `areaUnderROC`.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 7: Create a TrainValidationSplit instance:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: In this case the estimator is simply the linear regression that we created in
    *Step 4*. A `TrainValidationSplit` requires an Estimator, a set of Estimator `ParamMaps`,
    and an Evaluator. In this case, 70% of the data will be used as training and the
    remaining 30% for the validation.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 8: Run TrainValidationSplit, and chooses parameters**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the `TrainValidationSplit` and choose the best set of parameters for your
    problem using the training set. Just use the following code segment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 9: Making a prediction on the test set**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Make predictions on the test data where model is the model with the combination
    of parameters that performed best. Finally, to show the predictions, use the following
    code segment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '![Linear regression–based model selection for an OCR dataset](img/00036.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: Prediction against each feature and label'
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 11*, we showed the row prediction against the actual label. The first
    column is the actual label, the second column signifies the feature vector, and
    the third column shows the raw prediction based on the feature vectors the `TrainValidationSplitModel`
    created.
  prefs: []
  type: TYPE_NORMAL
- en: Logistic regression-based model selection for the cancer dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this sub-section, we will show how to perform train validation split tuning
    for OCR data. We will use the logistic regression in the first place; then we
    will move forward for other models. Finally, we will recommend the most suitable
    parameters for the OCR data classification.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1: Import the necessary packages/libraries/APIs**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 2: Initialize the necessary Spark environment**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Here we set the application name as `CancerDiagnosis`, the master URL as `local[*]`
    and the Spark Context as the entry point of the program. Please set these parameters
    accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 3: Create the Java RDD**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Parse the cancer diagnosis data and prepare the Java RDDs for strings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 4: Prepare the cancer diagnosis LabeledPoint RDDs**'
  prefs: []
  type: TYPE_NORMAL
- en: 'As already discussed in [Chapter 6](part0049_split_000.html#1ENBI2-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 6.  Building Scalable Machine Learning Pipelines"), *Building Scalable
    Machine Learning Pipelines*, the cancer diagnosis dataset contains two labels
    *B* and *M* for Benign and Malignant. However, we need to convert them into a
    numeric label. Just use the following code to convert all of them from label transforming
    to `LabeledPoint` RDDs preparation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: '![Logistic regression-based model selection for the cancer dataset](img/00082.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: The Label Point RDDs snapshot'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in *Figure 9*, *The top 20 rows*, the labels B and M have been
    converted into 1.0 and 0.0\. Now we need to create a data frame out of the label
    point RDDs.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 5: Create a Dataset and also prepare the training and test set**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a Dataset from the previous RDDs (that is, `linesRDD`) by specifying
    the Label Point class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: '![Logistic regression-based model selection for the cancer dataset](img/00136.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: The created Dataset showing the top 20 rows.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Note, that you will have to set the ratio of the random split based on your
    data and problem type accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 6: Configure an ML pipeline using logistic regression:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 7: Construct a grid of parameters to search over**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Currently, Spark uses a `ParamGridBuilder` to construct a grid of parameters
    to search over. In this regard, suppose we have three values for `hashingTF.numFeatures`
    and two values for `lr.regParam`, this grid will have 3 x 2 = 6 parameter settings
    for `CrossValidator` to choose from:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: Note, that you will have to set the values of above parameters based on your
    data and problem type accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 8: Create a TrainValidationSplit instance:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: In this case, the estimator is simply the linear regression that we created
    in *Step 4*. Prepare the cancer diagnosis `LabeledPoint` RDDs. A `TrainValidationSplit`
    requires an Estimator, a set of Estimator `ParamMaps`, and an Evaluator that supports
    binary classification since our dataset has only two classes where 80% is used
    for the purpose of training and the remaining 20% for the validation.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 9: Run the TrainValidationSplit and choose the parameters**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the `TrainValidationSplit`, and choose the best set of parameters for your
    problem using the training set. Just use the following code segments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 10: Make predictions on the test set**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Make predictions on the test data where the model is the model with a combination
    of parameters that performed the best. Finally, show the predictions. Just use
    the following code segment for doing so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: '![Logistic regression-based model selection for the cancer dataset](img/00038.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14: Prediction against each feature and label'
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, if you compare these results with those shown in *Figure 6, The newly
    created label and ID from the dataset; where ID is the number of rows*, you find
    that the prediction accuracy increases for more sophisticated methods of measuring
    predictive accuracy that can be used to identify places where the error rate can
    be optimized depending on the costs of each type of error.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Tuning an algorithm or machine learning application can be thought of as simply
    a process through which one goes when they optimise the parameters that impact
    the model in order to enable the algorithm to perform the best (in terms of run-time
    and memory usages). In this chapter, we have shown how to perform ML model tuning
    using train-validation split and cross-validation techniques of Spark ML.
  prefs: []
  type: TYPE_NORMAL
- en: We also want to mention that the tuning related support and algorithms are still
    not well enriched until the date of (14th October 2016) the current release of
    Spark. Interested readers are encouraged to visit the Spark tuning page at [http://spark.apache.org/docs/latest/ml-tuning.html](http://spark.apache.org/docs/latest/ml-tuning.html)
    for more updates since we believe that more features will be added to the Spark
    website and they will certainly provide enough documentation.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will discuss how to make your machine learning algorithm
    or models adaptable for new datasets. This chapter covers advanced machine learning
    techniques to be able to make algorithms adaptable to new data. It will mainly
    focus on batch/streaming architectures and on online learning algorithms by using
    Spark streaming.
  prefs: []
  type: TYPE_NORMAL
- en: The ultimate target is to bring dynamism to the static machine learning models.
    Readers will also see how machine learning algorithms learn incrementally over
    the data; that is to say the models are updated each time they see a new training
    instance.
  prefs: []
  type: TYPE_NORMAL
