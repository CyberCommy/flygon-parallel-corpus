- en: Chapter 4. Data Integrity in an Application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By now, you should be comfortable with the models and tools provided in Go's
    core to provide mostly race-free concurrency.
  prefs: []
  type: TYPE_NORMAL
- en: We can now create goroutines and channels with ease, manage basic communication
    across channels, coordinate data without race conditions, and detect such conditions
    as they arise.
  prefs: []
  type: TYPE_NORMAL
- en: However, we can neither manage larger distributed systems nor deal with potentially
    lower-level consistency problems. We've utilized a basic and simplistic mutex,
    but we are about to look at a more complicated and expressive way of handling
    mutual exclusions.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you should be able to expand your concurrency patterns
    from the previous chapter into distributed systems using a myriad of concurrency
    models and systems from other languages. We'll also look—at a high level—at some
    consistency models that you can utilize to further express your precoding strategies
    for single-source and distributed applications.
  prefs: []
  type: TYPE_NORMAL
- en: Getting deeper with mutexes and sync
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 2](part0021_split_000.html#page "Chapter 2. Understanding the Concurrency
    Model"), *Understanding the Concurrency Model*, we introduced `sync.mutex` and
    how to invoke a mutual exclusion lock within your code, but there's some more
    nuance to consider with the package and the mutex type.
  prefs: []
  type: TYPE_NORMAL
- en: We've mentioned that in an ideal world, you should be able to maintain synchronization
    in your application by using goroutines alone. In fact, this would probably be
    best described as the canonical method within Go, although the `sync` package
    does provide a few other utilities, including mutexes.
  prefs: []
  type: TYPE_NORMAL
- en: Whenever possible, we'll stick with goroutines and channels to manage consistency,
    but the mutex does provide a more traditional and granular approach to lock and
    access data. If you've ever managed another concurrent language (or package within
    a language), odds are you've had experience with either a mutex or a philosophical
    analog. In the following chapters, we'll look at ways of extending and exploiting
    mutexes to do a little more out of the box.
  prefs: []
  type: TYPE_NORMAL
- en: If we look at the `sync` package, we'll see there are a couple of different
    mutex structs.
  prefs: []
  type: TYPE_NORMAL
- en: The first is `sync.mutex`, which we've explored—but another is `RWMutex`. The
    `RWMutex` struct provides a multireader, single-writer lock. These can be useful
    if you want to allow reads to resources but provide mutex-like locks when a write
    is attempted. They can be best utilized when you expect a function or subprocess
    to do frequent reads but infrequent writes, but it still cannot afford a dirty
    read.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at an example that updates the date/time every 10 seconds (acquiring
    a lock), yet outputs the current value every other second, as shown in the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We don't explicitly run `Done()` on our `WaitGroup` struct, so this will run
    in perpetuity.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two different methods for performing locks/unlocks on `RWMutex`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Lock()`: This will block variables for both reading and writing until an `Unlock()`
    method is called'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`happenedRlock()`: This locks bound variables solely for reads'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second method is what we've used for this example, because we want to simulate
    a real-world lock. The net effect is the `interval` function that outputs the
    current time that will return a single dirty read before `rwLock` releases the
    read lock on the `currentTime` variable. The `Sleep()` method exists solely to
    give us time to witness the lock in motion. An `RWLock` struct can be acquired
    by many readers or by a single writer.
  prefs: []
  type: TYPE_NORMAL
- en: The cost of goroutines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As you work with goroutines, you might get to a point where you''re spawning
    dozens or even hundreds of them and wonder if this is going to be expensive. This
    is particularly true if your previous experience with concurrent and/or parallel
    programming was primarily thread-based. It''s commonly accepted that maintaining
    threads and their respective stacks can begin to bog down a program with performance
    issues. There are a few reasons for this, which are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Memory is required just for the creation of a thread
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Context switching at the OS level is more complex and expensive than in-process
    context switching
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Very often, a thread is spawned for a very small process that could be handled
    otherwise
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It's for these reasons that a lot of modern concurrent languages implement something
    akin to goroutines (C# uses the async and await mechanism, Python has greenlets/green
    threads, and so on) that simulate threads using small-scale context switching.
  prefs: []
  type: TYPE_NORMAL
- en: However, it's worth knowing that while goroutines are (or can be) cheap and
    cheaper than OS threads, they are not free. At a large (perhaps enormous) measure,
    even cheap and light goroutines can impact performance. This is particularly important
    to note as we begin to look at distributed systems, which often scale larger and
    at faster rates.
  prefs: []
  type: TYPE_NORMAL
- en: 'The difference between running a function directly and running it in a goroutine
    is negligible of course. However, keep in mind that Go''s documentation states:'
  prefs: []
  type: TYPE_NORMAL
- en: '*It is practical to create hundreds of thousands of goroutines in the same
    address space.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Given that stack creation uses a few kilobytes per goroutine, in a modern environment,
    it''s easy to see how that could be perceived as a nonfactor. However, when you
    start talking about thousands (or millions) of goroutines running, it can and
    likely will impact the performance of any given subprocess or function. You can
    test this by wrapping functions in an arbitrary number of goroutines and benchmarking
    the average execution time and—more importantly—memory usage. At approximately
    5KB per goroutine, you may find that memory can become a factor, particularly
    on low-RAM machines or instances. If you have an application that runs heavy on
    a high-powered machine, imagine it reaching criticality in one or more lower-powered
    machines. Consider the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Even if the overhead for the goroutine is cheap, what happens at 100 million
    or—as we have here—a billion goroutines running?
  prefs: []
  type: TYPE_NORMAL
- en: As always, doing this in an environment that utilizes more than a single core
    can actually increase the overhead of this application due to the costs of OS
    threading and subsequent context switching.
  prefs: []
  type: TYPE_NORMAL
- en: These issues are almost always the ones that are invisible unless and until
    an application begins to scale. Running on your machine is one thing, running
    at scale across a distributed system with what amounts to low-powered application
    servers is quite another.
  prefs: []
  type: TYPE_NORMAL
- en: The relationship between performance and data consistency is important, particularly
    if you start utilizing a lot of goroutines with mutual exclusions, locks, or channel
    communication.
  prefs: []
  type: TYPE_NORMAL
- en: This becomes a larger issue when dealing with external, more permanent memory
    sources.
  prefs: []
  type: TYPE_NORMAL
- en: Working with files
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Files are a great example of areas where data consistency issues such as race
    conditions can lead to more permanent and catastrophic problems. Let''s look at
    a piece of code that might continuously attempt to update a file to see where
    we could run into race conditions, which in turn could lead to bigger problems
    such as an application failing or losing data consistency:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Code involving file operations are rife for these sorts of potential issues,
    as mistakes are specifically *not ephemeral* and can be locked in time forever.
  prefs: []
  type: TYPE_NORMAL
- en: If our goroutines block at some critical point or the application fails midway
    through, we could end up with a file that has invalid data in it. In this case,
    we're simply iterating through some numbers, but you can also apply this situation
    to one involving database or datastore writes—the potential exists for persistent
    bad data instead of temporary bad data.
  prefs: []
  type: TYPE_NORMAL
- en: This is not a problem that is exclusively solved by channels or mutual exclusions;
    rather, it requires some sort of sanity check at every step to make certain that
    data is where you and the application expect it to be at every step in the execution.
    Any operation involving `io.Writer` relies on primitives, which Go's documentation
    explicitly notes that we should not assume they are safe for parallel execution.
    In this case, we have wrapped the file writing in a mutex.
  prefs: []
  type: TYPE_NORMAL
- en: Getting low – implementing C
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the most interesting developments in language design in the past decade
    or two is the desire to implement lower-level languages and language features
    via API. Java lets you do this purely externally, and Python provides a C library
    for interaction between the languages. It warrants mentioning that the reasons
    for doing this vary—among them applying Go's concurrency features as a wrapper
    for legacy C code—and you will likely have to deal with some of the memory management
    associated with introducing unmanaged code to garbage-collected applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'Go takes a hybrid approach, allowing you to call a C interface through an import,
    which requires a frontend compiler such as GCC:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: So why would we want to do this?
  prefs: []
  type: TYPE_NORMAL
- en: There are some good and bad reasons to implement C directly in your project.
    An example of a good reason might be to have direct access to the inline assembly,
    which you can do in C but not directly in Go. A bad reason could be any that has
    a solution inherent in Golang itself.
  prefs: []
  type: TYPE_NORMAL
- en: To be fair, even a bad reason is not bad if you build your application reliably,
    but it does impose an additional level of complexity to anyone else who might
    use your code. If Go can satisfy the technical and performance requirements, it's
    always better to use a single language in a single project.
  prefs: []
  type: TYPE_NORMAL
- en: 'There''s a famous quote from C++ creator Bjarne Stroustrup on C and C++:'
  prefs: []
  type: TYPE_NORMAL
- en: '*C makes it easy to shoot yourself in the foot; C++ makes it harder, but when
    you do, it blows your whole leg off.*'
  prefs: []
  type: TYPE_NORMAL
- en: Jokes aside (Stroustrup has a vast collection of such quips and quotes), the
    fundamental reasoning is that the complexity of C often prevents people from accidentally
    doing something catastrophic.
  prefs: []
  type: TYPE_NORMAL
- en: As Stroustrup says, C makes it easy to make big mistakes, but the repercussions
    are often smaller due to language design than higher-level languages. Issues dealing
    with security and stability are easy to be introduced in any low-level language.
  prefs: []
  type: TYPE_NORMAL
- en: By simplifying the language, C++ provides abstractions that make low-level operations
    easier to carry out. You can see how this might apply to using C directly in Go,
    given the latter language's syntactical sweetness and programmer friendliness.
  prefs: []
  type: TYPE_NORMAL
- en: 'That said, working with C can highlight some of the potential pitfalls with
    regard to memory, pointers, deadlocks, and consistency, so we''ll touch upon a
    simple example as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Touching memory in cgo
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The most important takeaway from the preceding example is to remember that anytime
    you go into or out of C, you need to manage memory manually (or at least more
    directly than with Go alone). If you've ever worked in C (or C++), you know that
    there's no automatic garbage collection, so if you request memory space, you must
    also free it. Calling C from Go does not preclude this.
  prefs: []
  type: TYPE_NORMAL
- en: The structure of cgo
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Importing C into Go will take you down a syntactical side route, as you probably
    noticed in the preceding code. The first thing that will appear glaringly different
    is the actual implementation of C code within your application.
  prefs: []
  type: TYPE_NORMAL
- en: 'Any code (in comments to stop Go''s compiler from failing) directly above the
    `import "C"` directive will be interpreted as C code. The following is an example
    of a C function declared above our Go code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Bear in mind that Go won't validate this, so if you make an error in your C
    code, it could lead to silent failure.
  prefs: []
  type: TYPE_NORMAL
- en: Another related warning is to remember your syntax. While Go and C share a lot
    of syntactical overlap, leave off a curly bracket or a semicolon and you could
    very well find yourself in one of those silent failure situations. Alternately,
    if you're working in the C part of your application and you go back to Go, you
    will undoubtedly find yourself wrapping loop expressions in parentheses and ending
    your lines with semicolons.
  prefs: []
  type: TYPE_NORMAL
- en: Also remember that you'll frequently have to handle type conversions between
    C and Go that don't have one-to-one analogs. For example, C does not have a built-in
    string type (you can, of course, include additional libraries for types), so you
    may need to convert between strings and char arrays. Similarly, `int` and `int64`
    might need some nonimplicit conversion, and again, you may not get the debugging
    feedback that you might expect when compiling these.
  prefs: []
  type: TYPE_NORMAL
- en: The other way around
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Using C within Go is obviously a potentially powerful tool for code migration,
    implementing lower-level code, and roping in other developers, but what about
    the inverse? Just as you can call C from within Go, you can call Go functions
    as external functions within your embedded C.
  prefs: []
  type: TYPE_NORMAL
- en: The end game here is the ability to work with and within C and Go in the same
    application. By far the easiest way to handle this is by using gccgo, which is
    a frontend for GCC. This is different than the built-in Go compiler; it is possible
    to go back and forth between C and Go without gccgo, but using it makes this process
    much simpler.
  prefs: []
  type: TYPE_NORMAL
- en: '**gopart.go**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the code for the Go part of the interaction, which the C part
    will call as an external function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '**cpart.c**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now for the C part, where we make our call to our Go application''s exported
    function `MyGoFunction`, as shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '**Makefile**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Unlike using C directly in Go, at present, doing the inverse requires the use
    of a makefile for C compilation. Here''s one that you can use to get an executable
    from the earlier simple example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Running the makefile here should produce an executable file that calls the function
    from within C.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, more fundamentally, cgo allows you to define your functions as external
    functions for C directly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, you''ll need to use the `cgo` tool directly to generate header files
    for C as shown in the following line of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, the Go function is available for use in your C application:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Note that if you export a Go function that contains more than one return value,
    it will be available as a struct in C rather than a function, as C does not provide
    multiple variables returned from a function.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, you may be realizing that the true power of this functionality
    is the ability to interface with a Go application directly from existing C (or
    even C++) applications.
  prefs: []
  type: TYPE_NORMAL
- en: While not necessarily a true API, you can now treat Go applications as linked
    libraries within C apps and vice versa.
  prefs: []
  type: TYPE_NORMAL
- en: 'One caveat about using `//export` directives: if you do this, your C code must
    reference these as extern-declared functions. As you may know, extern is used
    when a C application needs to call a function from another linked C file.'
  prefs: []
  type: TYPE_NORMAL
- en: 'When we build our Go code in this manner, cgo generates the header file `_cgo_export.h`,
    as you saw earlier. If you want to take a look at that code, it can help you understand
    how Go translates compiled applications into C header files for this type of use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: You may also run into a rare scenario wherein the C code is not exactly as you
    expect, and you're unable to cajole the compiler to produce what you expect. In
    that case, you're always free to modify the header file before the compilation
    of your C application, despite the `DO NOT EDIT` warning.
  prefs: []
  type: TYPE_NORMAL
- en: Getting even lower – assembly in Go
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you can shoot your foot off with C and you can blow your leg off with C++,
    just imagine what you can do with assembly in Go.
  prefs: []
  type: TYPE_NORMAL
- en: It isn't possible to use assembly directly in Go, but as Go provides access
    to C directly and C provides the ability to call inline assembly, you can indirectly
    use it in Go.
  prefs: []
  type: TYPE_NORMAL
- en: But again, just because something is possible doesn't mean it should be done—if
    you find yourself in need of assembly in Go, you should consider using assembly
    directly and connecting via an API.
  prefs: []
  type: TYPE_NORMAL
- en: Among the many roadblocks that you may encounter with assembly in (C and then
    in) Go is the lack of portability. Writing inline C is one thing—your code should
    be relatively transferable between processor instruction sets and operating systems—but
    assembly is obviously something that requires a lot of specificity.
  prefs: []
  type: TYPE_NORMAL
- en: All that said, it's certainly better to have the option to shoot yourself in
    the foot whether you choose to take the shot or not. Use great care when considering
    whether you need C or assembly directly in your Go application. If you can get
    away with communicating between dissonant processes through an API or interprocess
    conduit, always take that route first.
  prefs: []
  type: TYPE_NORMAL
- en: One very obvious drawback of using assembly in Go (or on its own or in C) is
    you lose the cross-compilation capabilities that Go provides, so you'd have to
    modify your code for every destination CPU architecture. For this reason, the
    only practical times to use Go in C are when there is a single platform on which
    your application should run.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s an example of what an ASM-in-C-in-Go application might look like. Keep
    in mind that we''ve included no ASM code, because it varies from one processor
    type to another. Experiment with some boilerplate assembly in the following `__asm__`
    section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: If nothing else, this may provide an avenue for delving deeper into ASM even
    if you're familiar with neither assembly nor C itself. The more high-level you
    consider C and Go to be, the more practical you might see this.
  prefs: []
  type: TYPE_NORMAL
- en: 'For most uses, Go (and certainly C) is low-level enough to be able to squeeze
    out any performance hiccups without landing at assembly. It''s worth noting again
    that while you do lose some immediate control of memory and pointers in Go when
    you invoke C applications, that caveat applies tenfold with assembly. All of those
    nifty tools that Go provides may not work reliably or not work at all. If you
    think about the Go race detector, consider the following application:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: You can see how tossing your pointers around between Go and C might leave you
    out in the dark when you don't get what you expect out of the program.
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind that here there is a somewhat unique and perhaps unexpected kicker
    to using goroutines with cgo; they are treated by default as blocking. This isn't
    to say that you can't manage concurrency within C, but it won't happen by default.
    Instead, Go may well launch another system thread. You can manage this to some
    degree by utilizing the runtime function `runtime.LockOSThread()`. Using `LockOSThread`
    tells Go that a particular goroutine should stay within the present thread and
    no other concurrent goroutine may use this thread until `runtime.UnlockOSThread()`
    is called.
  prefs: []
  type: TYPE_NORMAL
- en: The usefulness of this depends heavily on the necessity to call C or a C library
    directly; some libraries will play happily as new threads are created, a few others
    may segfault.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another useful runtime call you should find useful within your Go code is `NumGcoCall()`.
    This returns the number of cgo calls made by a current process. If you need to
    lock and unlock threads, you can also use this to build an internal queue report
    to detect and prevent deadlocks.
  prefs: []
  type: TYPE_NORMAL
- en: None of this precludes the possibility of race conditions should you choose
    to mix and match Go and C within goroutines.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, C itself has a few race detector tools available. Go's race detector
    itself is based on the `ThreadSanitizer` library. It should go without saying
    that you probably do not want several tools that accomplish the same thing within
    a single project.
  prefs: []
  type: TYPE_NORMAL
- en: Distributed Go
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we've talked quite a bit about managing data within single machines,
    though with one or more cores. This is complicated enough as is. Preventing race
    conditions and deadlocks can be hard to begin with, but what happens when you
    introduce more machines (virtual or real) to the mix?
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing that should come to mind is that you can throw out a lot of
    the inherent tools that Go provides, and to a large degree that''s true. You can
    mostly guarantee that Go can handle internal locking and unlocking of data within
    its own, singular goroutines and channels, but what about one or more additional
    instances of an application running? Consider the following model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Distributed Go](img/00018.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Here we see that either of these threads across either process could be reading
    from or writing to our **Critical Data** at any given point. With that in mind,
    there exists a need to coordinate access to that data.
  prefs: []
  type: TYPE_NORMAL
- en: At a very high level, there are two direct strategies for handling this, a distributed
    lock or consistency hash table (consistent hashing).
  prefs: []
  type: TYPE_NORMAL
- en: The first strategy is an extension of mutual exclusions except that we do not
    have direct and shared access to the same address space, so we need to create
    an abstraction. In other words, it's our job to concoct a lock mechanism that's
    visible to all available external entities.
  prefs: []
  type: TYPE_NORMAL
- en: The second strategy is a pattern designed specifically for caching and cache
    validation/invalidation, but it has relevancy here as well, because you can use
    it to manage where data lives in the more global address space.
  prefs: []
  type: TYPE_NORMAL
- en: However, when it comes to ensuring consistency across these systems, we need
    to go deeper than this general, high-level approach.
  prefs: []
  type: TYPE_NORMAL
- en: 'Split this model down the middle and it becomes easy: channels will handle
    the concurrent flow of data and data structures, and where they don''t, you can
    use mutexes or low-level atomicity to add additional safeguards.'
  prefs: []
  type: TYPE_NORMAL
- en: However, look to the right. Now you have another VM/instance or machine attempting
    to work with the same data. How can we make sure that we do not encounter reader/writer
    problems?
  prefs: []
  type: TYPE_NORMAL
- en: Some common consistency models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Luckily, there are some non-core Go solutions and strategies that we can utilize
    to improve our ability to control data consistency.
  prefs: []
  type: TYPE_NORMAL
- en: Let's briefly look at a few consistency models that we can employ to manage
    our data in distributed systems.
  prefs: []
  type: TYPE_NORMAL
- en: Distributed shared memory
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: On its own, a **Distributed Shared Memory** (**DSM**) system does not intrinsically
    prevent race conditions, as it is merely a method for more than one system to
    share real or partitioned memory.
  prefs: []
  type: TYPE_NORMAL
- en: 'In essence, you can imagine two systems with 1 GB of memory, each allocating
    500 MB to a shared memory space that is accessible and writable by each. Dirty
    reads are possible as are race conditions unless explicitly designed. The following
    figure is a visual representation of how two systems can coordinate using shared
    memory:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Distributed shared memory](img/00019.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: We'll look at one prolific but simple example of DSM shortly, and play with
    a library available to Go for test driving it.
  prefs: []
  type: TYPE_NORMAL
- en: First-in-first-out – PRAM
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Pipelined RAM** (**PRAM**) consistency is a form of first-in-first-out methodology,
    in which data can be read in order of the queued writes. This means that writes
    read by any given, separate process may be different. The following figure represents
    this concept:'
  prefs: []
  type: TYPE_NORMAL
- en: '![First-in-first-out – PRAM](img/00020.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Looking at the master-slave model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The master-slave consistency model is similar to the leader/follower model
    that we''ll look at shortly, except that the master manages all operations on
    data and broadcasts rather than receiving write operations from a slave. In this
    case, replication is the primary method of transmission of changes to data from
    the master to the slave. In the following diagram, you will find a representation
    of the master-slave model with a master server and four slaves:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Looking at the master-slave model](img/00021.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: While we can simply duplicate this model in Go, we have more elegant solutions
    available to us.
  prefs: []
  type: TYPE_NORMAL
- en: The producer-consumer problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the classic producer-consumer problem, the producer writes chunks of data
    to a conduit/buffer, while a consumer reads chunks. The issue arises when the
    buffer is full: if the producer adds to the stack, the data read will not be what
    you intend. To avoid this, we employ a channel with waits and signals. This model
    looks a bit like the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The producer-consumer problem](img/00022.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'If you''re looking for the semaphore implementation in Go, there is no explicit
    usage of the semaphore. However, think about the language here—fixed-size channels
    with waits and signals; sounds like a buffered channel. Indeed, by providing a
    buffered channel in Go, you give the conduit here an explicit length; the channel
    mechanism gives you the communication for waits and signals. This is incorporated
    in Go''s concurrency model. Let''s take a quick look at a producer-consumer model
    as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Looking at the leader-follower model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the leader/follower model, writes are broadcasted from a single source to
    any followers. Writes can be passed through any number of followers or be restricted
    to a single follower. Any completed writes are then broadcasted to the followers.
    This can be visually represented as the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Looking at the leader-follower model](img/00023.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: We can see a channel analog here in Go as well. We can, and have, utilized a
    single channel to handle broadcasts to and from other followers.
  prefs: []
  type: TYPE_NORMAL
- en: Atomic consistency / mutual exclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We've looked at atomic consistency quite a bit. It ensures that anything that
    is not created and used at essentially the same time will require serialization
    to guarantee the strongest form of consistency. If a value or dataset is not atomic
    in nature, we can always use a mutex to force linearizability on that data.
  prefs: []
  type: TYPE_NORMAL
- en: Serial or sequential consistency is inherently strong, but can also lead to
    performance issues and degradation of concurrency.
  prefs: []
  type: TYPE_NORMAL
- en: Atomic consistency is often considered the strongest form of ensuring consistency.
  prefs: []
  type: TYPE_NORMAL
- en: Release consistency
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The release consistency model is a DSM variant that can delay a write''s modifications
    until the time of first acquisition from a reader. This is known as lazy release
    consistency. We can visualize lazy release consistency in the following serialized
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Release consistency](img/00024.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: This model as well as an eager release consistency model both require an announcement
    of a release (as the name implies) when certain conditions are met. In the eager
    model, that condition requires that a write would be read by all read processes
    in a consistent manner.
  prefs: []
  type: TYPE_NORMAL
- en: In Go, there exists alternatives for this, but there are also packages out there
    if you're interested in playing with it.
  prefs: []
  type: TYPE_NORMAL
- en: Using memcached
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you're not familiar with memcache(d), it's a wonderful and seemingly obvious
    way to manage data across distributed systems. Go's built-in channels and goroutines
    are fantastic to manage communication and data integrity within a single machine's
    processes, but neither are built for distributed systems out of the box.
  prefs: []
  type: TYPE_NORMAL
- en: Memcached, as the name implies, allows data sharing memory among multiple instances
    or machines. Initially, memcached was intended to store data for quick retrieval.
    This is useful for caching data for systems with high turnover such as web applications,
    but it's also a great way to easily share data across multiple servers and/or
    to utilize shared locking mechanisms.
  prefs: []
  type: TYPE_NORMAL
- en: In our earlier models, memcached falls under DSM. All available and invoked
    instances share a common, mirrored memory space within their respective memories.
  prefs: []
  type: TYPE_NORMAL
- en: 'It''s worth pointing out that race conditions can and do exist within memcached,
    and you still need a way to deal with that. Memcached provides one method to share
    data across distributed systems, but does not guarantee data atomicity. Instead,
    memcached operates on one of two methods for invalidating cached data as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Data is explicitly assigned a maximum age (after which, it is removed from the
    stack)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Or data is pushed from the stack due to all available memory being used by newer
    data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It's important to note that storage within memcache(d) is, obviously, ephemeral
    and not fault resistant, so it should only be used where data should be passed
    without threat of critical application failure.
  prefs: []
  type: TYPE_NORMAL
- en: 'At the point where either of these conditions is met, the data disappears and
    the next call to this data will fail, meaning the data needs to be regenerated.
    Of course, you can work with some elaborate lock generation methods to make memcached
    operate in a consistent manner, although this is not standard built-in functionality
    of memcached itself. Let''s look at a quick example of memcached in Go using Brad
    Fitz''s gomemcache interface ([https://github.com/bradfitz/gomemcache](https://github.com/bradfitz/gomemcache)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: As you might note from the preceding example, if any of these memcached clients
    are writing to the shared memory at the same time, a race condition could still
    exist.
  prefs: []
  type: TYPE_NORMAL
- en: The key data can exist across any of the clients that have memcached connected
    and running at the same time.
  prefs: []
  type: TYPE_NORMAL
- en: Any client can also unset or overwrite the data at any time.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike a lot of implementations, you can set some more complex types through
    memcached, such as structs, assuming they are serialized. This caveat means that
    we're somewhat limited with the data we can share directly. We are obviously unable
    to use pointers as memory locations will vary from client to client.
  prefs: []
  type: TYPE_NORMAL
- en: One method to handle data consistency is to design a master-slave system wherein
    only one node is responsible for writes and the other clients listen for changes
    via a key's existence.
  prefs: []
  type: TYPE_NORMAL
- en: We can utilize any other earlier mentioned models to strictly manage a lock
    on this data, although it can get especially complicated. In the next chapter,
    we'll explore some ways by which we can build distributed mutual exclusion systems,
    but for now, we'll briefly look at an alternative option.
  prefs: []
  type: TYPE_NORMAL
- en: Circuit
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An interesting third-party library to handle distributed concurrency that has
    popped up recently is Petar Maymounkov's Go' circuit. Go' circuit attempts to
    facilitate distributed coroutines by assigning channels to listen to one or more
    remote goroutines.
  prefs: []
  type: TYPE_NORMAL
- en: The coolest part of Go' circuit is that simply including the package makes your
    application ready to listen and operate on remote goroutines and work with channels
    with which they are associated.
  prefs: []
  type: TYPE_NORMAL
- en: Go' circuit is in use at Tumblr, which proves it has some viability as a large-scale
    and relatively mature solutions platform.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Go' circuit can be found at [https://github.com/gocircuit/circuit](https://github.com/gocircuit/circuit).
  prefs: []
  type: TYPE_NORMAL
- en: Installing Go' circuit is not simple—you cannot run a simple `go get` on it—and
    requires Apache Zookeeper and building the toolkit from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once done, it''s relatively simple to have two machines (or two processes if
    running locally) running Go code to share a channel. Each cog in this system falls
    under a sender or listener category, just as with goroutines. Given that we''re
    talking about network resources here, the syntax is familiar with some minor modifications:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: You can see how this might make the communication between disparate machines
    playing with the same data a lot cleaner, whereas we used memcached primarily
    as a networked in-memory locking system. We're dealing with native Go code directly
    here; we have the ability to use circuits like we would in channels, without worrying
    about introducing new data management or atomicity issues. In fact, the circuit
    is built upon a goroutine itself.
  prefs: []
  type: TYPE_NORMAL
- en: 'This does, of course, still introduce some additional management issues, primarily
    as it pertains to knowing what remote machines are out there, whether they are
    active, updating the machines'' statuses, and so on. These types of issues are
    best suited for a suite such as Apache Zookeeper to handle coordination of distributed
    resources. It''s worth noting that you should be able to produce some feedback
    from a remote machine to a host: the circuit operates via passwordless SSH.'
  prefs: []
  type: TYPE_NORMAL
- en: That also means you may need to make sure that user rights are locked down and
    that they meet with whatever security policies you may have in place.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can find Apache Zookeeper at [http://zookeeper.apache.org/](http://zookeeper.apache.org/).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Equipped now with some methods and models to manage not only local data across
    single or multithreaded systems, but also distributed systems, you should start
    to feel pretty comfortable with protecting the validity of data in concurrent
    and parallel processes.
  prefs: []
  type: TYPE_NORMAL
- en: We've looked at both forms of mutual exclusions for read and read/write locks,
    and we have started to apply these to distributed systems to prevent blocks and
    race conditions across multiple networked systems.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we'll explore these exclusion and data consistency concepts
    a little deeper, building non-blocking networked applications and learn to work
    with timeouts and give parallelism with channels a deeper look.
  prefs: []
  type: TYPE_NORMAL
- en: We'll also dig a little deeper into the sync and OS packages, in particular
    looking at the `sync.atomic` operations.
  prefs: []
  type: TYPE_NORMAL
