- en: Concurrent Web Requests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter will focus on the application of concurrency in making web requests.
    Intuitively, making requests to a web page to collect information about it is
    independent to applying the same task to another web page. Concurrency, specifically
    threading in this case, therefore can be a powerful tool that provides a significant
    speedup in this process. In this chapter, we will learn the fundamentals of web
    requests and how to interact with websites using Python. We will also see how
    concurrency can help us make multiple requests in an efficient way. Finally, we
    will look at a number of good practices in web requests.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following concepts:'
  prefs: []
  type: TYPE_NORMAL
- en: The basics of web requests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The requests module
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Concurrent web requests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The problem of timeout
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Good practices in making web requests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following is a list of prerequisites for this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Python 3 must be installed on your computer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Download the GitHub repository at [https://github.com/PacktPublishing/Mastering-Concurrency-in-Python](https://github.com/PacktPublishing/Mastering-Concurrency-in-Python)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: During this chapter, we will be working with the subfolder named `Chapter05`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Check out the following video to see the Code in Action: [http://bit.ly/2Fy1ZcS](http://bit.ly/2Fy1ZcS)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The basics of web requests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The worldwide capacity to generate data is estimated to double in size every
    two years. Even though there is an interdisciplinary field known as data science
    that is entirely dedicated to the study of data, almost every programming task
    in software development also has something to do with collecting and analyzing
    data. A significant part of this is, of course, data collection. However, the
    data that we need for our applications is sometimes not stored nicely and cleanly
    in a database—sometimes, we need to collect the data we need from web pages.
  prefs: []
  type: TYPE_NORMAL
- en: For example, web scraping is a data extraction method that automatically makes
    requests to web pages and downloads specific information. Web scraping allows
    us to comb through numerous websites and collect any data we need in a systematic
    and consistent manner—the collected data can be analyzed later on by our applications
    or simply saved on our computers in various formats. An example of this would
    be Google, which programs and runs numerous web scrapers of its own to find and
    index web pages for the search engine.
  prefs: []
  type: TYPE_NORMAL
- en: The Python language itself provides a number of good options for applications
    of this kind. In this chapter, we will mainly work with the `requests` module
    to make client-side web requests from our Python programs. However, before we
    look into this module in more detail, we need to understand some web terminology
    in order to be able to effectively design our applications.
  prefs: []
  type: TYPE_NORMAL
- en: HTML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Hypertext Markup Language** (**HTML**) is the standard and most common markup
    language for developing web pages and web applications. An HTML file is simply
    a plaintext file with the `.html` file extension. In an HTML document, texts are
    surrounded and delimited by tags, written in angle brackets: `<p>`, `<img>`, `<i>`,
    and so on. These tags typically consist of pairs—an opening tag and a closing
    tag—indicating the styling or the'
  prefs: []
  type: TYPE_NORMAL
- en: nature of the data included inside.
  prefs: []
  type: TYPE_NORMAL
- en: It is also possible to include other forms of media in HTML code, such as images
    or videos. There are also numerous other tags that are used in common HTML documents.
    Some specify a group of elements that share some common characteristics, such
    as `<id></id>` and `<class></class>`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is an example of HTML code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/ce982a49-5538-4b5c-abee-b7580ca242dd.png)'
  prefs: []
  type: TYPE_IMG
- en: Sample HTML code
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, detailed knowledge on what each HTML tag accomplishes is not required
    for us to be able to make effective web requests. As we will see later on in this
    chapter, the more essential part of making web requests is the ability to interact
    with web pages efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: HTTP requests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In a typical communication process on the web, HTML texts are the data that
    is to be saved and/or further processed. This data needs to be first collected
    from web pages, but how can we go about doing that? Most of the communication
    is done via the internet—more specifically, the World Wide Web—and this utilizes
    the **Hypertext Transfer Protocol** (**HTTP**). In HTTP, request methods are used
    to convey the information of what data is being requested and should be sent back
    from a server.
  prefs: []
  type: TYPE_NORMAL
- en: For example, when you type `packtpub.com` in your browser, the browser sends
    a request method via HTTP to the Packt website's main server asking for data from
    the website. Now, if both your internet connection and Packt's server are working
    well, then your browser will receive a response back from the server, as shown
    in the following diagram. This response will be in the form of an HTML document,
    which will be interpreted by your browser, and your browser will display the corresponding
    HTML output to the screen.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/a850443e-e2d2-47d3-b1ca-b498d390fb95.png)'
  prefs: []
  type: TYPE_IMG
- en: Diagram of HTTP communication
  prefs: []
  type: TYPE_NORMAL
- en: 'Generally, request methods are defined as verbs that indicate the desired action
    to be performed while the HTTP client (web browsers) and the server communicate
    with each other: `GET`, `HEAD`, `POST`, `PUT`, `DELETE`, and so on. Of these methods,
    `GET` and `POST` are two of the most common request methods used in web-scraping
    applications; their function is described in the following list:'
  prefs: []
  type: TYPE_NORMAL
- en: The `GET` method makes a request for a specific data from the server. This method
    only retrieves data and has no other effect on the server and its databases.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `POST` method sends data in a specific form that is accepted by the server.
    This data could be, for example, a message to a bulletin board, mailing list,
    or a newsgroup; information to be submitted to a web form; or an item to be added
    to a database.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All general-purpose HTTP servers that we commonly see on the internet are actually required
    to implement at least the `GET` (and `HEAD`) method, while the `POST` method is
    considered optional.
  prefs: []
  type: TYPE_NORMAL
- en: HTTP status code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is not always the case that, when a web request is made and sent to a web
    server, the server will process the request and return the requested data without
    fail. Sometimes, the server might be completely down or already busy interacting
    with other clients and therefore unresponsive to a new request; sometimes, the
    client itself makes bad requests to a server (for example, incorrectly formatted
    or malicious requests).
  prefs: []
  type: TYPE_NORMAL
- en: As a way to categorize these problems as well as provide the most information
    as possible during the communication resulting from a web request, HTTP requires
    servers to respond to each request from its clients an **HTTP response** **status
    code**. A status code is typically a three-digit number that indicates the specific
    characteristics of the response that the server sends back to a client.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are in total five large categories of HTTP response status codes, indicated
    by the first digit of the code. They are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**1xx (informational status code)**: The request was received and the server
    is processing it. For example, 100 means the request header has been received
    and the server is waiting for the request body; 102 indicates that the request
    is currently being processed (this is used for large requests and to prevent clients
    from timing out).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**2xx (successful status code)**: The request was successfully received, understood,
    and processed by the server. For example, 200 means the request was successfully
    fulfilled; 202 indicates that the request has been accepted for processing, but
    the processing itself is not complete.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**3xx (redirectional status code)**: Additional actions need to be taken so
    that the request can be successfully processed. For example, 300 means that there
    are multiple options regarding how the response from the server should be processed
    (for example, giving the client multiple video format options when a video file
    is to be downloaded); 301 indicates that the server has been moved permanently
    and all requests should be directed to another address (provided in the response
    from the server).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**4xx (error status code for the client)**: The request was incorrectly formatted
    by the client and could not be processed. For example, 400 means that the client
    sent in a bad request (for example, syntax error or the size of the request is
    too large); 404 (arguably the most well-known status code) indicates that the
    request method is not supported by the server.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**5xx (error status code for the server)**: The request, although valid, could
    not be processed by the server. For example, 500 means there is an internal server
    error in which an unexpected condition was encountered; 504 (Gateway Timeout)
    means that the server, which was acting as a gateway or a proxy, did not receive
    a response from the final server in time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A lot more can be said about these status codes, but it is already sufficient
    for us to keep in mind the big five categories previously mentioned when making
    web requests from Python. If you would like to find more specific information
    about the above or other status codes, the **Internet Assigned Numbers Authority**
    (**IANA**) maintains the official registry of HTTP status codes.
  prefs: []
  type: TYPE_NORMAL
- en: The requests module
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `requests` module allows its users to make and send HTTP request methods.
    In the applications that we will be considering, it is mainly used to make contact
    with the server of the web pages we want to extract data from and obtain the response
    for the server.
  prefs: []
  type: TYPE_NORMAL
- en: According to the official documentation of the module, the use of Python 3 is
    **highly recommended** over Python 2 for `requests`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To install the module on your computer, run the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'You should use this code if you are using `pip` as your package manager. If,
    however, you are using Anaconda instead, simply use the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: These commands should install `requests` and any other required dependencies
    (`idna`, `certifi`, `urllib3`, and so on) for you if your system does not have
    those already. After this, run `import requests` in a Python interpreter to confirm
    that the module has been installed successfully.
  prefs: []
  type: TYPE_NORMAL
- en: Making a request in Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s look at an example usage of the module. If you already have the code
    for this book downloaded from the GitHub page, go ahead and navigate to the `Chapter05`
    folder. Let''s take a look at the `example1.py` file, as shown in the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we are using the `requests` module to download the HTML code
    of the web page, `www.google.com`. The `requests.get()` method sends a `GET` request
    method to `url` and we store the response to the `res` variable. After checking
    the status and headers of the response by printing them out, we create a file
    called `google.html` and write the HTML code, which is stored in the response
    text, to the file.
  prefs: []
  type: TYPE_NORMAL
- en: 'After running the programming (assuming that your internet is working and the
    Google server is not down), you should get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The response had a `200` status code, which we know means that the request has
    been successfully completed. The header of the response, stored in `res.headers`,
    additionally contains further specific information regarding the response. For
    example, we can see the date and time the request was made or that the content
    of the response is text and HTML and the total length of the content is `4958`.
  prefs: []
  type: TYPE_NORMAL
- en: The complete data sent from the server was also written to the `google.html`
    file. When you open the file in a text editor, you will be able to see the HTML
    code of the web page that we have downloaded using requests. On the other hand,
    if you use a web browser to open the file, you will see how **most** of the information
    from the original web page is now being displayed through a downloaded offline
    file.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, the following is how Google Chrome on my system interprets the
    HTML file:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/ddbf9c3e-2633-4f3c-914b-a2d868e5155f.png)'
  prefs: []
  type: TYPE_IMG
- en: Downloaded HTML opened offline
  prefs: []
  type: TYPE_NORMAL
- en: There is other information that is stored on the server that web pages of that
    server make reference to. This means that not all of the information that an online
    web page provides can be downloaded via a `GET` request, and this is why offline
    HTML code sometimes fails to contain all of the information available on the online
    web page that it was downloaded from. (For example, the downloaded HTML code in
    the preceding screenshot does not display the Google icon correctly.)
  prefs: []
  type: TYPE_NORMAL
- en: Running a ping test
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'With the basic knowledge of HTTP requests and the `requests` module in Python
    in mind, we will go through the rest of this chapter with a central problem: running
    a ping test. A ping test is a process in which you test the communication between
    your system and specific web servers, simply by making a request to each of the
    servers in question. By considering the HTTP response status code (potentially)
    returned by the server, the test is used to evaluate either the internet connection
    of your own system or the availability of the servers.'
  prefs: []
  type: TYPE_NORMAL
- en: Ping tests are quite common among web administrators, who usually have to manage
    a large number of websites simultaneously. Ping tests are a good tool to quickly
    identify pages that are unexpectedly unresponsive or down. There are many tools
    that provide you with powerful options in ping tests and, in this chapter, we
    will be designing a ping test application that can concurrently send multiple
    web requests at the same time.
  prefs: []
  type: TYPE_NORMAL
- en: To simulate different HTTP response status codes to be sent back to our program,
    we will be using [httpstat.us](http://www.httpstat.us), a website that can generate
    various status codes and is commonly used to test how applications that make web
    requests can handle varying response. Specifically, to use a request that will
    return a 200 status code in a program, we can simply make a request to [httpstat.us/200](http://www.httpstat.us/200) and
    the same applies for other status codes. In our ping test program, we will have
    a list of [httpstat.us](http://www.httpstat.us) URLs with different status codes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now a take look at the `Chapter05/example2.py` file, as shown in the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: In this program, the `ping()` function takes in a URL and attempts to make a
    `GET` request to the site. It will then print out the content of the response
    returned by the server. In our main program, we have a list of different status
    codes that we mentioned earlier, each of which we will go through and call the
    `ping()` function on.
  prefs: []
  type: TYPE_NORMAL
- en: 'The final output after running the preceding example should be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: We see that our ping test program was able to obtain corresponding responses
    from the server.
  prefs: []
  type: TYPE_NORMAL
- en: Concurrent web requests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the context of concurrent programming, we can see that the process of making
    a request to a web server and obtaining the returned response is independent from
    the same procedure for a different web server. This is to say that we could apply
    concurrency and parallelism to our ping test application to speed up our execution.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the concurrent ping test applications that we are designing, multiple HTTP
    requests will be made to the server simultaneously and corresponding responses
    will be sent back to our program, as shown in the following figure. As discussed
    before, concurrency and parallelism have significant applications in web development,
    and most servers nowadays have the ability to handle a large amount of requests
    at the same time:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/82ff2bcf-4fcb-47fd-9b08-0c1ebbf9a7bf.png)'
  prefs: []
  type: TYPE_IMG
- en: Parallel HTTP requests
  prefs: []
  type: TYPE_NORMAL
- en: Spawning multiple threads
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To apply concurrency, we simply use the `threading` module that we have been
    discussing to create separate threads to handle different web requests. Let''s
    take a look at the `Chapter05/example3.py` file, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we are including the sequential logic from the previous example
    to process our URL list, so that we can compare the improvement in speed when
    we apply threading to our ping test program. We are also creating a thread to
    ping each of the URLs in our URL list using the `threading` module; these threads
    will be executing independently from each other. Time taken to process the URLs
    sequentially and concurrently are also tracked using methods from the `time` module.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the program and your output should be similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: While the specific time that the sequential logic and threading logic take to
    process all the URLs might be different from system to system, there should still
    be a clear distinction between the two. Specifically, here we can see that the
    threading logic was almost six times faster than the sequential logic (which corresponds
    to the fact that we had six threads processing six URLs in parallel). There is
    no doubt, then, that concurrency can provide significant speedup for our ping
    test application specifically and for the process of making web requests in general.
  prefs: []
  type: TYPE_NORMAL
- en: Refactoring request logic
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The current version of our ping test application works as intended, but we
    can improve its readability by refactoring the logic where we make web requests
    into a thread class. Consider the `Chapter05/example4.py` file, specifically the
    `MyThread` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'In this example, `MyThread` inherits from the `threading.Thread` class and
    contains two additional attributes: `url` and `result`. The `url` attribute holds
    the URL that the thread instance should process, and the response returned from
    the web server to that thread will be written to the `result` attribute (in the
    `run()` function).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Outside of this class, we now can simply loop through the URL list, and create
    and manage the threads accordingly while not having to worry about the request
    logic in the main program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Note that we are now storing the responses in the `result` attribute of the
    `MyThread` class, instead of directly printing them out as in the old `ping()`
    function from the previous examples. This means that, after making sure that all
    threads have finished, we will need to loop through the threads one more time
    and print out those responses.
  prefs: []
  type: TYPE_NORMAL
- en: 'Refactoring the request logic should not greatly affect the performance of
    our current program; we are keeping track of the execution speed to see if this
    is actually the case. Execute the program and you will obtain the output similar
    to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Just as we expected, we are still achieving a significant speedup from the sequential
    version of the program with this refactored request logic. Again, our main program
    is now more readable, and further adjustments of the request logic (as we will
    see in the next section) can simply be directed to the `MyThread` class, without
    affecting the rest of the program.
  prefs: []
  type: TYPE_NORMAL
- en: The problem of timeout
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will explore a potential improvement to be made to our
    ping test application: timeout handling. Timeouts typically occur when the server
    takes an unusually long time to process a specific request, and the connection
    between the server and its client is terminated.'
  prefs: []
  type: TYPE_NORMAL
- en: In the context of a ping test application, we will be implementing a customized
    threshold for the timeout. Recall that a ping test is used to determine whether
    specific servers are still responsive, so we can specify in our program that,
    if a request takes more than our timeout threshold for the server to response,
    we will categorize that specific server with a timeout.
  prefs: []
  type: TYPE_NORMAL
- en: Support from httpstat.us and simulation in Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In addition to different options for status codes, the [httpstat.us](http://www.httpstat.us)
    website additionally provides a way to simulate a delay in its response when we
    send in requests. Specifically, we can customize the delay time (in milliseconds)
    with a query argument in our `GET` request. For example, [httpstat.us/200?sleep=5000](http://httpstat.us/200?sleep=5000)
    will return a response after five seconds of delay.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let us see how a delay like this would affect the execution of our program.
    Consider the `Chapter05/example5.py` file, which contains the current request
    logic of our ping test application but has a different URL list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Here we have a URL that will take around 20 seconds to return a response. Considering
    that we will block the main program until all threads finish their execution (with
    the `join()` method), our program will most likely appear to be hanging for 20
    seconds before any response is printed out.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the program to experience this for yourself. A 20 second delay will occur
    (which will make the execution take significantly longer to finish) and we will
    obtain the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Timeout specifications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An efficient ping test application should not be waiting for responses from
    its websites for a long time; it should have a set threshold for timeout that,
    if a server fails to return a response under that threshold, the application will
    deem that server non-responsive. We therefore need to implement a way to keep
    track of how much time has passed since a request is sent to a server. We will
    do this by counting down from the timeout threshold and, once that threshold is
    passed, all responses (whether returned or not yet returned) will be printed out.
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally, we will also be keeping track of how many requests are still
    pending and have not had their responses returned. We will be using the `isAlive()`
    method from the `threading.Thread` class to indirectly determine whether a response
    has been returned for a specific request: if, at one point, the thread processing
    a specific request is alive, we can conclude that that specific request is still
    pending.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Navigate to the `Chapter05/example6.py` file and consider the `process_requests()`
    function first:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The function takes in a list of threads that we have been using to make web
    requests in the previous examples, as well as an optional argument specifying
    the timeout threshold. Inside this function, we have an inner function, `alive_count()`,
    which returns the count of the threads that are still alive at the time of the
    function call.
  prefs: []
  type: TYPE_NORMAL
- en: In the `process_requests()` function, as long as there are threads that are
    currently alive and processing requests, we will allow the threads to continue
    with their execution (this is done in the `while` loop with the double condition).
    The `UPDATE_INTERVAL` variable, as you can see, specifies how often we check for
    this condition. If either condition fails (if there are no alive threads left
    or if the threshold timeout is passed), then we will proceed with printing out
    the responses (even if some might have not been returned).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s turn our attention to the new `MyThread` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: This class is almost identical to the one we considered in the previous example,
    except that the initial value for the `result` attribute is a message indicating
    a timeout. In the case that we discussed earlier where the timeout threshold specified
    in the `process_requests()` function is passed, this initial value will be used
    when the responses are printed out.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let''s consider our main program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Here, in our URL list, we have a request that would take 4 seconds and another
    that would take 20 seconds, aside from the ones that would respond immediately.
    As the timeout threshold that we are using is 5 seconds, theoretically we should
    be able to see that the 4-second-delay request will successfully obtain a response,
    while the 20-second-delay one will not.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is another point to be made about this program: daemon threads. In the
    `process_requests()` function, if the timeout threshold is passed while there
    is still at least one thread processing, then the function will proceed to print
    out the `result` attribute of each thread:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: This means that we do not block our program until all of the threads have finished
    their execution by using the `join()` function, and the program therefore can
    simply move forward if the timeout threshold is reached. However, this means that
    the threads themselves do not terminate at this point. The 20-second-delay request,
    specifically, will still most likely be running after our program exits out of
    the `process_requests()` function.
  prefs: []
  type: TYPE_NORMAL
- en: If the thread processing this request is not a daemon thread (as we know, daemon
    threads execute in the background and never terminate), it will block the main
    program from finishing until the thread itself finishes. By making this thread,
    and any other thread, a daemon thread, we allow the main program to finish as
    soon as it executes the last line of its instructions, even if there are threads
    still running.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us see this program in action. Execute the code and your output should
    be similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, it took around 5 seconds for our program to finish this time.
    This is because it spent 5 seconds waiting for the threads that were still running
    and, as soon as the 5-second threshold was passed, the program printed out the
    results. Here we see that the result from the 20-second-delay request was simply
    the default value of the `result` attribute of the `MyThread` class, while the
    rest of the requests were able to obtain the correct response from the server
    (including the 4-second-delay request, since it had enough time to obtain the
    response).
  prefs: []
  type: TYPE_NORMAL
- en: 'If you would like to see the effect of non-daemon threads that we discussed
    earlier, simply comment out the corresponding line of code in our main program,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: You will see that the main program will hang for around 20 seconds, as the non-daemon
    thread processing the 20-second-delay request is still running, before being able
    to finish its execution (even though the output produced will be identical).
  prefs: []
  type: TYPE_NORMAL
- en: Good practices in making web requests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are a few aspects of making concurrent web requests that require careful
    consideration and implementation. In this section, we will be going over those
    aspects and some of the best practices that you should use when developing your
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: Consider the terms of service and data-collecting policies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unauthorized data collection has been the topic of discussion in the technology
    world for the past few years, and it will continue to be for a long time—and for
    good reason too. It is therefore extremely important for developers who are making
    automated web requests in their applications to look for websites' policies on
    data collecting. You can find these policies in their terms of service or similar
    documents. When in doubt, it is generally a good rule of thumb to contact the
    website directly to ask for more details.
  prefs: []
  type: TYPE_NORMAL
- en: Error handling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Error is something that no one can easily avoid in the field of programming,
    and this is especially true in making web requests. Errors in these programs can
    include making bad requests (invalid requests or even bad internet connections),
    mishandling downloaded HTML code, or unsuccessfully parsing HTML code. It is therefore
    important to make use of `try...except` blocks and other error-handling tools
    in Python to avoid crashing your application. Avoiding crashes is especially important
    if your code/applications are used in production and larger applications.
  prefs: []
  type: TYPE_NORMAL
- en: Specifically in concurrent web scraping, it might be possible for some threads
    to collect data successfully, while others fail. By implementing error-handling
    functionalities in multithreaded parts of your program, you can make sure that
    a failed thread will not be able to crash the entirety of your program and ensure
    that successful threads can still return their results.
  prefs: []
  type: TYPE_NORMAL
- en: However, it is important to note that blind error-catching is still undesirable.
    This term indicates the practice where we have a large `try...expect` block in
    our program that will catch any and all errors that occur in the program execution,
    and no further information regarding the errors can be obtained; this practice
    might also be known as error swallowing. It's highly recommended to have specific
    error handling code in a program, so that not only appropriate actions can be
    taken with regards to that specific error, but other errors that have not been
    taken into account might also reveal themselves.
  prefs: []
  type: TYPE_NORMAL
- en: Update your program regularly
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is quite common for websites to change their request-handling logic as well
    as their displayed data regularly. If a program that makes requests to a website
    has considerably inflexible logic to interact with the server of the website (for
    example, structuring its requests in a specific format, only handling one kind
    of response), then if and when the website alters the way it handles its client
    requests, the program will most likely stop functioning correctly. This situation
    happens frequently with web scraping programs that look for data in specific HTML
    tags; when the HTML tags are changed, these programs will fail to find their data.
  prefs: []
  type: TYPE_NORMAL
- en: This practice is implemented to prevent automated data collecting programs from
    functioning. The only way to keep using a website that recently changed its request-handling
    logic is to analyze the updated protocols and alter our programs accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: Avoid making a large number of requests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Each time one of the programs that we have been discussing runs, it makes HTTP
    requests to a server that manages the site that you'd like to extract data from.
    This process happens significantly more frequently and over a shorter amount of
    time in a concurrent program, where multiple requests are being submitted to that
    server.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned before, servers nowadays have the ability to handle multiple requests
    simultaneously with ease. However, to avoid having to overwork and overconsume
    resources, servers are also designed to stop answering requests that come in too
    frequently. Websites of big tech companies, such as Amazon or Twitter, look for
    large amounts of automated requests that are made from the same IP address and
    implement different response protocols; some requests might be delayed, some might
    be refused a response, or the IP address might even be banned from making further
    requests for a specific amount of time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Interestingly, making repeated, heavy-duty requests to servers is actually
    a form of hacking a website. In **Denial of Service** (**DoS**) and **Distributed
    Denial of Service** (**DDoS**) attacks, a very large number of requests are made
    at the same time to the server, flooding the bandwidth of the targeted server
    with traffic, and as a result, normal, nonmalicious requests from other clients
    are denied because the servers are busy processing the concurrent requests, as
    illustrated in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/84cd0186-2860-4658-bc69-b0c6cf6a8fec.png)'
  prefs: []
  type: TYPE_IMG
- en: A of a DDoS attack
  prefs: []
  type: TYPE_NORMAL
- en: It is therefore important to space out the concurrent requests that your application
    makes to a server so that the application would not be considered an attacker
    and be potentially banned or treated as a malicious client. This could be as simple
    as limiting the maximum number of threads/requests that can be implemented at
    a time in your program or pausing the threading for a specific amount of time
    (for example, using the `time.sleep()` function) before making a request to the
    server.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have learned about the basics of HTML and web requests.
    The two most common web requests are `GET` and `POST` requests. There are five
    main categories for HTTP response status code, each indicating a different concept
    regarding the communication between the server and its client. By considering
    the status codes received from different websites, we can write a ping test application
    that effectively checks for the responsiveness of those websites.
  prefs: []
  type: TYPE_NORMAL
- en: Concurrency can be applied to the problem of making multiple web requests simultaneously
    via threading to provide a significant improvement in application speed. However,
    it is important to keep in mind a number of considerations when make concurrent
    web requests.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, we will start discussing another major player in concurrent
    programming: processes. We will be considering the concept of and the basic idea
    behind a process, and the options that Python provides for us to work with processes.'
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What is HTML?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are HTTP requests?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are HTTP response status codes?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How does the `requests` module help with making web requests?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is a ping test and how is one typically designed?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why is concurrency applicable in making web requests?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are the considerations that need to be made while developing applications
    that make concurrent web requests?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For more information, you can refer to the following links:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Automate the boring stuff with Python: practical programming for total beginners*, Al.
    Sweigart, No Starch Press, 2015'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Web Scraping with Python*, Richard Lawson, Packt Publishing Ltd, 2015'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Instant Web Scraping with Java*, Ryan Mitchell, Packt Publishing Ltd, 2013'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
