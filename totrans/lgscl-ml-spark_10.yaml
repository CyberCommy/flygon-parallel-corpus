- en: Chapter 10.  Configuring and Working with External Libraries
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter guides you on using external libraries to expand your data analysis
    to make the Spark more versatile. Examples will be given for deploying third-party-developed
    packages or libraries for machine learning applications with Spark core and ML/MLlib.
    We will also discuss how to compile and use external libraries with the core libraries
    of Spark for time series. As promised, we will also discuss how to configure SparkR
    to increase exploratory data manipulation and operations. In a nutshell, the following
    topics will be covered throughout this chapter:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: Third-party ML libraries with Spark
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using external libraries when deploying Spark ML on a cluster
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Time series analysis using the Spark-TS package of Cloudera
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuring SparkR with RStudio
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuring Hadoop run-time on Windows
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In order to provide a user-friendly environment for the developer, it is also
    possible to incorporate third-party APIs and libraries with Spark Core and other
    APIs such as Spark MLlib/ML, Spark Streaming, GraphX, and so on. Interested readers
    should refer to the following website that is listed on the Spark website as the
    **Third-Party Packages**: [https://spark-packages.org/](https://spark-packages.org/).'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: 'This website is a community index of third-party packages for Apache Spark.
    To date, there are total 252 packages registered on this site, as shown in *Table
    1*:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: '| **Domain** | **No. of packages** | **URL** |'
  id: totrans-9
  prefs: []
  type: TYPE_TB
- en: '| Spark core | 9 | [https://spark-packages.org/?q=tags%3A%22Core%22](https://spark-packages.org/?q=tags%3A%22Core%22)
    |'
  id: totrans-10
  prefs: []
  type: TYPE_TB
- en: '| Data sources | 39 | [https://spark-packages.org/?q=tags%3A%22Data%20Sources%22](https://spark-packages.org/?q=tags%3A%22Data%20Sources%22)
    |'
  id: totrans-11
  prefs: []
  type: TYPE_TB
- en: '| Machine learning | 55 | [https://spark-packages.org/?q=tags%3A%22Machine%20Learning%22](https://spark-packages.org/?q=tags%3A%22Machine%20Learning%22)
    |'
  id: totrans-12
  prefs: []
  type: TYPE_TB
- en: '| Streaming | 36 | [https://spark-packages.org/?q=tags%3A%22Streaming%22](https://spark-packages.org/?q=tags%3A%22Streaming%22)
    |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
- en: '| Graph processing | 13 | [https://spark-packages.org/?q=tags%3A%22Graph%22](https://spark-packages.org/?q=tags%3A%22Graph%22)
    |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
- en: '| Spark with Python | 5 | [https://spark-packages.org/?q=tags%3A%22PySpark%22](https://spark-packages.org/?q=tags%3A%22PySpark%22)
    |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
- en: '| Cluster deployment | 10 | [https://spark-packages.org/?q=tags%3A%22Deployment%22](https://spark-packages.org/?q=tags%3A%22Deployment%22)
    |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
- en: '| Data processing example | 18 | [https://spark-packages.org/?q=tags%3A%22Examples%22](https://spark-packages.org/?q=tags%3A%22Examples%22)
    |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
- en: '| Applications | 10 | [https://spark-packages.org/?q=tags%3A%22Applications%22](https://spark-packages.org/?q=tags%3A%22Applications%22)
    |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
- en: '| Tools | 24 | [https://spark-packages.org/?q=tags%3A%22Tools%22](https://spark-packages.org/?q=tags%3A%22Tools%22)
    |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
- en: '| Total Packages: 252 |   |   |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Third-party libraries for Spark based on application domain'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: Third-party ML libraries with Spark
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The 55 third-party machine learning libraries include libraries for neural data
    analysis, generalized clustering, streaming, topic modelling, feature selection,
    matrix factorization, distributed DataFrame for distributed ML, model matrix,
    Stanford Core NLP wrapper for Spark, social network analysis, deep learning module
    running, assembly of fundamental statistics, binary classifier calibration, and
    tokenizer for DataFrame.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: '*Table 2* provides a summary of the most useful packages based on use cases
    and application areas of machine learning. Interested readers should visit the
    respective websites for more insights:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: '| **Third party ML library for Spark** | **Use cases** |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
- en: '| thunderScalaNetwork | Neural networkLarge-scale neural data analysis with
    Spark where the neural network implementation is done with Scala. |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
- en: '| generalized-kmeans-clusteringpatchworkbisecting-kmeansspark-knn | ClusteringThis
    project generalizes the Spark MLLIB K-means cluster to support arbitrary distance
    functions.Highly scalable grid-density clustering algorithm for Spark MLlib.This
    is a prototype implementation of Bisecting K-Means Clustering on Spark.k-nearest
    neighbors algorithm on Spark. |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
- en: '| spark-ml-streamingstreaming-matrix-factorizationtwitter-stream-ml | StreamingVisualize
    streaming machine learning in Spark.Streaming Recommendation engine using matrix
    factorization with user and product bias.Machine learning over Twitter''s stream.
    Using Apache Spark, Web Server and Lightning Graph server. |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
- en: '| pipeline | Docker-based pipeliningEnd-to-End, real-time, advanced analytics
    big data reference pipeline using Spark, Spark SQL, Spark Streaming, ML, MLlib,
    GraphX, Kafka, Cassandra, Redis, Apache Zeppelin, Spark-Notebook, iPython/Jupyter
    Notebook, Tableau, H2O Flow, and Tachyon. |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
- en: '| dllibCaffeOnSpark`dl4j-spark-ml` | Deep learningdllib is a deep learning
    tool running on Apache Spark. Users need to download the tools as .jar and then
    can integrate with Spark and  develop deep-learning-based applications.CaffeOnSpark
    is a scalable deep learning running with the Spark executors. It is based on peer-to-peer
    (P2P) communication. `dl4j-spark-ml` can be used to develop deep-learning-based
    ML applications by integrating with Spark ML. |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
- en: '| kNN_ISsparkboostspark-calibration | ClassificationkNN-IS: An Iterative Spark-based
    design of the k-Nearest Neighbours classifier for big data.A distributed implementation
    of AdaBoost.MH and MP-Boost using Apache Spark.Assesses binary classifier calibration
    (that is, how well classifier outputs match observed class proportions) in Spark.
    |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
- en: '| Zen | RegressionZen provides a platform for large-scale and efficient machine
    learning on top of Spark. For example, logistic regression, linear regression,
    Latent Dirichlet Allocation (LDA), factorization machines and Deep Neural Network
    (DNN) are implemented in the current release. |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
- en: '| modelmatrixspark-infotheoretic-feature-selection | Feature engineeringspark-infotheoretic-feature-selection
    tools provide an alternative to Spark for developing large-scale machine learning
    applications. They provides robust feature engineering through the pipelining
    including the feature extractors, feature selectors. It is focused on building
    sparse feature-vector-based pipelines.On the other hand, it can be used as a feature
    selection framework based on Information Theory. Algorithms based on Information
    Theory include mRMR, InfoGain, JMI, and other commonly used FS filters. |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
- en: '| spark-knn-graphs | Graph processingSpark algorithms for building and processing
    k-nn graphs |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
- en: '| TopicModeling | Topic modellingDistributed Topic Modelling on Apache Spark
    |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
- en: '| Spark.statistics | StatisticsApart from SparkR, Spark.statistics works as
    an assembler of basic statistics implementation based on the Spark core |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Summary of the most useful third-party packages based on use cases
    and application areas of machine learning with Spark'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: Using external libraries with Spark Core
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to work with these external libraries, instead of placing the jars
    in any specific folder, a simple fix would be to start the `pyspark` shell or
    spark-shell with the following arguments:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This will automatically load the required `spark-csv` jars. However, these
    two jar files have to be downloaded to the Spark distribution using the following
    command in Ubuntu:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Then, to create an active Spark session, use the following line of codes:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Once you have instantiated an active Spark session, use the following lines
    of code to read the csv input file:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Note that here we define the `com.databricks.spark.csv` input format by using
    the `format()` method, dedicatedly developed by Databricks for faster CSV file
    reading and parsing, and by setting the auxiliary option for the header as true
    using the `option()` method. Finally, the `load()` method loads the input data
    from the `input/letterdata.data` location, for example.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: As a continuation, in the next section, we will discuss configuring the Spark-TS
    library for time series data analysis.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Interested readers should visit the third-party ML packages web page for Spark
    at [https://spark-packages.org/?q=tags%3A%22Machine%20Learning%22](https://spark-packages.org/?q=tags%3A%22Machine%20Learning%22)
    for the package-specific discussion, updates, and configuration procedures.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: Time series analysis using the Cloudera Spark-TS package
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As discussed in *[Chapter 9](part0073_split_000.html#25JP22-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 9.  Advanced Machine Learning with Streaming and Graph Data")*, *Advanced
    Machine Learning with Streaming and Graph Data*, we will see how to configure
    the Spark-TS package developed by Cloudera. Mainly, we will talk about the TimeSeriesRDD
    in this section.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: Time series data
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Time series data consists of sequences of measurements, each occurring at a
    point in time. A variety of terms are used to describe time series data, and many
    of them apply to conflicting or overlapping concepts. In the interest of clarity,
    in Spark-TS, Cloudera sticks to a particular vocabulary. Three objects are important
    in time series data analysis: time series, instant, and observation:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: A time series is a sequence of real (that is, floating-point) values, each linked
    to a specific timestamp. Particularly, this sticks with time series as meaning
    a univariate time series. In Scala, a time series is usually represented by a
    Breeze presented at [https://github.com/scalanlp/breeze](https://github.com/scalanlp/breeze)
    vector, and in Python, a 1-D NumPy array (refer to [http://www.numpy.org/](http://www.numpy.org/)
    for more), and has a `DateTimeIndex` as shown at [https://github.com/sryza/spark-timeseries/blob/master/src/main/scala/com/cloudera/sparkts/DateTimeIndex.scala](https://github.com/sryza/spark-timeseries/blob/master/src/main/scala/com/cloudera/sparkts/DateTimeIndex.scala).
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On the other hand, an instant is the vector of values in a collection of time
    series corresponding to a single point in time. In the Spark-TS library, each
    time series is typically labeled with a key that enables it to be identified among
    a collection of time series.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, an observation is a tuple of (timestamp, key, value), that is, a single
    value in a time series or instant.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, not all data with timestamps are time series data. For example, logs
    don't fit directly into time series since they consist of discrete events, not
    scalar measurements taken at intervals. However, measurements of log messages
    per hour would constitute a time series.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: Configuring Spark-TS
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The most straightforward way to access Spark-TS from Scala is to depend on
    it in a Maven project. Do this by including the following repo in `pom.xml`:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'To get the raw `pom.xml` file, interested readers should go to the following
    URL:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/sryza/spark-timeseries/blob/master/pom.xml](https://github.com/sryza/spark-timeseries/blob/master/pom.xml)'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: 'Alternatively, to access it in a spark-shell, download the JAR from [https://repository.cloudera.com/cloudera/libs-release-local/com/cloudera/sparkts/sparkts/0.1.0/sparkts-0.1.0-jar-with-dependencies.jar](https://repository.cloudera.com/cloudera/libs-release-local/com/cloudera/sparkts/sparkts/0.1.0/sparkts-0.1.0-jar-with-dependencies.jar),
    and then launch the shell with the following command as discussed in the *Using
    external libraries with Spark Core* section in this chapter:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: TimeSeriesRDD
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: According to the Spark-TS engineering blog written on the Cloudera website at
    [http://blog.cloudera.com/blog/2015/12/spark-ts-a-new-library-for-analyzing-time-series-data-with-apache-spark/](http://blog.cloudera.com/blog/2015/12/spark-ts-a-new-library-for-analyzing-time-series-data-with-apache-spark/),
    TimeSeriesRDD is central to Spark-TS, where each object in the RDD stores a full
    univariate series. Operations that tend to apply exclusively to time series are
    much more efficient. For example, if you want to generate a set of lagged time
    series from your original collection of time series, each lagged series can be
    computed just by looking at a single record in the input RDD.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, with imputing missing values based on surrounding values, or fitting
    time series models to each series, all of the data needed is present in a single
    array. Therefore, the central abstraction of the Spark-TS library is TimeSeriesRDD,
    which is simply a collection of time series on which you can operate in a distributed
    fashion. This approach allows you to avoid storing timestamps for each series
    and instead store a single `DateTimeIndex` to which all the series vectors conform.
    `TimeSeriesRDD[K]` extends `RDD[(K, Vector[Double])]`, where K is the key type
    (usually a string), and the second element in the tuple is a Breeze vector representing
    the time series.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: 'A more technical discussion can be found in the GitHub URL: [https://github.com/sryza/spark-timeseries](https://github.com/sryza/spark-timeseries).
    Since this is a Third Party Package, a detailed discussion is out of the scope
    of this book, we believe.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: Configuring SparkR with RStudio
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s assume you have RStudio installed on your machine. Follow the steps
    mentioned here:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: 'Now open RStudio and create a new R script; then write the following code:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Load the necessary package for SparkR by using this code:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Configure the SparkR environment as follows:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now let''s create the first DataFrame and print the first few rows, as follows:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'You might need to install the following packages in order to make the `devtools`
    package work:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Morever, you might need to install `libcurl` for RCurl, which devtools depends
    on. To do this, just run this command:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now configure the `ggplot2.SparkR` package from GitHub using the following
    code:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now let''s compute the skewness and kurtosis for the sample DataFrame that
    we have just created. Before that, load the necessary packages:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Let''s create the DataFrame for the daily exercise example shown in the *Feature
    engineering and data exploration* section in *[Chapter 4](part0038_split_000.html#147LC2-5afe140a04e845e0842b44be7971e11a
    "Chapter 4. Extracting Knowledge through Feature Engineering")*, *Extracting Knowledge
    through Feature Engineering*, and show the first few rows using `head` command:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now calculate the skewness and kurtosis, as follows:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: You are probably aware that we used the two terms `skewness` and `kurtosis`
    in [Chapter 4](part0038_split_000.html#147LC2-5afe140a04e845e0842b44be7971e11a
    "Chapter 4. Extracting Knowledge through Feature Engineering"), *Extracting Knowledge
    through Feature Engineering*. If you are not familiar with these two terms, here
    is a bit of definition of them. Well, from the statistical perspective, `skewness`
    is a measure of symmetry. Alternatively and more precisely, it signifies the lack
    of symmetry in a distribution of the dataset.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: Now you might be wondering what symmetric is. Well, a distribution of the dataset
    is symmetric if it looks the same to the left and right of the center point.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: 'Kurtosis, on the other hand, is a measure of whether the data are heavy-tailed
    or light-tailed relative to a normal distribution:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let''s plot the density plot graph by calling the `ggplot()` method
    of the `ggplot2.SparkR` package:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: If you are not familiar with the `ggplot2` R package, note that `ggplot2` is
    a plotting system for R based on the grammar of graphics of base and lattice graphics.
    It provides many fiddly details of the graphics that make plotting a hassle, for
    example, placing or drawing legends in a graph, as well as providing a powerful
    model of graphics. This will make your life easier in order to produce simple
    as well as complex multi-layered graphics.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'More info about `ggplot2` and its documentation can be found at the following
    website: [http://docs.ggplot2.org/current/](http://docs.ggplot2.org/current/).'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: Configuring Hadoop run-time on Windows
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you are developing your machine learning application on windows using Eclipse
    (as Maven project of course), probably you will face a problem since Spark expects
    that there is a runtime environment for Hadoop on Windows too.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: 'More specifically, suppose you are running a Spark project written in Java
    with main class as `JavaNaiveBayes_ML.java`, then you will experience an IO exception
    saying that:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![Configuring Hadoop run-time on Windows](img/00125.jpeg)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: IO exception due to the missing Hadoop runtime'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: The reason is that by default Hadoop is developed for the Linux environment
    and if you are developing your Spark applications on windows platform, a bridge
    is required that will provide the Hadoop environment for the Hadoop runtime for
    Spark to be properly executed.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: Now, how to get rid of this problem then? The solution is straight forward.
    As the error message says, we need to have an executable namely `winutils.exe`.
    Now download the `winutils.exe` file from the code directory of Packt for this
    chapter and copy and paste it in the Spark distribution directory and configure
    Eclipse.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: More specifically, suppose your Spark distribution containing Hadoop is located
    at `C:/Users/spark-2.0.0-bin-hadoop2.7`. Inside the Spark distribution there is
    a directory named `bin.` Now, paste the executable there (that is, `path = C:/Users/spark-2.0.0-binhadoop2.7/``bin/`).
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: The second phase of the solution is going to Eclipse, select the main class
    (that is, `JavaNaiveBayes_ML.java` in this case), and then go to the **Run** menu.
    From the **Run** menu go to the **Run Configurations** option and from this option
    select the **Environment** tab. If you select the tab, you a will have the option
    to create a new environmental variable for Eclipse suing the JVM.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: Now create a new environmental variable and put the value as `C:/Users/spark-2.0.0-bin-hadoop2.7/`.
    Now press on **apply** and re-run your application and your problem should be
    resolved.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: 'More technically, the details of the IO exception can be described as follows
    in Figure 1:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we showed how to use external libraries with Spark to expand
    data analyses.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: More and more Spark as well as third-party packages are being developed by open
    source contributors. Readers should be updated with the latest news and release
    on the Spark website. They also should be notified of about the latest machine
    learning APIs, since the development of Spark is continuous and innovative and,
    of course, sometimes after a certain package becomes obsolete or deprecated.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: Throughout this book, we have tried to guide you on how to use the most popular
    and widely used machine learning algorithms that have been developed by Spark.
    However, there are other algorithms too that we could not discuss, and more and
    more algorithms will be added to the Spark ML and MLlib packages.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: This is more or less the end of our little journey with Spark. Now a general
    suggestion from our side to you as readers, or if you are relatively new to machine
    learning, Java, or Spark at first try to understand whether a problem is really
    a machine learning problem. If it is a machine learning problem, try to guess
    what type of learning algorithms should be the best fit, that is, classification,
    clustering, regression, recommendation, or frequent pattern mining.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: Then define and formulate the problem. After that you should generate or download
    the appropriate data based on the feature engineering concept of Spark that we
    have discussed. Then you can select an ML model that will provide better results
    in terms of accuracy. However, as discussed earlier, the model selection really
    depends on your data and problem type.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 然后定义和规划问题。之后，您应该基于我们讨论过的Spark的特征工程概念生成或下载适当的数据。然后，您可以选择一个ML模型，该模型将在准确性方面提供更好的结果。但是，正如前面讨论的那样，模型选择确实取决于您的数据和问题类型。
- en: Now that you have your data ready to train the model, go straight to train the
    model towards making predictive analytics.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经准备好训练模型的数据，可以直接开始训练模型以进行预测分析。
- en: When your model is trained, evaluate it to see how it goes and fulfills your
    prediction expectations. Well, if you are not happy with the performance, try
    changing to other ML algorithms towards model selection. As discussed in [Chapter
    7](part0059_split_000.html#1O8H62-0b803698e2de424b8aa3c56ad52b005d "Chapter 7. Tuning
    Machine Learning Models"), *Tuning Machine Learning Models*, even proper model
    selection cannot provide the best result sometimes because of the nature of the
    data you have.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 当您的模型训练好后，评估它以查看其表现并满足您的预测期望。如果您对性能不满意，请尝试切换到其他ML算法以进行模型选择。正如在[第7章](part0059_split_000.html#1O8H62-0b803698e2de424b8aa3c56ad52b005d
    "第7章。调整机器学习模型")中讨论的那样，*调整机器学习模型*，即使适当的模型选择有时也无法提供最佳结果，因为您拥有的数据的性质。
- en: So what is to be done? It's simple. Tune your ML model using the available tuning
    algorithms to properly set the hyperparameters. You might also need to make your
    model adaptable for new data types, especially if you are developing an ML application
    for a dynamic environment such as time series analysis or streaming analytics.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 那么应该做什么呢？很简单。使用可用的调整算法来调整您的ML模型，以正确设置超参数。您可能还需要使您的模型适应新的数据类型，特别是如果您正在为时间序列分析或流式分析等动态环境开发ML应用程序。
- en: Finally, deploy your model and you have it as a robust ML application.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，部署您的模型，您将拥有一个强大的ML应用程序。
- en: Our final recommendation to the readers is to browse the Spark website (at [http://spark.apache.org/](http://spark.apache.org/))
    regularly to get updates and also try to incorporate the regular Spark provided
    APIs with other third-party applications to get the best result of the collaboration.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最终向读者推荐定期浏览Spark网站（位于[http://spark.apache.org/](http://spark.apache.org/)），以获取更新，并尝试将常规提供的Spark
    API与其他第三方应用程序结合起来，以获得合作的最佳结果。
- en: '***This eBook was posted by AlenMiler on AvaxHome!***'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这本电子书是由AlenMiler在AvaxHome上发布的！
- en: '***Many New eBooks in my Blog:*** [http://avxhome.in/blogs/AlenMiler](https://tr.im/fgrfegtr)'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '***我的博客中有许多新的电子书：*** [http://avxhome.in/blogs/AlenMiler](https://tr.im/fgrfegtr)'
- en: '***Mirror:*** [https://avxhome.unblocked.tw/blogs/AlenMiler](https://tr.im/geresttre)'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '***镜像：*** [https://avxhome.unblocked.tw/blogs/AlenMiler](https://tr.im/geresttre)'
