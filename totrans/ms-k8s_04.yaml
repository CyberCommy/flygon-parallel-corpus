- en: High Availability and Reliability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we looked at monitoring your Kubernetes cluster, detecting
    problems at the node level, identifying and rectifying performance problems, and
    general troubleshooting.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will dive into the topic of highly available clusters. This
    is a complicated topic. The Kubernetes project and the community haven't settled
    on one true way to achieve high-availability nirvana. There are many aspects to
    highly available Kubernetes clusters, such as ensuring that the control plane
    can keep functioning in the face of failures, protecting the cluster state in
    `etcd`, protecting the system's data, and recovering capacity and/or performance
    quickly. Different systems will have different reliability and availability requirements.
    How to design and implement a highly available Kubernetes cluster will depend
    on those requirements.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you will understand the various concepts associated
    with high availability and be familiar with Kubernetes high availability best
    practices and when to employ them. You will be able to upgrade live clusters using
    different strategies and techniques, and you will be able to choose between multiple
    possible solutions based on trade-offs between performance, cost, and availability.
  prefs: []
  type: TYPE_NORMAL
- en: High-availability concepts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will start our journey into high availability by exploring
    the concepts and building blocks of reliable and highly available systems. The
    million (trillion?) dollar question is how do we build reliable and highly available
    systems from unreliable components? Components will fail, you can take that to
    the bank; hardware will fail; networks will fail; configuration will be wrong;
    software will have bugs; people will make mistakes. Accepting that, we need to
    design a system that can be reliable and highly available even when components
    fail. The idea is to start with redundancy, detect component failure, and replace
    bad components quickly.
  prefs: []
  type: TYPE_NORMAL
- en: Redundancy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Redundancy** is the foundation of reliable and highly available systems at
    the hardware and data levels. If a critical component fails and you want the system
    to keep running, you must have another identical component ready to go. Kubernetes
    itself takes care of your stateless pods through replication controllers and replica
    sets. However, your cluster state in `etcd` and the master components themselves
    need redundancy to function when some components fail. In addition, if your system''s
    tasteful components are not backed up by redundant storage (for example, on a
    cloud platform), then you need to add redundancy to prevent data loss.'
  prefs: []
  type: TYPE_NORMAL
- en: Hot swapping
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Hot swapping** is the concept of replacing a failed component on the fly
    without taking the system down, with minimal (ideally, zero) interruption to users.
    If the component is stateless (or its state is stored in separate redundant storage),
    then hot swapping a new component to replace it is easy and just involves redirecting
    all clients to the new component. However, if it stores local state, including
    in memory, then hot swapping is important. There are the following two main options:'
  prefs: []
  type: TYPE_NORMAL
- en: Give up on in-flight transactions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keep a hot replica in sync
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first solution is much simpler. Most systems are resilient enough to cope
    with failures. Clients can retry failed requests, and the hot-swapped component
    will service them.
  prefs: []
  type: TYPE_NORMAL
- en: The second solution is more complicated and fragile, and will incur a performance
    overhead because every interaction must be replicated to both copies (and acknowledged).
    It may be necessary for some parts of the system.
  prefs: []
  type: TYPE_NORMAL
- en: Leader election
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Leader or master election is a common pattern in distributed systems. You often
    have multiple identical components that collaborate and share the load, but one
    component is elected as the leader and certain operations are serialized through
    the leader. You can think of distributed systems with leader election as a combination
    of redundancy and hot swapping. The components are all redundant and, when the
    current leader fails or becomes unavailable, a new leader is elected and hot-swapped
    in.
  prefs: []
  type: TYPE_NORMAL
- en: Smart load balancing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Load balancing is about distributing the workload across multiple components
    that service incoming requests. When some components fail the load balancer must
    first stop sending requests to failed or unreachable components. The second step
    is to provision new components to restore capacity and update the load balancer.
    Kubernetes provides great facilities to support this through services, endpoints,
    and labels.
  prefs: []
  type: TYPE_NORMAL
- en: Idempotency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many types of failure can be temporary. This is most common with networking
    issues or with too-stringent timeouts. A component that doesn't respond to a health
    check will be considered unreachable, and another component will take its place.
    Work that was scheduled to the presumably failed component may be sent to another
    component, but the original component may still be working and complete the same
    work. The end result is that the same work may be performed twice. It is very
    difficult to avoid this situation. To support exactly once semantics, you need
    to pay a heavy price in overhead, performance, latency, and complexity. Thus,
    most systems opt to support at least once semantics, which means it is OK for
    the same work to be performed multiple times without violating the system's data
    integrity. This property is named idempotency. Idempotent systems maintain their
    state if an operation is performed multiple times.
  prefs: []
  type: TYPE_NORMAL
- en: Self-healing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When component failures occur in dynamic systems, you usually want the system
    to be able to heal itself. Kubernetes replication controllers and replica sets
    are great examples of self-healing systems, but failure can extend well beyond
    pods. In the previous chapter, we discussed resource monitoring and node problem
    detection. The remedy controller is a great example of the concept of self-healing.
    Self-healing starts with automated detection of problems followed by automated
    resolution. Quotas and limits help create checks and balances to ensure an automated
    self-healing doesn't run amok due to unpredictable circumstances such as DDOS
    attacks.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we considered various concepts involved in creating reliable
    and highly available systems. In the next section, we will apply them and demonstrate
    best practices for systems deployed on Kubernetes clusters.
  prefs: []
  type: TYPE_NORMAL
- en: High-availability best practices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Building reliable and highly available distributed systems is an important endeavor.
    In this section, we will check some of the best practices that enable a Kubernetes-based
    system to function reliably and be available in the face of various failure categories.
  prefs: []
  type: TYPE_NORMAL
- en: Creating highly available clusters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To create a highly available Kubernetes cluster, the master components must
    be redundant. This means that `etcd` must be deployed as a cluster (typically
    across three or five nodes) and the Kubernetes API server must be redundant. Auxiliary
    cluster management services, such as Heapster''s storage, may be deployed redundantly
    too, if necessary. The following diagram depicts a typical reliable and highly
    available Kubernetes cluster. There are several load-balanced master nodes, each
    one containing whole master components as well as an `etcd` component:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/9a4726b1-7f50-4611-bed4-91d7663833b8.png)'
  prefs: []
  type: TYPE_IMG
- en: This is not the only way to configure highly available clusters. You may prefer,
    for example, to deploy a standalone `etcd` cluster to optimize the machines to
    their workload or if you require more redundancy for your `etcd` cluster than
    the rest of the master nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Self-hosted Kubernetes where control plane components are deployed as pods and
    stateful sets in the cluster is a great approach to simplify the robustness, disaster
    recovery, and self-healing of the control plane components by applying Kubernetes
    to Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Making your nodes reliable
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Nodes will fail, or some components will fail, but many failures are transient.
    The basic guarantee is to make sure that the Docker daemon (or whatever the CRI
    implementation is) and the Kubelet restart automatically in case of a failure.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you run CoreOS, a modern Debian-based OS (including Ubuntu >= 16.04), or
    any other OS that uses `systemd` as its `init` mechanism, then it''s easy to deploy
    `Docker` and the `kubelet` as self-starting daemons:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: For other operating systems, the Kubernetes project selected monit for their
    high-availability example, but you can use any process monitor you prefer.
  prefs: []
  type: TYPE_NORMAL
- en: Protecting your cluster state
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Kubernetes cluster state is stored in `etcd`. The `etcd` cluster was designed
    to be super reliable and distributed across multiple nodes. It's important to
    take advantage of these capabilities for a reliable and highly available Kubernetes
    cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering etcd
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You should have at least three nodes in your etcd cluster. If you need more
    reliability and redundancy, you can go to five, seven, or any other odd number
    of nodes. The number of nodes must be odd to have a clear majority in case of
    a network split.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to create a cluster, the `etcd` nodes should be able to discover each
    other. There are several methods to accomplish that. I recommend using the excellent
    `etcd-operator` from CoreOS:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/9f1020a1-9e4e-4d18-af97-d17c4af9f0dd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The operator takes care of many complicated aspects of `etcd` operation, such
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: Create and destroy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Resize
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Failover
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rolling upgrade
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Backup and restore
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing the etcd operator
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The easiest way to install the `etcd-operator` is using Helm-the Kubernetes
    package manager. If you don't have Helm installed yet, follow the instructions
    given at [https://github.com/kubernetes/helm#install](https://github.com/kubernetes/helm#install).
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, initialize `helm`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We will dive deep into Helm in [Chapter 13](74a06d16-4c3d-40b5-b87f-1f74e8c8ec25.xhtml),
    *Handling the Kubernetes Package Manager*. For now, we''ll just use it to install
    the `etcd` operator. On Minikube 0.24.1, which supports Kubernetes 1.8 (although
    Kubernetes 1.10 is already out), there are some permission issues out of the box.
    To overcome these issues, we need to create some roles and role bindings. Here
    is the `rbac.yaml` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'You can apply it like any other Kubernetes manifest:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can finally install the `etcd-operator`. I use `x` as a short release
    name to make the output less verbose. You may want to use more meaningful names:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Creating the etcd cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Save the following to `etcd-cluster.yaml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'To create the cluster type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Verifying the etcd cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once the `etcd` cluster is up and running, you can access the `etcdctl` tool
    to check on the cluster status and health. Kubernetes lets you execute commands
    directly inside pods or container through the `exec` command (similar to Docker
    exec).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is how to check whether the cluster is healthy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is to how to set and get key-value pairs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Yeah, it works!
  prefs: []
  type: TYPE_NORMAL
- en: Protecting your data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Protecting the cluster state and configuration is great, but even more important
    is protecting your own data. If somehow the cluster state gets corrupted, you
    can always rebuild the cluster from scratch (although the cluster will not be
    available during the rebuild). But if your own data is corrupted or lost, you're
    in deep trouble. The same rules apply; redundancy is king. However, while the
    Kubernetes cluster state is very dynamic, much of your data may be less dynamic.
    For example, a lot of historic data is often important and can be backed up and
    restored. Live data might be lost, but the overall system may be restored to an
    earlier snapshot and suffer only temporary damage.
  prefs: []
  type: TYPE_NORMAL
- en: Running redundant API servers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The API servers are stateless, fetching all the necessary data on the fly from
    the `etcd` cluster. This means that you can easily run multiple API servers without
    needing to coordinate between them. Once you have multiple API servers running,
    you can put a load balancer in front of them to make it transparent to clients.
  prefs: []
  type: TYPE_NORMAL
- en: Running leader election with Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Some master components, such as the scheduler and the controller manager, can't
    have multiple instances active at the same time. This will be chaos, as multiple
    schedulers try to schedule the same pod into multiple nodes or multiple times
    into the same node. The correct way to have a highly-scalable Kubernetes cluster
    is to have these components run in leader election mode. This means that multiple
    instances are running, but only one is active at a time, and if it fails, another
    one is elected as leader and takes its place.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes supports this mode through the `leader-elect` flag. The scheduler
    and the controller manager can be deployed as pods by copying their respective
    manifests to `/etc/kubernetes/manifests`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a snippet from a scheduler manifest that shows the use of the flag:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is a snippet from a controller manager manifest that shows the use of
    the flag:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Note that it is not possible to have these components restarted automatically
    by Kubernetes like other pods because these are exactly the Kubernetes components
    responsible for restarting failed pods, so they can't restart themselves if they
    fail. There must be a ready-to-go replacement already running.
  prefs: []
  type: TYPE_NORMAL
- en: Leader election for your application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Leader election can be very useful for your application too, but it is notoriously
    difficult to implement. Luckily, Kubernetes comes to the rescue. There is a documented
    procedure for supporting leader election for your application through the `leader-elector`
    container from Google. The basic concept is to use the Kubernetes endpoints combined
    with `ResourceVersion` and `Annotations`. When you couple this container as a
    sidecar in your application pod, you get leader-election capabilities in a very
    streamlined fashion.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s run the `leader-elector` container with three pods and an election called
    election:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'After a while, you''ll see three new pods in your cluster, named `leader-elector-xxx`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'OK. But who is the master? Let''s query the election endpoints:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'If you look really hard, you can see it buried in the `metadata.annotations`.
    To make it easy to detect, I recommend the fantastic `jq` program for slicing
    and dicing JSON ([https://stedolan.github.io/jq/](https://stedolan.github.io/jq/)).
    It is very useful to parse the output of the Kubernetes API or `kubectl`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'To prove that leader election works, let''s kill the leader and see if a new
    leader is elected:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'And we have a new leader:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also find the leader through HTTP, because each `leader-elector` container
    exposes the leader through a local web server (running on port `4040`) though
    a proxy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The local web server allows the leader-elector container to function as a sidecar
    container to your main application container within the same pod. Your application
    container shares the same local network as the `leader-elector` container, so
    it can access `http://localhost:4040` and get the name of the current leader.
    Only the application container that shares the pod with the elected leader will
    run the application; the other application containers in the other pods will be
    dormant. If they receive requests, they'll forward them to the leader, or some
    clever load-balancing tricks can be done to automatically send all requests to
    the current leader.
  prefs: []
  type: TYPE_NORMAL
- en: Making your staging environment highly available
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: High availability is important to set up. If you go to the trouble of setting
    up high availability, it means that there is a business case for a highly available
    system. It follows that you want to test your reliable and highly available cluster
    before you deploy it to production (unless you're Netflix, where you test in production).
    Also, any change to the cluster may, in theory, break your high availability without
    disrupting other cluster functions. The essential point is that, just like anything
    else, if you don't test it, assume it doesn't work.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ve established that you need to test reliability and high availability.
    The best way to do it is to create a staging environment that replicates your
    production environment as closely as possible. This can get expensive. There are
    several ways to manage the cost:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Ad hoc HA staging environment**: Create a large HA cluster only for the duration
    of HA testing'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Compress time**: Create interesting event streams and scenarios ahead of
    time, feed the input, and simulate the situations in rapid succession'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Combine HA testing with performance and stress testing**: At the end of your
    performance and stress tests, overload the system and see how the reliability
    and high-availability configuration handles the load'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing high availability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Testing high availability takes planning and a deep understanding of your system.
    The goal of every test is to reveal flaws in the system's design and/or implementation,
    and to provide good enough coverage that, if the tests pass, you'll be confident
    that the system behaves as expected.
  prefs: []
  type: TYPE_NORMAL
- en: In the realm of reliability and high availability, it means that you need to
    figure out ways to break the system and watch it put itself back together.
  prefs: []
  type: TYPE_NORMAL
- en: 'This requires several pieces, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: A comprehensive list of possible failures (including reasonable combinations)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For each possible failure, it should be clear how the system should respond
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A way to induce the failure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A way to observe how the system reacts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: None of the pieces are trivial. The best approach in my experience is to do
    it incrementally and try to come up with a relatively small number of generic
    failure categories and generic responses, rather than an exhaustive, ever-changing
    list of low-level failures.
  prefs: []
  type: TYPE_NORMAL
- en: For example, a generic failure category is node-unresponsive; the generic response
    could be rebooting the node. The way to induce the failure can be stopping the
    VM of the node (if it's a VM), and the observation should be that, while the node
    is down, the system still functions properly based on standard acceptance tests.
    The node is eventually up, and the system gets back to normal. There may be many
    other things you want to test, such as whether the problem was logged, relevant
    alerts went out to the right people, and various stats and reports were updated.
  prefs: []
  type: TYPE_NORMAL
- en: Note that, sometimes, a failure can't be resolved in a single response. For
    example, in our unresponsive node case, if it's a hardware failure, then reboot
    will not help. In this case, a second line of response comes into play and maybe
    a new VM is started, configured, and hooked up to the node. In this case, you
    can't be too generic, and you may need to create tests for specific types of pod/role
    that were on the node (etcd, master, worker, database, and monitoring).
  prefs: []
  type: TYPE_NORMAL
- en: If you have high-quality requirements, be prepared to spend much more time setting
    up the proper testing environments and the tests than even the production environment.
  prefs: []
  type: TYPE_NORMAL
- en: One last, important point is to try to be as nonintrusive as possible. This
    means that, ideally, your production system will not have testing features that
    allow shutting down parts of it or cause it to be configured to run in reduced
    capacity for testing. The reason is that it increases the attack surface of your
    system, and it can be triggered by accident by mistakes in configuration. Ideally,
    you can control your testing environment without resorting to modifying the code
    or configuration that will be deployed in production. With Kubernetes, it is usually
    easy to inject pods and containers with custom test functionality that can interact
    with system components in the staging environment, but will never be deployed
    in production.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we looked at what it takes to actually have a reliable and
    highly available cluster, including etcd, the API server, the scheduler, and the
    controller manager. We considered best practices for protecting the cluster itself
    as well as your data, and paid special attention to the issue of starting environments
    and testing.
  prefs: []
  type: TYPE_NORMAL
- en: Live cluster upgrades
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the most complicated and risky tasks involved in running a Kubernetes
    cluster is a live upgrade. The interactions between different parts of the system
    of different versions are often difficult to predict, but in many situations,
    it is required. Large clusters with many users can't afford to be offline for
    maintenance. The best way to attack complexity is to divide and conquer. Microservice
    architecture helps a lot here. You never upgrade your entire system. You just
    constantly upgrade several sets of related microservices, and if APIs have changed,
    then you upgrade their clients too. A properly-designed upgrade will preserve
    backward compatibility at least until all clients have been upgraded, and then
    deprecate old APIs across several releases.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will discuss how to go about upgrading your cluster using
    various strategies, such as rolling upgrades and blue-green upgrades. We will
    also discuss when it's appropriate to introduce breaking upgrades versus backward-compatible
    upgrades. Then, we will get into the critical topic of schema and data migration.
  prefs: []
  type: TYPE_NORMAL
- en: Rolling upgrades
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Rolling upgrades are upgrades where you gradually upgrade components from the
    current version to the next. This means that your cluster will run current and
    new components at the same time. There are two cases to consider here, where:'
  prefs: []
  type: TYPE_NORMAL
- en: New components are backward compatible
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: New components are not backward compatible
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If the new components are backward compatible, then the upgrade should be very
    easy. In earlier versions of Kubernetes, you had to manage rolling upgrades very
    carefully with labels and change the number of replicas gradually for both the
    old and new version (although `kubectl` rolling-update is a convenient shortcut
    for replication controllers). But, the deployment resource introduced in Kubernetes
    1.2 makes it much easier and supports replica sets. It has the following capabilities
    built-in:'
  prefs: []
  type: TYPE_NORMAL
- en: Running server-side (it keeps going if your machine disconnects)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Versioning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multiple concurrent rollouts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Updating deployments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Aggregating status across all pods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rollbacks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Canary deployments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multiple upgrade strategies (rolling upgrade is the default)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is a sample manifest for a deployment that deploys three NGINX pods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The resource kind is deployment, and it''s got the name `nginx-deployment`,
    which you can use to refer to this deployment later (for example, for updates
    or rollbacks). The most important part is, of course, the spec, which contains
    a pod template. The replicas determine how many pods will be in the cluster, and
    the template spec has the configuration for each container: in this case, just
    a single container.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To start the rolling update, you create the deployment resource:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'You can view the status of the deployment later, using:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Complex deployments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The deployment resource is great when you just want to upgrade one pod, but
    you may often need to upgrade multiple pods, and those pods sometimes have version
    interdependencies. In those situations, you sometimes must forego a rolling update
    or introduce a temporary compatibility layer. For example, suppose service A depends
    on service B. Service B now has a breaking change. The v1 pods of service A can't
    interoperate with the pods from service B v2\. It is also undesirable, from a
    reliability and change-management point of view, to make the v2 pods of service
    B support the old and new APIs. In this case, the solution may be to introduce
    an adapter service that implements the v1 API of the B service. This service will
    sit between A and B, and will translate requests and responses across versions.
    This adds complexity to the deployment process and requires several steps, but
    the benefit is that A and B services themselves are simple. You can do rolling
    updates across incompatible versions, and all indirection will go away once everybody
    upgrades to v2 (all A pods and all B pods).
  prefs: []
  type: TYPE_NORMAL
- en: Blue-green upgrades
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Rolling updates are great for availability, but, sometimes, the complexity involved
    in managing a proper rolling update is considered too high, or it adds a significant
    amount of work that pushes back more important projects. In these cases, blue-green
    upgrades provide a great alternative. With a blue-green release, you prepare a
    full copy of your production environment with the new version. Now you have two
    copies, old (blue) and new (green). It doesn't matter which one is blue and which
    one is green. The important thing is that you have two fully-independent production
    environments. Currently, blue is active and services all requests. You can run
    all your tests on green. Once you're happy, you flip the switch and green becomes
    active. If something goes wrong, rolling back is just as easy; just switch back
    from green to blue. I elegantly ignored the storage and in-memory state here.
    This immediate switch assumes that blue and green are composed of stateless components
    only and share a common persistence layer.
  prefs: []
  type: TYPE_NORMAL
- en: If there were storage changes or breaking changes to the API accessible to external
    clients, then additional steps need to be taken. For example, if blue and green
    have their own storage, then all incoming requests may need to be sent to both
    blue and green, and green may need to ingest historical data from blue to get
    in sync before switching.
  prefs: []
  type: TYPE_NORMAL
- en: Managing data-contract changes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data contracts describe how the data is organized. It's an umbrella term for
    structure metadata. A database schema is the most typical example. The most common
    example is a relational database schema. Other examples include network payloads,
    file formats, and even the content of string arguments or responses. If you have
    a configuration file, then this configuration file has both a file format (JSON,
    YAML, TOML, XML, INI, and custom format) and some internal structure that describes
    what kind of hierarchy, keys, values, and data types are valid. Sometimes, the
    data contract is explicit, and sometimes it's implicit. Either way, you need to
    manage it carefully, or else you'll get runtime errors when code that's reading,
    parsing, or validating encounters data with an unfamiliar structure.
  prefs: []
  type: TYPE_NORMAL
- en: Migrating data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data migration is a big deal. Many systems these days manage measured terabytes,
    petabytes, or more. The amount of collected and managed data will continue to
    increase for the foreseeable future. The pace of data collection exceeds the pace
    of hardware innovation. The essential point is that if you have a lot of data
    and you need to migrate it, it can take a while. In a previous company, I oversaw
    a project to migrate close to 100 terabytes of data from one Cassandra cluster
    of a legacy system to another Cassandra cluster.
  prefs: []
  type: TYPE_NORMAL
- en: The second Cassandra cluster had different schema and was accessed by a Kubernetes
    cluster 24/7\. The project was very complicated, and thus, it kept getting pushed
    back when urgent issues popped up. The legacy system was still in place side by
    side with the next-gen system long after the original estimate.
  prefs: []
  type: TYPE_NORMAL
- en: There were a lot of mechanisms in place to split the data and send it to both
    clusters, but then we ran into scalability issues with the new system and we had
    to address those before we could continue. The historical data was important,
    but it didn't have to be accessed with the same service level as recent hot data.
    So, we embarked on yet another project to send historical data to cheaper storage.
    This meant, of course, that client libraries or frontend services had to know
    how to query both stores and merge the results. When you deal with a lot of data,
    you can't take anything for granted. You run into scalability issues with your
    tools, your infrastructure, your third-party dependencies, and your processes.
    Large scale is not just quantity change, it is often qualitative change as well.
    Don't expect it to go smoothly. It is much more than copying some files from A
    to B.
  prefs: []
  type: TYPE_NORMAL
- en: Deprecating APIs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'API deprecation comes in two flavors: internal and external. Internal APIs
    are APIs used by components that are fully controlled by you and your team or
    organization. You can be sure that all API users will upgrade to the new API within
    a short time. External APIs are used by users or services outside your direct
    sphere of influence. There are a few gray-area situations where you work for a
    huge organization (think Google), and even internal APIs may need to be treated
    as external APIs. If you''re lucky, all your external APIs are used by self-updating
    applications or through a web interface which you control. In those cases, the
    API is practically hidden, and you don''t even need to publish it.'
  prefs: []
  type: TYPE_NORMAL
- en: If you have a lot of users (or a few very important users) using your API, you
    should consider deprecation very carefully. Deprecating an API means that you
    force your users to change their application to work with you or stay locked to
    an earlier version.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a few ways you can mitigate the pain:'
  prefs: []
  type: TYPE_NORMAL
- en: Don't deprecate. Extend the existing API or keep the previous API active. It
    is sometimes pretty simple although it adds testing burden.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Provide client libraries in all relevant programming languages to your target
    audience. This is always a good practice. It allows you to make many changes to
    the underlying API without disrupting users (as long as you keep the programming
    language interface stable).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you have to deprecate, explain why, allow ample time for users to upgrade
    and provide as much support as possible (for example, an upgrade guide with examples).
    Your users will appreciate it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Large-cluster performance, cost, and design trade-offs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we looked at live cluster upgrades. We explored various
    techniques and how Kubernetes supports them. We also discussed difficult problems,
    such as breaking changes, data contract changes, data migration, and API deprecation.
    In this section, we will consider the various options and configurations of large
    clusters with different reliability and high-availability properties. When you
    design your cluster, you need to understand your options and choose wisely based
    on the needs of your organization.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will cover various availability requirements, from best
    effort all the way to the Holy Grail of zero downtime, and for each category of
    availability, we will consider what it means from the perspectives of performance
    and cost.
  prefs: []
  type: TYPE_NORMAL
- en: Availability requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Different systems have very different requirements for reliability and availability.
    Moreover, different subsystems have very different requirements. For example,
    billing systems are always a high priority because if the billing system is down,
    you can't make money. However, even within the billing system, if the ability
    to dispute charges is sometimes unavailable, it may be OK from the business point
    of view.
  prefs: []
  type: TYPE_NORMAL
- en: Quick recovery
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Quick recovery is another important aspect of highly available clusters. Something
    will go wrong at some point. Your unavailability clock starts running. How quickly
    can you get back to normal?
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, it's not up to you. For example, if your cloud provider has an outage
    (and you didn't implement a federated cluster, as we will discuss later, then
    you just have to sit and wait until they sort it out. But the most likely culprit
    is a problem with a recent deployment. There are, of course, time-related issues,
    and even calendar-related issues. Do you remember the leap-year bug that took
    down Microsoft Azure on February 29, 2012?
  prefs: []
  type: TYPE_NORMAL
- en: The poster boy of quick recovery is, of course, the blue-green deployment-if
    you keep the previous version running when the problem is discovered.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, rolling updates mean that if the problem is discovered early,
    then most of your pods still run the previous version.
  prefs: []
  type: TYPE_NORMAL
- en: Data-related problems can take a long time to reverse, even if your backups
    are up to date and your restore procedure actually works (definitely test this
    regularly).
  prefs: []
  type: TYPE_NORMAL
- en: Tools such as Heptio Ark can help in some scenarios by creating snapshot backup
    of your cluster that you can just restore too, in case something goes wrong and
    you're not sure how to fix it.
  prefs: []
  type: TYPE_NORMAL
- en: Best effort
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Best effort means, counter intuitively, no guarantee whatsoever. If it works,
    great! If it doesn't work-oh well. What are you going to do? This level of reliability
    and availability may be appropriate for internal components that change often
    and the effort to make them robust is not worth it. It may also be appropriate
    for services released into the wild as beta.
  prefs: []
  type: TYPE_NORMAL
- en: Best effort is great for developers. Developers can move fast and break things.
    They are not worried about the consequences, and they don't have to go through
    a gauntlet of rigorous tests and approvals. The performance of best effort services
    may be better than more robust services because it can often skip expensive steps,
    such as verifying requests, persisting intermediate results, and replicating data.
    However, on the other hand, more robust services are often heavily optimized and
    their supporting hardware is fine-tuned to their workload. The cost of best-effort
    services is usually lower because they don't need to employ redundancy, unless
    the operators neglect to do basic capacity planning and just over-provision needlessly.
  prefs: []
  type: TYPE_NORMAL
- en: In the context of Kubernetes, the big question is whether all the services provided
    by the cluster are best effort. If this is the case, then the cluster itself doesn't
    have to be highly available. You can probably have a single master node with a
    single instance of `etcd`, and Heapster or another monitoring solution may not
    need to be deployed.
  prefs: []
  type: TYPE_NORMAL
- en: Maintenance windows
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In a system with maintenance windows, special times are dedicated for performing
    various maintenance activities, such as applying security patches, upgrading software,
    pruning log files, and database cleanups. With a maintenance window, the system
    (or a subsystem) becomes unavailable. This is planned off-time, and users are
    often notified. The benefit of maintenance windows is that you don't have to worry
    how your maintenance actions are going to interact with live requests coming into
    the system. It can drastically simplify operations. System administrators love
    maintenance windows just as much as developers love best-effort systems.
  prefs: []
  type: TYPE_NORMAL
- en: The downside, of course, is that the system is down during maintenance. It may
    only be acceptable for systems where user activity is limited to certain times
    (US office hours or week days only).
  prefs: []
  type: TYPE_NORMAL
- en: With Kubernetes, you can do maintenance windows by redirecting all incoming
    requests through the load balancer to a web page (or JSON response) that notifies
    users about the maintenance window.
  prefs: []
  type: TYPE_NORMAL
- en: But in most cases, the flexibility of Kubernetes should allow you to do live
    maintenance. In extreme cases, such as upgrading the Kubernetes version, or the
    switch from etcd v2 to etcd v3, you may want to resort to a maintenance window.
    Blue-green deployment is another alternative. But the larger the cluster, the
    more expansive the blue-green alternative because you must duplicate your entire
    production cluster, which is both costly and can cause you to run into insufficient
    quota issues.
  prefs: []
  type: TYPE_NORMAL
- en: Zero-downtime
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Finally, we arrive at the zero-downtime system. There is no such thing as a
    zero-downtime system. All systems fail and all software systems definitely fail.
    Sometimes, the failure is serious enough that the system or some of its services
    will be down. Think about zero-downtime as a best-effort distributed system design.
    You design for zero-downtime in the sense that you provide a lot of redundancy
    and mechanisms to address expected failures without bringing the system down.
    As always, remember that even if there is a business case for zero-downtime, it
    doesn't mean that every component must be.
  prefs: []
  type: TYPE_NORMAL
- en: 'The plan for zero-downtime is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Redundancy at every level**: This is a required condition. You can''t have
    a single point of failure in your design, because when it fails, your system is
    down.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Automated hot swapping of failed components**: Redundancy is only as good
    as the ability of the redundant components to kick into action as soon as the
    original component has failed. Some components can share the load (for example,
    stateless web servers), so there is no need for explicit action. In other cases,
    such as the Kubernetes scheduler and controller manager, you need leader election
    in place to make sure that the cluster keeps humming along.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tons of monitoring and alerts to detect problems early**: Even with a careful
    design, you may miss something or some implicit assumption might invalidate your
    design. Often such subtle issues creep up on you, and, with enough attention,
    you may discover it before it becomes an all-out system failure. For example,
    suppose that there is a mechanism in place to clean up old log files when disk
    space is over 90% full, but, for some reason, it doesn''t work. If you set an
    alert for when disk space is over 95% full, then you''ll catch it and be able
    to prevent the system failure.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tenacious testing before deployment to production**: Comprehensive tests
    have proven themselves as a reliable way to improve quality. It is hard work to
    have comprehensive tests for something as complicated as a large Kubernetes cluster
    running a massive distributed system, but you need it. What should you test? Everything.
    That''s right, for zero-downtime, you need to test both the application and the
    infrastructure together. Your 100% passing unit tests are a good start, but they
    don''t provide much confidence that when you deploy your application on your production
    Kubernetes cluster, it will still run as expected. The best tests are, of course,
    on your production cluster after a blue-green deployment or identical cluster.
    In lieu of a fully-fledged identical cluster, consider a staging environment with
    as much fidelity as possible to your production environment. Here is a list of
    tests you should run. Each of these tests should be comprehensive because if you
    leave something untested it might be broken:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unit tests
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: Acceptance tests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performance tests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stress tests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rollback tests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data restore tests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Penetration tests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Does that sound crazy? Good. Zero-downtime large-scale systems are hard. There
    is a reason why Microsoft, Google, Amazon, Facebook, and other big companies have
    tens of thousands of software engineers (combined) just working on infrastructure,
    operations, and making sure that things are up and running.
  prefs: []
  type: TYPE_NORMAL
- en: '**Keep the raw data**: For many systems, the data is the most critical asset.
    If you keep the raw data, you can recover from any data corruption and processed
    data loss that happens later. This will not really help you with zero-downtime
    because it can take a while to reprocess the raw data, but it will help with zero-data
    loss, which is often more important. The downside to this approach is that the
    raw data is often huge compared with the processed data. A good option may be
    to store the raw data in cheaper storage than the processed data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Perceived uptime as a last resort**: OK, some part of the system is down.
    You may still be able to maintain some level of service. In many situations, you
    may have access to a slightly stale version of the data or can let the user access
    some other part of the system. It is not a great user experience, but technically
    the system is still available.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performance and data consistency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When you develop or operate distributed systems, the CAP theorem should always
    be in the back of your mind. CAP stands for consistency, availability, and partition
    tolerance. The theorem says that you can have, at most, two out of the three.
    Since any distributed system can suffer from network partition in practice, you
    can choose between CP or AP. CP means that in order to remain consistent, the
    system will not be available in the event of a network partition. AP means that
    the system will always be available but might not be consistent. For example,
    reads from different partitions might return different results because one of
    the partitions didn't receive a write. In this section, we will focus on highly
    available systems, that is, AP. To achieve high availability, we must sacrifice
    consistency, but it doesn't mean that our system will have corrupt or arbitrary
    data. The keyword is eventual consistency. Our system may be a little bit behind
    and provide access to somewhat stale data, but, eventually, you'll get what you
    expect. When you start thinking in terms of eventual consistency, it opens the
    door to potentially significant performance improvements.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if some important value is updated frequently (for example, every
    second), but you send its value only every minute, you have reduced your network
    traffic by a factor of 60 and you're on, average, only 30 seconds behind real-time
    updates. This is very significant. This is huge. You have just scaled your system
    to handle 60 times more users or requests.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we looked at reliable and highly available large-scale Kubernetes
    clusters. This is arguably the sweet spot for Kubernetes. Although it is useful
    to be able to orchestrate a small cluster running a few containers, it is not
    necessary, but, at scale, you must have an orchestration solution in place which
    you can trust to scale with your system and provide the tools and the best practices
    to do that.
  prefs: []
  type: TYPE_NORMAL
- en: You now have a solid understanding of the concepts of reliability and high-availability
    in distributed systems. You have delved into the best practices for running reliable
    and highly available Kubernetes clusters. You have explored the nuances of live
    Kubernetes cluster upgrades, and you can make wise design choices regarding levels
    of reliability and availability, as well as their performance and cost.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will address the important topic of security in Kubernetes.
    We will also discuss the challenges of securing Kubernetes and the risks involved.
    You will learn all about namespaces, service accounts, admission control, authentication,
    authorization, and encryption.
  prefs: []
  type: TYPE_NORMAL
