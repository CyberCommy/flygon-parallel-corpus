- en: '*Chapter 4*: Turning the Source File into an Abstract Syntax Tree'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A compiler is typically divided into two parts: the frontend and the backend.
    In this chapter, we will implement the frontend of a programming language; that
    is, the part that deals with the source language. We will learn about the techniques
    real-world compilers use and apply them to our own programming languages.'
  prefs: []
  type: TYPE_NORMAL
- en: We'll begin our journey by defining our programming language's grammar and end
    it with an **abstract syntax tree (AST)**, which will become the basis for code
    generation. You can use this approach for every programming language that you
    would like to implement a compiler for.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, you will learn about the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Defining a real programming language introduces you to the `tinylang` language,
    which is a subset of a real programming language, and for which you must implement
    a compiler frontend.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating the project layout, in which you will create the project layout for
    the compiler.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Managing source files and user messages, which gives you knowledge of how to
    handle several input files and how to inform the user about problems in a pleasant
    way.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Structuring the lexer, which discusses how the lexer is broken down into modular
    pieces.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Constructing a recursive descent parser, which will talk about the rules you
    can use to derive a parser from grammar to perform syntax analysis.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating a parser and lexer with bison and flex, in which you will use tools
    to comfortably generate parsers and lexers from a specification.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing semantic analysis, in which you will create the AST and evaluate
    its attributes, which will be intertwined with the parser.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With the skills you will acquire in this chapter, you will be able to build
    a compiler frontend for any programming language.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The code files for the chapter are available at [https://github.com/PacktPublishing/Learn-LLVM-12/tree/master/Chapter04](https://github.com/PacktPublishing/Learn-LLVM-12/tree/master/Chapter04)
  prefs: []
  type: TYPE_NORMAL
- en: You can find the code in action videos at [https://bit.ly/3nllhED](https://bit.ly/3nllhED)
  prefs: []
  type: TYPE_NORMAL
- en: Defining a real programming language
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A real programming language brings up more challenges than the simple `tinylang`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a quick tour of subset of the `tinylang` grammar that will be used
    in this chapter. In the upcoming sections, we will derive the lexer and the parser
    from this grammar:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'A compilation unit in Modula-2 begins with the `MODULE` keyword, followed by
    the name of the module. The content of a module can be a list of imported modules,
    declarations, and a block containing statements that run at initialization time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'A declaration introduces constants, variables, and procedures. Constants that
    have been declared are prefixed with the `CONST` keyword. Similarly, variable
    declarations begin with the `VAR` keyword. Declaring a constant is very simple:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The identifier is the name of the constant. The value is derived from an expression,
    which must be computable at compile time. Declaring variables is a bit more complex:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'To be able to declare more than one variable in one go, a list of identifiers
    must be used. The type''s name can potentially come from another module and is
    prefixed with the module name in this case. This is called a qualified identifier.
    A procedure requires the most details:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, you can see how constants, variables, and procedures
    are declared. Procedures can have parameters and a return type. Normal parameters
    are passed as values, while `VAR` parameters are passed by reference. The other
    part missing from the preceding `block` rule is `statementSequence`, which is
    only a list of single statements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'A statement is delimited by a semicolon if it is followed by another statement.
    Again, only a subset of the *Modula-2* statements is supported:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The first part of this rule describes an assignment or procedure call. A qualified
    identifier followed by `:=` is an assignment. On the other hand, if it is followed
    by `(`, then it is a procedure call. The other statements are the usual control
    statements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The `IF` statement has a simplified syntax too, since it can only have a single
    `ELSE` block. With that statement, we can conditionally guard a statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The `WHILE` statement describes a loop, guarded by a condition. Together with
    the `IF` statement, this enables us to write simple algorithms in `tinylang`.
    Finally, the definition of an expression is missing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The expression syntax is very similar to that of calc in the previous chapter.
    Only the `INTEGER` and `BOOLEAN` data types are supported.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, the `identifier` and `integer_literal` tokens are used. An `H`.
  prefs: []
  type: TYPE_NORMAL
- en: That's already a lot of rules, and we're only covering a part of Modula-2 here!
    Nevertheless, it is possible to write small applications in this subset. Let's
    implement a compiler for `tinylang`!
  prefs: []
  type: TYPE_NORMAL
- en: Creating the project layout
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The project layout for `tinylang` follows the approach we laid out in [*Chapter
    2*](B15647_02_ePub_RK.xhtml#_idTextAnchor032), *Touring the LLVM Source*. The
    source code for each component is in a subdirectory of the `lib` directory, while
    the header files are in a subdirectory of `include/tinylang`. The subdirectory
    is named after the component. In [*Chapter 2*](B15647_02_ePub_RK.xhtml#_idTextAnchor032),
    *Touring the LLVM Source*, we only created the `Basic` component.
  prefs: []
  type: TYPE_NORMAL
- en: 'From the previous chapter, we know that we need to implement a lexer, a parser,
    an AST, and a semantic analyzer. Each is a component of its own, called `Lexer`,
    `Parser`, `AST`, and `Sema`. The directory layout that was used in the previous
    chapter looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.1 – The directory layout of the tinylang project'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_4.1_B15647.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.1 – The directory layout of the tinylang project
  prefs: []
  type: TYPE_NORMAL
- en: The components have clearly defined dependencies. Here, `Lexer` only depends
    on `Basic`. `Parser` depends on `Basic`, `Lexer`, `AST`, and `Sema`. Finally,
    `Sema` only depends on `Basic` and `AST`. These well-defined dependencies help
    with reusing components.
  prefs: []
  type: TYPE_NORMAL
- en: Let's have a closer look at their implementation!
  prefs: []
  type: TYPE_NORMAL
- en: Managing source files and user messages
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A real compiler must deal with many files. Usually, the developer calls the
    compiler with the name of the main compilation unit. This compilation unit can
    refer to other files, for example, via `#include` directives in C or `import`
    statements in Python or Modula-2\. An imported module can import other modules
    and so on. All these files must be loaded into memory and run through the analysis
    stages of the compiler. During development, a developer may make syntactical or
    semantical errors. When detected, an error message, including the source line
    and a marker, should be printed. At this point, it should be obvious that this
    essential component is not trivial.
  prefs: []
  type: TYPE_NORMAL
- en: 'Luckily, LLVM comes with a solution: the `llvm::SourceMgr` class. A new source
    file is added to `SourceMgr` with a call to the `AddNewSourceBuffer()` method.
    Alternatively, a file can be loaded with a call to the `AddIncludeFile()` method.
    Both methods return an ID to identify the buffer. You use this ID to retrieve
    a pointer to the memory buffer of the associated file. To define a location in
    the file, the `llvm::SMLoc` class must be used. This class encapsulates a pointer
    into the buffer. Various `PrintMessage()` methods allow us to emit errors and
    other informational messages to the user.'
  prefs: []
  type: TYPE_NORMAL
- en: Only a way to centrally define messages is missing. In a large piece of software
    (such as a compiler), you do not want to sprinkle message strings all over the
    place. If there is a request to change messages or translate them into another
    language, then you'd better have them in a central place!
  prefs: []
  type: TYPE_NORMAL
- en: 'A simple approach is that each message has an ID (an `enum` member), a severity
    level, and a string containing the messages. In your code, you only refer to the
    message ID. The severity level and message string are only used when the message
    is printed. These three items (the ID, the security level, and the message) must
    be managed consistently. The LLVM libraries use a preprocessor to solve this.
    The data is stored in a file with a`.def` suffix and is wrapped in a macro name.
    That file is usually included several times, with different definitions for the
    macro. The definition of this is in the `include/tinylang/Basic/Diagnostic.def`
    file path and looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The first macro parameter, `ID`, is the enumeration label, the second parameter,
    `Level`, is the severity, and the third parameter, `Msg`, is the message text.
    With this definition at hand, we can define a `DiagnosticsEngine` class to emit
    error messages. The interface is in the `include/tinylang/Basic/Diagnostic.h`
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'After including the necessary header files, `Diagnostic.def` is now used to
    define the enumeration. To not pollute the global namespace, a nested namespace,
    `diag`, must be used:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The `DiagnosticsEngine` class uses a `SourceMgr` instance to emit the messages
    via the `report()` method. Messages can have parameters. To implement this facility,
    the variadic-format support from LLVM must be used. The message text and the severity
    level are retrieved with the help of the `static` method. As a bonus, the number
    of emitted error messages is also counted:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The message string is returned by `getDiagnosticText()`, while the level is
    returned by `getDiagnosticKind()`. Both methods will be implemented in the `.cpp`
    file later:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Since messages can have a variable number of parameters, the solution in C++
    is to use a variadic template. Of course, this is also used by the `formatv()`
    function provided by LLVM. To get the formatted message, we need only to forward
    the template parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'With that, we have implemented most of the class. Only `getDiagnosticText()`
    and `getDiagnosticKind()` are missing. They are defined in the `lib/Basic/Diagnostic.cpp`
    file and also make use of the `Diagnostic.def` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'As in the header file, the `DIAG` macro is defined to retrieve the desired
    part. Here, we will define an array that will hold the text messages. Therefore,
    the `DIAG` macro only returns the `Msg` part. We will use the same approach for
    the level:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Not surprisingly, both functions simply index the array to return the desired
    data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: This combination of the `SourceMgr` and `DiagnosticsEngine` classes provides
    a good basis for the other components. Let's use them in the lexer first!
  prefs: []
  type: TYPE_NORMAL
- en: Structuring the lexer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we know from the previous chapter, we need a `Token` class and a `Lexer`
    class. Additionally, a `TokenKind` enumeration is required to give each token
    class a unique number. Having an all-in-one header and an implementation file
    does not scale, so let's restructure things. The `TokenKind` enumeration can be
    used universally and is placed in the `Basic` component. The `Token` and `Lexer`
    classes belong to the `Lexer` component but are placed in different header and
    implementation files.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are three different classes of tokens: `CONST` keyword, the `;` delimiter,
    and the `ident` token, which represent the identifiers in the source. Each token
    needs a member name for the enumeration. Keywords and punctuators have natural
    display names that can be used for messages.'
  prefs: []
  type: TYPE_NORMAL
- en: Like in many programming languages, the keywords are a subset of the identifiers.
    To classify a token as a keyword, we need a keyword filter, which checks if the
    identifier that's been found is indeed a keyword. This is the same behavior as
    in C or C++, where keywords are also a subset of identifiers. Programming languages
    evolve over time and new keywords may be introduced. As an example, the original
    K&R C language had no enumerations defined with the `enum` keyword. Due to this,
    a flag indicating the language level of a keyword should be present.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ve collected several pieces of information, all of which belong to a member
    of the `TokenKind` enumeration: the label for the enumeration member, the spelling
    of the punctuators, and a flag for the keywords. As for the diagnostic messages,
    we centrally store the information in a `.def` file called `include/tinylang/Basic/TokenKinds.def`,
    which looks as follows. One thing to note is that keywords are prefixed with `kw_`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'With these centralized definitions, it''s easy to create the `TokenKind` enumeration
    in the `include/tinylang/Basic/TokenKinds.h` file. Again, the enumeration is put
    into its own namespace, called `tok`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The pattern you must use to fill the array should be familiar by now. The `TOK`
    macro is defined to only return the enumeration label''s `ID`. As a useful addition,
    we also define `NUM_TOKENS` as the last member of the enumeration, denoting the
    number of defined tokens:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The implementation file, `lib/Basic/TokenKinds.cpp`, also uses the `.def` file
    to retrieve the names:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The textual name of a token is derived from its enumeration label''s `ID`.
    There are two particularities. First, we need two define the `TOK` and `KEYWORD`
    macros because the default definition of `KEYWORD` does not use the `TOK` macro.
    Second, a `nullptr` value is added at the end of the array, accounting for the
    added `NUM_TOKENS` enumeration member:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'We take a slightly different approach for the `getPunctuatorSpelling()` and
    `getKeywordSpelling()` functions. These functions only return meaningful values
    for a subset of the enumeration. This can be realized with a `switch` statement,
    which returns a `nullptr` value by default:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: Note how the macros are defined to retrieve the piece of information that's
    required from the file.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous chapter, the `Token` class was declared in the same header
    file as the `Lexer` class. To make this more modular, we will put the `Token`
    class into its own header file in `include/Lexer/Token.h`. As in the previous
    case, `Token` stores a pointer to the start of the token, the length, and the
    token''s kind, as defined previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The `SMLoc` instance, which denotes the source''s position in the messages,
    is created from the pointer to the token:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The `getIdentifier()` and `getLiteralData()` methods allow us to access the
    text of the token for identifiers and literal data. It is not necessary to access
    the text for any other token type, as this is implied by the token''s type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'We declare the `Lexer` class in the `include/Lexer/Lexer.h` header file and
    put the implementation in the `lib/Lexer/lexer.cpp` file. The structure is the
    same as for the calc language from the previous chapter. Here, we must take a
    closer look at two details:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, there are operators that share the same prefix; for example, `<` and
    `<=`. When the current character we''re looking at is a `<`, we must check the
    next character first, before deciding which token we found. Remember that we required
    that the input ends with a null byte. Therefore, the next character can always
    be used if the current character is valid:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The other detail is that at this point, there are far more keywords. How can
    we handle this? A simple and fast solution is to populate a hash table with the
    keywords, which are all stored in the `TokenKinds.def` file. This can be done
    while we instantiate the `Lexer` class. In this approach, it is also possible
    to support different levels of the language, as the keywords can be filtered with
    the attached flag. Here, this flexibility is not needed yet. In the header file,
    the keyword filter is defined as follows, using an instance of `llvm::StringMap`
    for the hash table:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The `getKeyword()` method returns the token kind of the given string, or a
    default value if the string does not represent a keyword:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'In the implementation file, the keyword table is filled in:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: With these techniques, it's not difficult to write an efficient lexer class.
    Since compilation speed matters, many compilers use a handwritten lexer, an example
    of which is Clang.
  prefs: []
  type: TYPE_NORMAL
- en: Constructing a recursive descent parser
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As shown in the previous chapter, the parser is derived from its grammar. Let''s
    recall all the *construction rules*. For each rule of grammar, you create a method
    that''s named after the non-terminal on the left-hand side of the rule in order
    to parse the right-hand side of the rule. Following the definition of the right-hand
    side, you must do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: For each non-terminal, the corresponding method is called.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each token is consumed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For alternatives and optional or repeating groups, the look-ahead token (the
    next unconsumed token) is examined to decide where we can continue from.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s apply these construction rules to the following rule of the grammar:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'We can easily translate this into the following C++ method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: The whole grammar of `tinylang` can be turned into C++ in this way. In general,
    you must be careful and avoid some pitfalls.
  prefs: []
  type: TYPE_NORMAL
- en: 'One issue to look out for is left-recursive rules. A rule is **left-recursive**
    if the right-hand side begins with the same terminal that''s on the left-hand
    side. A typical example can be found in the grammar for expressions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'If it''s not already clear from the grammar, then the following translation
    into C++ should make it obvious that this results in infinite recursion:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Left recursion can also indirectly occur and involve more rules, which is much
    more difficult to spot. That's why an algorithm exists that can detect and eliminate
    left recursion.
  prefs: []
  type: TYPE_NORMAL
- en: 'At each step, the parser decides how to continue just by using the look-ahead
    token. The grammar is said to have conflicts if this decision cannot be made deterministically.
    To illustrate this, let''s have a look at the `using` statement in C#. Like in
    C++, the `using` statement can be used to make a symbol visible in a namespace,
    such as in `using Math;`. It is also possible to define an alias name for the
    imported symbol; that is, `using M = Math;`. In grammar, this can be expressed
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Obviously, there's is a problem here. After the parser consumed the `using`
    keyword, the look-ahead token is `ident`. But this information is not enough for
    us to decide if the optional group must be skipped or parsed. This situation always
    arises if the set of tokens that the optional group can begin with overlap with
    the set of tokens that follow the optional group.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s rewrite the rule with an alternative instead of an optional group:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, there is a different conflict: both alternatives begin with the same token.
    Looking only at the look-ahead token, the parser can''t decide which of the alternatives
    is the right one.'
  prefs: []
  type: TYPE_NORMAL
- en: 'These conflicts are very common. Therefore, it''s good to know how to handle
    them. One approach is to rewrite the grammar in such a way that the conflict disappears.
    In the previous example, both alternatives begin with the same token. This can
    be factored out, resulting in the following rule:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: This formulation has no conflict. However, it should also be noted that it is
    less expressive. In the other two formulations, it is obvious which `ident` is
    the alias name and which `ident` is the namespace name. In this conflict-free
    rule, the left-most `ident` changes its role. First, it is the namespace name,
    but if an equals sign (`=`) follows it, then it turns into the alias name.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second approach is to add an additional predicate to distinguish between
    both cases. This predicate, often called a `Token &peek(int n)` method, which
    returns the nth token after the current look-ahead token. Here, the existence
    of an equals sign can be used as an additional predicate in the decision:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Now, let's incorporate error recovery. In the previous chapter, I introduced
    the so-called *panic mode* as a technique for error recovery. The basic idea is
    to skip tokens until one is found that is suitable for continuing parsing. For
    example, in `tinylang`, a statement is followed by a semicolon (`:`).
  prefs: []
  type: TYPE_NORMAL
- en: If there is a syntax problem in an `IF` statement, then you skip all the tokens
    until you find a semicolon. Then, you continue with the next statement. Instead
    of using an ad hoc definition for the token set, it's better to use a systematic
    approach.
  prefs: []
  type: TYPE_NORMAL
- en: For each non-terminal, you compute the set of tokens that can follow the non-terminal
    anywhere (called the `;`, `ELSE`, and `END` tokens can follow. So, you use this
    set in the error recovery part of `parseStatement()`. This method assumes that
    a syntax error can be handled locally. In general, this is not possible. Because
    the parser skips tokens, it could happen that so many are skipped that the end
    of the input is reached. At this point, local recovery is not possible.
  prefs: []
  type: TYPE_NORMAL
- en: 'To prevent meaningless error messages, the calling method needs to be informed
    that error recovery has still not finished. This can be done with the `bool` return
    value: `true` means that error recovery hasn''t finished yet, while `false` means
    that parsing (including possible error recovery) was successful.'
  prefs: []
  type: TYPE_NORMAL
- en: There are numerous ways to extend this error recovery scheme. One popular way
    is to also use the FOLLOW sets of the active callers. As a simple example, let's
    assume that `parseStatement()` was called by `parseStatementSequence()`, which
    was itself called by `parseBlock()` and that that was called from `parseModule()`.
  prefs: []
  type: TYPE_NORMAL
- en: Here, each of the corresponding non-terminals has a FOLLOW set. If the parser
    detects a syntax error in `parseStatement()`, then tokens are skipped until the
    token is in at least one of the FOLLOW sets of the active callers. If the token
    is in the FOLLOW set of a statement, then the error was recovered locally, and
    a `false` value is returned to the caller. Otherwise, a `true` value is returned,
    meaning that error recovery must continue. A possible implementation strategy
    for this extension is passing a `std::bitset` or `std::tuple` to represent the
    union of the current FOLLOW sets to the callee.
  prefs: []
  type: TYPE_NORMAL
- en: 'One last question is still open: how can we call error recovery? In the previous
    chapter, a `goto` was used to jump to the error recovery block. This works but
    is not a pleasing solution. Given the discussion earlier, we can skip tokens in
    a separate method. Clang has a method called `skipUntil()` for this purpose, and
    we can also use this for `tinylang`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Because the next step is to add semantic actions to the parser, it would be
    nice to have a central place to put cleanup code if necessary. A nested function
    would be ideal for this. C++ does not have a nested function. Instead, a lambda
    function can serve a similar purpose. The `parseIfStatement()` method with complete
    error recovery looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Generating a parser and lexer with bison and flex
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Manually constructing a lexer and a parser is not difficult and usually results
    in fast components. The disadvantage is that it is not easy to introduce changes,
    especially in the parser. This can be important if you are prototyping a new programming
    language. Using specialized tools can mitigate this issue.
  prefs: []
  type: TYPE_NORMAL
- en: There are many tools available that generate either a lexer or a parser from
    a specification file. In the Linux world, **flex** (https://github.com/westes/flex)
    and **bison** ([https://www.gnu.org/software/bison/](https://www.gnu.org/software/bison/))
    are the most commonly used tools. Flex generates a lexer from a set of regular
    expressions, while bison generates a parser from a grammar description. Usually,
    both tools are used together.
  prefs: []
  type: TYPE_NORMAL
- en: 'Bison produces an `tinylang`, stored in a `tinylang.yy` file, begins with the
    following prologue:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'We instruct bison to generate C++ code with the `%language` directive. Using
    the `%define` directive, we override some default values for the code generation:
    the generated class should be named `Parser` and be inside the `tinylang` namespace
    Additionally, the members of the enumeration representing the token kind should
    be prefixed with `T_`. We require version 3.2 or later, because some of these
    variables were introduced with this version. To be able to interact with flex,
    we tell bison to write a `Parser.h` header file with the `%defines` directive.
    Finally, we must declare all used tokens with the `%token` directive. The grammar
    rules come after `%%`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Please compare these rules with the grammar specification shown in the first
    section of this chapter. Bison does not know repeating groups, so we need to add
    a new rule called `imports` to model this repetition. In the `import` rule, we
    must introduce an alternative to model the optional group.
  prefs: []
  type: TYPE_NORMAL
- en: 'We also need to rewrite other rules of the `tinylang` grammar in this style.
    For example, the rule for the `IF` statement becomes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Again, we must introduce a new rule to model the optional `ELSE` statement.
    The `%empty` directive could be omitted, but the use of it makes it clear that
    this is an empty branch of the alternative.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we''ve rewritten all the grammar rules in the bison style, we can generate
    the parser with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: That's all it takes to create a parser that's similar to the handwritten one
    in the previous section!
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, flex is easy to use. The specification for flex is a list of regular
    expressions and the associated action, which is executed if the regular expression
    matches. The `tinylang.l` file specifies the lexer for `tinylang`. Like the bison
    specification, it begins with a prologue:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: The text inside `%{` `}%` is copied into the file generated by flex. We use
    this mechanism to include the header file generated by bison. With the `%option`
    directive, we control which features the generated lexer should have. We only
    read one file and do not want to continue to read another file once we've reached
    the end of it, so we specify `noyywrap` to disable this feature. We also do not
    need access to the underlying file stream and disable it with `nounput` and `noinout`.
    Finally, because we do not need an interactive lexer, we request that a `batch`
    scanner is generated.
  prefs: []
  type: TYPE_NORMAL
- en: 'Inside the prologue, we can also define character patterns for later use. After
    `%%` follows the definition section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: In the definition section, you specify a regular expression pattern and an action
    to execute if the pattern matches the input. The action can also be empty.
  prefs: []
  type: TYPE_NORMAL
- en: The `{space}+` pattern uses the `space` character pattern defined in the prologue.
    It matches one or more white space characters. We defined no action, so all white
    space will be ignored.
  prefs: []
  type: TYPE_NORMAL
- en: 'To match a number, we use the `{digit}+` pattern. As an action, we only return
    the associated token kind. The same is done for all the tokens. For example, we
    do the following for the arithmetic operators:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'If several patterns match the input, then the pattern with the longest match
    is selected. If there is still more than one pattern that matches the input, then
    the pattern that comes first lexicographically in the specification file is chosen.
    That''s why it is important to define the patterns for the keywords first and
    the pattern for identifiers only after all the keywords:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: The actions are not limited to just a `return` statement. If your code needs
    more than one line, then you must surround your code with curly braces `{` `}`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The scanner is generated with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: Which approach should you use for your language project? Parser generators usually
    generate LALR(1) parsers. The LALR(1) class is larger than the LL(1) class, which
    recursive descent parsers can be constructed for. If you can't tweak your grammar
    so that it fits in the LL(1) class, then you should consider using a parser generator.
    It's not feasible to construct such a bottom-up parser by hand. Even if your grammar
    is LL(1), a parser generator provides more comfort while producing similar code
    to what you could write by hand. Often, this is a choice that's influenced by
    many factors. Clang uses a handwritten parser, while GCC uses a bison-generated
    parser.
  prefs: []
  type: TYPE_NORMAL
- en: Performing semantic analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The parser that we constructed in the previous section only checks the syntax
    of the input. The next step is to add the ability to perform semantic analysis.
    In the calc example in the previous chapter, the parser constructed an AST. In
    a separate phase, the semantic analyzer worked on this tree. This approach can
    always be used. In this section, we will use a slightly different approach and
    intertwine the parser and the semantic analyzer more.
  prefs: []
  type: TYPE_NORMAL
- en: 'These are some of the tasks a semantic analyzer must perform:'
  prefs: []
  type: TYPE_NORMAL
- en: For each declaration, the semantic analyzer must check if the used name has
    not been declared elsewhere already.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For each occurrence of a name in an expression or statement, the semantic analyzer
    must check that the name is declared and that the desired use fits the declaration.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For each expression, the semantic analyzer must compute the resulting type.
    It is also necessary to compute if the expression is constant and if so, which
    value it has.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For assignment and parameter passing, the semantic analyzer must check that
    the types are compatible. Furthermore, we must check that the conditions in the
    `IF` and `WHILE` statements are of the `BOOLEAN` type.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: That's already a lot to check for such a small subset of a programming language!
  prefs: []
  type: TYPE_NORMAL
- en: Handling the scope of names
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s have a look at the scope of names first. The scope of a name is the
    range where the name is visible. Like C, `tinylang` uses a declare-before-use
    model. For example, the `B` and `X` variables are declared at the module level
    so that they''re of the `INTEGER` type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Before the declaration, the variables are not known and cannot be used. This
    is only possible after the declaration. Inside a procedure, more variables can
    be declared:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: Inside this procedure, at the point where the comment is, using `B` refers to
    the local variable `B`, while using `X` refers to the global variable `X`. The
    scope of the local variable, `B`, is the `Proc` procedure. If a name cannot be
    found in the current scope, then the search continues in the enclosing scope.
    Therefore, the `X` variable can be used inside the procedure. In `tinylang`, only
    modules and procedures open a new scope. Other language constructs such as `struct`
    and `class` usually also open a scope. Predefined entities such as the `INTEGER`
    type or the `TRUE` literal are declared in a global scope, enclosing the scope
    of the module.
  prefs: []
  type: TYPE_NORMAL
- en: 'In `tinylang`, only the name is crucial. Therefore, a scope can be implemented
    as a mapping from a name to its declaration. A new name can only be inserted if
    it is not already present. For the lookup, the enclosing or parent scope must
    also be known. The interface (in the `include/tinylang/Sema/Scope.h` file) is
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'The implementation in the `lib/Sema/Scope.cpp` file looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: Please note that the `StringMap::insert()` method does not override an existing
    entry. The `second` member of the resulting `std::pair` indicates whether the
    table was updated. This information is returned to the caller.
  prefs: []
  type: TYPE_NORMAL
- en: 'To implement the search for the declaration of a symbol, the `lookup()` method
    searches the current scope; if nothing is found, it searches the scopes that have
    been linked by the `parent` member:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'The variable declaration is then processed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The current scope is the module scope.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `INTEGER` type declaration is looked up. It's an error if no declaration
    is found or if it is not a type declaration.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A new AST node, `VariableDeclaration`, is instantiated, with the important attributes
    being the name, `B`, and the type.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The name, `B`, is inserted into the current scope, mapped to the declaration
    instance. If the name is already present in the scope, then this is an error.
    The content of the current scope is not changed in this case.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The same is done for the `X` variable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Two tasks are performed here. Like in the calc example, AST nodes are constructed.
    At the same time, the attributes of the node, such as its type, are computed.
    Why is this possible?
  prefs: []
  type: TYPE_NORMAL
- en: The semantic analyzer can fall back on two different sets of attributes. The
    scope is inherited from the caller. The type declaration can be computed (or synthesized)
    by evaluating the name of the type declaration. The language is designed in such
    a way that these two sets of attributes are sufficient to compute all the attributes
    of the AST node.
  prefs: []
  type: TYPE_NORMAL
- en: An important aspect of this is the *declare-before-use* model. If a language
    allows the use of names before declaration, such as the members inside a class
    in C++, then it is not possible to compute all the attributes of an AST node at
    once. In such a case, the AST node must be constructed with only partially computed
    attributes or just with plain information (such as in the calc example).
  prefs: []
  type: TYPE_NORMAL
- en: The AST must be visited one or more times to determine the missing information.
    In the case of `tinylang` (and Modula-2), it would also be possible to dispense
    with the AST construction – the AST is indirectly represented through the call
    hierarchy of the `parseXXX()` methods. Code generation from an AST is much more
    common, so we construct an AST here, too.
  prefs: []
  type: TYPE_NORMAL
- en: Before we put all the pieces together, we need to understand the LLVM style
    of using **runtime type information** (**RTTI**).
  prefs: []
  type: TYPE_NORMAL
- en: Using LLVM-style RTTI for the AST
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Naturally, the AST nodes are a part of a class hierarchy. A declaration always
    has a name. Other attributes depend on what is being declared. If a variable is
    declared, then a type is required. A constant declaration needs a type and a value,
    and so on. Of course, at runtime, you need to find out which kind of declaration
    you are working with. The `dynamic_cast<>` C++ operator could be used for this.
    The problem is that the required RTTI is only available if the C++ class has a
    virtual table attached to it; that is, it uses virtual functions. Another disadvantage
    is that C++ RTTI is bloated. To avoid these disadvantages, the LLVM developers
    introduced a self-made RTTI style that is used throughout the LLVM libraries.
  prefs: []
  type: TYPE_NORMAL
- en: 'The (abstract) base class of our hierarchy is `Decl`. To implement the LLVM-style
    RTTI, a public enumeration containing a label for each subclass is added. Also,
    a private member of this type and a public getter is required. The private member
    is usually called `Kind`. In our case, this looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Each subclass now needs a special function member called `classof`. The purpose
    of this function is to determine if a given instance is of the requested type.
    For a `VariableDeclaration`, it is implemented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: Now, you can use the `llvm::isa<>` special templates to check if an object is
    of the requested type and `llvm::dyn_cast<>` to dynamically cast the object. There
    are more templates that exist, but these two are the most commonly used ones.
    For the other templates, see [https://llvm.org/docs/ProgrammersManual.html#the-isa-cast-and-dyn-cast-templates](https://llvm.org/docs/ProgrammersManual.html#the-isa-cast-and-dyn-cast-templates)
    and for more information about the LLVM style, including more advanced uses, see
    [https://llvm.org/docs/HowToSetUpLLVMStyleRTTI.html](https://llvm.org/docs/HowToSetUpLLVMStyleRTTI.html).
  prefs: []
  type: TYPE_NORMAL
- en: Creating the semantic analyzer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Equipped with this knowledge, we can now implement the semantic analyzer, operating
    on AST nodes created by the parser. First, we will implement the definition of
    the AST node for a variable, which is stored in the `include/llvm/tinylang/AST/AST.h`
    file. Besides support for the LLVM-style RTTI, the base class stores the name
    of the declaration, the location of its name, and a pointer to the enclosing declaration.
    The latter is required to code-generate nested procedures. The `Decl` base class
    is declared as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'The declaration for a variable only adds a pointer to the type declaration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'The method in the parser needs to be extended with a semantic action and variables
    for the information that''s been collected:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: A `DeclList` is a list of declarations called `std::vector<Decl*>`, while `IdentList`
    is a list of locations and identifiers called `std::vector<std::pair<SMLoc, StringRef>>`.
  prefs: []
  type: TYPE_NORMAL
- en: The `parseQualident()` method returns a declaration, which in this case is expected
    to be a type declaration.
  prefs: []
  type: TYPE_NORMAL
- en: 'The parser class knows an instance of the semantic analyzer class, `Sema`,
    which is stored in the `Actions` member. A call to `actOnVariableDeclaration()`
    runs the semantic analyzer and the AST construction. The implementation is in
    the `lib/Sema/Sema.cpp` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: First, the type declaration is check with `llvm::dyn_cast<TypeDeclaration>`.
    If it is not a type declaration, then an error message is printed. Otherwise,
    for each name in the `Ids` list, a `VariableDeclaration` is instantiated and added
    to the list of declarations. If adding the variable to the current scope fails
    because the name has already been declared, then an error message is printed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Most of the other entities are constructed in the same way, with the complexity
    of their semantic analysis being the only difference. More work is required for
    modules and procedures because they open a new scope. Opening a new scope is easy:
    only a new `Scope` object must be instantiated. As soon as the module or procedure
    has been parsed, the scope must be removed.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This must be done in a reliable fashion because we do not want to add names
    to the wrong scope in case of a syntax error. This is a classic use of the **Resource
    Acquisition Is Initialization** (**RAII**) idiom in C++. Another complication
    comes from the fact that a procedure can recursively call itself. Due to this,
    the name of the procedure must be added to the current scope before it can be
    used. The semantic analyzer has two methods to enter and leave a scope. The scope
    is associated with a declaration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'A simple helper class is used to implement the RAII idiom:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'While parsing a module or procedure, there are now two interactions with the
    semantic analyzer. The first is after the name is parsed. Here, the (almost empty)
    AST node is constructed, and a new scope is established:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'The semantic analyzer does more than check the name in the current scope and
    return the AST node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'The real work is done once all the declarations and the procedure''s body have
    been parsed. Basically, the semantic analyzer must only check if the name at the
    end of the procedure declaration is equal to the name of the procedure, and also
    if the declaration that''s used for the return type is really a type declaration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'Some declarations are inherently present and cannot be defined by the developer.
    This includes the `BOOLEAN` and `INTEGER` types and the `TRUE` and `FALSE` literals.
    These declarations exist in the global scope and must be added programmatically.
    Modula-2 also predefines some procedures, such as `INC` or `DEC`, which should
    also be added to the global scope. Given our classes, the initialization of the
    global scope is simple:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'With this scheme, all the required calculations for `tinylang` can be done.
    For example, to compute if an expression results in a constant value, you must
    ensure the following occurs:'
  prefs: []
  type: TYPE_NORMAL
- en: A literal or a reference to a constant declaration is constant.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If both sides of an expression are constant, then applying the operator also
    yields a constant.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These rules are easily embedded into the semantic analyzer while creating the
    AST nodes for an expression. Likewise, the type and the constant value can be
    computed.
  prefs: []
  type: TYPE_NORMAL
- en: It should be noted that not all kinds of computations can be done in this way.
    For example, to detect the use of uninitialized variables, a method called symbolic
    interpretation can be used. In its general form, the method requires a special
    walk order through the AST, which is not possible during construction time. The
    good news is that the presented approach creates a fully decorated AST, which
    is ready for code generation. This AST can, of course, be used for further analysis,
    given the fact that costly analysis can be turned on or off on demand.
  prefs: []
  type: TYPE_NORMAL
- en: 'To play around with the frontend, you also need to update the driver. Since
    the code generation is missing, a correct `tinylang` program produces no output.
    Still, it can be used to explore error recovery and to provoke semantic errors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: Congratulations! You've finished implementing the frontend for `tinylang`!
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s try out what we have learned so far. Save the following source,
    which is an implementation of Euclid''s greatest common divisor algorithm, as
    a `Gcd.mod` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s run the compiler on this file with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: There is no output except the version number being printed. This is because
    only the frontend part has been implemented. However, if you change the source
    so that it contains syntax errors, then error messages will be printed.
  prefs: []
  type: TYPE_NORMAL
- en: We'll continue this fun by adding code generation, which is the topic of the
    next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned about the techniques a real-world compiler uses
    in the frontend. Starting with the project's layout, you created separate libraries
    for the lexer, the parser, and the semantic analyzer. To output messages to the
    user, you extended an existing LLVM class, which allowed the messages to be stored
    centrally. The lexer has now been separated into several interfaces.
  prefs: []
  type: TYPE_NORMAL
- en: You then learned how to construct a recursive descent parser from a grammar
    description, which pitfalls to avoid, and how to use generators to do the job.
    The semantic analyzer you constructed performs all the semantic checks that are
    required by the language while being intertwined with the parser and AST construction.
  prefs: []
  type: TYPE_NORMAL
- en: The result of your coding effort was a fully decorated AST, which will be used
    in the next chapter to generate IR code and object code.
  prefs: []
  type: TYPE_NORMAL
