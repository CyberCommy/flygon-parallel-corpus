- en: Chapter 7. Logging and Monitoring Microservices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the biggest challenges due to the very distributed nature of Internet-scale
    microservices deployment is the logging and monitoring of individual microservices.
    It is difficult to trace end-to-end transactions by correlating logs emitted by
    different microservices. As with monolithic applications, there is no single pane
    of glass to monitor microservices.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter will cover the necessity and importance of logging and monitoring
    in microservice deployments. This chapter will further examine the challenges
    and solutions to address logging and monitoring with a number of potential architectures
    and technologies.
  prefs: []
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you will learn about:'
  prefs: []
  type: TYPE_NORMAL
- en: The different options, tools, and technologies for log management
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The use of Spring Cloud Sleuth in tracing microservices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The different tools for end-to-end monitoring of microservices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The use of Spring Cloud Hystrix and Turbine for circuit monitoring
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The use of data lakes in enabling business data analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reviewing the microservice capability model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will explore the following microservice capabilities from
    the microservices capability model discussed in [Chapter 3](ch03.html "Chapter 3. Applying
    Microservices Concepts"), *Applying Microservices Concepts*:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Central Log Management**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Monitoring and Dashboards**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dependency Management** (part of Monitoring and Dashboards)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data Lake**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Reviewing the microservice capability model](img/B05447_07_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Understanding log management challenges
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Logs are nothing but streams of events coming from a running process. For traditional
    JEE applications, a number of frameworks and libraries are available to log. Java
    Logging (JUL) is an option off the shelf from Java itself. Log4j, Logback, and
    SLF4J are some of the other popular logging frameworks available. These frameworks
    support both UDP as well as TCP protocols for logging. Applications send log entries
    to the console or to the filesystem. File recycling techniques are generally employed
    to avoid logs filling up all the disk space.
  prefs: []
  type: TYPE_NORMAL
- en: One of the best practices of log handling is to switch off most of the log entries
    in production due to the high cost of disk IOs. Not only do disk IOs slow down
    the application, but they can also severely impact scalability. Writing logs into
    the disk also requires high disk capacity. An out-of-disk-space scenario can bring
    down the application. Logging frameworks provide options to control logging at
    runtime to restrict what is to be printed and what not. Most of these frameworks
    provide fine-grained control over the logging controls. They also provide options
    to change these configurations at runtime.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, logs may contain important information and have high value
    if properly analyzed. Therefore, restricting log entries essentially limits our
    ability to understand the application's behavior.
  prefs: []
  type: TYPE_NORMAL
- en: When moved from traditional to cloud deployment, applications are no longer
    locked to a particular, predefined machine. Virtual machines and containers are
    not hardwired with an application. The machines used for deployment can change
    from time to time. Moreover, containers such as Docker are ephemeral. This essentially
    means that one cannot rely on the persistent state of the disk. Logs written to
    the disk are lost once the container is stopped and restarted. Therefore, we cannot
    rely on the local machine's disk to write log files.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we discussed in [Chapter 1](ch01.html "Chapter 1. Demystifying Microservices"),
    *Demystifying Microservices*, one of the principles of the Twelve-Factor app is
    to avoid routing or storing log files by the application itself. In the context
    of microservices, they will run on isolated physical or virtual machines, resulting
    in fragmented log files. In this case, it is almost impossible to trace end-to-end
    transactions that span multiple microservices:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding log management challenges](img/B05447_07_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As shown in the diagram, each microservice emits logs to a local filesystem.
    In this case, microservice M1 calls M3\. These services write their logs to their
    own local filesystems. This makes it harder to correlate and understand the end-to-end
    transaction flow. Also, as shown in the diagram, there are two instances of M1
    and two instances of M2 running on two different machines. In this case, log aggregation
    at the service level is hard to achieve.
  prefs: []
  type: TYPE_NORMAL
- en: A centralized logging solution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to address the challenges stated earlier, traditional logging solutions
    require serious rethinking. The new logging solution, in addition to addressing
    the preceding challenges, is also expected to support the capabilities summarized
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: The ability to collect all log messages and run analytics on top of the log
    messages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ability to correlate and track transactions end to end
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ability to keep log information for longer time periods for trending and
    forecasting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ability to eliminate dependency on the local disk system
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ability to aggregate log information coming from multiple sources such as
    network devices, operating system, microservices, and so on
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The solution to these problems is to centrally store and analyze all log messages,
    irrespective of the source of log. The fundamental principle employed in the new
    logging solution is to detach log storage and processing from service execution
    environments. Big data solutions are better suited to storing and processing large
    numbers of log messages more effectively than storing and processing them in microservice
    execution environments.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the centralized logging solution, log messages will be shipped from the
    execution environment to a central big data store. Log analysis and processing
    will be handled using big data solutions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A centralized logging solution](img/B05447_07_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As shown in the preceding logical diagram, there are a number of components
    in the centralized logging solution, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Log streams**: These are streams of log messages coming out of source systems.
    The source system can be microservices, other applications, or even network devices.
    In typical Java-based systems, these are equivalent to streaming Log4j log messages.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Log shippers**: Log shippers are responsible for collecting the log messages
    coming from different sources or endpoints. The log shippers then send these messages
    to another set of endpoints, such as writing to a database, pushing to a dashboard,
    or sending it to stream-processing endpoint for further real-time processing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Log store**: A log store is the place where all log messages are stored for
    real-time analysis, trending, and so on. Typically, a log store is a NoSQL database,
    such as HDFS, capable of handling large data volumes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Log stream processor**: The log stream processor is capable of analyzing
    real-time log events for quick decision making. A stream processor takes actions
    such as sending information to a dashboard, sending alerts, and so on. In the
    case of self-healing systems, stream processors can even take actions to correct
    the problems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Log dashboard**: A dashboard is a single pane of glass used to display log
    analysis results such as graphs and charts. These dashboards are meant for the
    operational and management staff.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The benefit of this centralized approach is that there is no local I/O or blocking
    disk writes. It also does not use the local machine's disk space. This architecture
    is fundamentally similar to the lambda architecture for big data processing.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To read more on the Lambda architecture, go to [http://lambda-architecture.net](http://lambda-architecture.net).
  prefs: []
  type: TYPE_NORMAL
- en: It is important to have in each log message a context, message, and correlation
    ID. The context typically has the timestamp, IP address, user information, process
    details (such as service, class, and functions), log type, classification, and
    so on. The message will be plain and simple free text information. The correlation
    ID is used to establish the link between service calls so that calls spanning
    microservices can be traced.
  prefs: []
  type: TYPE_NORMAL
- en: The selection of logging solutions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are a number of options available to implement a centralized logging solution.
    These solutions use different approaches, architectures, and technologies. It
    is important to understand the capabilities required and select the right solution
    that meets the needs.
  prefs: []
  type: TYPE_NORMAL
- en: Cloud services
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are a number of cloud logging services available, such as the SaaS solution.
  prefs: []
  type: TYPE_NORMAL
- en: Loggly is one of the most popular cloud-based logging services. Spring Boot
    microservices can use Loggly's Log4j and Logback appenders to directly stream
    log messages into the Loggly service.
  prefs: []
  type: TYPE_NORMAL
- en: If the application or service is deployed in AWS, AWS CloudTrail can be integrated
    with Loggly for log analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Papertrial, Logsene, Sumo Logic, Google Cloud Logging, and Logentries are examples
    of other cloud-based logging solutions.
  prefs: []
  type: TYPE_NORMAL
- en: The cloud logging services take away the overhead of managing complex infrastructures
    and large storage solutions by providing them as simple-to-integrate services.
    However, latency is one of the key factors to be considered when selecting cloud
    logging as a service.
  prefs: []
  type: TYPE_NORMAL
- en: Off-the-shelf solutions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are many purpose-built tools to provide end-to-end log management capabilities
    that are installable locally in an on-premises data center or in the cloud.
  prefs: []
  type: TYPE_NORMAL
- en: Graylog is one of the popular open source log management solutions. Graylog
    uses Elasticsearch for log storage and MongoDB as a metadata store. Graylog also
    uses GELF libraries for Log4j log streaming.
  prefs: []
  type: TYPE_NORMAL
- en: Splunk is one of the popular commercial tools available for log management and
    analysis. Splunk uses the log file shipping approach, compared to log streaming
    used by other solutions to collect logs.
  prefs: []
  type: TYPE_NORMAL
- en: Best-of-breed integration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The last approach is to pick and choose best-of-breed components and build a
    custom logging solution.
  prefs: []
  type: TYPE_NORMAL
- en: Log shippers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are log shippers that can be combined with other tools to build an end-to-end
    log management solution. The capabilities differ between different log shipping
    tools.
  prefs: []
  type: TYPE_NORMAL
- en: Logstash is a powerful data pipeline tool that can be used to collect and ship
    log files. Logstash acts as a broker that provides a mechanism to accept streaming
    data from different sources and sync them to different destinations. Log4j and
    Logback appenders can also be used to send log messages directly from Spring Boot
    microservices to Logstash. The other end of Logstash is connected to Elasticsearch,
    HDFS, or any other database.
  prefs: []
  type: TYPE_NORMAL
- en: Fluentd is another tool that is very similar to Logstash, as is Logspout, but
    the latter is more appropriate in a Docker container-based environment.
  prefs: []
  type: TYPE_NORMAL
- en: Log stream processors
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Stream-processing technologies are optionally used to process log streams on
    the fly. For example, if a 404 error is continuously occurring as a response to
    a particular service call, it means there is something wrong with the service.
    Such situations have to be handled as soon as possible. Stream processors are
    pretty handy in such cases as they are capable of reacting to certain streams
    of events that a traditional reactive analysis can't.
  prefs: []
  type: TYPE_NORMAL
- en: A typical architecture used for stream processing is a combination of Flume
    and Kafka together with either Storm or Spark Streaming. Log4j has Flume appenders,
    which are useful to collect log messages. These messages are pushed into distributed
    Kafka message queues. The stream processors collect data from Kafka and process
    them on the fly before sending it to Elasticsearch and other log stores.
  prefs: []
  type: TYPE_NORMAL
- en: Spring Cloud Stream, Spring Cloud Stream Modules, and Spring Cloud Data Flow
    can also be used to build the log stream processing.
  prefs: []
  type: TYPE_NORMAL
- en: Log storage
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Real-time log messages are typically stored in Elasticsearch. Elasticsearch
    allows clients to query based on text-based indexes. Apart from Elasticsearch,
    HDFS is also commonly used to store archived log messages. MongoDB or Cassandra
    is used to store summary data, such as monthly aggregated transaction counts.
    Offline log processing can be done using Hadoop's MapReduce programs.
  prefs: []
  type: TYPE_NORMAL
- en: Dashboards
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The last piece required in the central logging solution is a dashboard. The
    most commonly used dashboard for log analysis is Kibana on top of an Elasticsearch
    data store. Graphite and Grafana are also used to display log analysis reports.
  prefs: []
  type: TYPE_NORMAL
- en: A custom logging implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The tools mentioned before can be leveraged to build a custom end-to-end logging
    solution. The most commonly used architecture for custom log management is a combination
    of Logstash, Elasticsearch, and Kibana, also known as the ELK stack.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The full source code of this chapter is available under the `Chapter 7` project
    in the code files. Copy `chapter5.configserver`, `chapter5.eurekaserver`, `chapter5.search`,
    `chapter5.search-apigateway`, and `chapter5.website` into a new STS workspace
    and rename them `chapter7.*`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows the log monitoring flow:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A custom logging implementation](img/B05447_07_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In this section, a simple implementation of a custom logging solution using
    the ELK stack will be examined.
  prefs: []
  type: TYPE_NORMAL
- en: 'Follow these steps to implement the ELK stack for logging:'
  prefs: []
  type: TYPE_NORMAL
- en: Download and install Elasticsearch, Kibana, and Logstash from [https://www.elastic.co](https://www.elastic.co).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Update the Search microservice (`chapter7.search`). Review and ensure that
    there are some log statements in the Search microservice. The log statements are
    nothing special but simple log statements using `slf4j`, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Add the `logstash` dependency to integrate `logback` to Logstash in the Search
    service''s `pom.xml` file, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Also, downgrade the `logback` version to be compatible with Spring 1.3.5.RELEASE
    via the following line:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Override the default Logback configuration. This can be done by adding a new
    `logback.xml` file under `src/main/resources`, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The preceding configuration overrides the default Logback configuration by adding
    a new TCP socket `appender`, which streams all the log messages to a Logstash
    service, which is listening on port `4560`. It is important to add an encoder,
    as mentioned in the previous configuration.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a configuration as shown in the following code and store it in a `logstash.conf`
    file. The location of this file is irrelevant as it will be passed as an argument
    when starting Logstash. This configuration will take input from the socket listening
    on `4560` and send the output to Elasticsearch running on `9200`. The `stdout`
    is optional and is set to debug:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Run Logstash, Elasticsearch, and Kibana from their respective installation
    folders, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Run the Search microservice. This will invoke the unit test cases and result
    in printing the log statements mentioned before.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Go to a browser and access Kibana, at `http://localhost:5601`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Go to **Settings** | **Configure an index pattern**, as shown here:![A custom
    logging implementation](img/B05447_07_05.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Go to the **Discover** menu to see the logs. If everything is successful, we
    will see the Kibana screenshot as follows. Note that the log messages are displayed
    in the Kibana screen.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Kibana provides out-of-the-box features to build summary charts and graphs
    using log messages:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A custom logging implementation](img/B05447_07_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Distributed tracing with Spring Cloud Sleuth
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The previous section addressed microservices' distributed and fragmented logging
    issue by centralizing the log data. With the central logging solution, we can
    have all the logs in a central storage. However, it is still almost impossible
    to trace end-to-end transactions. In order to do end-to-end tracking, transactions
    spanning microservices need to have a correlation ID.
  prefs: []
  type: TYPE_NORMAL
- en: Twitter's Zipkin, Cloudera's HTrace, and Google's Dapper systems are examples
    of distributed tracing systems. Spring Cloud provides a wrapper component on top
    of these using the Spring Cloud Sleuth library.
  prefs: []
  type: TYPE_NORMAL
- en: 'Distributed tracing works with the concepts of **span** and **trace**. The
    span is a unit of work; for example, calling a service is identified by a 64-bit
    span ID. A set of spans form a tree-like structure is called a trace. Using the
    trace ID, the call can be tracked end to end:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Distributed tracing with Spring Cloud Sleuth](img/B05447_07_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As shown in the diagram, **Microservice 1** calls **Microservice 2**, and **Microservice
    2** calls **Microservice 3**. In this case, as shown in the diagram, the same
    trace ID is passed across all microservices, which can be used to track transactions
    end to end.
  prefs: []
  type: TYPE_NORMAL
- en: In order to demonstrate this, we will use the Search API Gateway and Search
    microservices. A new endpoint has to be added in Search API Gateway (`chapter7.search-apigateway`)
    that internally calls the Search service to return data. Without the trace ID,
    it is almost impossible to trace or link calls coming from the Website to Search
    API Gateway to Search microservice. In this case, it only involves two to three
    services, whereas in a complex environment, there could be many interdependent
    services.
  prefs: []
  type: TYPE_NORMAL
- en: 'Follow these steps to create the example using Sleuth:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Update Search and Search API Gateway. Before this, the Sleuth dependency needs
    to be added to the respective POM files, which can be done via the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: In the case of building a new service, select **Sleuth** and **Web**, as shown
    here:![Distributed tracing with Spring Cloud Sleuth](img/B05447_07_08.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add the Logstash dependency to the Search service as well as the Logback configuration,
    as in the previous example.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The next step is to add two more properties in the Logback configuration, as
    follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The first property is the name of the application. The names given in this
    are the service IDs: `search-service` and `search-apigateway` in Search and Search
    API Gateway, respectively. The second property is an optional pattern used to
    print the console log messages with a trace ID and span ID. The preceding change
    needs to be applied to both the services.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Add the following piece of code to advise Sleuth when to start a new span ID
    in the Spring Boot Application class. In this case, `AlwaysSampler` is used to
    indicate that the span ID has to be created every time a call hits the service.
    This change needs to be applied in both the services:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Add a new endpoint to Search API Gateway, which will call the Search service
    as follows. This is to demonstrate the propagation of the trace ID across multiple
    microservices. This new method in the gateway returns the operating hub of the
    airport by calling the Search service, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Add another endpoint in the Search service, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Once added, run both the services. Hit the gateway's new hub on the gateway
    (`/hubongw`) endpoint using a browser ( `http://localhost:8095/hubongw`).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As mentioned earlier, the Search API Gateway service is running on `8095` and
    the Search service is running on `8090`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Look at the console logs to see the trace ID and span IDs printed. The first
    print is from Search API Gateway, and the second one came from the Search service.
    Note that the trace IDs are the same in both the cases, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Open the Kibana console and search for the trace ID using this trace ID printed
    in the console. In this case, it is `8a7e278f-7b2b-43e3-a45c-69d3ca66d663`. As
    shown in the following screenshot, with a trace ID, one can trace service calls
    that span multiple services:![Distributed tracing with Spring Cloud Sleuth](img/B05447_07_09.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Monitoring microservices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Microservices are truly distributed systems with a fluid deployment topology.
    Without sophisticated monitoring in place, operations teams may run into trouble
    managing large-scale microservices. Traditional monolithic application deployments
    are limited to a number of known services, instances, machines, and so on. This
    is easier to manage compared to the large number of microservices instances potentially
    running across different machines. To add more complication, these services dynamically
    change their topologies. A centralized logging capability only addresses part
    of the issue. It is important for operations teams to understand the runtime deployment
    topology and also the behavior of the systems. This demands more than a centralized
    logging can offer.
  prefs: []
  type: TYPE_NORMAL
- en: In general application, monitoring is more a collection of metrics, aggregation,
    and their validation against certain baseline values. If there is a service-level
    breach, then monitoring tools generate alerts and send them to administrators.
    With hundreds and thousands of interconnected microservices, traditional monitoring
    does not really offer true value. The one-size-fits-all approach to monitoring
    or monitoring everything with a single pane of glass is not easy to achieve in
    large-scale microservices.
  prefs: []
  type: TYPE_NORMAL
- en: One of the main objectives of microservice monitoring is to understand the behavior
    of the system from a user experience point of view. This will ensure that the
    end-to-end behavior is consistent and is in line with what is expected by the
    users.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring challenges
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Similar to the fragmented logging issue, the key challenge in monitoring microservices
    is that there are many moving parts in a microservice ecosystem.
  prefs: []
  type: TYPE_NORMAL
- en: 'The typical issues are summarized here:'
  prefs: []
  type: TYPE_NORMAL
- en: The statistics and metrics are fragmented across many services, instances, and
    machines.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Heterogeneous technologies may be used to implement microservices, which makes
    things even more complex. A single monitoring tool may not give all the required
    monitoring options.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Microservices deployment topologies are dynamic, making it impossible to preconfigure
    servers, instances, and monitoring parameters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Many of the traditional monitoring tools are good to monitor monolithic applications
    but fall short in monitoring large-scale, distributed, interlinked microservice
    systems. Many of the traditional monitoring systems are agent-based preinstall
    agents on the target machines or application instances. This poses two challenges:'
  prefs: []
  type: TYPE_NORMAL
- en: If the agents require deep integration with the services or operating systems,
    then this will be hard to manage in a dynamic environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If these tools impose overheads when monitoring or instrumenting the application,
    it may lead to performance issues
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Many traditional tools need baseline metrics. Such systems work with preset
    rules, such as if the CPU utilization goes above 60% and remains at this level
    for 2 minutes, then an alert should be sent to the administrator. It is extremely
    hard to preconfigure these values in large, Internet-scale deployments.
  prefs: []
  type: TYPE_NORMAL
- en: 'New-generation monitoring applications learn the application''s behavior by
    themselves and set automatic threshold values. This frees up administrators from
    doing this mundane task. Automated baselines are sometimes more accurate than
    human forecasts:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Monitoring challenges](img/B05447_07_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As shown in the diagram, the key areas of microservices monitoring are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Metrics sources and data collectors**: Metrics collection at the source is
    done either by the server pushing metrics information to a central collector or
    by embedding lightweight agents to collect information. Data collectors collect
    monitoring metrics from different sources, such as network, physical machines,
    containers, software components, applications, and so on. The challenge is to
    collect this data using autodiscovery mechanisms instead of static configurations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is done by either running agents on the source machines, streaming data
    from the sources, or polling at regular intervals.
  prefs: []
  type: TYPE_NORMAL
- en: '**Aggregation and correlation of metrics**: Aggregation capability is required
    for aggregating metrics collected from different sources, such as user transaction,
    service, infrastructure, network, and so on. Aggregation can be challenging as
    it requires some level of understanding of the application''s behavior, such as
    service dependencies, service grouping, and so on. In many cases, these are automatically
    formulated based on the metadata provided by the sources.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generally, this is done by an intermediary that accept the metrics.
  prefs: []
  type: TYPE_NORMAL
- en: '**Processing metrics and actionable insights**: Once data is aggregated, the
    next step is to do the measurement. Measurements are typically done using set
    thresholds. In the new-generation monitoring systems, these thresholds are automatically
    discovered. Monitoring tools then analyze the data and provide actionable insights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These tools may use big data and stream analytics solutions.
  prefs: []
  type: TYPE_NORMAL
- en: '**Alerting, actions, and dashboards**: As soon as issues are detected, they
    have to be notified to the relevant people or systems. Unlike traditional systems,
    the microservices monitoring systems should be capable of taking actions on a
    real-time basis. Proactive monitoring is essential to achieving self-healing.
    Dashboards are used to display SLAs, KPIs, and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dashboards and alerting tools are capable of handling these requirements.
  prefs: []
  type: TYPE_NORMAL
- en: 'Microservice monitoring is typically done with three approaches. A combination
    of these is really required for effective monitoring:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Application performance monitoring** (**APM**): This is more of a traditional
    approach to system metrics collection, processing, alerting, and dashboard rendering.
    These are more from the system''s point of view. Application topology discovery
    and visualization are new capabilities implemented by many of the APM tools. The
    capabilities vary between different APM providers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Synthetic monitoring**: This is a technique that is used to monitor the system''s
    behavior using end-to-end transactions with a number of test scenarios in a production
    or production-like environment. Data is collected to validate the system''s behavior
    and potential hotspots. Synthetic monitoring helps understand the system dependencies
    as well.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Real user monitoring** (**RUM**) or **user experience monitoring**: This
    is typically a browser-based software that records real user statistics, such
    as response time, availability, and service levels. With microservices, with more
    frequent release cycle and dynamic topology, user experience monitoring is more
    important.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring tools
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are many tools available to monitor microservices. There are also overlaps
    between many of these tools. The selection of monitoring tools really depends
    upon the ecosystem that needs to be monitored. In most cases, more than one tool
    is required to monitor the overall microservice ecosystem.
  prefs: []
  type: TYPE_NORMAL
- en: 'The objective of this section is to familiarize ourselves with a number of
    common microservices-friendly monitoring tools:'
  prefs: []
  type: TYPE_NORMAL
- en: AppDynamics, Dynatrace, and New Relic are top commercial vendors in the APM
    space, as per Gartner Magic Quadrant 2015\. These tools are microservice friendly
    and support microservice monitoring effectively in a single console. Ruxit, Datadog,
    and Dataloop are other commercial offerings that are purpose-built for distributed
    systems that are essentially microservices friendly. Multiple monitoring tools
    can feed data to Datadog using plugins.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cloud vendors come with their own monitoring tools, but in many cases, these
    monitoring tools alone may not be sufficient for large-scale microservice monitoring.
    For instance, AWS uses CloudWatch and Google Cloud Platform uses Cloud Monitoring
    to collect information from various sources.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some of the data collecting libraries, such as Zabbix, statd, collectd, jmxtrans,
    and so on operate at a lower level in collecting runtime statistics, metrics,
    gauges, and counters. Typically, this information is fed into data collectors
    and processors such as Riemann, Datadog, and Librato, or dashboards such as Graphite.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spring Boot Actuator is one of the good vehicles to collect microservices metrics,
    gauges, and counters, as we discussed in [Chapter 2](ch02.html "Chapter 2. Building
    Microservices with Spring Boot"), *Building Microservices with Spring Boot*. Netflix
    Servo, a metric collector similar to Actuator, and the QBit and Dropwizard metrics
    also fall in the same category of metric collectors. All these metrics collectors
    need an aggregator and dashboard to facilitate full-sized monitoring.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring through logging is popular but a less effective approach in microservices
    monitoring. In this approach, as discussed in the previous section, log messages
    are shipped from various sources, such as microservices, containers, networks,
    and so on to a central location. Then, we can use the logs files to trace transactions,
    identify hotspots, and so on. Loggly, ELK, Splunk, and Trace are candidates in
    this space.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sensu is a popular choice for microservice monitoring from the open source community.
    Weave Scope is another tool, primarily targeting containerized deployments. Spigo
    is one of the purpose-built microservices monitoring systems closely aligned with
    the Netflix stack.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pingdom, New Relic Synthetics, Runscope, Catchpoint, and so on provide options
    for synthetic transaction monitoring and user experience monitoring on live systems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Circonus is classified more as a DevOps monitoring tool but can also do microservices
    monitoring. Nagios is a popular open source monitoring tool but falls more into
    the traditional monitoring system.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prometheus provides a time series database and visualization GUI useful in building
    custom monitoring tools.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring microservice dependencies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When there are a large number of microservices with dependencies, it is important
    to have a monitoring tool that can show the dependencies among microservices.
    It is not a scalable approach to statically configure and manage these dependencies.
    There are many tools that are useful in monitoring microservice dependencies,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Mentoring tools such as AppDynamics, Dynatrace, and New Relic can draw dependencies
    among microservices. End-to-end transaction monitoring can also trace transaction
    dependencies. Other monitoring tools, such as Spigo, are also useful for microservices
    dependency management.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CMDB tools such as Device42 or purpose-built tools such as Accordance are useful
    in managing the dependency of microservices. **Veritas Risk Advisor** (**VRA**)
    is also useful for infrastructure discovery.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A custom implementation with a Graph database, such as Neo4j, is also useful.
    In this case, a microservice has to preconfigure its direct and indirect dependencies.
    At the startup of the service, it publishes and cross-checks its dependencies
    with a Neo4j database.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spring Cloud Hystrix for fault-tolerant microservices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section will explore Spring Cloud Hystrix as a library for a fault-tolerant
    and latency-tolerant microservice implementation. Hystrix is based on the *fail
    fast* and *rapid recovery* principles. If there is an issue with a service, Hystrix
    helps isolate it. It helps to recover quickly by falling back to another preconfigured
    fallback service. Hystrix is another battle-tested library from Netflix. Hystrix
    is based on the circuit breaker pattern.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Read more about the circuit breaker pattern at [https://msdn.microsoft.com/en-us/library/dn589784.aspx](https://msdn.microsoft.com/en-us/library/dn589784.aspx).
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will build a circuit breaker with Spring Cloud Hystrix.
    Perform the following steps to change the Search API Gateway service to integrate
    it with Hystrix:'
  prefs: []
  type: TYPE_NORMAL
- en: Update the Search API Gateway service. Add the Hystrix dependency to the service.
    If developing from scratch, select the following libraries:![Spring Cloud Hystrix
    for fault-tolerant microservices](img/B05447_07_11.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the Spring Boot Application class, add `@EnableCircuitBreaker`. This command
    will tell Spring Cloud Hystrix to enable a circuit breaker for this application.
    It also exposes the `/hystrix.stream` endpoint for metrics collection.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Add a component class to the Search API Gateway service with a method; in this
    case, this is `getHub` annotated with `@HystrixCommand`. This tells Spring that
    this method is prone to failure. Spring Cloud libraries wrap these methods to
    handle fault tolerance and latency tolerance by enabling circuit breaker. The
    Hystrix command typically follows with a fallback method. In case of failure,
    Hystrix automatically enables the fallback method mentioned and diverts traffic
    to the fallback method. As shown in the following code, in this case, `getHub`
    will fall back to `getDefaultHub`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The `getHub` method of `SearchAPIGatewayController` calls the `getHub` method
    of `SearchAPIGatewayComponent`, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The last part of this exercise is to build a Hystrix Dashboard. For this, build
    another Spring Boot application. Include Hystrix, Hystrix Dashboard, and Actuator
    when building this application.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the Spring Boot Application class, add the `@EnableHystrixDashboard` annotation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Start the Search service, Search API Gateway, and Hystrix Dashboard applications.
    Point the browser to the Hystrix Dashboard application's URL. In this example,
    the Hystrix Dashboard is started on port `9999`. So, open the URL `http://localhost:9999/hystrix`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A screen similar to the following screenshot will be displayed. In the Hystrix
    Dashboard, enter the URL of the service to be monitored.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In this case, Search API Gateway is running on port `8095`. Hence, the `hystrix.stream`
    URL will be `http://localhost:8095/hytrix.stream`, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Spring Cloud Hystrix for fault-tolerant microservices](img/B05447_07_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The Hystrix Dashboard will be displayed as follows:![Spring Cloud Hystrix for
    fault-tolerant microservices](img/B05447_07_13.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note that at least one transaction has to be executed to see the display. This
    can be done by hitting `http://localhost:8095/hubongw`.
  prefs: []
  type: TYPE_NORMAL
- en: Create a failure scenario by shutting down the Search service. Note that the
    fallback method will be called when hitting the URL `http://localhost:8095/hubongw`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If there are continuous failures, then the circuit status will be changed to
    open. This can be done by hitting the preceding URL a number of times. In the
    open state, the original service will no longer be checked. The Hystrix Dashboard
    will show the status of the circuit as **Open**, as shown in the following screenshot.
    Once a circuit is opened, periodically, the system will check for the original
    service status for recovery. When the original service is back, the circuit breaker
    will fall back to the original service and the status will be set to **Closed**:![Spring
    Cloud Hystrix for fault-tolerant microservices](img/B05447_07_14.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To know the meaning of each of these parameters, visit the Hystrix wiki at [https://github.com/Netflix/Hystrix/wiki/Dashboard](https://github.com/Netflix/Hystrix/wiki/Dashboard).
  prefs: []
  type: TYPE_NORMAL
- en: Aggregating Hystrix streams with Turbine
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous example, the `/hystrix.stream` endpoint of our microservice
    was given in the Hystrix Dashboard. The Hystrix Dashboard can only monitor one
    microservice at a time. If there are many microservices, then the Hystrix Dashboard
    pointing to the service has to be changed every time we switch the microservices
    to monitor. Looking into one instance at a time is tedious, especially when there
    are many instances of a microservice or multiple microservices.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have to have a mechanism to aggregate data coming from multiple `/hystrix.stream`
    instances and consolidate it into a single dashboard view. Turbine does exactly
    the same thing. Turbine is another server that collects Hystrix streams from multiple
    instances and consolidates them into one `/turbine.stream` instance. Now, the
    Hystrix Dashboard can point to `/turbine.stream` to get the consolidated information:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Aggregating Hystrix streams with Turbine](img/B05447_07_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Turbine currently works only with different hostnames. Each instance has to
    be run on separate hosts. If you are testing multiple services locally on the
    same host, then update the host file (`/etc/hosts`) to simulate multiple hosts.
    Once done, `bootstrap.properties` has to be configured as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: This example showcases how to use Turbine to monitor circuit breakers across
    multiple instances and services. We will use the Search service and Search API
    Gateway in this example. Turbine internally uses Eureka to resolve service IDs
    that are configured for monitoring.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps to build and execute this example:'
  prefs: []
  type: TYPE_NORMAL
- en: The Turbine server can be created as just another Spring Boot application using
    Spring Boot Starter. Select Turbine to include the Turbine libraries.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Once the application is created, add `@EnableTurbine` to the main Spring Boot
    Application class. In this example, both Turbine and Hystrix Dashboard are configured
    to be run on the same Spring Boot application. This is possible by adding the
    following annotations to the newly created Turbine application:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Add the following configuration to the `.yaml` or property file to point to
    the instances that we are interested in monitoring:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The preceding configuration instructs the Turbine server to look up the Eureka
    server to resolve the `search-service` and `search-apigateway` services. The `search-service`
    and `search-apigateways` service IDs are used to register services with Eureka.
    Turbine uses these names to resolve the actual service host and port by checking
    with the Eureka server. It will then use this information to read `/hystrix.stream`
    from each of these instances. Turbine will then read all the individual Hystrix
    streams, aggregate all of them, and expose them under the Turbine server's `/turbine.stream`
    URL.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The cluster name expression is pointing to the default cluster as there is
    no explicit cluster configuration done in this example. If the clusters are manually
    configured, then the following configuration has to be used:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Change the Search service''s `SearchComponent` to add another circuit breaker,
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Also, add `@EnableCircuitBreaker` to the main Application class in the Search
    service.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Add the following configuration to `bootstrap.properties` of the Search service.
    This is required because all the services are running on the same host:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, add the following in `bootstrap.properties` of the Search API Gateway
    service. This is to make sure that both the services use different hostnames:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'In this example, we will run two instances of `search-apigateway`: one on `localdomain1:8095`
    and another one on `localdomain2:8096`. We will also run one instance of `search-service`
    on `localdomain1:8090`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run the microservices with command-line overrides to manage different host
    addresses, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Open Hystrix Dashboard by pointing the browser to `http://localhost:9090/hystrix`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Instead of giving `/hystrix.stream`, this time, we will point to `/turbine.stream`.
    In this example, the Turbine stream is running on `9090`. Hence, the URL to be
    given in the Hystrix Dashboard is `http://localhost:9090/turbine.stream`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Fire a few transactions by opening a browser window and hitting the following
    two URLs: `http://localhost:8095/hubongw` and `http://localhost:8096/hubongw`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once this is done, the dashboard page will show the **getHub** service.
  prefs: []
  type: TYPE_NORMAL
- en: Run `chapter7.website`. Execute the search transaction using the website `http://localhost:8001`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'After executing the preceding search, the dashboard page will show **search-service**
    as well. This is shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Aggregating Hystrix streams with Turbine](img/B05447_07_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As we can see in the dashboard, **search-service** is coming from the Search
    microservice, and **getHub** is coming from Search API Gateway. As we have two
    instances of Search API Gateway, **getHub** is coming from two hosts, indicated
    by **Hosts 2**.
  prefs: []
  type: TYPE_NORMAL
- en: Data analysis using data lakes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Similarly to the scenario of fragmented logs and monitoring, fragmented data
    is another challenge in the microservice architecture. Fragmented data poses challenges
    in data analytics. This data may be used for simple business event monitoring,
    data auditing, or even deriving business intelligence out of the data.
  prefs: []
  type: TYPE_NORMAL
- en: A data lake or data hub is an ideal solution to handling such scenarios. An
    event-sourced architecture pattern is generally used to share the state and state
    changes as events with an external data store. When there is a state change, microservices
    publish the state change as events. Interested parties may subscribe to these
    events and process them based on their requirements. A central event store may
    also subscribe to these events and store them in a big data store for further
    analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the commonly followed architectures for such data handling is shown
    in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Data analysis using data lakes](img/B05447_07_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: State change events generated from the microservice—in our case, the **Search**,
    **Booking**, and **Check-In** events—are pushed to a distributed high-performance
    messaging system, such as Kafka. A data ingestion service, such as Flume, can
    subscribe to these events and update them to an HDFS cluster. In some cases, these
    messages will be processed in real time by Spark Streaming. To handle heterogeneous
    sources of events, Flume can also be used between event sources and Kafka.
  prefs: []
  type: TYPE_NORMAL
- en: Spring Cloud Streams, Spring Cloud Streams modules, and Spring Data Flow are
    also useful as alternatives for high-velocity data ingestion.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned about the challenges around logging and monitoring
    when dealing with Internet-scale microservices.
  prefs: []
  type: TYPE_NORMAL
- en: We explored the various solutions for centralized logging. You also learned
    about how to implement a custom centralized logging using Elasticsearch, Logstash,
    and Kibana (ELK). In order to understand distributed tracing, we upgraded BrownField
    microservices using Spring Cloud Sleuth.
  prefs: []
  type: TYPE_NORMAL
- en: In the second half of this chapter, we went deeper into the capabilities required
    for microservices monitoring solutions and different approaches to monitoring.
    Subsequently, we examined a number of tools available for microservices monitoring.
  prefs: []
  type: TYPE_NORMAL
- en: The BrownField microservices are further enhanced with Spring Cloud Hystrix
    and Turbine to monitor latencies and failures in inter-service communications.
    The examples also demonstrated how to use the circuit breaker pattern to fall
    back to another service in case of failures.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we also touched upon the importance of data lakes and how to integrate
    a data lake architecture in a microservice context.
  prefs: []
  type: TYPE_NORMAL
- en: Microservice management is another important challenge we need to tackle when
    dealing with large-scale microservice deployments. The next chapter will explore
    how containers can help in simplifying microservice management.
  prefs: []
  type: TYPE_NORMAL
