- en: '*Chapter 10*: Load Balancer Services for Linux'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we'll be discussing the load balancer services that are available
    for Linux, specifically HAProxy. Load balancers allow client workloads to be spread
    across multiple backend servers. This allows a single IP to scale larger than
    a single server may allow, and also allows for redundancy in the case of a server
    outage or maintenance window.
  prefs: []
  type: TYPE_NORMAL
- en: Once you've completed these examples, you should have the skills to deploy Linux-based
    load balancer services in your own environment via several different methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'In particular, we''ll cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to load balancing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Load balancing algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Server and service health checks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Datacenter load balancer design considerations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a HAProxy NAT/proxy load balancer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A final note on load balancer security
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Because of the complexity of setting up the infrastructure for this section,
    there are a few choices you can make with respect to the example configurations.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we''ll be exploring load balancer functions. As we work through
    the examples later in this book, you can follow along and implement our example
    configurations in your current Ubuntu host or virtual machine. However, to see
    our load balancing example in action, you''ll need a number of things:'
  prefs: []
  type: TYPE_NORMAL
- en: At least two target hosts to balance a load across
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another network adapter in the current Linux host
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another subnet to host the target hosts and this new network adapter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This configuration has a matching diagram, *Figure 10.2*, which will be shown
    later in this chapter that illustrates how all this will bolt together when we're
    done.
  prefs: []
  type: TYPE_NORMAL
- en: This adds a whole level of complexity to the configuration of our lab environment.
    When we get to the lab section, we'll offer some alternatives (downloading a pre-built
    virtual machine is one of them), but you may just choose to read along. If that's
    the case, I think you'll still get a good introduction to the topic, along with
    a solid background of the design, implementation, and security implications of
    various load balancer configurations in a modern datacenter.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to load balancing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In its simplest form, load balancing is all about spreading client load across
    multiple servers. These servers can be in one or several locations, and the method
    of distributing that load can vary quite a bit. In fact, how successful you are
    in spreading that load evenly can vary quite a bit as well (mostly depending on
    the method chosen). Let's explore some of the more common methods of load balancing.
  prefs: []
  type: TYPE_NORMAL
- en: Round Robin DNS (RRDNS)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can do simple load balancing just with a DNS server, in what''s called
    `a.example.com` hostname, the DNS server will return the IP of Server 1; then,
    when the next client requests it, it will return the IP for Server 2, and so on.
    This is the simplest load balancing method and works equally well for both co-located
    servers and servers in different locations. It can also be implemented with no
    changes at all to the infrastructure – no new components and no configuration
    changes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.1 – Simple load balancing with Round Robin DNS'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16336_10_001.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.1 – Simple load balancing with Round Robin DNS
  prefs: []
  type: TYPE_NORMAL
- en: 'Configuring RRDNS is simple – in BIND, simply configure multiple `A` records
    for the target hostname with multiple IPs. Successive DNS requests will return
    each `A` record in sequence. It''s a good idea to shorten the domain''s `A` records
    in sequence), random, or fixed (always return matching records in the same order).
    The syntax for changing the return order is as follows (`cyclic`, the default
    setting, is shown here):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'There are a few issues with this configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: There is no good way to incorporate any kind of health check in this model –
    are all the servers operating correctly? Are the services up? Are the hosts even
    up?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is no way of seeing if any DNS request is then actually followed up with
    a connection to the service. There are various reasons why DNS requests might
    be made, and the interaction might end there, with no subsequent connection.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is also no way to monitor when sessions end, which means there's no way
    to send the next request to the least used server – it's just a steady, even rotation
    between all servers. So, at the beginning of any business day, this may seem like
    a good model, but as the day progresses, there will always be longer-lived sessions
    and extremely short ones (or sessions that didn't occur at all), so it's common
    to see the server loads become "lop-sided" as the day progresses. This can become
    even more pronounced if there is no clear beginning or end of the day to effectively
    "zero things out."
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the same reason, if one server in the cluster is brought down for maintenance
    or an unplanned outage, there is no good way to bring it back to parity (as far
    as the session count goes).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With a bit of DNS reconnaissance, an attacker can collect the real IPs of all
    cluster members, then assess them or attack them separately. If any of them is
    particularly vulnerable or has an additional DNS entry identifying it as a backup
    host, this makes the attacker's job even easier.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Taking any of the target servers offline can be a problem – the DNS server will
    continue to serve that address up in the order it was requested. Even if the record
    is edited, any downstream clients and DNS servers will cache their resolved IPs
    and continue to try to connect to the down host.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Downstream DNS servers (that is, those on the internet) will cache whatever
    record they get for the zone's TTL period. So, all the clients of any of those
    DNS servers will get sent to the same target server.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For these reasons, RRDNS will do the job in a simple way "in a pinch," but this
    should not normally implemented as a long-term, production solution. That said,
    the **Global Server Load Balancer** (**GSLB**) products are actually based on
    this approach, with different load balancing options and health checks. The disconnect
    between the load balancer and the target server remains in GSLB, so many of these
    same disadvantages carry through to this solution.
  prefs: []
  type: TYPE_NORMAL
- en: What we see more often in datacenters is either proxy-based (Layer 7) or NAT-based
    (Layer 4) load balancing. Let's explore these two options.
  prefs: []
  type: TYPE_NORMAL
- en: Inbound proxy – Layer 7 load balancing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this architecture, the client's sessions are terminated at the proxy server,
    and a new session is started between the inside interface of the proxy and the
    real server IP.
  prefs: []
  type: TYPE_NORMAL
- en: 'This also brings up several architectural terms that are common to many of
    the load balancing solutions. In the following diagram, can we see the concept
    of the **FRONTEND**, which faces the clients, and the **BACKEND**, which faces
    the servers. We should also discuss IP addressing at this point. The frontend
    presents a **Virtual IP** (**VIP**) that is shared by all of target servers, and
    the servers'' **Real IPs** (**RIPs**) are not seen by the clients at all:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.2 – Load balancing using a reverse proxy'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16336_10_002.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.2 – Load balancing using a reverse proxy
  prefs: []
  type: TYPE_NORMAL
- en: 'This approach has a few disadvantages:'
  prefs: []
  type: TYPE_NORMAL
- en: It has the highest CPU load on the load balancer out of all the methods we will
    discuss in this chapter and can, in extreme cases, translate into a performance
    impact on the client's end.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition, because the client traffic on the target server all comes from
    the proxy server (or servers), without some special handling, the client IP seen
    on the target/application server will always be the backend IP of the load balancer.
    This makes logging direct client interaction in the application problematic. To
    parse out traffic from one session and relate it to a client's actual address,
    we must match the client session from the load balancer (which sees the client
    IP address but not the user identity) with the application/web server logs (which
    sees the user identity but not the client IP address). Matching the session between
    these logs can be a real issue; the common elements between them are the timestamp
    and the source port at the load balancer, and the source ports are often not on
    the web server.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This can be mitigated by having some application awareness. For instance, it's
    common to see a TLS frontend for a Citrix ICA Server or Microsoft RDP server backend.
    In those cases, the proxy server has some excellent "hooks" into the protocol,
    allowing the client IP address to be carried all the way through to the server,
    as well as the identity being detected by the load balancer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On the plus side, though, using a proxy architecture allows us to fully inspect
    the traffic for attacks, if the tooling is in place. In fact, because of the proxy
    architecture, that final "hop" between the load balancer and the target servers
    is an entirely new session – this means that invalid protocol attacks are mostly
    filtered out, without any special configuration being required at all.
  prefs: []
  type: TYPE_NORMAL
- en: We can mitigate some of the complexity of this proxy approach by running the
    load balancer as an inbound **Network Address Translation** (**NAT**) configuration.
    When decryption is not required, the NAT approach is commonly seen, built into
    most environments.
  prefs: []
  type: TYPE_NORMAL
- en: Inbound NAT – Layer 4 load balancing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This is the most common solution and is the one we''ll start with in our example.
    In a lot of ways, the architecture looks similar to the proxy solution, but with
    a few key differences. Note that in the following diagram, the TCP sessions on
    the frontend and backend now match – this is because the load balancer is no longer
    a proxy; it has been configured for an inbound NAT service. All the clients still
    attach to the single VIP and are redirected to the various server RIPs by the
    load balancer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.3 – Load balancing using inbound NAT'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16336_10_003.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.3 – Load balancing using inbound NAT
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several reasons why this is a preferred architecture in many cases:'
  prefs: []
  type: TYPE_NORMAL
- en: The servers see the real IPs of the client, and the server logs correctly reflect
    that.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The load balancer maintains the NAT table in memory, and the load balancer logs
    reflect the various NAT operations but can't "see" the session. For instance,
    if the servers are running an HTTPS session, if this is a simple Layer 4 NAT,
    then the load balancer can see the TCP session but can't decrypt the traffic.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have the option of terminating the HTTPS session on the frontend, then either
    running encrypted or clear text on the backend in this architecture. However,
    since we're maintaining two sessions (the frontend and backend), this starts to
    look more like a proxy configuration.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Because the entire TCP session (up to Layer 4) is seen by the load balancer,
    several load balancing algorithms are now available (see the next section on load
    balancing algorithms for more information).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This architecture allows us to place **Web Application Firewall** (**WAF**)
    functions on the load balancer, which can mask some vulnerabilities on target
    server web applications. For example, a WAF is a common defense against cross-site
    scripting or buffer overflow attacks, or any other attack that might rely on a
    lapse in input validation. For those types of attacks, the WAF identifies what
    is an acceptable input for any given field or URI, and then drops any inputs that
    don't match. However, WAFs are not limited to those attacks. Think of a WAF function
    as a web-specific IPS (see [*Chapter 14*](B16336_14_Final_NM_ePub.xhtml#_idTextAnchor252),
    *Honeypot Services on Linux*).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This architecture is well-suited to making sessions persistent or "sticky" –
    by that, we mean that once a client session is "attached" to a server, subsequent
    requests will be directed to that same server. This is well-suited for pages that
    have a backend database, for instance, where if you didn't keep the same backend
    server, your activity (for instance, a shopping cart on an ecommerce site) might
    be lost. Dynamic or parametric websites – where the pages are generated in real
    time as you navigate (for instance, most sites that have a product catalog or
    inventory) – also usually need session persistence.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can also load balance each successive request independently, so, for instance,
    as a client navigates a site, their session might be terminated by a different
    server for each page. This type of approach is well-suited to static websites.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can layer other functions on top of this architecture. For instance, these
    are often deployed in parallel with a firewall or even with a native interface
    on the public internet. Because of this, you'll often see load balancer vendors
    with VPN clients to go with their load balancers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As shown in the preceding diagram, an inbound NAT and proxy load balancer have
    a very similar topology – the connections all look very similar. This is carried
    through to the implementation, where it's possible to see some things proxied
    and some things running through the NAT process on the same load balancer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, even though the CPU impact of this configuration is much lower than
    the proxy solution, every workload packet must route through the load balancer,
    in both directions. We can reduce this impact dramatically using the **Direct
    Server Return** (**DSR**) architecture.
  prefs: []
  type: TYPE_NORMAL
- en: DSR load balancing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In DSR, all the incoming traffic is still load balanced from the VIP on the
    load balancer to the various server RIPs. However, the return traffic goes directly
    from the server back to the client, bypassing the load balancer.
  prefs: []
  type: TYPE_NORMAL
- en: 'How can this work? Here''s how:'
  prefs: []
  type: TYPE_NORMAL
- en: On the way in, the load balancer rewrites the MAC address of each packet, load
    balancing them across the MAC addresses of the target servers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each server has a loopback address, a configured address that matches the VIP
    address. This is the interface that returns all the traffic (because the client
    is expecting return traffic from the VIP address). However, it must be configured
    to not reply to ARP requests (otherwise, the load balancer will be bypassed on
    the inbound path).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This may seem convoluted, but the following diagram should make things a bit
    clearer. Note that there is only one target host in this diagram, to make the
    traffic flow a bit easier to see:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.4 – DSR load balancing'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16336_10_004.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.4 – DSR load balancing
  prefs: []
  type: TYPE_NORMAL
- en: 'There are some pretty strict requirements for building this out:'
  prefs: []
  type: TYPE_NORMAL
- en: The load balancer and all the target servers need to be on the same subnet.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This mechanism requires some games on the default gateway, since, on the way
    in, it must direct all the client traffic to the VIP on the load balancer, but
    it also has to accept replies from multiple target servers with the same address
    but different MAC addresses. The Layer 3 default gateway for this must have an
    ARP entry for each of the target servers, all with the same IP address. In many
    architectures, this is done via multiple static ARP entries. For instance, on
    a Cisco router, we would do the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Note in this example that `192.168.124.21` and `22` are the target hosts being
    load balanced. Also, the MAC addresses have an OUI, indicating that they're both
    VMware virtual hosts, which is typical in most datacenters.
  prefs: []
  type: TYPE_NORMAL
- en: Why would you go through all this bother and unusual network configuration?
  prefs: []
  type: TYPE_NORMAL
- en: A DSR configuration has the advantage of minimizing the traffic through the
    load balancer by a wide margin. In web applications, for instance, it's common
    to see the return traffic outweigh the incoming traffic by a factor of 10 or more.
    This means that for that traffic model, a DSR implementation will see 90% or less
    of the traffic that a NAT or proxy load balancer will see.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No "backend" subnet is required; the load balancer and the target servers are
    all in the same subnet – in fact, that's a requirement. This has some disadvantages
    too, as we've already discussed. We'll cover this in more detail in the *Specific
    Server Settings for DSR* section.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'However, there are some downsides:'
  prefs: []
  type: TYPE_NORMAL
- en: The relative load across the cluster, or the individual load on any one server,
    is, at best, inferred by the load balancer. If a session ends gracefully, the
    load balancer will catch enough of the "end of session" handshake to figure out
    that a session has ended, but if a session doesn't end gracefully, it depends
    entirely on timeouts to end a session.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All the hosts must be configured with the same IP (the original target) so that
    the return traffic doesn't come from an unexpected address. This is normally done
    with a loopback interface, and usually requires some additional configuration
    on the host.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The upstream router (or Layer 3 switch, if that's the gateway for the subnet)
    needs to be configured to allow all the possible MAC addresses for the target
    IP address. This is a manual process, and if it's possible to see MAC addresses
    change unexpectedly, this can be a problem.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If any function that needs proxying or full visibility of the session (as in
    the NAT implementation) can't work, the load balancer only sees half of the session.
    This means that any HTTP header parsing, cookie manipulation (for instance, for
    session persistence), or SYN cookies can't be implemented.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition, because (as far as the router is concerned), all the target hosts
    have different MAC addresses but the same IP address, and the target hosts cannot
    reply to any ARP requests (otherwise, they'd bypass the load balancer), there's
    a fair amount of work that needs to be done on the target hosts.
  prefs: []
  type: TYPE_NORMAL
- en: Specific server settings for DSR
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For a Linux client, ARP suppression for the "VIP" addressed interface (whether
    it's a loopback or a logical Ethernet) must be done. This can be completed with
    `sudo ip link set <interface name> arp off` or (using the older `ifconfig` syntax)
    `sudo ifconfig <interface name> -arp`.
  prefs: []
  type: TYPE_NORMAL
- en: You'll also need to implement the `strong host` and `weak host` settings on
    the target servers. A server interface is configured as a `strong host` if it's
    not a router and cannot send or receive packets from an interface, unless the
    source or destination IP in the packet matches the interface IP. If an interface
    has been configured as a `weak host`, this restriction doesn't apply – it can
    receive or send packets on behalf of other interfaces.
  prefs: []
  type: TYPE_NORMAL
- en: 'Linux and BSD Unix have `weak host` enabled by default on all interfaces (`sysctl
    net.ip.ip.check_interface = 0`). Windows 2003 and older also have this enabled.
    However, Windows Server 2008 and newer have a `strong host` model for all interfaces.
    To change this for DSR in newer Windows versions, execute the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: You'll also need to disable any IP checksum offload and TCP checksum offload
    functions on the target servers. On a Windows host, these two settings are in
    the `Network Adapter/Advanced` settings. On a Linux host, the `ethtool` command
    can manipulate these settings, but these hardware-based offload features are disabled
    by default in Linux, so normally, you won't need to adjust them.
  prefs: []
  type: TYPE_NORMAL
- en: With the various architectures described, we still need to work out how exactly
    we want to distribute the client load across our group of target servers.
  prefs: []
  type: TYPE_NORMAL
- en: Load balancing algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far, we''ve touched on a few load balancing algorithms, so let''s explore
    the more common approaches in a bit more detail (note that this list is not exhaustive;
    just the most commonly seen methods have been provided here):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16336_10_Table_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Least Connections**, as you may expect, is the most often assigned algorithm.
    We''ll use this method in the configuration examples later in this chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we've seen some options for how to balance a workload, how can we make
    sure that those backend servers are working correctly?
  prefs: []
  type: TYPE_NORMAL
- en: Server and service health checks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One of the issues we discussed in the section on DNS load balancing was health
    checks. Once you start load balancing, you usually want some method of knowing
    which servers (and services) are operating correctly. Methods for checking the
    *health* of any connection include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Use ICMP to effectively "ping" the target servers periodically. If no pings
    return with an ICMP echo reply, then they are considered down, and they don't
    receive any new clients. Existing clients will be spread across the other servers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the TCP handshake and check for an open port (for instance `80/tcp` and
    `443/tcp` for a web service). Again, if the handshake doesn't complete, then the
    host is considered down.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In UDP, you would typically make an application request. For instance, if you
    are load balancing DNS servers, the load balancer would make a simple DNS query
    – if a DNS response is received, then the sever is considered up.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, when balancing a web application, you may make an actual web request.
    Often, you'll request the index page (or any known page) and look for known text
    on that page. If that text doesn't appear, then that host and service combination
    is considered down. In a more complex environment, the test page you check may
    make a known call to a backend database to verify it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Testing the actual application (as in the preceding two points) is, of course,
    the most reliable way to verify that the application is working.
  prefs: []
  type: TYPE_NORMAL
- en: We'll show a few of these health checks in our example configuration. Before
    we get to that, though, let's dive into how you might see load balancers implemented
    in a typical datacenter – both in a "legacy" configuration and in a more modern
    implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Datacenter load balancer design considerations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Load balancing has been part of larger architectures for decades, which means
    that we've gone through several common designs.
  prefs: []
  type: TYPE_NORMAL
- en: The "legacy" design that we still frequently see is a single pair (or cluster)
    of physical load balancers that service all the load balanced workloads in the
    datacenter. Often, the same load balancer cluster is used for internal and external
    workloads, but sometimes, you'll see one internal pair of load balancers on the
    internal network, and one pair that only services DMZ workloads (that is, for
    external clients).
  prefs: []
  type: TYPE_NORMAL
- en: This model was a good approach in the days when we had physical servers, and
    load balancers were expensive pieces of hardware.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a virtualized environment, though, the workload VMs are tied to the physical
    load balancers, which complicates the network configuration, limits disaster recovery
    options, and can often result in traffic making multiple "loops" between the (physical)
    load balancers and the virtual environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.5 – Legacy load balancing architecture'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16336_10_005.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.5 – Legacy load balancing architecture
  prefs: []
  type: TYPE_NORMAL
- en: 'This has all changed with the advent of virtualization. The use of physical
    load balancers now makes very little sense – you are far better off sticking to
    a dedicated, small VM for each workload, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.6 – Modern load balancing architecture'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16336_10_006.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.6 – Modern load balancing architecture
  prefs: []
  type: TYPE_NORMAL
- en: 'This approach has several advantages:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Cost** is one advantage as these small virtual load balancers are much cheaper
    if licensed, or free if you use a solution such as HAProxy (or any other free/open
    source solution). This is probably the advantage that should have the least impact
    but is unsurprisingly usually the one factor that changes opinions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Configurations are much simpler** and easier to maintain as each load balancer
    only services one workload. If a change is made and possibly needs subsequent
    debugging, it''s much simpler to "pick out" something from a smaller configuration.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the event of a failure or, more likely, a configuration error, the **splatter
    zone** or **blast radius** is much smaller. If you tie each load balancer to a
    single workload, any errors or failures are more likely to affect only that workload.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Also, from an operational standpoint, **using an orchestration platform or API
    used to scale the workload is much easier** (adding or removing backend servers
    to the cluster as demand rises and falls). This approach makes it much simpler
    to build those playbooks – mainly because of the simpler configuration and smaller
    blast radius in the event of a playbook error.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**More rapid deployments for developers**. Since you are keeping this configuration
    simple, in the development environment, you can provide developers with exactly
    this configuration as they are writing or modifying the applications. This means
    that the applications are written with the load balancer in mind. Also, most of
    the testing is done during the development cycle, rather than the configuration
    being shoe-horned and tested in a single change window at the end of development.
    Even if the load balancers are licensed products, most vendors have a free (low
    bandwidth) tier of licensing for exactly this scenario.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Providing **securely configured templates** to developers or for deployments
    is much easier with a smaller configuration.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Security testing during the development or DevOps cycle** includes the load
    balancer, not just the application and the hosting server.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Training and testing is much simpler**. Since the load balancing products
    are free, setting up a training or test environment is quick and easy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Workload optimization** is a significant advantage since, in a virtualized
    environment, you can usually "tie" groups of servers together. In a VMware vSphere
    environment, for instance, this is called a **vApp**. This construct allows you
    to keep all vApp members together if, for example, you vMotion them to another
    hypervisor server. You may need to do this for maintenance, or this may happen
    automatically using **Dynamic Resource Scheduling** (**DRS**), which balances
    CPU or memory load across multiple servers. Alternatively, the migration might
    be part of a disaster recovery workflow, where you migrate the vApp to another
    datacenter, either using vMotion or simply by activating a replica set of VMs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cloud deployments lend themselves even more strongly to this distributed
    model**. This is taken to the extreme in the larger cloud service providers, where
    load balancing is simply a service that you subscribe to, rather than a discrete
    instance or virtual machine. Examples of this include the AWS Elastic Load Balancing
    Service, Azure Load Balancer, and Google''s Cloud Load Balancing service.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Load balancing brings with it several management challenges, though, most of
    which stem from one issue – if all the target hosts have a default gateway for
    the load balancer, how can we monitor and otherwise manage those hosts?
  prefs: []
  type: TYPE_NORMAL
- en: Datacenter network and management considerations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If a workload is load balanced using the NAT approach, routing becomes a concern.
    The route to the potential application clients must point to the load balancer.
    If those targets are internet-based, this makes administering the individual servers
    a problem – you don't want your server administrative traffic to be load balanced.
    You also don't want to have unnecessary traffic (such as backups or bulk file
    copies) route through the load balancer – you want it to route application traffic,
    not all the traffic!
  prefs: []
  type: TYPE_NORMAL
- en: This is commonly dealt with by adding static routes and possibly a management
    VLAN.
  prefs: []
  type: TYPE_NORMAL
- en: This is a good time to bring up that the management VLAN should have been there
    from the start – my "win the point" phrase on management VLANs is "does your accounting
    group (or receptionist or manufacturing group) need access to your SAN or hypervisor
    login?" If you can get an answer that leads you toward protecting sensitive interfaces
    from internal attacks, then a management VLAN is an easy thing to implement.
  prefs: []
  type: TYPE_NORMAL
- en: 'In any case, in this model, the default gateway remains pointed toward the
    load balancer (to service internet clients), but specific routes are added to
    the servers to point toward internal or service resources. In most cases, this
    list of resources remains small, so even if internal clients plan to use the same
    load balanced application, this can still work:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.7 – Routing non-application traffic (high level)'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16336_10_007.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.7 – Routing non-application traffic (high level)
  prefs: []
  type: TYPE_NORMAL
- en: If this model can't work for one reason or another, then you might want to consider
    adding **policy-based routing** (**PBR**).
  prefs: []
  type: TYPE_NORMAL
- en: 'In this situation, say, for example, that your servers are load balancing HTTP
    and HTTPS – `80/tcp` and `443/tcp`, respectively. Your policy might look like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: Route all traffic `80/tcp` and `443/tcp` to the load balancer (in other words,
    reply traffic from the application).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Route all other traffic through the subnet's router.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This policy route could be put on the server subnet''s router, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.8 – Routing non-application traffic – policy routing on an upstream
    router'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16336_10_008.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.8 – Routing non-application traffic – policy routing on an upstream
    router
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding diagram, the servers all have a default gateway based on the
    router''s interface (`10.10.10.1`, in this example):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This has the benefit of simplicity, but this subnet default gateway device must
    have sufficient horsepower to service the demand of all that reply traffic, without
    affecting the performance of any of its other workloads. Luckily, many modern
    10G switches do have that horsepower. However, this also has the disadvantage
    that your reply traffic now leaves the hypervisor, hits that default gateway router,
    then likely goes back into the virtual infrastructure to reach the load balancer.
    In some environments, this can still work performance-wise, but if not, consider
    moving the policy route to the servers themselves.
  prefs: []
  type: TYPE_NORMAL
- en: 'To implement this same policy route on a Linux host, follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, add the route to `table 5`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the traffic that matches the load balancer (source `10.10.10.0/24`,
    source port `443`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Add the lookup, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This method adds more complexity and CPU overhead than most people want. Also,
    for "network routing issues," support staff are more likely to start any future
    troubleshooting on routers and switches than looking at the host configurations.
    For these reasons, putting the policy route on a router or Layer 3 switch is what
    we often see being implemented.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using a management interface solves this problem much more elegantly. Also,
    if management interfaces are not already widely in use in the organization, this
    approach nicely introduces them into the environment. In this approach, we keep
    the target hosts configured with their default gateway pointed to the load balancer.
    We then add a management VLAN interface to each host, likely with some management
    services directly in that VLAN. In addition, we can still add specific routes
    to things such as SNMP servers, logging servers, or other internal or internet
    destinations as needed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.9 – Adding a management VLAN'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16336_10_009.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.9 – Adding a management VLAN
  prefs: []
  type: TYPE_NORMAL
- en: Needless to say, this is what is commonly implemented. Not only is it the simplest
    approach, but it also adds a much-needed management VLAN to the architecture.
  prefs: []
  type: TYPE_NORMAL
- en: With most of the theory covered, let's get on with building a few different
    load balancing scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Building a HAProxy NAT/proxy load balancer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First, we likely don't want to use our example host for this, so we must add
    a new network adapter to demonstrate a NAT/proxy (L4/L7) load balancer.
  prefs: []
  type: TYPE_NORMAL
- en: If your example host is a virtual machine, building a new one should be quick.
    Or, better yet, clone your existing VM and use that. Alternatively, you can download
    an `haproxy –v`.
  prefs: []
  type: TYPE_NORMAL
- en: Or, if you choose not to "build along" with our example configuration, by all
    means you can still "follow along." While building the plumbing for a load balancer
    can take a bit of work, the actual configuration is pretty simple, and introducing
    you to that configuration is our goal here. You can certainly attain that goal
    without having to build out the supporting virtual or physical infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are installing this on a fresh Linux host, ensure that you have two
    network adapters (one facing the clients and one facing the servers). As always,
    we''ll start by installing the target application:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '*<Start here if you are using the OVA-based installation:>*'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can verify that the installation worked by checking the version number
    using the `haproxy` application itself:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Note that any version newer that the one shown here should work fine.
  prefs: []
  type: TYPE_NORMAL
- en: With the package installed, let's look at our example network build.
  prefs: []
  type: TYPE_NORMAL
- en: Before you start configuring – NICs, addressing, and routing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You are welcome to use any IP addressing you choose, but in our example, the
    frontend `192.168.122.21/24` (note that this is different than the interface IP
    of the host), while the backend address of the load balancer will be `192.168.124.1/24`
    – this will be the default gateway of the target hosts. Our target web servers
    will have `192.168.124.10` and `192.168.124.20`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our final build will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.10 – Load balancer example build'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16336_10_010.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.10 – Load balancer example build
  prefs: []
  type: TYPE_NORMAL
- en: Before we start building our load balancer, this is the perfect time to adjust
    some settings in Linux (some of which will require a system reload).
  prefs: []
  type: TYPE_NORMAL
- en: Before you start configuring – performance tuning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A basic "out of the box" Linux installation must make several assumptions for
    various settings, though many of them result in compromises from a performance
    or security perspective. For a load balancer, there are several Linux settings
    that need to be addressed. Luckily, the HAProxy installation does a lot of this
    work for us (if we installed the licensed version). Once the installation is complete,
    edit the `/etc/sysctl.d/30-hapee-2.2.conf` file and uncomment the lines in the
    following code (in our case, we're installing the Community Edition, so create
    this file and uncomment the lines). As with all basic system settings, test these
    as you go, making these changes one at a time or in logical groups. Also, as expected,
    this may be an iterative process where you may go back and forth from one setting
    to another. As noted in the file comments, not all these values are recommended
    in all or even most cases.
  prefs: []
  type: TYPE_NORMAL
- en: These settings and their descriptions can all be found at [https://www.haproxy.com/documentation/hapee/2-2r1/getting-started/system-tuning/](https://www.haproxy.com/documentation/hapee/2-2r1/getting-started/system-tuning/).
  prefs: []
  type: TYPE_NORMAL
- en: 'Limit the per-socket default receive/send buffers to limit memory usage when
    running with a lot of concurrent connections. The values are in bytes and represent
    the minimum, default, and maximum. The defaults are `4096`, `87380`, and `4194304`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Allow early reuse of the same source port for outgoing connections. This is
    required if you have a few hundred connections per second. The default is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Extend the source port range for outgoing TCP connections. This limits early
    port reuse and makes use of `64000` source ports. The defaults are `32768` and
    `61000`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Increase the TCP SYN backlog size. This is generally required to support very
    high connection rates, as well as to resist SYN flood attacks. Setting it too
    high will delay SYN cookie usage though. The default is `1024`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Set the timeout in seconds for the `tcp_fin_wait` state. Lowering it speeds
    up the release of dead connections, though it will cause issues below 25-30 seconds.
    It is preferable not to change it if possible. The default is `60`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Limit the number of outgoing SYN-ACK retries. This value is a direct amplification
    factor of SYN floods, so it is important to keep it reasonably low. However, making
    it too low will prevent clients on lossy networks from connecting.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using `3` as a default value provides good results (4 SYN-ACK total), while
    lowering it to `1` under a SYN flood attack can save a lot of bandwidth. The default
    is `5`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Set this to `1` to allow local processes to bind to an IP that is not present
    on the system. This is typically what happens with a shared VRRP address, where
    you want both the primary and backup to be started, even though the IP is not
    present. Always leave it as `1`. The default is `0`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The following serves as a higher bound for all the system''s SYN backlogs.
    Put it at least as high as `tcp_max_syn_backlog`; otherwise, clients may experience
    difficulties connecting at high rates or under SYN attacks. The default is `128`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Again, note that if you make any of these changes, you may end up coming back
    to this file later to back out or adjust your settings. With all this complete
    (for now, at least), let's configure our load balancer so that it works with our
    two target web servers.
  prefs: []
  type: TYPE_NORMAL
- en: Load balancing TCP services – web services
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The configuration for load balancing services is extremely simple. Let's start
    by load balancing between two web server hosts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s edit the `/etc/haproxy/haproxy.cfg` file. We''ll create a `frontend`
    section that defines the service that faces clients and a `backend` section that
    defines the two downstream web servers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Take note of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The frontend section has a `default backend` line in it, which tells it which
    services to tie to that frontend.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The frontend has a `bind` statement that allows the load to be balanced across
    all the IPs on that interface. So, in this case, if we're load balancing with
    just one VIP, we can do this on the physical IP of the load balancer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The backend has `roundrobin` as the load balancing algorithm. This means that
    as users connect, they'll get directed to server1, then server2, then server1,
    and so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `check` parameter tells the service to check the target server to ensure
    that it's up. This is much simpler when load balancing TCP services as a simple
    TCP "connect" does the trick, at least to verify that the host and service are
    running.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fall 3` marks the service as offline after three consecutive failed checks,
    while `rise 2` marks it as online after two successful checks. These rise/fall
    keywords can be used regardless of what check type is being used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We also want a global section in this file so that we can set some server parameters
    and defaults:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Note that we define the user and group in this section. Going all the way back
    to [*Chapter 3*](B16336_03_Final_NM_ePub.xhtml#_idTextAnchor053), *Using Linux
    and Linux Tools for Network Diagnostics*, we mentioned that you need to have root
    privileges to start a listening port if that port number is less than `1024`.
    What this means for HAProxy is that it needs root rights to start the service.
    The user and group directives in the global section allow the service to "downgrade"
    its rights. This is important because if the service is ever compromised, having
    lower rights gives the attackers far fewer options, likely increases the time
    required for their attack, and hopefully increases their likelihood of being caught.
  prefs: []
  type: TYPE_NORMAL
- en: The `log` line is very straightforward – it tells `haproxy` where to send its
    logs. If you have any problems you need to solve with load balancing, this is
    a good place to start, followed by the target service logs.
  prefs: []
  type: TYPE_NORMAL
- en: The `stats` directive tells `haproxy` where to store its various performance
    statistics.
  prefs: []
  type: TYPE_NORMAL
- en: The `nbproc` and `nbpthread` directives tell the HAProxy service how many processors
    and threads it has available for use. These numbers should be at least one process
    less than what's available so that in the event of a denial-of-service attack,
    the entire load balancer platform is not incapacitated.
  prefs: []
  type: TYPE_NORMAL
- en: 'The various timeout parameters are there to prevent protocol-level denial-of-service
    attacks. In these situations, the attacker sends the initial requests, but then
    never continues the session – they just continually send requests, "eating up"
    load balancer resources until the memory is entirely consumed. These timeouts
    put a limit on how long the load balancer will keep any one session alive. The
    following table outlines a short description of each of the keep-alive parameters
    we''re discussing here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16336_10_Table_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Also, the SSL directives are pretty self-explanatory:'
  prefs: []
  type: TYPE_NORMAL
- en: '`ssl-default-bind-ciphers` lists the ciphers that are allowed in any TLS sessions,
    if the load balancer is terminating or starting a session (that is, if your session
    is in proxy or Layer 7 mode).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ssl-default-bind-options` is there to set a lower bound on TLS versions that
    are supported. At the time of writing, all SSL versions, as well as TLS version
    1.0, are no longer recommended. SSL in particular is susceptible to a number of
    attacks. With all modern browsers capable of negotiating up to TLS version 3,
    most environments elect to support TLS version 1.2 or higher (as shown in the
    example).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, from a client machine, you can browse to the HAProxy host, and you'll see
    that you'll connect to one of the backends. If you try again from a different
    browser, you should connect to the second.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s expand on this and add support for HTTPS (on `443/tcp`). We''ll add
    an IP to the frontend interface and bind to that. We''ll change the balancing
    algorithm to least connections. Finally, we''ll change the name of the frontend
    and the backend so that they include the port number. This allows us to add the
    additional configuration sections for `443/tcp`. This traffic is load balanced
    nicely if we just monitor at the Layer 4 TCP sessions; no decryption is required:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Note that we're still just checking that the TCP port is open for a "server
    health" check. This is often referred to as a Layer 3 health check. We put ports
    `80` and `443` into two sections – these can be combined into one section for
    the frontend stanza, but it's normally a best practice to keep these separate
    so that they can be tracked separately. The side effect of this is that the counts
    for the two backend sections aren't aware of each other, but this normally isn't
    an issue since these days, the entire HTTP site is usually just a redirect to
    the HTTPS site.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another way to phrase this would be on a `listen` stanza, rather than on the
    frontend and backend stanzas. This approach combines the frontend and backend
    sections into a single stanza and adds a "health check":'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: This default HTTP health check simply opens the default page and ensures that
    something comes back by checking the header for the phrase `HTTP/1.0`. If this
    is not seen in the returned page, it counts as a failed check. You can expand
    this by checking any URI on the site and looking for an arbitrary text string
    on that page. This is often referred to as a "Layer 7" health check since it is
    checking the application. Ensure that you keep your checks simple, though – if
    the application changes even slightly, the text that's returned on a page may
    change enough to have your health check fail and accidentally mark the whole cluster
    as offline!
  prefs: []
  type: TYPE_NORMAL
- en: Setting up persistent (sticky) connections
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s inject a cookie into the HTTP sessions by using a variant of the server''s
    name. Let''s also do a basic check of the HTTP service rather than just the open
    port. We will go back to our "frontend/backend" configuration file approach for
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Make sure that you don't use the IP address or real name of the server as your
    cookie value. If the real server's name is used, attackers may be able to get
    access to that server by looking for that server name in DNS, or in sites that
    have databases of historical DNS entries (`dnsdumpster.com`, for instance). Server
    names can also be used to gain intelligence about the target from certificate
    transparency logs (as we discussed in [*Chapter 8*](B16336_08_Final_NM_ePub.xhtml#_idTextAnchor133),
    *Certificate Services on Linux*). Finally, if the server IP address is used in
    the cookie value, that information gives the attacker some intelligence on your
    internal network architecture, and if the disclosed network is publicly routable,
    it may give them their next target!
  prefs: []
  type: TYPE_NORMAL
- en: Implementation note
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have covered a basic configuration, a very common step is to have
    a "placeholder" website on each server, each identified to match the server. Using
    "1-2-3," "a-b-c," or "red-green-blue" are all common approaches, just enough to
    tell each server session from the next. Now, with different browsers or different
    workstations, browse to the shared address multiple times to ensure that you are
    directed to the correct backend server, as defined by your rule set.
  prefs: []
  type: TYPE_NORMAL
- en: This is, of course, a great way to ensure things are working as you progressively
    build your configuration, but it's also a great troubleshooting mechanism to help
    you decide on simple things such as "is this still working after the updates?"
    or "I know what the helpdesk ticket says, but is there even a problem to solve?"
    months or even years later. Test pages like this are a great thing to keep up
    long-term for future testing or troubleshooting.
  prefs: []
  type: TYPE_NORMAL
- en: HTTPS frontending
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In days gone by, server architects were happy to set up load balancers to offload
    HTTPS processing, moving that encrypt/decrypt processing from the server to the
    load balancer. This saved on server CPU, and it also moved the responsibility
    of implementing and maintaining the certificates to whoever was managing the load
    balancer. These reasons are mostly no longer valid though, for several reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: If the servers and load balancer are all virtual (as is recommended in most
    cases), this just moves the processing around between different VMs on the same
    hardware – there's no net gain.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Modern processors are much more efficient in performing encryption and decryption
    – the algorithms are written with CPU performance in mind. In fact, depending
    on the algorithm, the encrypt/decrypt operations may be native to the CPU, which
    is a huge performance gain.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The use of wildcard certificates makes the whole "certificate management" piece
    much simpler.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, we still do HTTPS frontending with load balancers, usually to get reliable
    session persistence using cookies – you can't add a cookie to an HTTPS response
    (or read one on the next request) unless you can read and write to the data stream,
    which implies that, at some point, it's been decrypted.
  prefs: []
  type: TYPE_NORMAL
- en: Remember from our previous discussion that in this configuration, each TLS session
    will be terminated on the frontend side, using a valid certificate. Since this
    is now a proxy setup (Layer 7 load balancing), the backend session is a separate
    HTTP or HTTPS session. In days past, the backend would often be HTTP (mostly to
    save CPU resources), but in modern times, this will be rightly seen as a security
    exposure, especially if you are in the finance, healthcare, or government sectors
    (or any sector that hosts sensitive information). For this reason, in a modern
    build, the backend will almost always be HTTPS as well, often with the same certificate
    on the target web server.
  prefs: []
  type: TYPE_NORMAL
- en: Again, the disadvantage of this setup is that since the actual client of the
    target web server is the load balancer, the `X-Forwarded-*` HTTPS header will
    be lost, and the IP address of the actual client will not be available to the
    web server (or its logs).
  prefs: []
  type: TYPE_NORMAL
- en: 'How do we go about configuring this setup? First, we must obtain the site certificate
    and private key, whether that''s a "named certificate" or a wildcard. Now, combine
    these into one file (not as a `pfx` file, but as a chain) by simply concatenating
    them together using the `cat` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Note that we're using `sudo` in the second half of the command, to give the
    command rights to the `/etc/ssl/sitename.com` directory. Also, note the `tee`
    command, which echoes the command's output to the screen. It also directs the
    output to the desired location.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can bind the certificate to the address in the frontend file stanza:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Take note of the following in this configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: We can now use cookies for session persistence (in the backend section), which
    is generally the primary objective in this configuration.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We use the `redirect scheme` line in the frontend to instruct the proxy to use
    SSL/TLS on the backend.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `forwardfor` keyword adds the actual client IP to the `X-Forwarded-For`
    HTTP header field on the backend request. Note that it's up to the web server
    to parse this out and log it appropriately so that you can use it later.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Depending on the application and browsers, you can also add the client IP to
    the backend HTTP request in the `X-Client-IP` header field with a clause:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: This approach sees mixed results.
  prefs: []
  type: TYPE_NORMAL
- en: Note, however, that whatever you add or change in the HTTP header, the actual
    client IP that the target server "sees" remains the backend address of the load
    balancer – these changed or added header values are simply fields in the HTTPS
    request. If you intend to use these header values for logging, troubleshooting,
    or monitoring, it's up to the web server to parse them out and log them appropriately.
  prefs: []
  type: TYPE_NORMAL
- en: That covers our example configurations – we've covered NAT-based and proxy-based
    load balancing, as well as session persistence for both HTTP and HTTPS traffic.
    After all the theory, actually configuring the load balancer is simple – the work
    is all in the design and in setting up the supporting network infrastructure.
    Let's discuss security briefly before we close out this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: A final note on load balancer security
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we've discussed how an attacker might be able to gain insight or access
    to the internal network if they can get server names or IP addresses. We discussed
    how a malicious actor can get that information using information disclosed by
    the cookies used in a local balancer configuration for persistent settings. How
    else can an attacker gain information about our target servers (which are behind
    the load balancer and should be hidden)?
  prefs: []
  type: TYPE_NORMAL
- en: Certificate transparency information is another favorite method for getting
    current or old server names, as we discussed in [*Chapter 8*](B16336_08_Final_NM_ePub.xhtml#_idTextAnchor133),
    *Certificate Services on Linux*. Even if the old server names are no longer in
    use, the records of their past certificates are immortal.
  prefs: []
  type: TYPE_NORMAL
- en: The Internet Archive site at [https://archive.org](https://archive.org) takes
    "snapshots" of websites periodically, and allows them to be searched and viewed,
    allowing people to go "back in time" and view older versions of your infrastructure.
    If older servers are disclosed in your old DNS or in the older code of your web
    servers, they're likely available on this site.
  prefs: []
  type: TYPE_NORMAL
- en: DNS archive sites such as `dnsdumpster` collect DNS information using passive
    methods such as packet analysis and present it via a web or API interface. This
    allows an attacker to find both older IP addresses and older (or current) hostnames
    that an organization might have used, which sometimes allows them to still access
    those services by IP if the DNS entries are removed. Alternatively, they can access
    them individually by hostname, even if they are behind a load balancer.
  prefs: []
  type: TYPE_NORMAL
- en: '*Google Dorks* are another method of obtaining such information – these are
    terms for finding specific information that can be used in search engines (not
    just Google). Often, something as simple as a search term such as `inurl:targetdomain.com`
    will find hostnames that the target organization would rather keep hidden. Some
    Google Dorks that are specific to `haproxy` include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Note that where we say `site:`, you can also specify `inurl:`. In that case,
    you can also shorten the search term to just the domain rather than the full site
    name.
  prefs: []
  type: TYPE_NORMAL
- en: Sites such as `shodan.io` will also index historic versions of your servers,
    focusing on server IP addresses, hostnames, open ports, and the services running
    on those ports. Shodan is unique in how well it identifies the service running
    on an open port. While they are not, of course, 100% successful in that (think
    of it as someone else's NMAP results), when they do identify a service, the "proof"
    is posted with it, so if you are using Shodan for reconnaissance, you can use
    that to verify how accurate that determination might be. Shodan has both a web
    interface and a comprehensive API. With this service, you can often find improperly
    secured load balancers by organization or by geographic area.
  prefs: []
  type: TYPE_NORMAL
- en: 'A final comment on search engines: if Google (or any search engine) can reach
    your real servers directly, then that content will be indexed, making it easily
    searchable. If a site may have an authentication bypass issue, the "protected
    by authentication" content will also be indexed and available for anyone who uses
    that engine.'
  prefs: []
  type: TYPE_NORMAL
- en: That said, it's always a good idea to use tools like the ones we've just discussed
    to periodically look for issues on your perimeter infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: Another important security issue to consider is management access. It's important
    to restrict access to the management interface of the load balancers (in other
    words, SSH), restricting it to permitted hosts and subnets on all interfaces.
    Remember that if your load balancer is parallel to your firewall, the whole internet
    has access to it, and even if not, everyone on your internal network has access
    to it. You'll want to whittle that access down to just trusted management hosts
    and subnets. If you need a reference for that, remember that we covered this in
    [*Chapter 4*](B16336_04_Final_NM_ePub.xhtml#_idTextAnchor071), *The Linux Firewall*,
    and [*Chapter 5*](B16336_05_Final_NM_ePub.xhtml#_idTextAnchor085), *Linux Security
    Standards with Real-Life Examples*.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hopefully, this chapter has served as a good introduction to load balancers,
    how to deploy them, and the reasons you might choose to make various design and
    implementation decisions around them.
  prefs: []
  type: TYPE_NORMAL
- en: If you used new VMs to follow along with the examples in this chapter, we won't
    need them in subsequent chapters, but you might wish to keep the HAProxy VMs in
    particular if you need an example to reference later. If you followed the examples
    in this chapter just by reading them, then the examples in this chapter remain
    available to you. Either way, as you read this chapter, I hope you were mentally
    working out how load balancers might fit into your organization's internal or
    perimeter architecture.
  prefs: []
  type: TYPE_NORMAL
- en: With this chapter completed, you should have the skills needed to build a load
    balancer in any organization. These skills were discussed in the context of the
    (free) version of HAProxy, but the design and implementation considerations are
    almost all directly usable in any vendor's platform, the only changes being the
    wording and syntax in the configuration options or menus. In the next chapter,
    we'll look at enterprise routing implementations based on Linux platforms.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we conclude, here is a list of questions for you to test your knowledge
    regarding this chapter''s material. You will find the answers in the *Assessments*
    section of the *Appendix*:'
  prefs: []
  type: TYPE_NORMAL
- en: When would you choose to use a **Direct Server Return** (**DSR**) load balancer?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why would you choose to use a proxy-based load balancer as opposed to one that
    is a pure NAT-based solution?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Take a look at the following links to learn more about the topics that were
    covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: 'HAProxy documentation: [http://www.haproxy.org/#docs](http://www.haproxy.org/#docs)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'HAProxy documentation (Commercial version): [https://www.haproxy.com/documentation/hapee/2-2r1/getting-started/](https://www.haproxy.com/documentation/hapee/2-2r1/getting-started/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'HAProxy GitHub: [https://github.com/haproxytech](https://github.com/haproxytech)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'HAProxy GitHub, OVA VM download: [https://github.com/haproxytech/vmware-haproxy#download](https://github.com/haproxytech/vmware-haproxy#download)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'HAProxy Community versus Enterprise Differences: [https://www.haproxy.com/products/community-vs-enterprise-edition/](https://www.haproxy.com/products/community-vs-enterprise-edition/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'More on Load Balancing Algorithms: [http://cbonte.github.io/haproxy-dconv/2.4/intro.html#3.3.5](http://cbonte.github.io/haproxy-dconv/2.4/intro.html#3.3.5)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
