- en: '*Chapter 10*: Load Balancer Services for Linux'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第10章*：Linux负载均衡器服务'
- en: In this chapter, we'll be discussing the load balancer services that are available
    for Linux, specifically HAProxy. Load balancers allow client workloads to be spread
    across multiple backend servers. This allows a single IP to scale larger than
    a single server may allow, and also allows for redundancy in the case of a server
    outage or maintenance window.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论适用于Linux的负载均衡器服务，具体来说是HAProxy。负载均衡器允许客户端工作负载分布到多个后端服务器。这允许单个IP扩展到比单个服务器更大，并且在服务器故障或维护窗口的情况下也允许冗余。
- en: Once you've completed these examples, you should have the skills to deploy Linux-based
    load balancer services in your own environment via several different methods.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 完成这些示例后，您应该具备通过几种不同的方法在自己的环境中部署基于Linux的负载均衡器服务的技能。
- en: 'In particular, we''ll cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，我们将涵盖以下主题：
- en: Introduction to load balancing
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 负载均衡简介
- en: Load balancing algorithms
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 负载均衡算法
- en: Server and service health checks
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务器和服务健康检查
- en: Datacenter load balancer design considerations
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据中心负载均衡器设计考虑
- en: Building a HAProxy NAT/proxy load balancer
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建HAProxy NAT/代理负载均衡器
- en: A final note on load balancer security
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于负载均衡器安全性的最后说明
- en: Because of the complexity of setting up the infrastructure for this section,
    there are a few choices you can make with respect to the example configurations.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 由于设置此部分的基础设施的复杂性，您可以在示例配置方面做出一些选择。
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'In this chapter, we''ll be exploring load balancer functions. As we work through
    the examples later in this book, you can follow along and implement our example
    configurations in your current Ubuntu host or virtual machine. However, to see
    our load balancing example in action, you''ll need a number of things:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨负载均衡器功能。当我们在本书的后面示例中工作时，您可以跟着进行，并在当前Ubuntu主机或虚拟机中实施我们的示例配置。但是，要看到我们的负载均衡示例的实际效果，您需要一些东西：
- en: At least two target hosts to balance a load across
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 至少有两个目标主机来平衡负载
- en: Another network adapter in the current Linux host
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当前Linux主机中的另一个网络适配器
- en: Another subnet to host the target hosts and this new network adapter
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一个子网来托管目标主机和这个新的网络适配器
- en: This configuration has a matching diagram, *Figure 10.2*, which will be shown
    later in this chapter that illustrates how all this will bolt together when we're
    done.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 此配置有一个匹配的图表，*图10.2*，将在本章后面显示，说明了当我们完成时所有这些将如何连接在一起。
- en: This adds a whole level of complexity to the configuration of our lab environment.
    When we get to the lab section, we'll offer some alternatives (downloading a pre-built
    virtual machine is one of them), but you may just choose to read along. If that's
    the case, I think you'll still get a good introduction to the topic, along with
    a solid background of the design, implementation, and security implications of
    various load balancer configurations in a modern datacenter.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们的实验室环境的配置增加了一整个层次的复杂性。当我们到达实验室部分时，我们将提供一些替代方案（下载预构建的虚拟机是其中之一），但您也可以选择跟着阅读。如果是这种情况，我认为您仍然会对这个主题有一个很好的介绍，以及对现代数据中心中各种负载均衡器配置的设计、实施和安全影响有一个扎实的背景。
- en: Introduction to load balancing
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 负载均衡简介
- en: In its simplest form, load balancing is all about spreading client load across
    multiple servers. These servers can be in one or several locations, and the method
    of distributing that load can vary quite a bit. In fact, how successful you are
    in spreading that load evenly can vary quite a bit as well (mostly depending on
    the method chosen). Let's explore some of the more common methods of load balancing.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在其最简单的形式中，负载均衡就是将客户端负载分布到多个服务器上。这些服务器可以在一个或多个位置，以及分配负载的方法可以有很大的不同。事实上，您在均匀分配负载方面的成功程度也会有很大的不同（主要取决于所选择的方法）。让我们探讨一些更常见的负载均衡方法。
- en: Round Robin DNS (RRDNS)
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 循环DNS（RRDNS）
- en: 'You can do simple load balancing just with a DNS server, in what''s called
    `a.example.com` hostname, the DNS server will return the IP of Server 1; then,
    when the next client requests it, it will return the IP for Server 2, and so on.
    This is the simplest load balancing method and works equally well for both co-located
    servers and servers in different locations. It can also be implemented with no
    changes at all to the infrastructure – no new components and no configuration
    changes:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以只使用DNS服务器进行简单的负载均衡，即所谓的`a.example.com`主机名，DNS服务器将返回服务器1的IP；然后，当下一个客户端请求时，它将返回服务器2的IP，依此类推。这是最简单的负载均衡方法，对于共同放置的服务器和不同位置的服务器同样有效。它也可以在基础设施上不做任何更改-没有新组件，也没有配置更改：
- en: '![Figure 10.1 – Simple load balancing with Round Robin DNS'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.1-循环DNS的简单负载均衡'
- en: '](img/B16336_10_001.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16336_10_001.jpg)'
- en: Figure 10.1 – Simple load balancing with Round Robin DNS
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.1-循环DNS的简单负载均衡
- en: 'Configuring RRDNS is simple – in BIND, simply configure multiple `A` records
    for the target hostname with multiple IPs. Successive DNS requests will return
    each `A` record in sequence. It''s a good idea to shorten the domain''s `A` records
    in sequence), random, or fixed (always return matching records in the same order).
    The syntax for changing the return order is as follows (`cyclic`, the default
    setting, is shown here):'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 配置RRDNS很简单-在BIND中，只需为目标主机名配置多个`A`记录，其中包含多个IP。连续的DNS请求将按顺序返回每个`A`记录。将域的`A`记录缩短是个好主意，以便按顺序（顺序返回匹配的记录）、随机或固定（始终以相同顺序返回匹配的记录）。更改返回顺序的语法如下（`cyclic`，默认设置，如下所示）：
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'There are a few issues with this configuration:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这种配置存在一些问题：
- en: There is no good way to incorporate any kind of health check in this model –
    are all the servers operating correctly? Are the services up? Are the hosts even
    up?
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在这种模型中，没有好的方法来整合任何类型的健康检查-所有服务器是否正常运行？服务是否正常？主机是否正常？
- en: There is no way of seeing if any DNS request is then actually followed up with
    a connection to the service. There are various reasons why DNS requests might
    be made, and the interaction might end there, with no subsequent connection.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有办法看到任何DNS请求是否实际上会跟随连接到服务。有各种原因可能会发出DNS请求，并且交互可能就此结束，没有后续连接。
- en: There is also no way to monitor when sessions end, which means there's no way
    to send the next request to the least used server – it's just a steady, even rotation
    between all servers. So, at the beginning of any business day, this may seem like
    a good model, but as the day progresses, there will always be longer-lived sessions
    and extremely short ones (or sessions that didn't occur at all), so it's common
    to see the server loads become "lop-sided" as the day progresses. This can become
    even more pronounced if there is no clear beginning or end of the day to effectively
    "zero things out."
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 也没有办法监视会话何时结束，这意味着没有办法将下一个请求发送到最少使用的服务器 - 它只是在所有服务器之间稳定地轮换。因此，在任何工作日的开始，这可能看起来像一个好模型，但随着一天的进展，总会有持续时间更长的会话和极短的会话（或根本没有发生的会话），因此很常见看到服务器负载在一天进展过程中变得“不平衡”。如果没有明确的一天开始或结束来有效地“清零”，这种情况可能会变得更加明显。
- en: For the same reason, if one server in the cluster is brought down for maintenance
    or an unplanned outage, there is no good way to bring it back to parity (as far
    as the session count goes).
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 出于同样的原因，如果集群中的一个服务器因维护或非计划中断而下线，没有好的方法将其恢复到与会话计数相同的状态。
- en: With a bit of DNS reconnaissance, an attacker can collect the real IPs of all
    cluster members, then assess them or attack them separately. If any of them is
    particularly vulnerable or has an additional DNS entry identifying it as a backup
    host, this makes the attacker's job even easier.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过一些DNS侦察，攻击者可以收集所有集群成员的真实IP，然后分别评估它们或对它们进行攻击。如果其中任何一个特别脆弱或具有额外的DNS条目标识它为备用主机，这将使攻击者的工作变得更加容易。
- en: Taking any of the target servers offline can be a problem – the DNS server will
    continue to serve that address up in the order it was requested. Even if the record
    is edited, any downstream clients and DNS servers will cache their resolved IPs
    and continue to try to connect to the down host.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将任何目标服务器下线可能会成为一个问题 - DNS服务器将继续按请求的顺序提供该地址。即使记录被编辑，任何下游客户端和DNS服务器都将缓存其解析的IP，并继续尝试连接到已关闭的主机。
- en: Downstream DNS servers (that is, those on the internet) will cache whatever
    record they get for the zone's TTL period. So, all the clients of any of those
    DNS servers will get sent to the same target server.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下游DNS服务器（即互联网上的服务器）将在区域的TTL周期内缓存它们获取的任何记录。因此，任何这些DNS服务器的客户端都将被发送到同一个目标服务器。
- en: For these reasons, RRDNS will do the job in a simple way "in a pinch," but this
    should not normally implemented as a long-term, production solution. That said,
    the **Global Server Load Balancer** (**GSLB**) products are actually based on
    this approach, with different load balancing options and health checks. The disconnect
    between the load balancer and the target server remains in GSLB, so many of these
    same disadvantages carry through to this solution.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，RRDNS可以在紧急情况下简单地完成工作，但通常不应将其实施为长期的生产解决方案。也就是说，**全局服务器负载均衡器**（**GSLB**）产品实际上是基于这种方法的，具有不同的负载均衡选项和健康检查。负载均衡器与目标服务器之间的断开在GSLB中仍然存在，因此许多相同的缺点也适用于这种解决方案。
- en: What we see more often in datacenters is either proxy-based (Layer 7) or NAT-based
    (Layer 4) load balancing. Let's explore these two options.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据中心中，我们更经常看到基于代理（第7层）或基于NAT（第4层）的负载均衡。让我们探讨这两个选项。
- en: Inbound proxy – Layer 7 load balancing
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 入站代理 - 第7层负载均衡
- en: In this architecture, the client's sessions are terminated at the proxy server,
    and a new session is started between the inside interface of the proxy and the
    real server IP.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种架构中，客户端的会话在代理服务器上终止，并在代理的内部接口和真实服务器IP之间启动新的会话。
- en: 'This also brings up several architectural terms that are common to many of
    the load balancing solutions. In the following diagram, can we see the concept
    of the **FRONTEND**, which faces the clients, and the **BACKEND**, which faces
    the servers. We should also discuss IP addressing at this point. The frontend
    presents a **Virtual IP** (**VIP**) that is shared by all of target servers, and
    the servers'' **Real IPs** (**RIPs**) are not seen by the clients at all:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这也提出了许多在许多负载均衡解决方案中常见的架构术语。在下图中，我们可以看到**前端**的概念，面向客户端，以及**后端**，面向服务器。我们还应该在这一点上讨论IP地址。前端呈现了所有目标服务器共享的**虚拟IP**（**VIP**），客户端根本看不到服务器的**真实IP**（**RIPs**）：
- en: '![Figure 10.2 – Load balancing using a reverse proxy'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.2 - 使用反向代理进行负载均衡'
- en: '](img/B16336_10_002.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16336_10_002.jpg)'
- en: Figure 10.2 – Load balancing using a reverse proxy
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.2 - 使用反向代理进行负载均衡
- en: 'This approach has a few disadvantages:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法有一些缺点：
- en: It has the highest CPU load on the load balancer out of all the methods we will
    discuss in this chapter and can, in extreme cases, translate into a performance
    impact on the client's end.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在本章讨论的所有方法中，它对负载均衡器的CPU负载最高，并且在极端情况下可能会对客户端产生性能影响。
- en: In addition, because the client traffic on the target server all comes from
    the proxy server (or servers), without some special handling, the client IP seen
    on the target/application server will always be the backend IP of the load balancer.
    This makes logging direct client interaction in the application problematic. To
    parse out traffic from one session and relate it to a client's actual address,
    we must match the client session from the load balancer (which sees the client
    IP address but not the user identity) with the application/web server logs (which
    sees the user identity but not the client IP address). Matching the session between
    these logs can be a real issue; the common elements between them are the timestamp
    and the source port at the load balancer, and the source ports are often not on
    the web server.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This can be mitigated by having some application awareness. For instance, it's
    common to see a TLS frontend for a Citrix ICA Server or Microsoft RDP server backend.
    In those cases, the proxy server has some excellent "hooks" into the protocol,
    allowing the client IP address to be carried all the way through to the server,
    as well as the identity being detected by the load balancer.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On the plus side, though, using a proxy architecture allows us to fully inspect
    the traffic for attacks, if the tooling is in place. In fact, because of the proxy
    architecture, that final "hop" between the load balancer and the target servers
    is an entirely new session – this means that invalid protocol attacks are mostly
    filtered out, without any special configuration being required at all.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: We can mitigate some of the complexity of this proxy approach by running the
    load balancer as an inbound **Network Address Translation** (**NAT**) configuration.
    When decryption is not required, the NAT approach is commonly seen, built into
    most environments.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: Inbound NAT – Layer 4 load balancing
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This is the most common solution and is the one we''ll start with in our example.
    In a lot of ways, the architecture looks similar to the proxy solution, but with
    a few key differences. Note that in the following diagram, the TCP sessions on
    the frontend and backend now match – this is because the load balancer is no longer
    a proxy; it has been configured for an inbound NAT service. All the clients still
    attach to the single VIP and are redirected to the various server RIPs by the
    load balancer:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.3 – Load balancing using inbound NAT'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16336_10_003.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.3 – Load balancing using inbound NAT
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several reasons why this is a preferred architecture in many cases:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: The servers see the real IPs of the client, and the server logs correctly reflect
    that.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The load balancer maintains the NAT table in memory, and the load balancer logs
    reflect the various NAT operations but can't "see" the session. For instance,
    if the servers are running an HTTPS session, if this is a simple Layer 4 NAT,
    then the load balancer can see the TCP session but can't decrypt the traffic.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have the option of terminating the HTTPS session on the frontend, then either
    running encrypted or clear text on the backend in this architecture. However,
    since we're maintaining two sessions (the frontend and backend), this starts to
    look more like a proxy configuration.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Because the entire TCP session (up to Layer 4) is seen by the load balancer,
    several load balancing algorithms are now available (see the next section on load
    balancing algorithms for more information).
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This architecture allows us to place **Web Application Firewall** (**WAF**)
    functions on the load balancer, which can mask some vulnerabilities on target
    server web applications. For example, a WAF is a common defense against cross-site
    scripting or buffer overflow attacks, or any other attack that might rely on a
    lapse in input validation. For those types of attacks, the WAF identifies what
    is an acceptable input for any given field or URI, and then drops any inputs that
    don't match. However, WAFs are not limited to those attacks. Think of a WAF function
    as a web-specific IPS (see [*Chapter 14*](B16336_14_Final_NM_ePub.xhtml#_idTextAnchor252),
    *Honeypot Services on Linux*).
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这种架构允许我们在负载均衡器上放置**Web应用程序防火墙**（**WAF**）功能，这可以掩盖目标服务器Web应用程序上的一些漏洞。例如，WAF是对跨站脚本或缓冲区溢出攻击的常见防御，或者任何可能依赖输入验证中断的攻击。对于这些类型的攻击，WAF识别任何给定字段或URI的可接受输入，然后丢弃任何不匹配的输入。但是，WAF并不局限于这些攻击。将WAF功能视为Web特定的IPS（见[*第14章*](B16336_14_Final_NM_ePub.xhtml#_idTextAnchor252)，*Linux上的蜜罐服务*）。
- en: This architecture is well-suited to making sessions persistent or "sticky" –
    by that, we mean that once a client session is "attached" to a server, subsequent
    requests will be directed to that same server. This is well-suited for pages that
    have a backend database, for instance, where if you didn't keep the same backend
    server, your activity (for instance, a shopping cart on an ecommerce site) might
    be lost. Dynamic or parametric websites – where the pages are generated in real
    time as you navigate (for instance, most sites that have a product catalog or
    inventory) – also usually need session persistence.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这种架构非常适合使会话持久或“粘性” - 我们的意思是一旦客户端会话“附加”到服务器，随后的请求将被定向到同一台服务器。这非常适合具有后端数据库的页面，例如，如果您不保持相同的后端服务器，您的活动（例如，电子商务网站上的购物车）可能会丢失。动态或参数化网站
    - 在这些网站上，页面在您导航时实时生成（例如，大多数具有产品目录或库存的网站） - 通常也需要会话持久性。
- en: You can also load balance each successive request independently, so, for instance,
    as a client navigates a site, their session might be terminated by a different
    server for each page. This type of approach is well-suited to static websites.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您还可以独立地负载均衡每个连续的请求，因此，例如，当客户端浏览网站时，他们的会话可能会在每个页面由不同的服务器终止。这种方法非常适合静态网站。
- en: You can layer other functions on top of this architecture. For instance, these
    are often deployed in parallel with a firewall or even with a native interface
    on the public internet. Because of this, you'll often see load balancer vendors
    with VPN clients to go with their load balancers.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以在这种架构的基础上叠加其他功能。例如，这些通常与防火墙并行部署，甚至与公共互联网上的本机接口并行部署。因此，您经常会看到负载均衡器供应商配备VPN客户端以配合其负载均衡器。
- en: As shown in the preceding diagram, an inbound NAT and proxy load balancer have
    a very similar topology – the connections all look very similar. This is carried
    through to the implementation, where it's possible to see some things proxied
    and some things running through the NAT process on the same load balancer.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如前图所示，入站NAT和代理负载均衡器具有非常相似的拓扑结构 - 连接看起来都非常相似。这一点延续到了实现中，可以看到一些东西通过代理和一些东西通过同一负载均衡器上的NAT过程。
- en: However, even though the CPU impact of this configuration is much lower than
    the proxy solution, every workload packet must route through the load balancer,
    in both directions. We can reduce this impact dramatically using the **Direct
    Server Return** (**DSR**) architecture.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，即使这种配置的CPU影响远低于代理解决方案，每个工作负载数据包仍必须通过负载均衡器在两个方向上进行路由。我们可以使用**直接服务器返回**（**DSR**）架构大大减少这种影响。
- en: DSR load balancing
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DSR负载平衡
- en: In DSR, all the incoming traffic is still load balanced from the VIP on the
    load balancer to the various server RIPs. However, the return traffic goes directly
    from the server back to the client, bypassing the load balancer.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在DSR中，所有传入的流量仍然从负载均衡器上的VIP负载均衡到各个服务器的RIP。然而，返回流量直接从服务器返回到客户端，绕过负载均衡器。
- en: 'How can this work? Here''s how:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这怎么可能？这是怎么回事：
- en: On the way in, the load balancer rewrites the MAC address of each packet, load
    balancing them across the MAC addresses of the target servers.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在进入时，负载均衡器会重写每个数据包的MAC地址，将它们负载均衡到目标服务器的MAC地址上。
- en: Each server has a loopback address, a configured address that matches the VIP
    address. This is the interface that returns all the traffic (because the client
    is expecting return traffic from the VIP address). However, it must be configured
    to not reply to ARP requests (otherwise, the load balancer will be bypassed on
    the inbound path).
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每台服务器都有一个环回地址，这是一个配置的地址，与VIP地址匹配。这是返回所有流量的接口（因为客户端期望从VIP地址返回流量）。但是，它必须配置为不回复ARP请求（否则，负载均衡器将在入站路径上被绕过）。
- en: 'This may seem convoluted, but the following diagram should make things a bit
    clearer. Note that there is only one target host in this diagram, to make the
    traffic flow a bit easier to see:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能看起来很复杂，但以下图表应该使事情变得更清晰一些。请注意，此图表中只有一个目标主机，以使流量流动更容易看到：
- en: '![Figure 10.4 – DSR load balancing'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.4 - DSR负载平衡'
- en: '](img/B16336_10_004.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16336_10_004.jpg)'
- en: Figure 10.4 – DSR load balancing
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.4 - DSR负载平衡
- en: 'There are some pretty strict requirements for building this out:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 构建这个的要求非常严格：
- en: The load balancer and all the target servers need to be on the same subnet.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 负载均衡器和所有目标服务器都需要在同一个子网上。
- en: 'This mechanism requires some games on the default gateway, since, on the way
    in, it must direct all the client traffic to the VIP on the load balancer, but
    it also has to accept replies from multiple target servers with the same address
    but different MAC addresses. The Layer 3 default gateway for this must have an
    ARP entry for each of the target servers, all with the same IP address. In many
    architectures, this is done via multiple static ARP entries. For instance, on
    a Cisco router, we would do the following:'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这种机制需要在默认网关上进行一些设置，因为在进入时，它必须将所有客户端流量定向到负载均衡器上的VIP，但它还必须接受来自具有相同地址但不同MAC地址的多个目标服务器的回复。这个必须有一个ARP条目，对于每个目标服务器，都有相同的IP地址。在许多架构中，这是通过多个静态ARP条目来完成的。例如，在Cisco路由器上，我们会这样做：
- en: '[PRE1]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Note in this example that `192.168.124.21` and `22` are the target hosts being
    load balanced. Also, the MAC addresses have an OUI, indicating that they're both
    VMware virtual hosts, which is typical in most datacenters.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在这个例子中，`192.168.124.21`和`22`是被负载均衡的目标主机。此外，MAC地址具有OUI，表明它们都是VMware虚拟主机，在大多数数据中心都是典型的。
- en: Why would you go through all this bother and unusual network configuration?
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么要经历所有这些麻烦和不寻常的网络配置？
- en: A DSR configuration has the advantage of minimizing the traffic through the
    load balancer by a wide margin. In web applications, for instance, it's common
    to see the return traffic outweigh the incoming traffic by a factor of 10 or more.
    This means that for that traffic model, a DSR implementation will see 90% or less
    of the traffic that a NAT or proxy load balancer will see.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DSR配置的优势在于大大减少了通过负载均衡器的流量。例如，在Web应用程序中，通常会看到返回流量超过传入流量的10倍以上。这意味着对于这种流量模型，DSR实现将看到负载均衡器将看到的流量的90%或更少。
- en: No "backend" subnet is required; the load balancer and the target servers are
    all in the same subnet – in fact, that's a requirement. This has some disadvantages
    too, as we've already discussed. We'll cover this in more detail in the *Specific
    Server Settings for DSR* section.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不需要“后端”子网；负载均衡器和目标服务器都在同一个子网中 - 实际上，这是一个要求。这也有一些缺点，正如我们已经讨论过的那样。我们将在*DSR的特定服务器设置*部分详细介绍这一点。
- en: 'However, there are some downsides:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，也有一些缺点：
- en: The relative load across the cluster, or the individual load on any one server,
    is, at best, inferred by the load balancer. If a session ends gracefully, the
    load balancer will catch enough of the "end of session" handshake to figure out
    that a session has ended, but if a session doesn't end gracefully, it depends
    entirely on timeouts to end a session.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集群中的相对负载，或者任何一个服务器上的个体负载，最多只能由负载均衡器推断出来。如果一个会话正常结束，负载均衡器将捕捉到足够的“会话结束”握手来判断会话已经结束，但如果一个会话没有正常结束，它完全依赖超时来结束会话。
- en: All the hosts must be configured with the same IP (the original target) so that
    the return traffic doesn't come from an unexpected address. This is normally done
    with a loopback interface, and usually requires some additional configuration
    on the host.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有主机必须配置相同的IP（原始目标），以便返回流量不会来自意外的地址。这通常是通过环回接口完成的，并且通常需要对主机进行一些额外的配置。
- en: The upstream router (or Layer 3 switch, if that's the gateway for the subnet)
    needs to be configured to allow all the possible MAC addresses for the target
    IP address. This is a manual process, and if it's possible to see MAC addresses
    change unexpectedly, this can be a problem.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上游路由器（或者如果它是子网的网关，则是第3层交换机）需要配置为允许目标IP地址的所有可能的MAC地址。这是一个手动过程，如果可能看到MAC地址意外更改，这可能是一个问题。
- en: If any function that needs proxying or full visibility of the session (as in
    the NAT implementation) can't work, the load balancer only sees half of the session.
    This means that any HTTP header parsing, cookie manipulation (for instance, for
    session persistence), or SYN cookies can't be implemented.
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果任何需要代理或完全可见会话的功能（如NAT实现中）无法工作，负载均衡器只能看到会话的一半。这意味着任何HTTP头解析、cookie操作（例如会话持久性）或SYN
    cookie都无法实现。
- en: In addition, because (as far as the router is concerned), all the target hosts
    have different MAC addresses but the same IP address, and the target hosts cannot
    reply to any ARP requests (otherwise, they'd bypass the load balancer), there's
    a fair amount of work that needs to be done on the target hosts.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，因为（就路由器而言）所有目标主机具有不同的MAC地址但相同的IP地址，而目标主机不能回复任何ARP请求（否则，它们将绕过负载均衡器），因此需要在目标主机上进行大量的工作。
- en: Specific server settings for DSR
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: DSR的特定服务器设置
- en: For a Linux client, ARP suppression for the "VIP" addressed interface (whether
    it's a loopback or a logical Ethernet) must be done. This can be completed with
    `sudo ip link set <interface name> arp off` or (using the older `ifconfig` syntax)
    `sudo ifconfig <interface name> -arp`.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Linux客户端，必须对“VIP”寻址接口（无论是环回还是逻辑以太网）进行ARP抑制。可以使用`sudo ip link set <interface
    name> arp off`或（使用较旧的`ifconfig`语法）`sudo ifconfig <interface name> -arp`来完成。
- en: You'll also need to implement the `strong host` and `weak host` settings on
    the target servers. A server interface is configured as a `strong host` if it's
    not a router and cannot send or receive packets from an interface, unless the
    source or destination IP in the packet matches the interface IP. If an interface
    has been configured as a `weak host`, this restriction doesn't apply – it can
    receive or send packets on behalf of other interfaces.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 您还需要在目标服务器上实现“强主机”和“弱主机”设置。如果服务器接口不是路由器，并且不能发送或接收来自接口的数据包，除非数据包中的源或目的IP与接口IP匹配，则将其配置为“强主机”。如果接口已配置为“弱主机”，则不适用此限制-它可以代表其他接口接收或发送数据包。
- en: 'Linux and BSD Unix have `weak host` enabled by default on all interfaces (`sysctl
    net.ip.ip.check_interface = 0`). Windows 2003 and older also have this enabled.
    However, Windows Server 2008 and newer have a `strong host` model for all interfaces.
    To change this for DSR in newer Windows versions, execute the following code:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: You'll also need to disable any IP checksum offload and TCP checksum offload
    functions on the target servers. On a Windows host, these two settings are in
    the `Network Adapter/Advanced` settings. On a Linux host, the `ethtool` command
    can manipulate these settings, but these hardware-based offload features are disabled
    by default in Linux, so normally, you won't need to adjust them.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: With the various architectures described, we still need to work out how exactly
    we want to distribute the client load across our group of target servers.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: Load balancing algorithms
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far, we''ve touched on a few load balancing algorithms, so let''s explore
    the more common approaches in a bit more detail (note that this list is not exhaustive;
    just the most commonly seen methods have been provided here):'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16336_10_Table_01.jpg)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
- en: '**Least Connections**, as you may expect, is the most often assigned algorithm.
    We''ll use this method in the configuration examples later in this chapter.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: Now that we've seen some options for how to balance a workload, how can we make
    sure that those backend servers are working correctly?
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: Server and service health checks
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One of the issues we discussed in the section on DNS load balancing was health
    checks. Once you start load balancing, you usually want some method of knowing
    which servers (and services) are operating correctly. Methods for checking the
    *health* of any connection include the following:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: Use ICMP to effectively "ping" the target servers periodically. If no pings
    return with an ICMP echo reply, then they are considered down, and they don't
    receive any new clients. Existing clients will be spread across the other servers.
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the TCP handshake and check for an open port (for instance `80/tcp` and
    `443/tcp` for a web service). Again, if the handshake doesn't complete, then the
    host is considered down.
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In UDP, you would typically make an application request. For instance, if you
    are load balancing DNS servers, the load balancer would make a simple DNS query
    – if a DNS response is received, then the sever is considered up.
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, when balancing a web application, you may make an actual web request.
    Often, you'll request the index page (or any known page) and look for known text
    on that page. If that text doesn't appear, then that host and service combination
    is considered down. In a more complex environment, the test page you check may
    make a known call to a backend database to verify it.
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Testing the actual application (as in the preceding two points) is, of course,
    the most reliable way to verify that the application is working.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: We'll show a few of these health checks in our example configuration. Before
    we get to that, though, let's dive into how you might see load balancers implemented
    in a typical datacenter – both in a "legacy" configuration and in a more modern
    implementation.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: Datacenter load balancer design considerations
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Load balancing has been part of larger architectures for decades, which means
    that we've gone through several common designs.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: The "legacy" design that we still frequently see is a single pair (or cluster)
    of physical load balancers that service all the load balanced workloads in the
    datacenter. Often, the same load balancer cluster is used for internal and external
    workloads, but sometimes, you'll see one internal pair of load balancers on the
    internal network, and one pair that only services DMZ workloads (that is, for
    external clients).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: This model was a good approach in the days when we had physical servers, and
    load balancers were expensive pieces of hardware.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: 'In a virtualized environment, though, the workload VMs are tied to the physical
    load balancers, which complicates the network configuration, limits disaster recovery
    options, and can often result in traffic making multiple "loops" between the (physical)
    load balancers and the virtual environment:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.5 – Legacy load balancing architecture'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16336_10_005.jpg)'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.5 – Legacy load balancing architecture
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: 'This has all changed with the advent of virtualization. The use of physical
    load balancers now makes very little sense – you are far better off sticking to
    a dedicated, small VM for each workload, as shown here:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.6 – Modern load balancing architecture'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16336_10_006.jpg)'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.6 – Modern load balancing architecture
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: 'This approach has several advantages:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '**Cost** is one advantage as these small virtual load balancers are much cheaper
    if licensed, or free if you use a solution such as HAProxy (or any other free/open
    source solution). This is probably the advantage that should have the least impact
    but is unsurprisingly usually the one factor that changes opinions.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Configurations are much simpler** and easier to maintain as each load balancer
    only services one workload. If a change is made and possibly needs subsequent
    debugging, it''s much simpler to "pick out" something from a smaller configuration.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the event of a failure or, more likely, a configuration error, the **splatter
    zone** or **blast radius** is much smaller. If you tie each load balancer to a
    single workload, any errors or failures are more likely to affect only that workload.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Also, from an operational standpoint, **using an orchestration platform or API
    used to scale the workload is much easier** (adding or removing backend servers
    to the cluster as demand rises and falls). This approach makes it much simpler
    to build those playbooks – mainly because of the simpler configuration and smaller
    blast radius in the event of a playbook error.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**More rapid deployments for developers**. Since you are keeping this configuration
    simple, in the development environment, you can provide developers with exactly
    this configuration as they are writing or modifying the applications. This means
    that the applications are written with the load balancer in mind. Also, most of
    the testing is done during the development cycle, rather than the configuration
    being shoe-horned and tested in a single change window at the end of development.
    Even if the load balancers are licensed products, most vendors have a free (low
    bandwidth) tier of licensing for exactly this scenario.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Providing **securely configured templates** to developers or for deployments
    is much easier with a smaller configuration.
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Security testing during the development or DevOps cycle** includes the load
    balancer, not just the application and the hosting server.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Training and testing is much simpler**. Since the load balancing products
    are free, setting up a training or test environment is quick and easy.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Workload optimization** is a significant advantage since, in a virtualized
    environment, you can usually "tie" groups of servers together. In a VMware vSphere
    environment, for instance, this is called a **vApp**. This construct allows you
    to keep all vApp members together if, for example, you vMotion them to another
    hypervisor server. You may need to do this for maintenance, or this may happen
    automatically using **Dynamic Resource Scheduling** (**DRS**), which balances
    CPU or memory load across multiple servers. Alternatively, the migration might
    be part of a disaster recovery workflow, where you migrate the vApp to another
    datacenter, either using vMotion or simply by activating a replica set of VMs.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cloud deployments lend themselves even more strongly to this distributed
    model**. This is taken to the extreme in the larger cloud service providers, where
    load balancing is simply a service that you subscribe to, rather than a discrete
    instance or virtual machine. Examples of this include the AWS Elastic Load Balancing
    Service, Azure Load Balancer, and Google''s Cloud Load Balancing service.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Load balancing brings with it several management challenges, though, most of
    which stem from one issue – if all the target hosts have a default gateway for
    the load balancer, how can we monitor and otherwise manage those hosts?
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: Datacenter network and management considerations
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If a workload is load balanced using the NAT approach, routing becomes a concern.
    The route to the potential application clients must point to the load balancer.
    If those targets are internet-based, this makes administering the individual servers
    a problem – you don't want your server administrative traffic to be load balanced.
    You also don't want to have unnecessary traffic (such as backups or bulk file
    copies) route through the load balancer – you want it to route application traffic,
    not all the traffic!
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: This is commonly dealt with by adding static routes and possibly a management
    VLAN.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: This is a good time to bring up that the management VLAN should have been there
    from the start – my "win the point" phrase on management VLANs is "does your accounting
    group (or receptionist or manufacturing group) need access to your SAN or hypervisor
    login?" If you can get an answer that leads you toward protecting sensitive interfaces
    from internal attacks, then a management VLAN is an easy thing to implement.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: 'In any case, in this model, the default gateway remains pointed toward the
    load balancer (to service internet clients), but specific routes are added to
    the servers to point toward internal or service resources. In most cases, this
    list of resources remains small, so even if internal clients plan to use the same
    load balanced application, this can still work:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.7 – Routing non-application traffic (high level)'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16336_10_007.jpg)'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.7 – Routing non-application traffic (high level)
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: If this model can't work for one reason or another, then you might want to consider
    adding **policy-based routing** (**PBR**).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: 'In this situation, say, for example, that your servers are load balancing HTTP
    and HTTPS – `80/tcp` and `443/tcp`, respectively. Your policy might look like
    this:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: Route all traffic `80/tcp` and `443/tcp` to the load balancer (in other words,
    reply traffic from the application).
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Route all other traffic through the subnet's router.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This policy route could be put on the server subnet''s router, as shown here:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.8 – Routing non-application traffic – policy routing on an upstream
    router'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16336_10_008.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.8 – Routing non-application traffic – policy routing on an upstream
    router
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding diagram, the servers all have a default gateway based on the
    router''s interface (`10.10.10.1`, in this example):'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This has the benefit of simplicity, but this subnet default gateway device must
    have sufficient horsepower to service the demand of all that reply traffic, without
    affecting the performance of any of its other workloads. Luckily, many modern
    10G switches do have that horsepower. However, this also has the disadvantage
    that your reply traffic now leaves the hypervisor, hits that default gateway router,
    then likely goes back into the virtual infrastructure to reach the load balancer.
    In some environments, this can still work performance-wise, but if not, consider
    moving the policy route to the servers themselves.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: 'To implement this same policy route on a Linux host, follow these steps:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: 'First, add the route to `table 5`:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Define the traffic that matches the load balancer (source `10.10.10.0/24`,
    source port `443`):'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Add the lookup, as follows:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This method adds more complexity and CPU overhead than most people want. Also,
    for "network routing issues," support staff are more likely to start any future
    troubleshooting on routers and switches than looking at the host configurations.
    For these reasons, putting the policy route on a router or Layer 3 switch is what
    we often see being implemented.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: 'Using a management interface solves this problem much more elegantly. Also,
    if management interfaces are not already widely in use in the organization, this
    approach nicely introduces them into the environment. In this approach, we keep
    the target hosts configured with their default gateway pointed to the load balancer.
    We then add a management VLAN interface to each host, likely with some management
    services directly in that VLAN. In addition, we can still add specific routes
    to things such as SNMP servers, logging servers, or other internal or internet
    destinations as needed:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.9 – Adding a management VLAN'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16336_10_009.jpg)'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.9 – Adding a management VLAN
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: Needless to say, this is what is commonly implemented. Not only is it the simplest
    approach, but it also adds a much-needed management VLAN to the architecture.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: With most of the theory covered, let's get on with building a few different
    load balancing scenarios.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: Building a HAProxy NAT/proxy load balancer
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First, we likely don't want to use our example host for this, so we must add
    a new network adapter to demonstrate a NAT/proxy (L4/L7) load balancer.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: If your example host is a virtual machine, building a new one should be quick.
    Or, better yet, clone your existing VM and use that. Alternatively, you can download
    an `haproxy –v`.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: Or, if you choose not to "build along" with our example configuration, by all
    means you can still "follow along." While building the plumbing for a load balancer
    can take a bit of work, the actual configuration is pretty simple, and introducing
    you to that configuration is our goal here. You can certainly attain that goal
    without having to build out the supporting virtual or physical infrastructure.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are installing this on a fresh Linux host, ensure that you have two
    network adapters (one facing the clients and one facing the servers). As always,
    we''ll start by installing the target application:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '*<Start here if you are using the OVA-based installation:>*'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: 'You can verify that the installation worked by checking the version number
    using the `haproxy` application itself:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Note that any version newer that the one shown here should work fine.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: With the package installed, let's look at our example network build.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: Before you start configuring – NICs, addressing, and routing
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You are welcome to use any IP addressing you choose, but in our example, the
    frontend `192.168.122.21/24` (note that this is different than the interface IP
    of the host), while the backend address of the load balancer will be `192.168.124.1/24`
    – this will be the default gateway of the target hosts. Our target web servers
    will have `192.168.124.10` and `192.168.124.20`.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: 'Our final build will look like this:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.10 – Load balancer example build'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16336_10_010.jpg)'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.10 – Load balancer example build
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: Before we start building our load balancer, this is the perfect time to adjust
    some settings in Linux (some of which will require a system reload).
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: Before you start configuring – performance tuning
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A basic "out of the box" Linux installation must make several assumptions for
    various settings, though many of them result in compromises from a performance
    or security perspective. For a load balancer, there are several Linux settings
    that need to be addressed. Luckily, the HAProxy installation does a lot of this
    work for us (if we installed the licensed version). Once the installation is complete,
    edit the `/etc/sysctl.d/30-hapee-2.2.conf` file and uncomment the lines in the
    following code (in our case, we're installing the Community Edition, so create
    this file and uncomment the lines). As with all basic system settings, test these
    as you go, making these changes one at a time or in logical groups. Also, as expected,
    this may be an iterative process where you may go back and forth from one setting
    to another. As noted in the file comments, not all these values are recommended
    in all or even most cases.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: These settings and their descriptions can all be found at [https://www.haproxy.com/documentation/hapee/2-2r1/getting-started/system-tuning/](https://www.haproxy.com/documentation/hapee/2-2r1/getting-started/system-tuning/).
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: 'Limit the per-socket default receive/send buffers to limit memory usage when
    running with a lot of concurrent connections. The values are in bytes and represent
    the minimum, default, and maximum. The defaults are `4096`, `87380`, and `4194304`:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Allow early reuse of the same source port for outgoing connections. This is
    required if you have a few hundred connections per second. The default is as follows:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Extend the source port range for outgoing TCP connections. This limits early
    port reuse and makes use of `64000` source ports. The defaults are `32768` and
    `61000`:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Increase the TCP SYN backlog size. This is generally required to support very
    high connection rates, as well as to resist SYN flood attacks. Setting it too
    high will delay SYN cookie usage though. The default is `1024`:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Set the timeout in seconds for the `tcp_fin_wait` state. Lowering it speeds
    up the release of dead connections, though it will cause issues below 25-30 seconds.
    It is preferable not to change it if possible. The default is `60`:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Limit the number of outgoing SYN-ACK retries. This value is a direct amplification
    factor of SYN floods, so it is important to keep it reasonably low. However, making
    it too low will prevent clients on lossy networks from connecting.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: 'Using `3` as a default value provides good results (4 SYN-ACK total), while
    lowering it to `1` under a SYN flood attack can save a lot of bandwidth. The default
    is `5`:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Set this to `1` to allow local processes to bind to an IP that is not present
    on the system. This is typically what happens with a shared VRRP address, where
    you want both the primary and backup to be started, even though the IP is not
    present. Always leave it as `1`. The default is `0`:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The following serves as a higher bound for all the system''s SYN backlogs.
    Put it at least as high as `tcp_max_syn_backlog`; otherwise, clients may experience
    difficulties connecting at high rates or under SYN attacks. The default is `128`:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Again, note that if you make any of these changes, you may end up coming back
    to this file later to back out or adjust your settings. With all this complete
    (for now, at least), let's configure our load balancer so that it works with our
    two target web servers.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: Load balancing TCP services – web services
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The configuration for load balancing services is extremely simple. Let's start
    by load balancing between two web server hosts.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s edit the `/etc/haproxy/haproxy.cfg` file. We''ll create a `frontend`
    section that defines the service that faces clients and a `backend` section that
    defines the two downstream web servers:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Take note of the following:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: The frontend section has a `default backend` line in it, which tells it which
    services to tie to that frontend.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The frontend has a `bind` statement that allows the load to be balanced across
    all the IPs on that interface. So, in this case, if we're load balancing with
    just one VIP, we can do this on the physical IP of the load balancer.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The backend has `roundrobin` as the load balancing algorithm. This means that
    as users connect, they'll get directed to server1, then server2, then server1,
    and so on.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `check` parameter tells the service to check the target server to ensure
    that it's up. This is much simpler when load balancing TCP services as a simple
    TCP "connect" does the trick, at least to verify that the host and service are
    running.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fall 3` marks the service as offline after three consecutive failed checks,
    while `rise 2` marks it as online after two successful checks. These rise/fall
    keywords can be used regardless of what check type is being used.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We also want a global section in this file so that we can set some server parameters
    and defaults:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Note that we define the user and group in this section. Going all the way back
    to [*Chapter 3*](B16336_03_Final_NM_ePub.xhtml#_idTextAnchor053), *Using Linux
    and Linux Tools for Network Diagnostics*, we mentioned that you need to have root
    privileges to start a listening port if that port number is less than `1024`.
    What this means for HAProxy is that it needs root rights to start the service.
    The user and group directives in the global section allow the service to "downgrade"
    its rights. This is important because if the service is ever compromised, having
    lower rights gives the attackers far fewer options, likely increases the time
    required for their attack, and hopefully increases their likelihood of being caught.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: The `log` line is very straightforward – it tells `haproxy` where to send its
    logs. If you have any problems you need to solve with load balancing, this is
    a good place to start, followed by the target service logs.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: The `stats` directive tells `haproxy` where to store its various performance
    statistics.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: The `nbproc` and `nbpthread` directives tell the HAProxy service how many processors
    and threads it has available for use. These numbers should be at least one process
    less than what's available so that in the event of a denial-of-service attack,
    the entire load balancer platform is not incapacitated.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: 'The various timeout parameters are there to prevent protocol-level denial-of-service
    attacks. In these situations, the attacker sends the initial requests, but then
    never continues the session – they just continually send requests, "eating up"
    load balancer resources until the memory is entirely consumed. These timeouts
    put a limit on how long the load balancer will keep any one session alive. The
    following table outlines a short description of each of the keep-alive parameters
    we''re discussing here:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16336_10_Table_02.jpg)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
- en: 'Also, the SSL directives are pretty self-explanatory:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: '`ssl-default-bind-ciphers` lists the ciphers that are allowed in any TLS sessions,
    if the load balancer is terminating or starting a session (that is, if your session
    is in proxy or Layer 7 mode).'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ssl-default-bind-options` is there to set a lower bound on TLS versions that
    are supported. At the time of writing, all SSL versions, as well as TLS version
    1.0, are no longer recommended. SSL in particular is susceptible to a number of
    attacks. With all modern browsers capable of negotiating up to TLS version 3,
    most environments elect to support TLS version 1.2 or higher (as shown in the
    example).'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, from a client machine, you can browse to the HAProxy host, and you'll see
    that you'll connect to one of the backends. If you try again from a different
    browser, you should connect to the second.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s expand on this and add support for HTTPS (on `443/tcp`). We''ll add
    an IP to the frontend interface and bind to that. We''ll change the balancing
    algorithm to least connections. Finally, we''ll change the name of the frontend
    and the backend so that they include the port number. This allows us to add the
    additional configuration sections for `443/tcp`. This traffic is load balanced
    nicely if we just monitor at the Layer 4 TCP sessions; no decryption is required:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Note that we're still just checking that the TCP port is open for a "server
    health" check. This is often referred to as a Layer 3 health check. We put ports
    `80` and `443` into two sections – these can be combined into one section for
    the frontend stanza, but it's normally a best practice to keep these separate
    so that they can be tracked separately. The side effect of this is that the counts
    for the two backend sections aren't aware of each other, but this normally isn't
    an issue since these days, the entire HTTP site is usually just a redirect to
    the HTTPS site.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: 'Another way to phrase this would be on a `listen` stanza, rather than on the
    frontend and backend stanzas. This approach combines the frontend and backend
    sections into a single stanza and adds a "health check":'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: This default HTTP health check simply opens the default page and ensures that
    something comes back by checking the header for the phrase `HTTP/1.0`. If this
    is not seen in the returned page, it counts as a failed check. You can expand
    this by checking any URI on the site and looking for an arbitrary text string
    on that page. This is often referred to as a "Layer 7" health check since it is
    checking the application. Ensure that you keep your checks simple, though – if
    the application changes even slightly, the text that's returned on a page may
    change enough to have your health check fail and accidentally mark the whole cluster
    as offline!
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: Setting up persistent (sticky) connections
  id: totrans-232
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s inject a cookie into the HTTP sessions by using a variant of the server''s
    name. Let''s also do a basic check of the HTTP service rather than just the open
    port. We will go back to our "frontend/backend" configuration file approach for
    this:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Make sure that you don't use the IP address or real name of the server as your
    cookie value. If the real server's name is used, attackers may be able to get
    access to that server by looking for that server name in DNS, or in sites that
    have databases of historical DNS entries (`dnsdumpster.com`, for instance). Server
    names can also be used to gain intelligence about the target from certificate
    transparency logs (as we discussed in [*Chapter 8*](B16336_08_Final_NM_ePub.xhtml#_idTextAnchor133),
    *Certificate Services on Linux*). Finally, if the server IP address is used in
    the cookie value, that information gives the attacker some intelligence on your
    internal network architecture, and if the disclosed network is publicly routable,
    it may give them their next target!
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: Implementation note
  id: totrans-236
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have covered a basic configuration, a very common step is to have
    a "placeholder" website on each server, each identified to match the server. Using
    "1-2-3," "a-b-c," or "red-green-blue" are all common approaches, just enough to
    tell each server session from the next. Now, with different browsers or different
    workstations, browse to the shared address multiple times to ensure that you are
    directed to the correct backend server, as defined by your rule set.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: This is, of course, a great way to ensure things are working as you progressively
    build your configuration, but it's also a great troubleshooting mechanism to help
    you decide on simple things such as "is this still working after the updates?"
    or "I know what the helpdesk ticket says, but is there even a problem to solve?"
    months or even years later. Test pages like this are a great thing to keep up
    long-term for future testing or troubleshooting.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: HTTPS frontending
  id: totrans-239
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In days gone by, server architects were happy to set up load balancers to offload
    HTTPS processing, moving that encrypt/decrypt processing from the server to the
    load balancer. This saved on server CPU, and it also moved the responsibility
    of implementing and maintaining the certificates to whoever was managing the load
    balancer. These reasons are mostly no longer valid though, for several reasons:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: If the servers and load balancer are all virtual (as is recommended in most
    cases), this just moves the processing around between different VMs on the same
    hardware – there's no net gain.
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Modern processors are much more efficient in performing encryption and decryption
    – the algorithms are written with CPU performance in mind. In fact, depending
    on the algorithm, the encrypt/decrypt operations may be native to the CPU, which
    is a huge performance gain.
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The use of wildcard certificates makes the whole "certificate management" piece
    much simpler.
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, we still do HTTPS frontending with load balancers, usually to get reliable
    session persistence using cookies – you can't add a cookie to an HTTPS response
    (or read one on the next request) unless you can read and write to the data stream,
    which implies that, at some point, it's been decrypted.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: Remember from our previous discussion that in this configuration, each TLS session
    will be terminated on the frontend side, using a valid certificate. Since this
    is now a proxy setup (Layer 7 load balancing), the backend session is a separate
    HTTP or HTTPS session. In days past, the backend would often be HTTP (mostly to
    save CPU resources), but in modern times, this will be rightly seen as a security
    exposure, especially if you are in the finance, healthcare, or government sectors
    (or any sector that hosts sensitive information). For this reason, in a modern
    build, the backend will almost always be HTTPS as well, often with the same certificate
    on the target web server.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: Again, the disadvantage of this setup is that since the actual client of the
    target web server is the load balancer, the `X-Forwarded-*` HTTPS header will
    be lost, and the IP address of the actual client will not be available to the
    web server (or its logs).
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: 'How do we go about configuring this setup? First, we must obtain the site certificate
    and private key, whether that''s a "named certificate" or a wildcard. Now, combine
    these into one file (not as a `pfx` file, but as a chain) by simply concatenating
    them together using the `cat` command:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Note that we're using `sudo` in the second half of the command, to give the
    command rights to the `/etc/ssl/sitename.com` directory. Also, note the `tee`
    command, which echoes the command's output to the screen. It also directs the
    output to the desired location.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can bind the certificate to the address in the frontend file stanza:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Take note of the following in this configuration:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: We can now use cookies for session persistence (in the backend section), which
    is generally the primary objective in this configuration.
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We use the `redirect scheme` line in the frontend to instruct the proxy to use
    SSL/TLS on the backend.
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `forwardfor` keyword adds the actual client IP to the `X-Forwarded-For`
    HTTP header field on the backend request. Note that it's up to the web server
    to parse this out and log it appropriately so that you can use it later.
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Depending on the application and browsers, you can also add the client IP to
    the backend HTTP request in the `X-Client-IP` header field with a clause:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Note
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: This approach sees mixed results.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: Note, however, that whatever you add or change in the HTTP header, the actual
    client IP that the target server "sees" remains the backend address of the load
    balancer – these changed or added header values are simply fields in the HTTPS
    request. If you intend to use these header values for logging, troubleshooting,
    or monitoring, it's up to the web server to parse them out and log them appropriately.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: That covers our example configurations – we've covered NAT-based and proxy-based
    load balancing, as well as session persistence for both HTTP and HTTPS traffic.
    After all the theory, actually configuring the load balancer is simple – the work
    is all in the design and in setting up the supporting network infrastructure.
    Let's discuss security briefly before we close out this chapter.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: A final note on load balancer security
  id: totrans-262
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we've discussed how an attacker might be able to gain insight or access
    to the internal network if they can get server names or IP addresses. We discussed
    how a malicious actor can get that information using information disclosed by
    the cookies used in a local balancer configuration for persistent settings. How
    else can an attacker gain information about our target servers (which are behind
    the load balancer and should be hidden)?
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: Certificate transparency information is another favorite method for getting
    current or old server names, as we discussed in [*Chapter 8*](B16336_08_Final_NM_ePub.xhtml#_idTextAnchor133),
    *Certificate Services on Linux*. Even if the old server names are no longer in
    use, the records of their past certificates are immortal.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: The Internet Archive site at [https://archive.org](https://archive.org) takes
    "snapshots" of websites periodically, and allows them to be searched and viewed,
    allowing people to go "back in time" and view older versions of your infrastructure.
    If older servers are disclosed in your old DNS or in the older code of your web
    servers, they're likely available on this site.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: DNS archive sites such as `dnsdumpster` collect DNS information using passive
    methods such as packet analysis and present it via a web or API interface. This
    allows an attacker to find both older IP addresses and older (or current) hostnames
    that an organization might have used, which sometimes allows them to still access
    those services by IP if the DNS entries are removed. Alternatively, they can access
    them individually by hostname, even if they are behind a load balancer.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: '*Google Dorks* are another method of obtaining such information – these are
    terms for finding specific information that can be used in search engines (not
    just Google). Often, something as simple as a search term such as `inurl:targetdomain.com`
    will find hostnames that the target organization would rather keep hidden. Some
    Google Dorks that are specific to `haproxy` include the following:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Note that where we say `site:`, you can also specify `inurl:`. In that case,
    you can also shorten the search term to just the domain rather than the full site
    name.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: Sites such as `shodan.io` will also index historic versions of your servers,
    focusing on server IP addresses, hostnames, open ports, and the services running
    on those ports. Shodan is unique in how well it identifies the service running
    on an open port. While they are not, of course, 100% successful in that (think
    of it as someone else's NMAP results), when they do identify a service, the "proof"
    is posted with it, so if you are using Shodan for reconnaissance, you can use
    that to verify how accurate that determination might be. Shodan has both a web
    interface and a comprehensive API. With this service, you can often find improperly
    secured load balancers by organization or by geographic area.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: 'A final comment on search engines: if Google (or any search engine) can reach
    your real servers directly, then that content will be indexed, making it easily
    searchable. If a site may have an authentication bypass issue, the "protected
    by authentication" content will also be indexed and available for anyone who uses
    that engine.'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: That said, it's always a good idea to use tools like the ones we've just discussed
    to periodically look for issues on your perimeter infrastructure.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: Another important security issue to consider is management access. It's important
    to restrict access to the management interface of the load balancers (in other
    words, SSH), restricting it to permitted hosts and subnets on all interfaces.
    Remember that if your load balancer is parallel to your firewall, the whole internet
    has access to it, and even if not, everyone on your internal network has access
    to it. You'll want to whittle that access down to just trusted management hosts
    and subnets. If you need a reference for that, remember that we covered this in
    [*Chapter 4*](B16336_04_Final_NM_ePub.xhtml#_idTextAnchor071), *The Linux Firewall*,
    and [*Chapter 5*](B16336_05_Final_NM_ePub.xhtml#_idTextAnchor085), *Linux Security
    Standards with Real-Life Examples*.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-274
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hopefully, this chapter has served as a good introduction to load balancers,
    how to deploy them, and the reasons you might choose to make various design and
    implementation decisions around them.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: If you used new VMs to follow along with the examples in this chapter, we won't
    need them in subsequent chapters, but you might wish to keep the HAProxy VMs in
    particular if you need an example to reference later. If you followed the examples
    in this chapter just by reading them, then the examples in this chapter remain
    available to you. Either way, as you read this chapter, I hope you were mentally
    working out how load balancers might fit into your organization's internal or
    perimeter architecture.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: With this chapter completed, you should have the skills needed to build a load
    balancer in any organization. These skills were discussed in the context of the
    (free) version of HAProxy, but the design and implementation considerations are
    almost all directly usable in any vendor's platform, the only changes being the
    wording and syntax in the configuration options or menus. In the next chapter,
    we'll look at enterprise routing implementations based on Linux platforms.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  id: totrans-278
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we conclude, here is a list of questions for you to test your knowledge
    regarding this chapter''s material. You will find the answers in the *Assessments*
    section of the *Appendix*:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: When would you choose to use a **Direct Server Return** (**DSR**) load balancer?
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why would you choose to use a proxy-based load balancer as opposed to one that
    is a pure NAT-based solution?
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  id: totrans-282
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Take a look at the following links to learn more about the topics that were
    covered in this chapter:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: 'HAProxy documentation: [http://www.haproxy.org/#docs](http://www.haproxy.org/#docs)'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'HAProxy documentation (Commercial version): [https://www.haproxy.com/documentation/hapee/2-2r1/getting-started/](https://www.haproxy.com/documentation/hapee/2-2r1/getting-started/)'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'HAProxy GitHub: [https://github.com/haproxytech](https://github.com/haproxytech)'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'HAProxy GitHub, OVA VM download: [https://github.com/haproxytech/vmware-haproxy#download](https://github.com/haproxytech/vmware-haproxy#download)'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'HAProxy Community versus Enterprise Differences: [https://www.haproxy.com/products/community-vs-enterprise-edition/](https://www.haproxy.com/products/community-vs-enterprise-edition/)'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'More on Load Balancing Algorithms: [http://cbonte.github.io/haproxy-dconv/2.4/intro.html#3.3.5](http://cbonte.github.io/haproxy-dconv/2.4/intro.html#3.3.5)'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
