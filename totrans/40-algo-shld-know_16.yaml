- en: Large-Scale Algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Large-scale algorithms  are designed to solve gigantic complex problems. The
    characterizing feature of large-scale algorithms is their need to have more than
    one execution engine due to the scale of their data and processing requirements.
    This chapter starts by discussing what types of algorithms are best suited to
    be run in parallel. Then, it discusses the issues related to parallelizing algorithms.
    Next, it presents the  **Compute Unified Device Architecture** (**CUDA**)architecture
    and discusses how a single **graphics processing unit** (**GPU**) or an array
    of GPUs can be used to accelerate the algorithms. It also discusses what changes
    need to be made to the algorithm to effectively utilize the power of the GPU.
    Finally, this chapter discusses cluster computing and discusses how Apache Spark
    creates  **Resilient Distributed Datasets**  (**RDDs**) to create an extremely
    fast parallel implementation of standard algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you will be able to understand the basic strategies
    related to the design of large-scale algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics are covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to large-scale algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The design of parallel algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Algorithms to utilize the GPU
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding algorithms utilizing cluster computing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to use the GPU to run large-scale algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to use the power of clusters to run large-scale algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's start with the introduction.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to large-scale algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Human beings like to be challenged. For centuries, various human innovations
    have allowed us to solve really complex problems in different ways. From predicting
    the next target area of a locust attack to calculating the largest prime number,
    the methods to provide answers for complex problems around us kept on evolving.
    With the advent of the computer, we found a powerful new way to solve complex
    algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Defining a well-designed, large-scale algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A well-designed, large-scale algorithm has the following two characteristics:'
  prefs: []
  type: TYPE_NORMAL
- en: It is designed to handle a huge amount of data and processing requirements using
    an available pool of resources optimally.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is scalable. As the problem becomes more complex, it can handle the complexity
    simply by provisioning more resources.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One of the most practical ways of implementing large-scale algorithms is by
    using the divide and conquer strategy, that is, divide the larger problem into
    smaller problems that can be solved independently of each other.
  prefs: []
  type: TYPE_NORMAL
- en: Terminology
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's look into some of the terminology that can be used to quantify the quality
    of large-scale algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Latency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Latency is the end-to-end time taken to perform a single computation. If *Compute[1]*  represents
    a single computation that starts at *t[1]*  and ends at *t[2]*,  then we can say
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Latency = t[2]-t[1]*'
  prefs: []
  type: TYPE_NORMAL
- en: Throughput
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the context of parallel computing, throughput is the number of single computations
    that can be performed simultaneously. For example, if, at *t[1]*, we can perform
    four simultaneous computations, *C[1]*, *C[2]*, *C[3]*, and C[4], then the throughput
    is four.
  prefs: []
  type: TYPE_NORMAL
- en: Network bisection bandwidth
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The bandwidth between two equal parts of a network is called the  **network
    bisection bandwidth**. For distributed computing to work efficiently, this is
    the most important parameter to consider. If we do not have enough network bisection
    bandwidth, the benefits gained by the availability of multiple execution engines
    in distributed computing will be overshadowed by slow communication links.
  prefs: []
  type: TYPE_NORMAL
- en: Elasticity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The ability of the infrastructure to react to a sudden increase in processing
    requirements by provisioning more resources is called elasticity.
  prefs: []
  type: TYPE_NORMAL
- en: The three cloud computing giants, Google, Amazon, and Microsoft can provide
    highly elastic infrastructures. Due to the gigantic size of their shared resource
    pools, there are very few companies that have the potential to match the elasticity
    of infrastructure of these three companies.
  prefs: []
  type: TYPE_NORMAL
- en: If the infrastructure is elastic, it can create a scalable solution to the problem.
  prefs: []
  type: TYPE_NORMAL
- en: The design of parallel algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is important to note that parallel algorithms are not a silver bullet. Even
    the best designed parallel architectures may not give the performance that we
    may expect. One law that is widely used to design parallel algorithms is Amdahl's
    law.
  prefs: []
  type: TYPE_NORMAL
- en: Amdahl's law
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Gene Amdahl was one of the first people who studied parallel processing in
    the 1960s. He proposed Amdahl''s law, which is still applicable today and can
    become a basis to understand the various trade-offs involved when designing a
    parallel computing solution. Amdahl''s law can be explained as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: It is based on the concept that in any computing process, not all of the processes
    can be executed in parallel. There will be a sequential portion of the process
    that cannot be parallelized.
  prefs: []
  type: TYPE_NORMAL
- en: Let's look at a particular example. Assume that we want to read a large number
    of files stored on a computer and want to train a machine learning model using
    the data found in these files.
  prefs: []
  type: TYPE_NORMAL
- en: 'This whole process is called P. It is obvious that P can be divided into the
    following two subprocesses:'
  prefs: []
  type: TYPE_NORMAL
- en: '*P1*: Scan the files in the directory, create a list of filenames that matches
    the input file, and pass it on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*P2*: Read the files, create the data processing pipeline, process the files,
    and train the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conducting sequential process analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The time to run *P* is represented by *T[seq]**(P)*. The times to run *P1*
    and *P2* are represented by *T[seq](P1)* and *T[seq](P2)*. It is obvious that,
    when running on a single node, we can observe two things:'
  prefs: []
  type: TYPE_NORMAL
- en: '*P2* cannot start running before P1 is complete. This is represented by *P1*
    -- > *P2*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*T[seq](P) = T[seq](P1) + T[seq](P2)*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s assume that P overall takes 10 minutes to run on a single node. Out
    of these 10 minutes, P1 takes 2 minutes to run and P2 takes 8 minutes to run on
    a single node. This is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/28852de3-0a22-4dbc-877e-ac90073c894e.png)'
  prefs: []
  type: TYPE_IMG
- en: Now the important thing to note is that *P1* is sequential in nature. We cannot
    make it faster by making it parallel. On the other hand, *P2* can easily be split
    into parallel subtasks that can run in parallel. So, we can make it run faster
    by running it in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: The major benefit of using cloud computing is the availability of a large pool
    of resources and many of them are used in parallel. The plan to use these resources
    for a given problem is called an execution plan. Amdahl's law is used comprehensively
    to identify the bottlenecks for a given problem and a pool of resources.
  prefs: []
  type: TYPE_NORMAL
- en: Conducting parallel execution analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If we want to use more than one node to speed up *P*, it will only affect *P2*
    by a factor of *s>1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/fa78d698-b932-406a-9a95-6bb07cebff10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The speedup of the process P can be easily calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/e9f45310-5be0-4c7b-b3e3-37f68fc8ba09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The ratio of the parallelizable portion of a process to its total is represented
    by *b* and is calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/b0a3962b-be67-47a2-8678-7d948493ec94.png)'
  prefs: []
  type: TYPE_IMG
- en: For example, in the preceding scenario, *b = 8/10 = 0.8.*
  prefs: []
  type: TYPE_NORMAL
- en: 'Simplifying these equations will give us Amdahl''s law:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/65eafd8e-1313-4267-b1c7-b65390f7e3fa.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*P* is the overall process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*b* is the ratio of the parallelizable portion of *P*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*s* is the speedup achieved in the parallelizable portion of *P*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s assume that we plan to run the process P on three parallel nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '*P1* is the sequential portion and cannot be reduced by using parallel nodes.
    It will remain at 2 seconds.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*P2* now takes 3 seconds instead of 9 seconds.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So, the total time taken by process *P* is reduced to 5 seconds, as shown in
    the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/26d817e9-bdb3-4a6e-98cd-7ef4073ea5d4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding example, we can calculate the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*n[p]*  = the number of processors = 3'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*b* = the parallel portion = 9/11 = 81.81%'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*s* = the speedup = 3'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, let''s look at a typical graph that explains Amdahl''s law:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/6075c179-8dae-4a80-aa66-27dd2f492939.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding diagram, we draw the graph between *s* and *n*[*p*] for different
    values of *b*.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding task granularity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we parallelize an algorithm, a larger job is divided into multiple parallel
    tasks. It is not always straightforward to find the optimal number of parallel
    tasks into which a job should be divided. If there are too few parallel tasks,
    we will not get much benefit from parallel computing. If there are too many tasks,
    then it will generate too much overhead.  This is a challenge also termed as task
    granularity.
  prefs: []
  type: TYPE_NORMAL
- en: Load balancing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In parallel computing, a scheduler is responsible for selecting the resources
    to execute the tasks. Optimal load balancing is a difficult thing to achieve,
    in the absence of which, the resources are not fully utilized.
  prefs: []
  type: TYPE_NORMAL
- en: Locality issues
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In parallel processing, the movement of data should be discouraged. Whenever
    possible, instead of moving data, it should be processed locally on the node where
    it resides, otherwise, it reduces the quality of the parallelization.
  prefs: []
  type: TYPE_NORMAL
- en: Enabling concurrent processing in Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The easiest way to enable parallel processing in Python is by cloning a current
    process that will start a new concurrent process called the  **child process**.
  prefs: []
  type: TYPE_NORMAL
- en: Python programmers, although not biologists, have created their own process
    of cloning. Just like a cloned sheep, the clone copy is the exact copy of the
    original process.
  prefs: []
  type: TYPE_NORMAL
- en: Strategizing multi-resource processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Initially, large-scale algorithms were used to run on huge machines called  **supercomputers**.
    These supercomputers shared the same memory space. The resources were all local—physically
    placed in the same machine. It means that the communications between the various
    processors were very fast and they were able to share the same variable through
    the common memory space. As the systems evolved and the need to run large-scale
    algorithms grew, the supercomputers evolved into  **Distributed Shared Memory**  (**DSM**)
    where each processing node used to own a portion of the physical memory. Eventually,
    clusters were developed, which are loosely coupled and rely on message passing
    among processing nodes. For large-scale algorithms, we need to find more than
    one execution engines running in parallel to solve a complex problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/6b9c2b35-14a6-4162-a4d8-6b6a6cd4274f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'There are three strategies to have more than one execution engine:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Look within**:  Exploit the resources already on the computer. Use the hundreds
    of cores of the GPU to run a large-scale algorithm.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Look outside**:  Use distributed computing to find more computing resources
    that can be collectively used to solve the large-scale problem at hand.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hybrid strategy**: Use distributed computing  and, on each  of the nodes,
    use the GPU or an array of GPUs to expedite the running of the algorithm.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing CUDA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'GPUs have been designed originally for graphic processing. They have been designed
    to suit the needs of optimization dealing with the multimedia data of a typical
    computer. To do so, they have developed certain characteristics that differentiate
    them from CPUs. For example, they have thousands of cores as compared to the limited
    number of CPU cores. Their clock speed is much slower than a CPU. A GPU has its
    own DRAM. For example, Nvidia''s RTX 2080 has 8GB of RAM.  Note that GPUs are
    specialized processing devices and do not have general processing unit features,
    including interrupts or means of addressing devices, for example, a keyboard and
    mouse. Here is the architecture of GPUs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/3f8ba45d-46a9-4345-b8ff-08ef34554310.png)'
  prefs: []
  type: TYPE_IMG
- en: Soon after GPUs became mainstream, data scientists started exploring GPUs for
    their potential to efficiently perform parallel operations. As a typical GPU has
    thousands of ALUs, it has the potential to spawn 1,000s concurrent processes.
    This makes GPUs the architecture optimized for data-parallel computation. Hence,
    algorithms that can perform parallel computations are best suited for GPUs. For
    example, an object search in a video is known to be at least 20 times faster in
    GPUs, compared to CPUs. Graph algorithms, which were discussed in [Chapter 5](051e9b32-f15f-4e88-a63a-ae3c14696492.xhtml)
    , *Graph Algorithms*, are known to run much faster on GPUs than on CPUs.
  prefs: []
  type: TYPE_NORMAL
- en: 'To realize the dreams of data scientists to fully utilize GPUs for algorithms,
    in 2007, Nvidia created an open source framework called CUDA, which stands for
    Compute Unified Device Architecture.  CUDA abstracts  the working of the CPU and
    GPU as a host and device respectively. The host, that is, the CPU, is responsible
    for calling the device, which is the GPU. The CUDA architecture has various layers
    of abstractions that can be presented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/fe199de5-75f7-4fa7-8815-0eb6a9630652.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that CUDA runs on top of Nvidia's GPUs. It needs support in the OS kernel.
    CUDA initially started with support in the Linux kernel. More recently, Windows
    is now fully supported. Then, we have the CUDA Driver API that acts as a bridge
    between the programming language API and the CUDA driver. On the top layer, we
    have support for C, C+, and Python.
  prefs: []
  type: TYPE_NORMAL
- en: Designing parallel algorithms on CUDA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's look deeper into how the GPU accelerates certain processing operations.
    As we know, CPUs are designed for the sequential execution of data that results
    in significant running time for certain classes of applications. Let's look into
    the example of processing an image of a size of 1,920 x 1,200\. It can be calculated
    that there are 2,204,000 pixels to process. Sequential processing means that it
    will take a long time to process them on a traditional CPU. Modern GPUs such as
    Nvidia's Tesla are capable of spawning this unbelievable amount of 2,204,000 parallel
    threads to process the pixels. For most multimedia applications, the pixels can
    be processed independently of each other and will achieve a significant speedup.
    If we map each pixel with a thread, they can all be processed in O(1) constant
    time.
  prefs: []
  type: TYPE_NORMAL
- en: 'But image processing is not the only application where we can use data parallelism
    to speed up the process. Data parallelism can be used in preparing data for machine
    learning libraries. In fact, the GPU can massively reduce the execution time of
    parallelizable algorithms, which include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Mining money for bitcoins
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Large-scale simulations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DNA analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Video and photos analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GPUs are not designed for **Single Program, Multiple Data** (**SPMD**). For
    example, if we want to calculate the hash for a block of data, it is a single
    program that cannot run in parallel. GPUs will perform slower in such scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: The code that we want to run on the GPU is marked with special CUDA keywords
    called **kernels**. These kernels are used to mark the functions that we intend
    to run on GPUs for parallel processing. Based on the kernels, the GPU compiler
    separates which code needs to run on the GPU and the CPU.
  prefs: []
  type: TYPE_NORMAL
- en: Using GPUs for data processing in Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'GPUs are great for data processing in a multidimensional data structure. These
    data structures are inherently parallelizable. Let''s see how we can use the GPU
    for multidimensional data processing in Python:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s import the Python packages that are needed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: We will be using a multidimensional array in NumPy, which is a traditional Python
    package that uses the CPU.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Then, we create a multidimensional array using a CuPy array, which uses the
    GPU. Then, we will compare the timings:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'If we will run this code, it will generate the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/20d041ea-1df5-4075-898e-dd19cfe68e37.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that it took around 1.13 seconds to create this array in NumPy and around
    0.012 in CuPy, which makes the initialization of this array 92 times faster in
    the GPU.
  prefs: []
  type: TYPE_NORMAL
- en: Cluster computing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Cluster computing is one of the ways of implementing parallel processing for
    large-scale algorithms. In cluster computing, we have multiple nodes connected
    via a very high-speed network. Large-scale algorithms are submitted as jobs. Each
    job is divided into various tasks and each task is run on a separate node.
  prefs: []
  type: TYPE_NORMAL
- en: 'Apache Spark is one of the most popular ways of implementing cluster computing.
    In Apache Spark, the data is converted into distributed fault-tolerant datasets,
    which are  called  **Resilient Distributed Datasets**  (**RDDs**). RDDs are the
    core Apache Spark abstraction. They are immutable collections of elements that
    can be operated in parallel. They are split into partitions and are distributed
    across the nodes, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/88c9702a-23ec-45b0-a223-4050254b50e1.png)'
  prefs: []
  type: TYPE_IMG
- en: Through this parallel data structure, we can run algorithms in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing data processing in Apache Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see how we can create an RDD in Apache Spark and run distributed processing
    on it across the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For this, first, we need to create a new Spark session, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have created a Spark session, we use a CSV file for the source of the
    RDD. Then, we will run the following function—it will create an RDD that is abstracted
    as a DataFrame called `df`. The ability to abstract an RDD as a DataFrame was
    added in Spark 2.0 and this makes it easier to process the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s look into the columns of the DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/ffc20c87-5cdf-4c78-a235-2483f9c50655.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, we can create a temporary table from the DataFrame, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the temporary table is created, we can run SQL statements to process the
    data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/dada91d3-357d-4523-9c0d-3fb41a6d2691.png)'
  prefs: []
  type: TYPE_IMG
- en: An important point to note is that although it looks like a regular DataFrame,
    it is just a high-level data structure. Under the hood, it is the RDD that spreads
    data across the cluster. Similarly, when we run SQL functions, under the hood,
    they are converted into parallel transformer and reducers and they fully use the
    power of the cluster to process the code.
  prefs: []
  type: TYPE_NORMAL
- en: The hybrid strategy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Increasingly, cloud computing is becoming more and more popular to run large-scale
    algorithms. This provides us with the opportunity to combine *look outside* and
    *look within* strategies. This can be done by provisioning one or more GPUs in
    multiple virtual machines, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/3c81bb15-6d1a-4f10-9bd7-a575497280e4.png)'
  prefs: []
  type: TYPE_IMG
- en: Making the best use of hybrid architecture is a non-trivial task. This is approached
    by first dividing the data into multiple partitions. Compute-intensive tasks that
    require less data are parallelized within each node at the GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we looked into the design of parallel algorithms and the design
    issues of large-scale algorithms. We looked into using parallel computing and
    GPUs to implement large-scale algorithms. We also looked into how we can use Spark
    clusters  to implement large-scale algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we learned about issues related to large-scale algorithms.
    We looked into the issues related to parallelizing algorithms and the potential
    bottlenecks created in the process.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will look at some practical aspects of implementing
    algorithms.
  prefs: []
  type: TYPE_NORMAL
