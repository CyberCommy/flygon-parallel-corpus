- en: Creating a Highly Available Self-Healing Architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will go through how the IT industry has evolved from using
    monolithic applications to cloud-native, containerized, and highly available microservices.
  prefs: []
  type: TYPE_NORMAL
- en: With open source, we can provide solutions that will enable us to create highly
    available and on-demand scales of our applications based on our user consumption.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Describing microservices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why containers are the home of microservices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How we can orchestrate our containers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring the most commonly-used orchestrator in Open Source, Kubernetes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Microservices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Microservices are used to design applications in a modular way, where each module
    is deployed independently, and they communicate with each other through APIs.
    All these modules work together to deliver a single application where each function
    has its own purpose.
  prefs: []
  type: TYPE_NORMAL
- en: For example, let's take a look at an online store. All we can see is the main
    website; however, on the backend there are several microservices that come into
    play, one service to take orders, another to suggest items for you based on your
    previous browsing, payment processing, review and comment handlers, and more.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram is an example of a microservice application:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a4ce4515-7320-4dd9-910d-043557a5767b.png)'
  prefs: []
  type: TYPE_IMG
- en: By nature, microservice applications do not require a huge team to support the
    application as a whole. One single team supports only one or two modules in the
    big picture, creating a more granular approach in terms of support and expertise
    of each moving part of the final product. Support and development are not only
    granular, but there are also failures. In the case of a single microservice failure,
    only that portion of the application will fail.
  prefs: []
  type: TYPE_NORMAL
- en: Continuing with our online store example, let's say that the microservice that
    handles the reviews and comments fails. This is due to the fact that our website
    is constructed using microservices, so only that component of our site will be
    unavailable to our customers.
  prefs: []
  type: TYPE_NORMAL
- en: They will, however, still be able to continue purchasing and using the website
    with no issues, and while users will not be able to see the reviews for the products
    they are interested in, this does not mean that our entire website usability is
    compromised. Depending on what caused the issue, you can either patch the microservice
    or restart it. Bringing down the entire website for a patch or restart is no longer
    necessary.
  prefs: []
  type: TYPE_NORMAL
- en: As an infrastructure engineer you might think, why do I have to know what a
    microservice is or what its benefits are? Well, the reason is simple. As an architect
    or infrastructure engineer, you are building the underlying infrastructure for
    this type of application. Whether they are monolithic applications running on
    a single host or microservices spread out across multiple containers, it will
    certainly impact the way you design your customer's architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Linux will be your best friend here, as you will find multiple open source tools
    that will help you to maintain high availability, load balancing, and **continuous
    integration** (**CI**)/**continuous delivery** (**CD**) with tools such as Docker,
    Kubernetes, Jenkins, Salt, and Puppet.  So whenever a customer asks you for which
    OS environment he should start designing his microserviced applications, Linux
    will be your  answer.
  prefs: []
  type: TYPE_NORMAL
- en: Currently, Docker Swarm and Kubernetes are leaders when it comes to container
    orchestration. When it comes to microservices, containers will be also your go-to
    when designing an infrastructure for a customer.
  prefs: []
  type: TYPE_NORMAL
- en: We will be diving into Kubernetes in [Chapter 7](d89f650b-f4ea-4cda-9111-a6e6fa6c2256.xhtml), *Understanding
    the Core Components of a Kubernetes Cluster*, and showing how it will help you
    orchestrate and deliver an elegant but complex solution for hosting microservices
    and other types of applications.
  prefs: []
  type: TYPE_NORMAL
- en: However, before talking about Kubernetes or container orchestration, we need
    to explain the concept of a container in order to understand why they are perfect
    for housing microservice apps.
  prefs: []
  type: TYPE_NORMAL
- en: Containers in Linux have been available for some time now, but it was not until
    a few years ago (with the release of the Docker Engine) that they gained momentum
    and admiration across all the tech communities. Containers came into play at the
    right time, and with the rise of microservices architecture they came to stay,
    and are shaping the way that we design and perform it.
  prefs: []
  type: TYPE_NORMAL
- en: Let's take a step back so that you can understand the benefits of such technology.
    Imagine that you have a simple monolith application that is running an API from
    which you can consult a list of users and what they have bought from a website
    that you are hosting on the same application bundle.
  prefs: []
  type: TYPE_NORMAL
- en: After a while, your customer sees that their API is becoming really popular
    among other applications, who are now making thousands of HTTP `GET` requests
    during peak hours. The current infrastructure is not able to handle so many requests,
    so your customer asks you to scale their infrastructure in a way that can handle
    more requests. The problem here is that because this is a monolithic application,
    you will not only need to calculate the resources required for the API, but you
    will have to also take into account the web-store frontend that is hosted alongside
    the API—even though the API is the only thing that you actually need to scale.
  prefs: []
  type: TYPE_NORMAL
- en: This will be a waste of resources as you are taking the web-store frontend as
    well, which does not require any additional replicas or resources. You are wasting
    precious, and sometimes expensive (if you are in the public cloud) storage, memory,
    and CPU resources on something that doesn't really require it.
  prefs: []
  type: TYPE_NORMAL
- en: So, this is where microservices, and also containers for hosting such types
    of applications, come into play. With microservices in container images, you don't
    have to provision a new server every time you need to scale up your services due
    to demand, nor do you have to restart the server, or struggle with package dependencies
    every time you perform an update of the app or the OS. With a simple single command
    (`docker container run companyreg.io/storeapi:latest`), your application is up
    and ready to serve requests. Similarly, if your application fails, just restart
    your container or provision a new one, and you are ready to go. What if an update
    that was made to the microservice had a bug? Just go ahead and revert to the previous
    image version and you can be up and running again; there is no need to start uninstalling
    updated libraries or dealing with dependency issues.
  prefs: []
  type: TYPE_NORMAL
- en: Containers also allow consistency across application deployments because, as
    you may know, there are multiple ways of installing a package. You can do so through
    a package manager such as `apt`, `yum`, and `apk`, or through `git`, `/curl/wget`,
    `pip`, and `juju`, and depending on how you install it, it will also define the
    way you maintain it.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine a production environment where the developers send their package to
    the **open profiling standard** (**OPS**) team for deployment, and every OPS engineer
    deploys the app in a different way! This will become unsupportable and very hard
    to track. A container image with your app on it will create consistency because,
    no matter where you deploy it as a container, it will have the same location for
    all the configuration files, binaries, libraries, and dependencies everywhere
    you deploy it. Everything will be isolated into a container running with its own
    **process namespace** (**PID namespace**), network namespace, and **mount namespace**
    (**MNT namespace**).
  prefs: []
  type: TYPE_NORMAL
- en: The point of having an app architected in microservices is to provide isolation
    to each of the microservices in the app so that they can be easily managed and
    maintained—and a container achieves exactly that. You can even define how you
    want to start your application every time the container comes up—again, consistency
    plays a leading role here.
  prefs: []
  type: TYPE_NORMAL
- en: Creating container images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The way you build a container is through something called a **Dockerfile**.
    A Dockerfile is basically a set of instructions on how to build your container
    image; a typical Dockerfile is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, it is a very readable set of instructions. Without even knowing
    what each instruction does, we can assume its function because it's very similar
    to English. This Dockerfile is just an example and by far the most efficient way
    to do it.
  prefs: []
  type: TYPE_NORMAL
- en: An image is essentially like a template in the **virtual machine** (**VM**)
    world; it is a set of read-only layers that contain all the information that you
    need to deploy your containers—from a single image you can deploy multiple containers
    as they all work on their own writable layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, whenever you pull an image you will see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Each `Pull complete` instance that you see corresponds to a layer of the image.
    So, what are these layers and where do they come from?
  prefs: []
  type: TYPE_NORMAL
- en: When we perform the build of the image, some of the instructions that we define
    in the Dockerfile will create a new layer. Each instruction in the file is executed
    in a read-write layer in a container that, at the end of the build, will be committed
    to the final layer stack that shapes the final image. One thing to note is that
    even if each instruction during the build is executed in a container, not all
    commands will create data that will make the image larger in terms of size and
    layers—some of them will only write to something called the **image manifest**,
    which is essentially a file that contains all the images' metadata.
  prefs: []
  type: TYPE_NORMAL
- en: Let's explore each command a little bit more.
  prefs: []
  type: TYPE_NORMAL
- en: FROM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `FROM` instruction indicates what your initial image will be and, essentially,
    the grounds on which you will start building your own image.
  prefs: []
  type: TYPE_NORMAL
- en: What you put here will depend on your needs, for instance, which image has the
    libraries preinstalled that my application needs, which image already has the
    compiler that I need to compile my application, or which image has the least impact
    on our final size. For example, your application is built on Python 2\. Instead
    of using CentOS or Ubuntu as the initial image and then installing Python manually,
    you can just use the `python:2.7` image, and it will already come with Python
    preinstalled for you.
  prefs: []
  type: TYPE_NORMAL
- en: Clearly, there are more things to consider here, but we will be going through
    them later in this chapter when we look at the best practices of image building.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since this instruction takes another image and uses it as its basis, your final
    image will inherit the layers of your base; so, the total number of final layers
    will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Final image layers = base image layers + the layers you create*'
  prefs: []
  type: TYPE_NORMAL
- en: LABEL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `LABEL` instruction is very self-explanatory—it labels your images with
    key-value pairs as metadata that you will later be able to retrieve through the
    `docker inspect` command. You can use this to add data that you would like the
    user of your image to know. Usually, it is used to add the information about the
    author of the image, such as their email or company:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Because this instruction is just metadata, no extra layers will be added to
    your image.
  prefs: []
  type: TYPE_NORMAL
- en: RUN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With `RUN`, you will run the commands that you need to prepare your container
    to run your application; for example, to install packages, compile your code,
    and create users or directories. `RUN` has two ways of running commands.
  prefs: []
  type: TYPE_NORMAL
- en: 'The shell form is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'In this form, all your commands will be run with the `/bin/sh -c` shell by
    default, although you can change the shell by using the `SHELL` instruction, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The `SHELL` keyword can only be run in the JSON array format, which leads us
    to the second form that you can use to run the `RUN` instruction.
  prefs: []
  type: TYPE_NORMAL
- en: 'The exec form is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The main difference here, besides the formatting, is that in the exec form
    the shell is not invoked, so normal variable substitution will not happen—instead,
    you will have to invoke the shell as a command for the shell to be able to provide
    variable expansion:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Due to the nature of the `RUN` keyword, each instance of it will be executed
    on a new layer and committed to the final image, therefore, every time you use
    `RUN` it will add a new layer to your image.
  prefs: []
  type: TYPE_NORMAL
- en: ENV
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For `ENV`, there is not much to say—this instruction sets variables for the
    environment. They will be used during build time and will be available during
    container runtime. `ENV` does not generate extra layers to the container as it
    stores the environment variables on the image manifest as metadata:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The parameters for `ENV` are treated in `<key>` /`<value>` pairs, where the
    `<key>` parameter is the variable name and the `<value>` parameter is its contents
    or value. You can either declare them by using the `=` sign or without it. Quote
    marks and backslashes can be used to escape spaces in the value field.
  prefs: []
  type: TYPE_NORMAL
- en: 'All the following variations are valid:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: COPY
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With `COPY`, we can copy files or directories from our local host (where you
    are executing the Docker build) to our image. This is very useful as you are actually
    moving content to the image, so that you can copy your applications, files, or
    anything that you might need for your container to work. As we previously mentioned,
    any instructions that add actual data to the container will create a new layer,
    therefore, increasing the storage footprint of your final image.
  prefs: []
  type: TYPE_NORMAL
- en: 'This instruction shares the same forms as `RUN`; you can either use JSON formatting
    or just space the `<src>` source separately from the `<dst>` destination:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: There are several catches that we need to go through. First, if any of the filenames
    or directories has a space on its name, you have to use the JSON array format.
  prefs: []
  type: TYPE_NORMAL
- en: 'Second, by default, all files and directories will be copied with **user identifier**
    (**UID**) and **group identifier** (**GID**) `0` (root). To override this, you
    can use the `--chown=<UID>:<GID>` flag as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '`chown` accepts either the numerical ID or the name of the user or group. If
    there is only one of them, then it is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '`COPY` will assume that both the user and the group are the same.'
  prefs: []
  type: TYPE_NORMAL
- en: If you are copying similarly-named files, then you can always use wildcards—`COPY`
    will use the Go `filepath.Match` rule, which can be found at [http://golang.org/pkg/path/filepath#Match](http://golang.org/pkg/path/filepath#Match).
  prefs: []
  type: TYPE_NORMAL
- en: 'How you define the `<src>` and `<dst>` entries is very important because they
    follow these three rules:'
  prefs: []
  type: TYPE_NORMAL
- en: The path that you define in `<src>` must be inside the context of the build,
    essentially, all files and directories that are located in the directory that
    you specified when running the Docker build `PATH` command.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you are copying directories, then always end them with `/`. In this way,
    Docker knows that this is a directory and not a single file that you are copying.
    Additionally, if it's a directory, all of the files inside of it will be copied
    as well.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The path defined in `<dst>` will always have to be an absolute path, unless
    you specify a working directory to be relative to with the `WORKDIR` instruction.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finishing with the `COPY` instruction, I must add that `COPY` only supports
    copying locally-located files. If you want to copy files from a remote server
    using URLs, you must use the `ADD` instruction, which follows the same rules that
    `COPY` does but with some other caveats for URLs. This is beyond the scope of
    this chapter, but you can learn more about it at [https://docs.docker.com](https://docs.docker.com).
  prefs: []
  type: TYPE_NORMAL
- en: EXPOSE
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With the `EXPOSE` keyword, we are not actually publishing the container port
    that we specify here; instead, we are creating a guideline for the container's
    user to know which ports to publish when they start the container.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, this is only metadata that is again created in the image's manifest,
    which can later be retrieved with `docker inspect`. No additional layers are created
    with this keyword.
  prefs: []
  type: TYPE_NORMAL
- en: Ports defined in the `EXPOSE` instruction can be either **user datagram protocol**
    (**UDP**) or **transmission control protocol** (**TCP**), but, by default, TCP
    is assumed if no protocol is specified.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some examples of the `EXPOSE` instruction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: CMD and ENTRYPOINT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: These are probably the most important instructions in a Dockerfile, since they
    tell the container what to run when it's started. We will go through both of them
    and explore how they interact with one another and how they differ from one another.
  prefs: []
  type: TYPE_NORMAL
- en: Let's start with `ENTRYPOINT` first. This instruction, as we mentioned before,
    allows you to define the executable that you want to run when starting the container.
    You can add multiple `ENTRYPOINT` definitions in a Dockerfile, but only the last
    one will be executed on `docker container run`.
  prefs: []
  type: TYPE_NORMAL
- en: When you run a container with the `run` argument, you can usually add command-line
    arguments. These arguments will be appended to the `ENTRYPOINT` parameter unless
    you use the `--entrypoint` flag while using `docker container run` to overwrite
    the `ENTRYPOINT` executable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at some examples. Let''s say that we are using a container with
    the following Dockerfile:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s assume that we built the image and tagged it `entrypointexample`.
    When we run this container without extra command-line arguments, it will appear
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'If we add command-line arguments to the `run` command, we will see something
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, it is not actually executing a BASH shell, but it''s taking
    `/bin/bash` as though it was a string for the `echo` command that we defined in
    our Dockerfile. Let''s consider a more explicit example as, with the previous
    one, I only wanted to demonstrate that even if you pass an actual command or try
    to execute a shell, it will still take it and pass it as arguments for `ENTRYPOINT`.
    Here is a more clear example with a simple string:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, if we pass the `--entrypoint` flag, we will overwrite the `ENTRYPOINT`
    executable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Okay, so why is the formatting of this command this way? As we saw earlier,
    the `--entrypoint` flag only replaces the executable—all additional parameters
    have to be passed as arguments. This is the reason why our `ls` has its `-lath
    /var` arguments at the very end. There are some additional things that we need
    to see here, and they correspond to the forms that the `ENTRYPOINT` instruction
    has.
  prefs: []
  type: TYPE_NORMAL
- en: 'As with the other Dockerfile instruction, `ENTRYPOINT` has two forms, shell
    and exec:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: For the exec form, the same rules that apply to the previous Dockerfile instructions
    apply here as well.
  prefs: []
  type: TYPE_NORMAL
- en: No shell is invoked in exec form, therefore, the `$PATH` variable is not present,
    and you will not be able to use the executables without providing their full path—this
    is why we used `/bin/ls` instead of just `ls`. Also, you can see that you first
    define the executable in the JSON array and then its parameters, this first field
    is what the `--entrypoint` flag will replace. Any additional parameters when using
    the flag will have to be passed to the `docker container run` command arguments
    as we did in our example.
  prefs: []
  type: TYPE_NORMAL
- en: 'Shell form, on the other hand, will load `/bin/sh` so that environment variables
    are available. Let''s take a look at an example; here is a container with the
    following Dockerfile using the exec form:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s assume that we built the image and tagged it `pathexampleexec`. When
    we run the container, we will see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is a container with the following Dockerfile using the shell form:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'When we run the container, we will see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Now, let's say that you want to have some default arguments for your application,
    but you want your user to be able to overwrite and use different arguments if
    they require. This is where `CMD` comes in; with `CMD`, you can specify default
    parameters for your executable, but they will be overwritten if a user runs the
    container with command arguments on `docker container run`. You have to be careful
    of how you declare `ENTRYPOINT`, because if `ENTRYPOINT` is declared using the
    shell form, all `CMD` definitions will be ignored.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at a couple of examples; the following is a Dockerfile of
    the container to run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the running of the previously mentioned container, assuming that it
    was built and tagged as `cmdexample`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, if we use the exec form for `ENTRYPOINT`, the CMD parameters will be appended
    to the `ENTRYPOINT`. Dockerfile for reference:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output, assuming that the image was built and tagged as `execcmdexample`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Notice that this time the `CMD` entries were appended to `ENTRYPOINT` as arguments.
    However, remember that the contents of `CMD` are just defaults; if we specify
    the arguments on `docker container run`, these will overwrite those in `CMD`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the same Dockerfile as the preceding example, we will something similar
    to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'There are several combinations between `CMD` and `ENTRYPOINT`, and you can
    see all of them in the following chart taken from [https://docs.docker.com](https://docs.docker.com):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/78bc3880-5c3b-4744-90a5-727d6a982c94.png)'
  prefs: []
  type: TYPE_IMG
- en: Building container images using best practices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Dockerfiles are like recipes for your applications, but you can't just throw
    in the ingredients and hope for the best. Creating an efficient image requires
    you to be careful about how you utilize the tools at your disposal.
  prefs: []
  type: TYPE_NORMAL
- en: The whole point of containers is to have a small footprint—having a 1 GB+ image
    for a 100 MB application is not indicative of a small footprint, nor is it efficient
    at all. Microservices are all about this as well; having small container images
    for your microservices not only improves performance, but storage utilization
    decreases security vulnerabilities and points of failure, and it also saves you
    money.
  prefs: []
  type: TYPE_NORMAL
- en: Container images are stored locally in your host and remotely in a container
    registry. Public cloud providers charge you for the storage utilization of your
    registry and not by the image quantity that you have stored there. Think of a
    registry as the GitHub of containers. Let's say that you have to pull an image
    from your cloud provider's registry; which image do you think it will be faster
    to pull? A 1 GB image or a 100 MB image? The image size is essential.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing to consider when building an image is the base image that you
    are going to use. Instead of using large images (such as full Linux distributions,
    Ubuntu, Debian, or CentOS) that have a lot of tools and executables that you will
    not need for your application to run, use smaller ones such as Alpine:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **REPOSITORY ** | **SIZE** |'
  prefs: []
  type: TYPE_TB
- en: '| `centos` | 200 MB |'
  prefs: []
  type: TYPE_TB
- en: '| `ubuntu` | 83.5 MB |'
  prefs: []
  type: TYPE_TB
- en: '| `debian` | 101 MB |'
  prefs: []
  type: TYPE_TB
- en: '| `alpine ` | 4.41 MB |'
  prefs: []
  type: TYPE_TB
- en: 'You will find that most of the images have a slimmer version of themselves,
    for example, `httpd` and `nginx`:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **REPOSITORY** | **TAG** | **SIZE** |'
  prefs: []
  type: TYPE_TB
- en: '| `httpd` | `alpine` | 91.4 MB |'
  prefs: []
  type: TYPE_TB
- en: '| `httpd` | `latest` | 178 MB |'
  prefs: []
  type: TYPE_TB
- en: '| `nginx` | `alpine` | 18.6 MB |'
  prefs: []
  type: TYPE_TB
- en: '| `nginx` | `latest` | 109 MB |'
  prefs: []
  type: TYPE_TB
- en: As you can see, `httpd`:`alpine` is almost 50% smaller than `httpd`:`latest`,
    while `nginx`:`alpine` is 80% smaller!
  prefs: []
  type: TYPE_NORMAL
- en: Smaller images will not only reduce your storage consumption, but they will
    also reduce your attack surface. This is because a smaller container has a lower
    attack surface; let's take a look at the latest Ubuntu image versus the latest
    Alpine.
  prefs: []
  type: TYPE_NORMAL
- en: 'For Ubuntu, we can see an increased count for vulnerabilities as per the Docker
    Hub page for the latest tag; this is captured in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b425f5cd-5234-44ea-9d34-9838f950efa0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For Alpine Linux, the count goes down to zero, as demonstrated in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b3203d39-5afb-4e50-907f-5fda932e443a.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding screenshot, we can see the vulnerability count when compared
    to Ubuntu. Even today, the latest Alpine image has no vulnerabilities whatsoever.
    In comparison, Ubuntu has seven vulnerable components that are not even needed
    for our application to run.
  prefs: []
  type: TYPE_NORMAL
- en: Another thing to take into account is the layering of your image; each time
    you run a `RUN` statement in the build it will add one more layer and size to
    your final image. Reducing the number of `RUN` statements and what you run on
    them will dramatically decrease your image size.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take our first Dockerfile, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'We can modify the `RUN` instruction into the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Now instead of creating three layers, we will be producing only one, by running
    all our commands in a single statement.
  prefs: []
  type: TYPE_NORMAL
- en: Remember that everything you do in `RUN` is executed with `/bin/sh -c` or any
    other shell that you specified with `SHELL`, so `&`, `;`, and `\` are accepted
    as they would be in a regular shell.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, we didn''t only remove the extra `RUN` instructions; we also added
    `apt clean` to clean the cache of our container before it commits, and used the
    `--no-install-recommend` flag to avoid installing any unnecessary packages, thus
    reducing both storage space and the attack surface:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the details of the original image:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **REPOSITORY ** | **SIZE** |'
  prefs: []
  type: TYPE_TB
- en: '| `bigimage` | 221 MB |'
  prefs: []
  type: TYPE_TB
- en: 'Here are the details of the smaller image:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **REPOSITORY ** | **SIZE** |'
  prefs: []
  type: TYPE_TB
- en: '| `smallerimage` | 214 MB |'
  prefs: []
  type: TYPE_TB
- en: Of course, this is not a huge difference, but this is only an example and no
    real application was being installed. In a production image, you will have to
    do more than just install `apache2`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s use both of the techniques that we have learned and slim our image
    down:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the final size of the image:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **REPOSITORY ** | **SIZE** |'
  prefs: []
  type: TYPE_TB
- en: '| `finalimage` | 5.79 MB |'
  prefs: []
  type: TYPE_TB
- en: Now, you can see there is a great difference in sizes—we passed from 221 MB
    to 217 MB, and finally ended up with a 5.79-MB image! Both images did the exact
    same thing, which was to serve a web page, but with an entirely different footprint.
  prefs: []
  type: TYPE_NORMAL
- en: Container orchestration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we know how to create our images, we need a way to maintain the desired
    state of our applications. Here''s where container orchestrators come in. Container
    orchestrators answer questions such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: How do I maintain my applications so that they are highly available?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do I scale each microservice on demand?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do I load balance my application across multiple hosts?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do I limit my application's resource consumption on my hosts?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do I easily deploy multiple services?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With container orchestrators, administrating your containers has never been
    as easy or efficient as it is now. There are several orchestrators available,
    but the most widely used are Docker Swarm and Kubernetes. We will discuss Kubernetes
    later on in this chapter and take a more in-depth look at it in the [Chapter 7](d89f650b-f4ea-4cda-9111-a6e6fa6c2256.xhtml),
    *Understanding the Core Components of a Kubernetes Cluster*.
  prefs: []
  type: TYPE_NORMAL
- en: What all orchestrators have in common is that their basic architecture is a
    cluster that is composed of some master nodes watching for your desired state,
    which will be saved in a database. Masters will then start or stop your containers
    depending on the state of the worker nodes that are in charge of the container
    workloads. Each master node will also be in charge of dictating which container
    has to run on which node, based on your predefined requirements, and to scale
    or restart any failed instances.
  prefs: []
  type: TYPE_NORMAL
- en: However, orchestrators not only provide high availability by restarting and
    bringing up containers on demand, both Kubernetes and Docker Swarm also have mechanisms
    to control traffic to the backend containers, in order to provide load balancing
    for incoming requests to your application services.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram demonstrates the traffic going to an orchestrated cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e5135807-8184-484f-b09d-84618971f17e.png)'
  prefs: []
  type: TYPE_IMG
- en: Let's explore Kubernetes a little bit more.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes is by far the most popular container orchestrator out there. Many
    public cloud providers are now adopting it as the de facto container orchestrator;
    for instance, Azure with its **Azure Kubernetes Services** (**AKS**), Amazon Web
    Services with **elastic container service for Kubernetes** (**EKS**), and Google
    Cloud with **Google Kubernetes Engine** (**GKE**). Most of these solutions are
    managed, abstracting the management plane for the user for ease of use, and adopting
    cloud-native solutions such as integration with public cloud load balancers and
    DNS services.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes sits in the middle of a **platform as a service** (**PaaS**) solution
    and an **infrastructure as a service** (**IaaS**) solution because it provides
    you with a platform to run your containers and manage your data, but it still
    lets you provision software-defined infrastructures such as load balancers, network
    management, ingress controls, and resource allocation.
  prefs: []
  type: TYPE_NORMAL
- en: With Kubernetes, we can automate the process of deploying our containers and
    maintaining our desired state while controlling the resource consumption of our
    applications and providing high availability and isolation across our different
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes has the basic orchestrator components that we mentioned before; it
    has worker nodes, master nodes, and a database that saves the status of our cluster.
    We will start exploring Kubernetes concepts in depth in [Chapter 7](d89f650b-f4ea-4cda-9111-a6e6fa6c2256.xhtml), *Understanding
    the Core Components of a Kubernetes Cluster*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows the basic architecture of Kubernetes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bbdec0c7-d304-4b05-8631-31fa286aa2f1.png)'
  prefs: []
  type: TYPE_IMG
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed how IT is evolving from a monolithic design to
    microservices, and how containers are helping us achieve this type of architecture
    by allowing a modularized infrastructure. We used the example of an online store
    to demonstrate how microservices allow for the scalability of specific components
    without the need to bring down the entire application. Additionally, we explored
    how the same example has a highly available design by discussing how the microservices
    approach allows for just a portion of the application to fail without impacting
    the entire solution (that is, how only the reviews part failed without bringing
    down the entire online store).
  prefs: []
  type: TYPE_NORMAL
- en: Later, we learned how containers are created from images through the use of
    a Dockerfile, which uses a readable set of instructions to create the base image.
    An image can be seen as the counterpart of a template in the context of VMs.
  prefs: []
  type: TYPE_NORMAL
- en: From this Dockerfile, we learned that a `FROM` statement indicates what will
    be the initial image, how the `LABEL` instruction adds metadata to the container,
    how `RUN` executes the commands that you need to prepare your container to run
    your application, and how `ENV` sets variables for the environment used for container
    building.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, we discussed some of the best practices when building container
    images, such as the use of smaller images (such as Alpine), and how choosing a
    smaller image helps to reduce the number of vulnerabilities present in the built
    containers.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we quickly glanced over some of the more popular orchestration tools
    that are available, these being Docker Swarm and Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will jump into exploring the core components of a Kubernetes
    cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What are the components of Kubernetes?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the difference between GKE, EKS, and AKS?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How secure are containers from exploits?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How easy is to deploy an application in a container?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Are Docker containers and Kubernetes exclusive to Linux?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Mastering Kubernetes* by Gigi Sayfan: [https://www.packtpub.com/virtualization-and-cloud/mastering-kubernetes](https://www.packtpub.com/virtualization-and-cloud/mastering-kubernetes)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Kubernetes for Developers* by Joseph Heck: [https://www.packtpub.com/virtualization-and-cloud/kubernetes-developers](https://www.packtpub.com/virtualization-and-cloud/kubernetes-developers)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Hands-On Microservices with Kubernetes* by Gigi Sayfan: [https://www.packtpub.com/virtualization-and-cloud/hands-microservices-kubernetes](https://www.packtpub.com/virtualization-and-cloud/hands-microservices-kubernetes)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Getting Started with Kubernetes – Third Edition* by Jonathan Baier, Jesse
    White: [https://www.packtpub.com/virtualization-and-cloud/getting-started-kubernetes-third-edition](https://www.packtpub.com/virtualization-and-cloud/getting-started-kubernetes-third-edition)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Mastering Docker - Second Edition* by Russ McKendrick, Scott Gallagher: [https://www.packtpub.com/virtualization-and-cloud/mastering-docker-second-edition](https://www.packtpub.com/virtualization-and-cloud/mastering-docker-second-edition)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Docker Bootcamp* by Russ McKendrick et al: [https://www.packtpub.com/virtualization-and-cloud/docker-bootcamp](https://www.packtpub.com/virtualization-and-cloud/docker-bootcamp)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bibliography/sources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'What are microservices?: [http://microservices.io/](http://microservices.io/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Docker Hub: [https://hub.docker.com/](https://hub.docker.com/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Production-Grade Container Orchestration: [http://kubernetes.io/](http://kubernetes.io/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
