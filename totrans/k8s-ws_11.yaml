- en: 11\. Build Your Own HA Cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Overview
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will learn how Kubernetes enables us to deploy infrastructure
    with remarkable resilience and how to set up a high-availability Kubernetes cluster
    in the AWS cloud. This chapter will help you understand what enables Kubernetes
    to be used for highly available deployments and, in turn, enable you to make the
    right choices while architecting a production environment for your use case. By
    the end of the chapter, you will be able to set up a suitable cluster infrastructure
    on AWS to support your **highly available** (**HA**) Kubernetes cluster. You will
    also be able to deploy an application in a production environment.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapters, you learned about application containerization, how
    Kubernetes works, and some of the "proper nouns" or "objects" in Kubernetes that
    allow you to create a declarative-style application architecture that Kubernetes
    will execute on your behalf.
  prefs: []
  type: TYPE_NORMAL
- en: Software and hardware instability are a reality in all environments. As applications
    need higher and higher availability, shortcomings in the infrastructure become
    more obvious. Kubernetes was purpose-built to help solve this challenge for containerized
    applications. But what about Kubernetes itself? As cluster operators, do we shift
    from watching our individual servers like hawks to watching our single Kubernetes
    control infrastructure?
  prefs: []
  type: TYPE_NORMAL
- en: 'As it turns out, this aspect was one of the design considerations for Kubernetes.
    One of the design goals of Kubernetes is to be able to withstand instability in
    its own infrastructure. This means that when set up properly, the Kubernetes control
    plane could withstand quite a few disasters, including:'
  prefs: []
  type: TYPE_NORMAL
- en: Network splits/partitions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Control plane (master) server failure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data corruption in etcd
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Many other less severe events that impact availability events
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Not only can Kubernetes help your application tolerate failure, but you can
    rest easy at night knowing that Kubernetes can also tolerate failures in its own
    control infrastructure. In this chapter, we are going to build a cluster of our
    very own and make sure that it is highly available. High availability implies
    that the system is very reliable and almost always available. This does not mean
    that everything in it always works perfectly; it just means that whenever the
    user or client wants something, the architecture stipulates that the API server
    should be **available** to do the job. This means that we have to design a system
    for our applications to automatically respond to and take corrective measures
    in response to any faults.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will look at how Kubernetes integrates such measures to
    tolerate faults in its own control architecture. Then, you will have the chance
    to extend this concept a bit further by designing your application to take advantage
    of this horizontally scalable, fault-tolerant architecture. But first, let's look
    at how the different cogs in the machine turn together to enable it to be highly
    available.
  prefs: []
  type: TYPE_NORMAL
- en: How the Components of Kubernetes Work Together to Achieve High Availability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You have learned in *Chapter 2*, *An Overview of Kubernetes*, how the pieces
    of Kubernetes work together to provide a runtime for your application containers.
    But we need to investigate deeper how these components work together to achieve
    high availability. To do that, we'll start with the memory bank of Kubernetes,
    otherwise known as etcd.
  prefs: []
  type: TYPE_NORMAL
- en: etcd
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As you have learned in earlier chapters, etcd is the place where all Kubernetes
    configuration is stored. This makes it arguably the single most important component
    of the cluster since changes in etcd affect the state of everything. More specifically,
    any change to a key-value pair in etcd will cause the other components of Kubernetes
    to react to this change, which could mean disruptions to your application. In
    order to achieve high availability for Kubernetes, it is wise to have more than
    one etcd node.
  prefs: []
  type: TYPE_NORMAL
- en: But many more challenges arise when you add multiple nodes to an eventually
    consistent datastore like etcd. Do you have to write to every node to persist
    a change of state? How does replication work? Do we read from just one node or
    as many as are available? How does it handle networking failures and partitions?
    Who is the master of the cluster and how does leader election work? The short
    answer is that, by design, etcd makes these challenges either non-existent or
    easy to deal with. etcd uses a consensus algorithm called **Raft** to achieve
    replication and fault tolerance in relation to many of the aforementioned issues.
    Thus, if we're building a Kubernetes HA cluster, we need to make sure that we
    set up multiple nodes (preferably an odd number to make leader election tie-breaking
    easier) of an etcd cluster properly, and we can rely on that from there.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'Leader election in etcd is a process where multiple instances of the database
    software collectively vote on which host will be an authority for dealing with
    any issues that arise in achieving database consensus. For more details, refer
    to this link: [https://raft.github.io/](https://raft.github.io/)'
  prefs: []
  type: TYPE_NORMAL
- en: Networking and DNS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Many of the applications that run on Kubernetes require some form of network
    to be useful. Therefore, networking is an important consideration when designing
    a topology for your clusters. For example, your network should be able to support
    all of the protocols that your application uses, including the ones for Kubernetes.
    Kubernetes itself uses TCP for all of its communication between masters, nodes,
    and etcd, and it uses UDP for internal domain name resolution, which is otherwise
    known as service discovery. Your network should also be provisioned to have at
    least as many IP addresses as the number of nodes that you plan to have in the
    cluster. For example, if you planned to have more than 256 machines (nodes) in
    your cluster, you probably shouldn't use an IP CIDR address space of /24 or higher
    since that only has 255 or fewer available IP addresses.
  prefs: []
  type: TYPE_NORMAL
- en: Later in this workshop, we will talk about the security decisions you will need
    to make as a cluster operator. However, in this section, we will not discuss them
    because they do not directly relate to Kubernetes' ability to achieve high availability.
    We will deal with the security of Kubernetes in *Chapter 13*, *Runtime and Network
    Security in Kubernetes*.
  prefs: []
  type: TYPE_NORMAL
- en: One final thing to take into consideration about the network where your master
    and worker nodes will run is that every master node should be able to communicate
    with every worker node. The reason this is important is that each master node
    communicates with the Kubelet process running on the worker node in order to determine
    the state of the full cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Nodes' and Master Servers' Locations and Resources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Because of the design of etcd's Raft algorithm, which allows distributed consensus
    to happen in the key-value store of Kubernetes, we are able to run multiple master
    nodes, each of which is capable of controlling the entire cluster without the
    fear of them behaving independently from each other (in other words, going rogue).
    As a reminder of why master nodes being out of sync is a problem in Kubernetes,
    consider that the runtime of your application is being controlled by commands
    that Kubernetes issues on your behalf. If those commands conflict with each other
    because of state sync problems between master nodes, then your application runtime
    will suffer as a result. By introducing multiple master nodes, we again provide
    resistance to faults and network partitions that could potentially sacrifice the
    availability of the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes is actually able to run in a "headless" mode. This means whatever
    instructions the Kubelets (worker nodes) have last received from the master nodes
    will continue to be carried out until communication with the master nodes can
    be re-established. In theory, this means an application that was deployed on Kubernetes
    could run indefinitely, even if the entire control plane (all master nodes) went
    down and nothing else changed on the worker nodes where the Pods running the application
    were scheduled. Obviously, this is a worst-case scenario for the availability
    of a cluster, but it is reassuring to know that, even in the worst case, applications
    don't necessarily have to suffer downtime.
  prefs: []
  type: TYPE_NORMAL
- en: 'When you are planning the design and capacity for a high-availability deployment
    of Kubernetes, it is important to know a few things about the design of your network,
    which we discussed previously. For example, if you are running a cluster in a
    popular cloud provider, they likely have a concept of "availability zones". A
    similar concept for data center environments would be physically isolated data
    centers. If possible, there should be at least one master node and multiple worker
    nodes per availability zone. This is important because, in the event of an availability
    zone (data center) outage, your cluster is still able to operate within the remaining
    availability zones. This is illustrated in the following diagrams:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.1: The cluster before the outage of an availability zone'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B14870_11_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 11.1: The cluster before the outage of an availability zone'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s assume that there is a total outage of Availability Zone – C, or at
    least we are no longer able to communicate with any servers that are running inside
    it. Here is how the cluster now behaves:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.2: The cluster following the outage of an availability zone'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B14870_11_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 11.2: The cluster following the outage of an availability zone'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in the diagram, Kubernetes can still execute. Additionally, if
    the loss of the nodes running in Availability Zone - C causes an application to
    no longer be in its desired state, which is dictated by the application's Kubernetes
    manifest, the remaining master nodes will work to schedule the interrupted workload
    on the remaining worker nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'Depending on the number of worker nodes in your Kubernetes cluster, you may
    have to plan for additional resource constraints because of the amount of CPU
    power needed to run a master connected to several worker nodes. You can use the
    chart at this link to determine the resource requirements of the master nodes
    you should deploy for controlling your cluster: [https://kubernetes.io/docs/setup/best-practices/cluster-large/](https://kubernetes.io/docs/setup/best-practices/cluster-large/)'
  prefs: []
  type: TYPE_NORMAL
- en: Container Network Interface and Cluster DNS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The next decision you need to make with respect to your cluster is how the containers
    themselves communicate across each of the nodes. Kubernetes itself has a container
    network interface called **kubenet**, which is what we will use in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: For smaller deployments and simple operations, kubenet more than exceeds the
    needs of those clusters from a **Container Network Interface** (**CNI**) perspective.
    However, it does not work for every workload and network topology. So, Kubernetes
    provides support for several different CNIs. When considering container network
    interfaces from a high-availability perspective, you will want the most performant
    and stable option possible. It is beyond the scope of this introduction to Kubernetes
    to discuss each of the CNI offerings at length.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'If you plan to use a managed Kubernetes service provider or plan to have a
    more complex network topology such as multiple subnets inside a single VPC, kubenet
    will not work for you. In this case, you will have to pick one of the more advanced
    options. More information on selecting the right CNI for your environment can
    be found here: [https://chrislovecnm.com/kubernetes/cni/choosing-a-cni-provider/](https://chrislovecnm.com/kubernetes/cni/choosing-a-cni-provider/)'
  prefs: []
  type: TYPE_NORMAL
- en: Container Runtime Interfaces
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the final decisions you will have to make is how your containers will
    run on your worker nodes. The Kubernetes default for this is the Docker container
    runtime interface, and Kubernetes was initially built to work with Docker. Since
    then, however, open standards have been developed and other container runtime
    interfaces are now compatible with the Kubernetes API. Generally, cluster operators
    tend to stick with Docker because it is extremely well established. Even if you
    want to explore alternatives, keep in mind when designing a topology capable of
    maintaining high availability for your workloads and Kubernetes that you'll probably
    want to go with more established and stable options like Docker.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find some of the other container runtime interfaces that are compatible
    with Kubernetes on this page: [https://kubernetes.io/docs/setup/production-environment/container-runtimes/](https://kubernetes.io/docs/setup/production-environment/container-runtimes/)'
  prefs: []
  type: TYPE_NORMAL
- en: Container Storage Interfaces
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Recent versions of Kubernetes have introduced improved ways of interacting
    with the persistence tools that are available in data centers and cloud providers
    such as storage arrays and blob storage. The most important improvement has been
    the introduction and standardization of the container storage interface for managing
    `StorageClass`, `PersistentVolume`, and `PersistentVolumeClaim` in Kubernetes.
    The consideration for highly available clusters you will need to make with regard
    to storage is more specific per application. For example, if your application
    makes use of Amazon EBS volumes, which must reside within an availability zone,
    then you will have to ensure appropriate redundancy is available in your worker
    nodes so that the Pod that depends on that volume can be rescheduled in the event
    of an outage. More information on CSI drivers and implementations can be found
    here: [https://kubernetes-csi.github.io/docs/](https://kubernetes-csi.github.io/docs/)'
  prefs: []
  type: TYPE_NORMAL
- en: Building a High-Availability Focused Kubernetes Cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hopefully, by reading the previous section, you're starting to realize that
    Kubernetes is less magical than it may seem when you first approached the topic.
    It is an extremely powerful tool on its own, but Kubernetes really shines when
    we take full advantage of its capability of running in a highly available configuration.
    So now we're going to see how to implement it and actually build a cluster using
    a cluster life cycle management tool. But before we do that, we need to know the
    different ways that we can deploy and manage a Kubernetes cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Self-Managed versus Vendor-Managed Kubernetes Solutions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Amazon Web Services, Google Cloud Platform, Microsoft Azure, and practically
    every other major cloud services provider has a managed Kubernetes offering. So,
    when you are deciding how you are going to build and run your cluster, you should
    consider some of the different managed providers and their strategic offerings
    to see whether or not they align with your business needs and goals. For example,
    if you use Amazon Web Services, then Amazon EKS might be a viable solution for
    you.
  prefs: []
  type: TYPE_NORMAL
- en: There are trade-offs with choosing a managed service provider over an open-source
    and self-managed solution. For example, a lot of the hard work of cluster assembly
    is done for you, but you forfeit a great deal of control in the process. So, you
    need to decide how much value you place on being able to control the Kubernetes
    master plane and whether or not you would like to be able to pick your container
    networking interface or container runtime interface. For the purposes of this
    tutorial, we are going to use an open-source solution because it can be deployed
    anywhere, and it also helps us understand how Kubernetes works and how it is supposed
    to be configured.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'Please ensure that you have an AWS account and are able to access it using
    the AWS CLI: [https://aws.amazon.com/cli](https://aws.amazon.com/cli).'
  prefs: []
  type: TYPE_NORMAL
- en: If you are unable to access it, then please follow the instructions at the preceding
    link.
  prefs: []
  type: TYPE_NORMAL
- en: Assuming for now that we want more control over our cluster and are comfortable
    with managing it by ourselves, let's look at some open-source tools that can be
    used for setting up a cluster.
  prefs: []
  type: TYPE_NORMAL
- en: kops
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will use one of the more popular open-source installation tools to do this
    called **kops**, which stands for **Kubernetes Operations**. It is a complete
    cluster life cycle management tool and has a very easy API to understand. As a
    part of the cluster creation/updating process, kops can generate Terraform configuration
    files so you can run the infrastructure upgrade process as part of your own pipeline.
    It also has good tooling to support the upgrade path between versions of Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Terraform is an infrastructure life cycle management tool that we will briefly
    learn about in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Some of the drawbacks of kops are that it tends to be about two versions of
    Kubernetes behind, it has not always been able to respond to vulnerability announcements
    as fast as other tools, and it is currently limited to creating clusters in AWS,
    GCP, and OpenStack.
  prefs: []
  type: TYPE_NORMAL
- en: 'The reason we have decided to use kops for our cluster life cycle management
    in this chapter is four-fold:'
  prefs: []
  type: TYPE_NORMAL
- en: We wanted to select a tool that would abstract away some of the more confusing
    bits of the Kubernetes setup as we ease you into cluster administration.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It supports more cloud platforms than just AWS, so you don't have to be locked
    into Amazon if you choose not to be.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It supports a broad array of customizations to the Kubernetes infrastructure,
    such as choosing CNI providers, deciding on a VPC network topology, and node instance
    group customizations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It has first-class support for zero-downtime cluster version upgrades and handles
    the process automatically.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Other Commonly Used Tools
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Besides kops, there are several other tools that can be used to set up a Kubernetes
    cluster. You can find the full list at this link: [https://kubernetes.io/docs/setup/#production-environment](https://kubernetes.io/docs/setup/#production-environment).'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will mention a couple of them here so you get an idea of what''s available:'
  prefs: []
  type: TYPE_NORMAL
- en: '**kubeadm**: This is generated from the Kubernetes source code and is the tool
    that will allow the greatest level of control over each component of Kubernetes.
    It can be deployed in any environment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using kubeadm requires an expert level knowledge of Kubernetes to be useful.
    It gives cluster administrators little room for error, and it is complicated to
    upgrade a cluster using kubeadm.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kubespray**: This uses Ansible/Vagrant-style configuration management, which
    is familiar to many IT professionals. It is better for environments where the
    infrastructure is more static rather than dynamic (such as the cloud). Kubespray
    is very composable and configurable from a tooling perspective. It also allows
    the deployment of a cluster on bare-metal servers. The key to watch out for here
    is coordinating software upgrades of cluster components and hardware and operating
    systems. Since you are providing much of the functionality a cloud provider does,
    you have to make sure your upgrade processes won''t break the applications running
    on top of the cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Because Kubespray uses Ansible for provisioning, you are restricted by the
    underlying limitations of Ansible for provisioning large clusters and keeping
    them in spec. Currently, Kubespray is limited to the following environments: AWS,
    GCP, Azure, OpenStack, vSphere, Packet, Oracle Cloud Infrastructure, or your own
    bare-metal installations.'
  prefs: []
  type: TYPE_NORMAL
- en: Authentication and Identity in Kubernetes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Kubernetes uses two concepts for authentication: ServiceAccounts are meant
    to identify processes running inside Pods, and User Accounts are meant to identify
    human users. We will take a look at ServiceAccounts in a later topic in this chapter,
    but first, let''s understand User Accounts.'
  prefs: []
  type: TYPE_NORMAL
- en: From the very beginning, Kubernetes has tried to remain incredibly agnostic
    to any form of authentication and identity for user accounts, because most companies
    have a very specific way of authenticating users. Some use Microsoft Active Directory
    and Kerberos, some may use Unix passwords and UGW permission sets, and some may
    use a cloud provider or software as a service-based IAM solution. In addition,
    there are a number of different authentication strategies that may be used by
    an organization.
  prefs: []
  type: TYPE_NORMAL
- en: Because of this, Kubernetes does not have built-in identity management or a
    required single way of authenticating those identities. Instead, it has a concept
    of authentication "strategies." A strategy is essentially a way for Kubernetes
    to delegate the verification of identity to another system or method.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will be using x509 certificate-based authentication. X509
    certificate authentication essentially makes use of the Kubernetes Certificate
    Authority and common names/organization names. Since Kubernetes RBAC rules use
    `usernames` and `group names` to map authenticated identities to permission sets,
    x509 `common names` become the `usernames` of Kubernetes, and `organization names`
    become the `group names` in Kubernetes. kops automatically provisions x509-based
    authentication certificates for you so there is little to worry about; but when
    it comes to adding your own users, you will want to be aware of this.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes RBAC stands for Role-Based Access Control, which allows us to allow
    or deny certain access to our users based on their roles. This will be covered
    in more depth in *Chapter 13*, *Runtime and Network Security in Kubernetes*.
  prefs: []
  type: TYPE_NORMAL
- en: An interesting feature of kops is that you can use it in a similar way to manage
    cluster resources as you would use kubectl to manage cluster resources. kops handles
    a node similar to how Kubernetes would handle a Pod. Just as Kubernetes has a
    resource called "Deployment" to manage a bunch of Pods, kops has a resource called
    **InstanceGroup** (which can also be referred to by its short form, `ig`) to manage
    a bunch of nodes. In the case of AWS, a kops InstanceGroup effectively creates
    an AWS EC2 Autoscaling group.
  prefs: []
  type: TYPE_NORMAL
- en: Extending this comparison, `kops get instancegroups` or `kops get ig` is analogous
    to `kubectl get deployments`, and `kops edit` works similarly to `kubectl edit`.
    We will make use of this feature in the activity later in the chapter, but first,
    let's get our basic HA cluster infrastructure up and running in the following exercise.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, the commands have been run using the Zsh shell. However, they
    are completely compatible with Bash.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 11.01: Setting up Our Kubernetes Cluster'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'This exercise will exceed the free tier of AWS that is normally given to new
    account holders for the first 12 months. Pricing information on EC2 can be found
    here: [https://aws.amazon.com/ec2/pricing/](https://aws.amazon.com/ec2/pricing/)'
  prefs: []
  type: TYPE_NORMAL
- en: Also, you should remember to delete your instances at the end of the chapter
    to stop being billed for your consumed AWS resources.
  prefs: []
  type: TYPE_NORMAL
- en: In this exercise, we will prepare our infrastructure for running a Kubernetes
    cluster on AWS. There's nothing particularly special about the choice of AWS;
    Kubernetes is platform-agnostic, though it already has code that allows it to
    integrate with native AWS services (EBS, EC2, and IAM) on behalf of cluster operators.
    This is also true for Azure, GCP, IBM Cloud, and many other cloud platforms.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will set up a cluster with the following specifications:'
  prefs: []
  type: TYPE_NORMAL
- en: Three master nodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Three etcd nodes (to keep things simple, we will run these on the master nodes)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Two worker nodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At least two availability zones
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Once we have our cluster set up, we will deploy an application on it in the
    next exercise. Now follow these steps to complete this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Ensure that you have installed kops as per the instructions in the *Preface*.
    Verify that kops is properly installed and configured using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see the following response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Now before we move on to the following steps, we need to do some setup in AWS.
    Most of the following settings are configurable, but we will be making a few decisions
    for you for the sake of convenience.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will set up an AWS IAM user that kops will use to provision your
    infrastructure. Run the following commands one after the other in your terminal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see output similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.3: Setting up an IAM user for kops'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B14870_11_03.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 11.3: Setting up an IAM user for kops'
  prefs: []
  type: TYPE_NORMAL
- en: Note the highlighted `AccessKeyID` and `SecretAccessKey` fields you will receive
    for your output. This is sensitive information, and the keys in the preceding
    screenshot will, of course, be invalidated by the author. We will need the highlighted
    information for our next step.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we need to export the created credentials for kops as environment variables
    for our terminal session. Use the highlighted information from the screenshot
    in the previous step:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we need to create an S3 bucket for kops to store its state. To create
    a random bucket name, run the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The second command outputs the name of the S3 bucket created, and you should
    see a response similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the following command to create the required bucket using the AWS CLI:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we are using the `us-west-2` region. You can use a region closer to you
    if you want. You should see the following response for a successful bucket creation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have our S3 bucket, we can begin to set our cluster up. There are
    numerous options we can choose, but right now we're going to work with the defaults.
  prefs: []
  type: TYPE_NORMAL
- en: 'Export the name of your cluster and the S3 bucket that kops will use to store
    its state:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Generate all the config and store it in the S3 bucket from earlier to create
    a Kubernetes cluster using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: By passing the `--zones` argument, we are specifying the availability zones
    we want our cluster to span, and by specifying the `master-count=3` parameter,
    we are effectively saying we want to use a highly available Kubernetes cluster.
    By default, kops will create two worker nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Note that this did not actually create the cluster, but it created a pre-flight
    set of checks so we can create a cluster in just a moment. It is informing us
    that in order to access our AWS instances, we need to provide a public key – the
    default search location is `~/.ssh/id_rsa.pub`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we need to create an SSH key to be added to all of the master and worker
    nodes so we can log in to them with SSH. Use the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The type of secret (`sshpublickey`) is a special keyword reserved to kops for
    this operation. More information can be found at this link: [https://github.com/kubernetes/kops/blob/master/docs/cli/kops_create_secret_sshpublickey.md](https://github.com/kubernetes/kops/blob/master/docs/cli/kops_create_secret_sshpublickey.md).'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The key being specified here at `~/.ssh/id_rsa.pub` will be the key that kops
    is going to distribute to all master and worker nodes and can be used for SSH
    from your local computer to the running server for diagnostic or maintenance purposes.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can use the following command to use the key to log in with an admin account:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: While this is not required for this exercise, you will find this useful for
    a later chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'To view our configuration, let''s run the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'This will open your text editor with the definition of our cluster, as shown
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.4: Examining the definition of our cluster'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B14870_11_04.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 11.4: Examining the definition of our cluster'
  prefs: []
  type: TYPE_NORMAL
- en: 'We have truncated this screenshot for brevity. At this point, you can make
    any edits, though, for this exercise, we will proceed without making any changes.
    We will keep the description of this spec out of the scope of this workshop for
    brevity. If you want more details about the various elements in the `clusterSpec`
    of kops, you can find more details here: [https://github.com/kubernetes/kops/blob/master/docs/cluster_spec.md](https://github.com/kubernetes/kops/blob/master/docs/cluster_spec.md).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, take the configuration we generated and stored in S3 and actually run
    commands to reconcile the AWS infrastructure with what we said we wanted it to
    be in our config files:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: All commands in kops are dry-run (nothing will actually happen except some validation
    steps) by default unless you specify the `--yes` flag. This is a protectionary
    measure, so you don't accidentally do something harmful to your cluster in production.
  prefs: []
  type: TYPE_NORMAL
- en: 'This will take a long time, but after it''s done, we''ll have a working Kubernetes
    HA cluster. You should see the following response:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.5: Updating the cluster to match the generated definition'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B14870_11_05.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 11.5: Updating the cluster to match the generated definition'
  prefs: []
  type: TYPE_NORMAL
- en: 'To validate that our cluster is running, let''s run the following command.
    This may take up to 5-10 minutes to fully work:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see the following response:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.6: Validating our cluster'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B14870_11_06.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 11.6: Validating our cluster'
  prefs: []
  type: TYPE_NORMAL
- en: From this screenshot, we can see we have three Kubernetes master nodes running
    in separate availability zones, and two worker nodes spread across two of the
    three availability zones (making this a highly available cluster). Also, all of
    the nodes as well as the cluster appear to be healthy.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'Remember your cluster resources are still running. If you plan to proceed to
    the next exercise after a significant amount of time, you may want to delete this
    cluster to stop the billing for the AWS resources. To delete this cluster, you
    can use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '`kops delete cluster --name ${NAME} --yes`'
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes Service Accounts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we learned earlier, a Kubernetes ServiceAccount object serves as an identification
    marker for a process inside a Pod. While Kubernetes does not manage and authenticate
    the identity of human users, it does manage and authenticate ServiceAccount objects.
    And then, similar to users, you can allow role-based access to Kubernetes resources
    for ServiceAccount.
  prefs: []
  type: TYPE_NORMAL
- en: ServiceAccount acts as a way of authenticating to the cluster using **JSON Web
    Token** (**JWT**) style, header-based authentication. Every ServiceAccount is
    paired with a token stored in a secret that is created by the Kubernetes API and
    then mounted into the Pod associated with that ServiceAccount. Whenever any process
    in the Pod needs to make an API request, it passes the token along with it to
    the API server, and Kubernetes maps that request to the ServiceAccount. Based
    on that identity, Kubernetes can then determine the level of access to the resources/objects
    (authorization) that a process should be granted. Typically, service accounts
    are given to Pods inside the cluster as they are intended only to be used internally.
    A ServiceAccount is a Kubernetes namespace-scoped object.
  prefs: []
  type: TYPE_NORMAL
- en: 'An example spec for a ServiceAccount would look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We will use this example in the next exercise. You would attach this ServiceAccount
    to an object by including this field in the definition of an object such as a
    Kubernetes deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: If you create a Kubernetes object without specifying a service account, it will
    be created with the `default` service account. A `default` service account is
    created by Kubernetes for each namespace.
  prefs: []
  type: TYPE_NORMAL
- en: In the following exercise, we will deploy the Kubernetes Dashboard on our cluster.
    Kubernetes Dashboard is arguably one of the most helpful tools to have running
    in any Kubernetes cluster. It is useful for debugging issues with configuring
    workloads in Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find more information about it here: [https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/](https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 11.02: Deploying an Application on Our HA Cluster'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this exercise, we will use the same cluster that we deployed in the previous
    exercise and deploy Kubernetes Dashboard. If you have deleted your cluster resources,
    then please rerun the previous exercise. kops will automatically add the required
    information to connect to the cluster in your local Kube config file (found at
    `~/.kube/config`) and set that cluster as the default context.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since the Kubernetes Dashboard is an application that helps us in administration
    tasks, the `default` ServiceAccount does not have sufficient privileges. We will
    be creating a new ServiceAccount with generous privileges in this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To begin with, we will apply the Kubernetes Dashboard manifest sourced directly
    from the official Kubernetes repository. This manifest defines all the objects
    that we will need for our application. Run the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see the following response:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.7: Applying the manifest for Kubernetes Dashboard'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B14870_11_07.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 11.7: Applying the manifest for Kubernetes Dashboard'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we need to configure a ServiceAccount to access the dashboard. To do
    this, create a file called `sa.yaml` with the following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: We are giving this user very liberal permissions, so please treat the access
    token with care. ClusterRole and ClusterRoleBinding objects are a part of RBAC
    policies, which are covered in *Chapter 13*, *Runtime and Network Security in
    Kubernetes*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, run the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see this response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s confirm the ServiceAccount details by running the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see the following response:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.8: Examining our ServiceAccount'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B14870_11_08.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 11.8: Examining our ServiceAccount'
  prefs: []
  type: TYPE_NORMAL
- en: When you create a ServiceAccount in Kubernetes, it will also create a Secret
    in the same namespace with the contents of the JWT needed to make API calls against
    the API server. As we can see from the previous screenshot, the Secret in this
    case is named `admin-user-token-vx84g`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s examine the `secret` object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.9: Examining the token in our ServiceAccount'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B14870_11_09.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 11.9: Examining the token in our ServiceAccount'
  prefs: []
  type: TYPE_NORMAL
- en: This is a truncated screenshot of the output. As we can see, we have a token
    here in this secret. Note that this is Base64 encoded, which we will decode in
    the next step.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we need the content of the token for the account Kubernetes just created
    for us, so let''s use this command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s break this command down. The command gets the secret called `admin-user`
    because we created a ServiceAccount with that name. When a ServiceAccount is created
    in Kubernetes, it places a secret named the same with the token we use to authenticate
    to the cluster. The rest of the command is syntactic sugar to decode the result
    in a useful form for copying and pasting into the dashboard. You should get an
    output as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.10: Getting the content of the token associated'
  prefs: []
  type: TYPE_NORMAL
- en: with the admin-user ServiceAccount
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B14870_11_10.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 11.10: Getting the content of the token associated with the admin-user
    ServiceAccount'
  prefs: []
  type: TYPE_NORMAL
- en: Copy the output you receive, while being careful not to copy the `$` or `%`
    signs (seen in Bash or Zsh, respectively) seen at the very end of the output.
  prefs: []
  type: TYPE_NORMAL
- en: 'By default, Kubernetes Dashboard is not exposed to the public internet outside
    our cluster. So, in order to access it with our browser, we need a way to allow
    our browser to communicate with Pods inside the Kubernetes container network.
    One useful way is to use the proxy built into `kubectl`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see this response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Open your browser and navigate to the following URL:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see the following prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.11: Entering the token to sign in to Kubernetes Dashboard'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B14870_11_11.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 11.11: Entering the token to sign in to Kubernetes Dashboard'
  prefs: []
  type: TYPE_NORMAL
- en: Paste your token copied from *step 4*, and then click on the `SIGN IN` button.
  prefs: []
  type: TYPE_NORMAL
- en: 'After logging in successfully, you should see the dashboard as shown in the
    following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.12: Kubernetes Dashboard landing page'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B14870_11_12.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 11.12: Kubernetes Dashboard landing page'
  prefs: []
  type: TYPE_NORMAL
- en: In this exercise, we have deployed Kubernetes Dashboard to the cluster to allow
    you to administer your application from a convenient GUI. During the course of
    deploying this application, we have seen how we can create ServiceAccounts for
    our cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout this chapter, you've learned how to create the cloud infrastructure
    using kops to make a highly available Kubernetes cluster. Then, we deployed the
    Kubernetes Dashboard and learned about ServiceAccounts in the process. Now that
    you have seen the steps required to make a cluster and get an application running
    on it, we will make another cluster and see its resilience in action in the following
    activity.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 11.01: Testing the Resilience of a Highly Available Cluster'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this activity, we will test out the resiliency of a Kubernetes cluster we
    create ourselves. Here are some guidelines for proceeding with this activity:'
  prefs: []
  type: TYPE_NORMAL
- en: Deploy Kubernetes Dashboard. But this time, set the replica count of the deployment
    running the application to something higher than `1`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Kubernetes Dashboard application is run on Pods managed by a deployment
    named `kubernetes-dashboard`, which runs in a namespace called `kubernetes-dashboard`.
    This is the deployment that you need to manipulate.
  prefs: []
  type: TYPE_NORMAL
- en: Now, start shutting down various nodes from the AWS console to remove nodes,
    delete Pods, and do what you can to make the underlying system unstable.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'After each attempt you make to take down the cluster, refresh the Kubernetes
    console if the console is still accessible. So long as you get any response from
    the application, this means that the cluster and our application (in this case,
    Kubernetes Dashboard) is still online. As long as the application is online, you
    should be able to access the Kubernetes Dashboard as shown in the following screenshot:![Figure
    11.13: Kubernetes Dashboard prompt for entering a token'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](image/B14870_11_13.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 11.13: Kubernetes Dashboard prompt for entering a token'
  prefs: []
  type: TYPE_NORMAL
- en: This screenshot shows just the prompt where you need to enter your token, but
    it is a good enough indicator that our application is online. If your request
    times out, this means that our cluster is no longer functional.
  prefs: []
  type: TYPE_NORMAL
- en: Join another node to this cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To achieve this, you need to find and edit the InstanceGroup resource that
    is managing the nodes. The spec contains `maxSize` and `minSize` fields, which
    you can manipulate to control the number of nodes. When you update your cluster
    to match the modified specification, you should be able to see three nodes, as
    shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.14: Number of master and worker nodes in the cluster'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B14870_11_14.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 11.14: Number of master and worker nodes in the cluster'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'The solution to this activity can be found at the following address: [https://packt.live/304PEoD](https://packt.live/304PEoD).
    Make sure you have deleted your clusters once you have completed the activity.
    More details on how to delete your clusters are presented in the following section
    (*Deleting Our Cluster*).'
  prefs: []
  type: TYPE_NORMAL
- en: Deleting Our Cluster
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Once we''re done with all the exercises and activities in this chapter, you
    should delete the cluster by running the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see this response:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.15: Deleting our cluster'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B14870_11_15.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 11.15: Deleting our cluster'
  prefs: []
  type: TYPE_NORMAL
- en: At this point, you should no longer be receiving charges from AWS for the Kubernetes
    infrastructure you have spun up in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Highly available infrastructure is one of the key components to achieving high
    availability for applications. Kubernetes is an extremely well-designed tool and
    has many built-in resiliency features that make it able to withstand major networking
    and compute events. It works to keep those events from impacting your application.
    During our exploration of high-availability systems, we investigated some components
    of Kubernetes and how they work together to achieve high availability. Then, we
    constructed a cluster of our own on AWS that was designed to be highly available
    using the kops cluster life cycle management tool.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we're going to take a look at how we make our applications
    more resilient by leveraging Kubernetes primitives to ensure high availability.
  prefs: []
  type: TYPE_NORMAL
