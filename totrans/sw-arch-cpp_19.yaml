- en: Cloud-Native Design
  prefs: []
  type: TYPE_NORMAL
- en: As the name suggests, cloud-native design describes the application's architecture
    built, first and foremost, to operate in the cloud. It is not defined by a single
    technology or language, but rather takes advantage of all that the modern cloud
    platforms offer.
  prefs: []
  type: TYPE_NORMAL
- en: This may mean a combination of using **Platform-as-a-Service** (**PaaS**) whenever
    necessary, multi-cloud deployments, edge computing, **Function-as-a-Service**
    (**FaaS**), static file hosting, microservices, and managed services. It transcends
    the boundaries of traditional operating systems. Instead of targeting the POSIX
    API and UNIX-like operating systems, cloud-native developers build on higher-level
    concepts using libraries and frameworks such as boto3, Pulumi, or Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding cloud-native
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Kubernetes to orchestrate cloud-native workloads
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Connecting services with a service mesh
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Observability in distributed systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Going GitOps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of the chapter, you'll have a good understanding of how modern trends
    in software architecture can be used in your applications.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Some of the examples in this chapter require Kubernetes 1.18.
  prefs: []
  type: TYPE_NORMAL
- en: The code present in the chapter has been placed on GitHub at [https://github.com/PacktPublishing/Software-Architecture-with-Cpp/tree/master/Chapter15](https://github.com/PacktPublishing/Software-Architecture-with-Cpp/tree/master/Chapter15).
  prefs: []
  type: TYPE_NORMAL
- en: Understanding cloud-native
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Whereas it is possible to migrate an existing application to run in the cloud,
    such migration won't make the application cloud-native. It would be running in
    the cloud, but the architectural choices would still be based on the on-premises
    model.
  prefs: []
  type: TYPE_NORMAL
- en: In short, cloud-native applications are distributed by nature, loosely coupled,
    and are scalable. They're not tied to any particular physical infrastructure and
    don't require the developers to even think about specific infrastructure. Such
    applications are usually web-centric.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we'll go over some examples of cloud-native building blocks
    and describe some cloud-native patterns.
  prefs: []
  type: TYPE_NORMAL
- en: Cloud-Native Computing Foundation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One proponent of cloud-native design is the **Cloud Native Computing Foundation**
    (**CNCF**), which hosts the Kubernetes project. CNCF is home to various technologies,
    making it easier to build cloud-native applications independent of the cloud vendor.
    Examples of such technologies include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Fluentd**, a unified logging layer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Jaeger**, for distributed tracing'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prometheus**, for monitoring'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**CoreDNS**, for service discovery'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cloud-native applications are typically built with application containers, often
    running on top of the Kubernetes platform. However, this is not a requirement,
    and it's entirely possible to use many of the CNCF frameworks outside Kubernetes
    and containers.
  prefs: []
  type: TYPE_NORMAL
- en: Cloud as an operating system
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The main trait of cloud-native design is to treat the various cloud resources
    as the building blocks of your application. Individual **virtual machines** (**VMs**)
    are seldom used in cloud-native design. Instead of targeting a given operating
    system running on some instances, with a cloud-native approach, you target either
    the cloud API directly (for example, with FaaS) or some intermediary solution
    such as Kubernetes. In this sense, the cloud becomes your operating system, as
    the POSIX API no longer limits you.
  prefs: []
  type: TYPE_NORMAL
- en: As containers changed the approach to building and distributing software, it
    is now possible to free yourself from thinking about the underlying hardware infrastructure.
    Your software is not working in isolation, so it's still necessary to connect
    different services, monitor them, control their life cycle, store data, or pass
    the secrets. This is something that Kubernetes provides and it's one of the reasons
    why it became so popular.
  prefs: []
  type: TYPE_NORMAL
- en: As you can probably imagine, cloud-native applications are web- and mobile-first.
    Desktop applications can also benefit from having some cloud-native components,
    but it's a less common use case.
  prefs: []
  type: TYPE_NORMAL
- en: It's still possible to use hardware and other low-level access in cloud-native
    applications. If your workload requires the use of the GPU, this should not prevent
    you from going cloud-native. What's more, cloud-native applications can be built
    on-premises if you want access to custom hardware unavailable elsewhere. The term
    is not limited to the public cloud, but rather to the way of thinking about different
    resources.
  prefs: []
  type: TYPE_NORMAL
- en: Load balancing and service discovery
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Load balancing is an essential part of distributed applications. It not only
    spreads the incoming requests across a cluster of services, which is essential
    for scaling, but can also help the responsiveness and availability of the applications.
    A smart load balancer can gather metrics to react to patterns in incoming traffic,
    monitor the state of the servers in its cluster, and forward requests to the less
    loaded and faster responding nodes – avoiding the currently unhealthy ones.
  prefs: []
  type: TYPE_NORMAL
- en: Load balancing brings more throughput and less downtime. By forwarding requests
    to many servers, a single point of failure is eliminated, especially if multiple
    load balancers are used, for example, in an active-passive scheme.
  prefs: []
  type: TYPE_NORMAL
- en: 'Load balancers can be used anywhere in your architecture: you can balance the
    requests coming from the web, requests done by web servers to other services,
    requests to cache or database servers, and whatever else suits your requirements.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a few things to remember when introducing load balancing. One of
    them is session persistence—make sure all requests from the same customer go to
    the same server, so the carefully chosen pink stilettos won''t disappear from
    their basket in your e-commerce site. Sessions can get tricky with load balancing:
    take extra care to not mix sessions, so customers won''t suddenly start being
    logged into each other''s profiles – countless companies stumbled upon this error
    before, especially when adding caching into the mix. It''s a great idea to combine
    the two; just make sure it is done the right way.'
  prefs: []
  type: TYPE_NORMAL
- en: Reverse proxies
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Even if you want to deploy just one instance of your server, it might be a good
    idea to add yet another service in front of it instead of the load balancer—a
    reverse proxy. While a proxy usually acts on behalf of the client sending some
    requests, a reverse proxy acts on behalf of the servers handling those requests,
    hence the name.
  prefs: []
  type: TYPE_NORMAL
- en: 'Why use it, you ask? There are several reasons and uses for such a proxy:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Security**: The address of your server is now hidden, and the server can
    be protected by the proxy''s DDoS prevention capabilities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Flexibility and scalability**: You can modify the infrastructure hidden behind
    the proxy in any way you want and when you want.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Caching**: Why bother the server if you already know what answer it will
    give?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Compression**: Compressing data will reduce the bandwidth needed, which may
    be especially useful for mobile users with poor connectivity. It can also lower
    your networking costs (but will likely cost you compute power).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SSL termination**: Reduce the backend server''s load by taking its burden
    to encrypt and decrypt network traffic.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An example of a reverse proxy is **NGINX**. It also provides load balancing
    capabilities, A/B testing, and much more. One of its other capabilities is service
    discovery. Let's see how it can be helpful.
  prefs: []
  type: TYPE_NORMAL
- en: Service Discovery
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As the name suggests, **Service Discovery** (**SD**) allows for automatically
    detecting instances of specific services in a computer network. Instead of hardcoding
    a domain name or IP where the service should be hosted, the caller must only be
    pointed to a service registry. Using this approach, your architecture gets a lot
    more flexible, as now all the services you use can be easily found. If you design
    a microservice-based architecture, introducing SD really goes a long way.
  prefs: []
  type: TYPE_NORMAL
- en: There are several approaches to SD. In client-side discovery, the caller contacts
    the SD instance directly. Each service instance has a registry client, which registers
    and de-registers the instance, handles heartbeats, and others. While quite straightforward,
    in this approach, each client has to implement the service discovery logic. Netflix
    Eureka is an example of a service registry commonly used in this approach.
  prefs: []
  type: TYPE_NORMAL
- en: An alternative is to use server-side discovery. Here, a service registry is
    also present, along with the registry clients in each service instance. The callers,
    however, don't contact it directly. Instead, they connect to a load balancer,
    for example, the AWS Elastic Load Balancer, which, in turn, either calls a service
    registry or uses its built-in service registry before dispatching the client calls
    to specific instances. Aside from AWS ELB, NGINX and Consul can be used to provide
    server-side SD capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: We now know how to find and use our services efficiently, so let's learn how
    best to deploy them.
  prefs: []
  type: TYPE_NORMAL
- en: Using Kubernetes to orchestrate cloud-native workloads
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes is an extensible open source platform for automating and managing
    container applications. It is sometimes referred to as k8s since it starts with
    'k,' ends with 's,' and there are eight letters in the middle.
  prefs: []
  type: TYPE_NORMAL
- en: 'Its design is based on Borg, a system used internally by Google. Some of the
    features present in Kubernetes are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Autoscaling of applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configurable networking
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Batch job execution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unified upgrading of applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ability to run highly available applications on top of it
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The declarative configuration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are different ways to run Kubernetes in your organization. Choosing one
    over the other requires you to analyze additional costs and benefits related to
    them.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes structure
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While it is possible to run Kubernetes on a single machine (for example, using
    minikube, k3s, or k3d), it is not recommended to do so in production. Single-machine
    clusters have limited functionality and no failover mechanisms. A typical size
    for a Kubernetes cluster is six machines or more. Three of the machines then form
    the control plane. The other three are worker nodes.
  prefs: []
  type: TYPE_NORMAL
- en: The minimum requirement of three machines comes from the fact that this is the
    minimal number to provide high availability. It is possible to have the control
    plane nodes also available as worker nodes, although this is not encouraged.
  prefs: []
  type: TYPE_NORMAL
- en: Control plane
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In Kubernetes, you rarely interact with individual worker nodes. Instead, all
    the API requests go to the control plane. The control plane then decides on the
    actions to take based on the requests, and then it communicates with the worker
    nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The interaction with the control plane can take several forms:'
  prefs: []
  type: TYPE_NORMAL
- en: Using the kubectl CLI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using a web dashboard
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the Kubernetes API from inside an application other than kubectl
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Control plane nodes usually run the API server, scheduler, a configuration store
    (etcd), and possibly some additional processes to handle the specific needs. For
    example, Kubernetes clusters deployed in a public cloud such as Google Cloud Platform
    have cloud controllers running on control plane nodes. The cloud controller interacts
    with the cloud provider's API to replace the failed machines, provision load balancers,
    or assign external IP addresses.
  prefs: []
  type: TYPE_NORMAL
- en: Worker nodes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The nodes that form the control plane and the worker pool are the actual machines
    the workload will run on. They may be physical servers that you host on-premises,
    VMs hosted privately, or VMs from your cloud provider.
  prefs: []
  type: TYPE_NORMAL
- en: 'Every node in a cluster runs at least the three programs as listed follows:'
  prefs: []
  type: TYPE_NORMAL
- en: A container runtime (for example, Docker Engine or cri-o) that allows the machine
    to handle the application containers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A kubelet, which is responsible for receiving requests from the control plane
    and manages the individual containers based on those requests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A kube-proxy, which is responsible for networking and load balancing on the
    node level
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Possible approaches to deploying Kubernetes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As you may have realized from reading the previous section, there are different
    possible ways to deploy Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: One of them is to deploy it to bare-metal servers hosted on-premises. One of
    the benefits is that this may be cheaper for large-scale applications than what
    the cloud providers offer. This approach has one major drawback—you will require
    an operator to provide the additional nodes whenever necessary.
  prefs: []
  type: TYPE_NORMAL
- en: To mitigate this issue, you can run a virtualization appliance on top of your
    bare-metal servers. This makes it possible to use the Kubernetes built-in cloud
    controller to provision the necessary resources automatically. You still have
    the same control over the costs, but there's less manual work. Virtualization
    adds some overhead, but in most cases, this should be a fair trade-off.
  prefs: []
  type: TYPE_NORMAL
- en: If you are not interested in hosting the servers yourself, you can deploy Kubernetes
    to run on top of VMs from a cloud provider. By choosing this route, you can use
    some of the existing templates for optimal setup. There are Terraform and Ansible
    modules available to build a cluster on popular cloud platforms.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, there are the managed services available from the major cloud players.
    You only have to pay for the worker nodes in some of them, while the control plane
    is free of charge.
  prefs: []
  type: TYPE_NORMAL
- en: Why would you choose self-hosted Kubernetes over the managed services when operating
    in a public cloud? One of the reasons may be a specific version of Kubernetes
    that you require. Cloud providers are typically a bit slow when it comes to introducing
    updates.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the Kubernetes concepts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Kubernetes introduces some concepts that may sound unfamiliar or be confusing
    if you hear them for the first time. When you learn their purpose, it should be
    easier to grasp what makes Kubernetes special. Here are some of the most common
    Kubernetes objects:'
  prefs: []
  type: TYPE_NORMAL
- en: A *container*, specifically, an application container, is a method of distributing
    and running a single application. It contains the code and configuration necessary
    to run the unmodified application anywhere.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A *Pod* is a basic Kubernetes building block. It is atomic and consists of one
    or more containers. All the containers inside the pod share the same network interfaces,
    volumes (such as persistent storage or secrets), and resources (CPU and memory).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A *deployment* is a higher-level object that describes the workload and its
    life cycle features. It typically manages a set of pod replicas, allows for rolling
    upgrades, and manages the rollbacks in case of failure. This is what makes it
    easy to scale and manage the life cycle of Kubernetes applications.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A *DaemonSet* is a controller similar to a deployment in that it manages where
    the pods are distributed. While deployments are concerned with keeping a given
    number of replicas, DaemonSets spreads the pods across all worker nodes. The primary
    use case is to run a system-level service, such as a monitoring or logging agent
    on each node.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Jobs* are designed for one-off tasks. Pods in deployments restart automatically
    when the containers inside them terminate. They are suitable for all the always-on
    services that listen on network ports for requests. However, deployments are unsuited
    for batch jobs, such as thumbnail generation, which you want to run only when
    required. Jobs create one or more pods and watch them until they complete a given
    task. When a specific number of successful pods terminate, the job is considered
    complete.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*CronJobs*, as the name suggests, are the jobs that are run periodically within
    the cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Services* represent a particular function performed within a cluster. They
    have a network endpoint associated with them (which is usually load balanced).
    Services may be performed by one or more pods. The life cycle of services is independent
    of the life cycles of the many pods. Since pods are transient, they may be created
    and destroyed at any time. Services abstract the individual pods to allow for
    high availability. Services have their own IP addresses and DNS names for ease
    of use.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Declarative approach
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We've covered the differences between declarative and imperative approaches
    earlier in [Chapter 9](https://cdp.packtpub.com/hands_on_software_architecture_with_c__/wp-admin/post.php?post=33&action=edit),
    *Continuous Integration/Continuous Deployment*[.](https://cdp.packtpub.com/hands_on_software_architecture_with_c__/wp-admin/post.php?post=33&action=edit)
    Kubernetes takes the declarative approach. Instead of giving instructions regarding
    the steps that need to be taken, you provide the resources that describe your
    cluster's desired state. It is up to the control plane to allocate internal resources
    so that they fulfill your needs.
  prefs: []
  type: TYPE_NORMAL
- en: It is possible to add the resources using the command line directly. This can
    be quick for testing, but you want to have a trail of the resources you created
    most of the time. Thus, most people work with manifest files, which provide a
    coded description of the resources required. Manifests are typically YAML files,
    but it is also possible to use JSON.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s an example YAML manifest with a single Pod:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The first line is mandatory, and it tells which API version will be used in
    the manifest. Some resources are only available in extensions, so this is the
    information for the parser on how to behave.
  prefs: []
  type: TYPE_NORMAL
- en: The second line describes what resource we are creating. Next, there is metadata
    and the specification of the resource.
  prefs: []
  type: TYPE_NORMAL
- en: A name is mandatory in metadata as this is the way to distinguish one resource
    from another. If we wanted to create another pod with the same name, we would
    get an error stating that such a resource already exists. The label is optional
    and useful when writing selectors. For example, if we wanted to create a service
    that allows connection to the pod, we would use a selector matching label app
    with a value equal to `dominican-front`.
  prefs: []
  type: TYPE_NORMAL
- en: The specification is also the mandatory part as it describes the actual content
    of the resource. In our example, we list all the containers that are running inside
    the pod. To be precise, one container named `webserver` using an image, `nginx`,
    from Docker Hub. Since we want to connect to the Nginx web server from the outside,
    we also expose the container port `80` on which the server is listening. The name
    in the port description is optional.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes networking
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Kubernetes allows for pluggable network architectures. Several drivers exist
    that may be used depending on requirements. Whichever driver you select, some
    concepts are universal. The following are the typical networking scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Container-to-container communication
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A single pod may host several different containers. Since the network interface
    is tied to the pod and not to the containers, each container operates in the same
    networking namespace. This means various containers may address one another using
    localhost networking.
  prefs: []
  type: TYPE_NORMAL
- en: Pod-to-pod communication
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Each pod has an internal cluster-local IP address assigned. The address does
    not persist once the pod has been deleted. One pod can connect to another's exposed
    ports when it knows the other's address as they share the same flat network. You
    can think of pods as VMs hosting containers with regard to this communication
    model. This is rarely used as the preferred method is pod-to-service communication.
  prefs: []
  type: TYPE_NORMAL
- en: Pod-to-service communication
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Pod-to-service communication is the most popular use case for communication
    within the cluster. Each service has an individual IP address and a DNS name assigned
    to it. When a pod connects to a service, the connection is proxied to one of the
    pods in the group selected by the service. Proxying is a task of the kube-proxy
    tool described earlier.
  prefs: []
  type: TYPE_NORMAL
- en: External-to-internal communication
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: External traffic typically comes to the cluster via the means of load balancers.
    These are either tied to or handled by specific services or ingress controllers.
    When the externally exposed services handle the traffic, it behaves like pod-to-service
    communication. With the ingress controller, you have additional features available
    that allow for routing, observability, or advanced load balancing.
  prefs: []
  type: TYPE_NORMAL
- en: When is using Kubernetes a good idea?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Introducing Kubernetes within an organization requires some investment. There
    are many benefits provided by Kubernetes, such as autoscalability, automation,
    or deployment scenarios. However, these benefits may not justify the necessary
    investment.
  prefs: []
  type: TYPE_NORMAL
- en: 'This investment concerns several areas:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Infrastructure costs**: The costs associated with running the control plane
    and the worker nodes may be relatively high. Additionally, the costs may rise
    if you want to use various Kubernetes expansions, such as GitOps or a service
    mesh (described later). They also require additional resources to run and provide
    more overhead on top of your application''s regular services. Apart from the nodes
    themselves, you should also factor in other costs. Some of the Kubernetes features
    work best when deployed to a supported cloud provider. This means that in order
    to benefit from those features, you''d have to go down one of the following routes:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a. Move your workload to the specifically supported cloud.
  prefs: []
  type: TYPE_NORMAL
- en: b. Implement your own drivers for a cloud provider of your choice.
  prefs: []
  type: TYPE_NORMAL
- en: c. Migrate your on-premises infrastructure to a virtualized API-enabled environment
    such as VMware vSphere or OpenStack.
  prefs: []
  type: TYPE_NORMAL
- en: '**Operations costs**: The Kubernetes cluster and associated services require
    maintenance. Even though you get less maintenance for your applications, this
    benefit is slightly offset by the cost of keeping the cluster running.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Education costs**: Your entire product team has to learn new concepts. Even
    if you have a dedicated platform team that will provide developers with easy-to-use
    tools, developers would still require a basic understanding of how the work they
    do influences the entire system and which API they should use.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Before you decide on introducing Kubernetes, consider first whether you can
    afford the initial investment it requires.
  prefs: []
  type: TYPE_NORMAL
- en: Observability in distributed systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Distributed systems such as cloud-native architecture pose some unique challenges.
    The sheer number of different services working at any given time makes it very
    inconvenient to investigate how well the components perform.
  prefs: []
  type: TYPE_NORMAL
- en: In monolithic systems, logging and performance monitoring are usually enough.
    With a distributed system, even logging requires a design choice. Different components
    produce different log formats. Those logs have to be stored somewhere. Keeping
    them together with a service that delivers them will make it challenging to get
    the big picture in an outage case. Besides, since microservices may be short-lived,
    you will want to decouple the life cycle of logs from the life cycle of a service
    that provides them or a machine that hosts the service.
  prefs: []
  type: TYPE_NORMAL
- en: In [C](ccc9ef2c-747a-4b56-9009-21382c7838d5.xhtml)[hapter 13](ccc9ef2c-747a-4b56-9009-21382c7838d5.xhtml),
    *Designing Microservices*, we described how a unified logging layer helps manage
    the logs. But logs only show what happens at a given point in the system. To see
    the picture from a single transaction point of view, you require a different approach.
  prefs: []
  type: TYPE_NORMAL
- en: This is where tracing comes in.
  prefs: []
  type: TYPE_NORMAL
- en: How tracing differs from logging
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Tracing is a specialized form of logging. It provides lower-level information
    than logs. This may include all the function calls, their parameters, their size,
    and execution time. They also contain the unique ID of the transaction being processed.
    These details make it possible to reassemble them and see the life cycle of a
    given transaction as it passes through your system.
  prefs: []
  type: TYPE_NORMAL
- en: Performance information present in tracing helps you with uncovering bottlenecks
    and sub-optimal components in the system.
  prefs: []
  type: TYPE_NORMAL
- en: While logs are often read by operators and developers, they tend to be human-readable.
    There are no such requirements for tracing. To view the traces, you will use a
    dedicated visualization program. This means that even though traces are more detailed,
    they may also take up less space than logs.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram is an overview of a single trace:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/99975346-44c1-441b-a623-58d5513d59d7.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15.1 – Single trace
  prefs: []
  type: TYPE_NORMAL
- en: Two services communicate over a network. In *Service A*, we have one parent
    span that contains a child span and a single log. Child spans usually correspond
    to deeper function calls. A log represents the smallest piece of information.
    Each of them is timed and may contain additional information.
  prefs: []
  type: TYPE_NORMAL
- en: The network call to *Service B* preserves the span context. Even though *Service
    B* is executed in a different process on another machine, all of the information
    can be later reassembled as the transaction ID is preserved.
  prefs: []
  type: TYPE_NORMAL
- en: A piece of bonus information that we get from reassembling traces is the dependency
    graph between the services in our distributed system. As traces contain the entire
    call chain, it is possible to visualize this information and inspect unexpected
    dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing a tracing solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are several possible solutions to choose from when implementing tracing.
    As you may imagine, there are both self-hosted and managed tools that you can
    use to instrument your applications. We will briefly describe the managed ones
    and focus on the self-hosted ones.
  prefs: []
  type: TYPE_NORMAL
- en: Jaeger and OpenTracing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'One of the standards in distributed tracing is OpenTracing proposed by the
    authors of Jaeger. Jaeger is a tracer built for cloud-native applications. It
    addresses the problems of monitoring distributed transactions and propagating
    the tracing context. It''s useful for the following purposes:'
  prefs: []
  type: TYPE_NORMAL
- en: Performance or latency optimization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing a root cause analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyzing the inter-service dependencies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenTracing is an open standard presenting an API that is independent of the
    tracer used. This means that when your application is instrumented using OpenTracing,
    you avoid lock-in to one particular vendor. If, at some point, you decide to switch
    from Jaeger to Zipkin, DataDog, or any other compatible tracer, you won't have
    to modify the entire instrumentation code.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many client libraries compatible with OpenTracing. You can also find
    many resources, including tutorials and articles that explain how to implement
    the API for your needs. OpenTracing officially supports the following languages:'
  prefs: []
  type: TYPE_NORMAL
- en: Go
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: JavaScript
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Java
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ruby
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PHP
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Objective-C
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: C++
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: C#
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are also unofficial libraries available, and specific applications can
    export OpenTracing data as well. This includes Nginx and Envoy, both popular web
    proxies.
  prefs: []
  type: TYPE_NORMAL
- en: Jaeger also accepts samples in Zipkin format. We will cover Zipkin in the next
    section. What it means is that you don't have to rewrite the instrumentation from
    one format to another if you (or any of your dependencies) already use Zipkin.
    For all new applications, OpenTracing is the recommended approach.
  prefs: []
  type: TYPE_NORMAL
- en: Jaeger scales well. You can run it as a single binary or a single application
    container if you want to evaluate it. You may configure Jaeger for production
    use to use its own backend or a supported external one, such as Elasticsearch,
    Cassandra, or Kafka.
  prefs: []
  type: TYPE_NORMAL
- en: Jaeger is a CNCF graduated project. This means it has reached a similar level
    of maturity to Kubernetes, Prometheus, or Fluentd. Because of this, we expect
    it to gain even more support in other CNCF applications.
  prefs: []
  type: TYPE_NORMAL
- en: Zipkin
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The main competitor for Jaeger is Zipkin. It's an older project, which also
    means it is more mature. Usually, more senior projects are also better supported,
    but in this case, the endorsement of CNCF plays in Jaeger's favor.
  prefs: []
  type: TYPE_NORMAL
- en: Zipkin uses its proprietary protocol to handle tracing. It has OpenTracing support
    available, but it may not be at the same maturity and support level as the native
    Jaeger protocol. As we've mentioned earlier, it is also possible to configure
    Jaeger to collect traces in Zipkin format. This means the two are, at least to
    some point, interchangeable.
  prefs: []
  type: TYPE_NORMAL
- en: The project is hosted under the Apache foundation, but is not considered a CNCF
    project. When developing cloud-native applications, Jaeger is a better alternative.
    If you are looking instead for an all-purpose tracing solution, it is worth considering
    Zipkin as well.
  prefs: []
  type: TYPE_NORMAL
- en: One drawback is that Zipkin doesn't have a supported C++ implementation. There
    are unofficial libraries, but they don't seem to be well-supported. Using a C++
    OpenTracing library is the preferred way to instrument the C++ code.
  prefs: []
  type: TYPE_NORMAL
- en: Instrumenting an application with OpenTracing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section will illustrate how to add instrumentation with Jaeger and OpenTracing
    to an existing application. We'll use the `opentracing-cpp` and `jaeger-client-cpp`
    libraries.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we want to set up the tracer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The two preferred methods for configuring a sampling server are either by using
    the environment variable, as we did, or by using a YAML configuration file. When
    using environment variables, we will have to set them up before running the application.
    The most important ones are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`JAEGER_AGENT_HOST`: The hostname where the Jaeger agent is located'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`JAEGER_AGENT_POR`: The port on which the Jaeger agent is listening'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`JAEGER_SERVICE_NAME`: The name of our application'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we configure the tracer and supply the logging implementation. It is possible
    to implement a custom logging solution if the available `ConsoleLogger` is not
    enough. For container-based applications with a unified logging layer, the ConsoleLogger
    should be enough.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we have the tracer set up, we want to add spans to the functions that
    we want to be instrumented. The following code does just that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'This span may be used later to create child spans within a given function.
    It may also be propagated to deeper function calls as a parameter. This is how
    it appears:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Context propagation happens when we call the `opentracing::ChildOf` function.
    We may also pass the context over network calls using the `inject()` and `extract()`
    calls.
  prefs: []
  type: TYPE_NORMAL
- en: Connecting services with a service mesh
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Microservices and cloud-native design come with their own set of problems. Communication
    between different services, observability, debugging, rate limiting, authentication,
    access control, and A/B testing may be challenging even with a limited number
    of services. When the number of services rises, so does the complexity of the
    aforementioned requirements.
  prefs: []
  type: TYPE_NORMAL
- en: That's where a service mesh enters the fray. In short, a service mesh trades
    off some resources (necessary to run the control plane and sidecars) for an automated
    and centrally controlled solution to the aforementioned challenges.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing a service mesh
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: All the requirements we mentioned in the introduction to this chapter used to
    be coded within the application itself. As it turns out, many may be abstracted
    as they are shared across many different applications. When your application consists
    of many services, adding new features to all of them starts to be costly. With
    a service mesh, you may control these features from a single point instead.
  prefs: []
  type: TYPE_NORMAL
- en: Since a containerized workflow already abstracts some of the runtime and some
    networking, a service mesh takes the abstraction to another level. This way, the
    application within a container is only aware of what happens at the application
    level of the OSI networking model. The service mesh handles lower levels.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up a service mesh allows you to control all network traffic in a new
    way and gives you better insights into this traffic. The dependencies become visible,
    as does the flow, shape, and amount of traffic.
  prefs: []
  type: TYPE_NORMAL
- en: Not only is the flow of traffic handled by the service mesh. Other popular patterns,
    such as circuit breaking, rate limiting, or retries, don't have to be implemented
    by each application and configured separately. This is also a feature that can
    be outsourced to the service mesh. Similarly, A/B testing or canary deployments
    are the use cases that a service mesh is able to fulfill.
  prefs: []
  type: TYPE_NORMAL
- en: One of the benefits of the service mesh, as previously mentioned, is greater
    control. Its architecture typically consists of a manageable edge proxy for external
    traffic and internal proxies usually deployed as sidecars along each microservice.
    This way, the networking policies can be written as code and stored alongside
    all the other configuration in a single place. Rather than having to switch on
    mutual TLS encryption for two of the services you want to connect, you only have
    to enable the feature once in your service mesh configuration.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we'll cover some of the service mesh solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Service mesh solutions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: All of the solutions described here are self-hosted.
  prefs: []
  type: TYPE_NORMAL
- en: Istio
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Istio is a powerful collection of service mesh tools. It allows you to connect
    microservices through the deployment of Envoy proxies as sidecar containers. Because
    Envoy is programmable, the Istio control plane's configuration changes are communicated
    to all the proxies, which then reconfigure themselves accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: The Envoy proxies are, among other things, responsible for handing encryption
    and authentication. With Istio, enabling mutual TLS between your services requires
    a single switch in the configuration for the majority of the time. If you don't
    want mTLS between all your services, you may also select those that demand this
    additional protection while allowing unencrypted traffic between everything else.
  prefs: []
  type: TYPE_NORMAL
- en: 'Istio also helps with observability. First of all, the Envoy proxies export
    proxy-level metrics compatible with Prometheus. There are also service-level metrics
    and control plane metrics exported by Istio. Next, there are distributed traces
    that describe the traffic flow within the mesh. Istio can serve the traces to
    different backends: Zipkin, Jaeger, Lightstep, and Datadog. Finally, there are
    Envoy access logs, which show every call in a format similar to Nginx.'
  prefs: []
  type: TYPE_NORMAL
- en: It's possible to visualize your mesh using Kiali, an interactive web interface.
    This way, you can see a graph of your services, including information such as
    whether the encryption is enabled, what the size of the flow between different
    services is, or what's the health check status of each of them is.
  prefs: []
  type: TYPE_NORMAL
- en: The authors of Istio claim that this service mesh should be compatible with
    different technologies. At the time of writing, the best documented, best integrated,
    and best tested is the integration with Kubernetes. Other supported environments
    are on-premises, general-purpose clouds, Mesos, and Nomad with Consul.
  prefs: []
  type: TYPE_NORMAL
- en: If you work in an industry concerned with compliance (such as financial institutions),
    then Istio can help in these aspects.
  prefs: []
  type: TYPE_NORMAL
- en: Envoy
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: While Envoy is not a service mesh in itself, it is worth mentioning in this
    section due to its use in Istio.
  prefs: []
  type: TYPE_NORMAL
- en: Envoy is a service proxy that acts much like Nginx or HAProxy. The main difference
    is that it can be reconfigured on the fly. This happens programmatically via an
    API and does not require the configuration file to be changed and the daemon to
    then be reloaded.
  prefs: []
  type: TYPE_NORMAL
- en: Interesting facts regarding Envoy are its performance and popularity. According
    to tests performed by SolarWinds, Envoy beats the competition when it comes to
    performance as a service proxy. This competition includes HAProxy, Nginx, Traefik,
    and AWS Application Load Balancer. Envoy is much younger than the established
    leaders in this space, such as Nginx, HAProxy, Apache, and Microsoft IIS, but
    this didn't stop Envoy from entering the top 10 list of most-used web servers,
    according to Netcraft.
  prefs: []
  type: TYPE_NORMAL
- en: Linkerd
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before Istio became synonymous with a service mesh, this field was represented
    by Linkerd. There is some confusion regarding the naming, as the original Linkerd
    project was designed to be platform-agnostic and targeted the Java VM. This meant
    that it was resource-heavy and often sluggish. The newer version, called Linkerd2,
    has been rewritten to address these issues. Linkerd2, as opposed to the original
    Linkerd, is only focused on Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Both Linkerd and Linkerd2 use their own proxy solution instead of relying on
    an existing project such as Envoy. The rationale for that is that a dedicated
    proxy (versus a general-purpose Envoy) offers better security and performance.
    An interesting feature of Linkerd2 is that the company that developed it also
    offers paid support.
  prefs: []
  type: TYPE_NORMAL
- en: Consul service mesh
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A recent addition to the service mesh space is the Consul service mesh. This
    is a product from HashiCorp, a well-established cloud company known for such tools
    as Terraform, Vault, Packer, Nomad, and Consul.
  prefs: []
  type: TYPE_NORMAL
- en: Just like the other solutions, it features mTLS and traffic management. It's
    advertised as a multi-cloud, multi-data center, and multi-region mesh. It integrates
    with different platforms, data plane products, and observability providers. At
    the time of writing, the reality is a bit more modest as the main supported platforms
    are Nomad and Kubernetes, while the supported proxies are either the built-in
    proxy or Envoy.
  prefs: []
  type: TYPE_NORMAL
- en: If you are considering using Nomad for your application, then the Consul service
    mesh may be a great choice and a good fit as both are HashiCorp products.
  prefs: []
  type: TYPE_NORMAL
- en: Going GitOps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The last topic that we would like to cover in this chapter is GitOps. Even though
    the term sounds new and trendy, the idea behind it is not entirely novel. It's
    an extension of the well-known **Continuous Integration**/**Continuous Deployment**
    (**CI**/**CD**) pattern. Or maybe an extension is not a good description.
  prefs: []
  type: TYPE_NORMAL
- en: While CI/CD systems usually aim to be very flexible, GitOps seeks to minimize
    the number of possible integrations. The two main constants are Git and Kubernetes.
    Git is used for version control, release management, and environment separation.
    Kubernetes is used as a standardized and programmable deployment platform.
  prefs: []
  type: TYPE_NORMAL
- en: 'This way, the CI/CD pipeline becomes almost transparent. It''s the opposite
    approach to that of imperative code handling all the stages of the build. To allow
    such a level of abstraction, you will typically need the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Infrastructure as Code to allow the automated deployment of all the necessary
    environments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Git workflow with feature branches and pull requests or merge requests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A declarative workflow configuration, which is already available in Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The principles of GitOps
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since GitOps is an extension of the established CI/CD pattern, it may not be
    very clear to distinguish between the two. Here are some of the GitOps principles
    that differentiate this approach from general-purpose CI/CD.
  prefs: []
  type: TYPE_NORMAL
- en: Declarative description
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The main difference between a classical CI/CD system and GitOps lies in the
    mode of operation. Most CI/CD systems are imperative: they consist of a sequence
    of steps to be taken in order for a pipeline to succeed.'
  prefs: []
  type: TYPE_NORMAL
- en: Even the pipeline's notion is imperative as it implies an object that has an
    entry, a set of connections, and a sink. Some of the steps may be performed in
    parallel, but a process has to stop and wait for the depending step to finish
    whenever there is a dependency.
  prefs: []
  type: TYPE_NORMAL
- en: In GitOps, the configuration is declarative. This refers to the entire state
    of your system – the applications, their configuration, monitoring, and dashboards.
    It is all treated as code, giving it the same features as regular application
    code.
  prefs: []
  type: TYPE_NORMAL
- en: The system's state versioned in Git
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Since the state of your system is written in code, you derive some benefits
    from that fact. Features such as easier auditing, code reviews, and version control
    are now applicable not just to the application code. The consequence is that in
    case anything goes wrong, reverting back to a working state requires a single
    `git revert` command.
  prefs: []
  type: TYPE_NORMAL
- en: You can use the power of Git's signed commits and SSH and GPG keys to give control
    over different environments. By adding a gating mechanism that makes sure only
    the commits meeting required standards can be pushed to the repository, you also
    eliminate many accidental errors that may result from running commands manually
    using `ssh` or `kubectl`.
  prefs: []
  type: TYPE_NORMAL
- en: Auditable
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Everything that you store in your version control systems becomes auditable.
    Before introducing a new code, you perform a code review. When you notice a bug,
    you can revert the change that introduced it or get back to the last working version.
    Your repository becomes the single point of truth regarding your entire system.
  prefs: []
  type: TYPE_NORMAL
- en: It's already useful when applied to the application code. However, extending
    the ability to audit configuration, helper services, metrics, dashboards, and
    even deployment strategies makes it even more powerful. You no longer have to
    ask yourself, "*OK, so why did this configuration end up in production?*" All
    you have to do is check the Git log.
  prefs: []
  type: TYPE_NORMAL
- en: Integrated with established components
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Most CI/CD tools introduce proprietary configuration syntax. Jenkins uses Jenkins
    DSL. Each of the popular SaaS solutions uses YAML, but the YAML files are incompatible
    with each other. You can't switch from Travis to CircleCI or from CircleCI to
    GitLab CI without rewriting your pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: This has two drawbacks. One is the obvious vendor lock-in. The other is the
    need to learn the configuration syntax to use the given tool. Even if most of
    your pipeline is already defined elsewhere (shell scripts, Dockerfiles, or Kubernetes
    manifests), you still need to write some glue code to instruct the CI/CD tool
    to use it.
  prefs: []
  type: TYPE_NORMAL
- en: It's different with GitOps. Here, you don't write explicit instructions or use
    proprietary syntax. Instead, you reuse other common standards, such as Helm or
    Kustomize. There's less to learn, and the migration process is much more comfortable.
    Also, GitOps tools usually integrate well with other components from the CNCF
    ecosystem, so you can get your deployment metrics stored in Prometheus and auditable
    with Grafana.
  prefs: []
  type: TYPE_NORMAL
- en: Configuration drift prevention
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Configuration drift happens when a given system's current state differs from
    the desired state as described in the repository. Multiple causes are contributing
    to the configuration drift.
  prefs: []
  type: TYPE_NORMAL
- en: For example, let's consider a configuration management tool with a VM-based
    workload. All of the VMs start in the same state. As the CM runs for the first
    time, it brings the machines to the desired state. But if an auto-update agent
    is running on those machines by default, this agent may update some of the packages
    on its own, without considering the desired state from the CM. Moreover, as network
    connectivity may be fragile, some of the machines may update to a newer version
    of a package, while others won't.
  prefs: []
  type: TYPE_NORMAL
- en: One of the updated packages may be incompatible with the pinned package that
    your application requires in extreme cases. Such a situation will break the entire
    CM workflow and leave your machine in an unusable state.
  prefs: []
  type: TYPE_NORMAL
- en: With GitOps, an agent is always running inside your system that keeps track
    of the current state and the desired state of the system. If the current state
    suddenly differs from the desired one, an agent may fix it or issue an alert regarding
    configuration drift.
  prefs: []
  type: TYPE_NORMAL
- en: Preventing configuration drift adds another layer of self-healing to your system.
    If you're running Kubernetes, you already have self-healing on the pod level.
    Whenever a pod fails, another one is recreated in its place. If you are using
    a programmable infrastructure underneath (such as a cloud provider or OpenStack
    on-premises), you also have self-healing capabilities of your nodes. With GitOps,
    you get the self-healing for workloads and its configuration.
  prefs: []
  type: TYPE_NORMAL
- en: The benefits of GitOps
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As you can imagine, the described features of GitOps afford several benefits.
    Here are some of them.
  prefs: []
  type: TYPE_NORMAL
- en: Increased productivity
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: CI/CD pipelines already automate a lot of usual tasks. They reduce lead time
    by helping get more deployments. GitOps adds a feedback loop that prevents configuration
    drift and allows self-healing. This means that your team can ship quicker and
    worry less about introducing potential problems as they are easy to revert. This,
    in turn, means that the development throughput increases and you can introduce
    new features faster and with more confidence.
  prefs: []
  type: TYPE_NORMAL
- en: Better developer experience
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With GitOps, developers don't have to worry about building containers or using
    kubectl to control the cluster. Deploying new features requires just the use of
    Git, which is already a familiar tool in most environments.
  prefs: []
  type: TYPE_NORMAL
- en: This also means that onboarding is quicker since new hires don't have to learn
    a lot of new tools in order to be productive. GitOps uses standard and consistent
    components, so introducing changes to the operations side should not impact developers.
  prefs: []
  type: TYPE_NORMAL
- en: Higher stability and reliability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Using Git to store the state of your system means you have access to an audit
    log. This log contains a description of all the changes introduced. If your task
    tracking system integrates with Git (which is a good practice), you can typically
    tell which business feature is related to the system's change.
  prefs: []
  type: TYPE_NORMAL
- en: With GitOps, there is less need to allow manual access to the nodes or the entire
    cluster, which reduces the chance of accidental errors originating from running
    an invalid command. Those random errors that get into the system are easily fixed
    by using Git's powerful revert feature.
  prefs: []
  type: TYPE_NORMAL
- en: Recovery from a severe disaster (such as losing the entire control plane) is
    also a lot easier. All it requires is setting up a new clean cluster, installing
    a GitOps operator there, and pointing it to the repository with your configuration.
    After a short while, you have an exact replica of your previous production system,
    all without manual intervention.
  prefs: []
  type: TYPE_NORMAL
- en: Improved security
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A reduced need to give access to the cluster and nodes means improved security.
    There is less to worry about in terms of lost or stolen keys. You avoid a situation
    where someone retains access to your production environment even though this person
    is no longer working on the team (or in the company).
  prefs: []
  type: TYPE_NORMAL
- en: When it comes to access to the system, the single point of truth is handled
    by the Git repository. Even if a malicious actor decides to introduce a backdoor
    into your system, the change required will undergo a code review. Impersonating
    another developer is also more challenging when your repository uses GPG-signed
    commits with strong verification.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we've mainly covered the benefits from the development and operations
    point of view. But GitOps also benefits the business. It affords business observability
    in the system, something that was hard to achieve before.
  prefs: []
  type: TYPE_NORMAL
- en: It's easy to track the features present in a given release as they are all stored
    in Git. Since Git commits a link to the task tracker, business people can get
    preview links to see how the application looks in various development stages.
  prefs: []
  type: TYPE_NORMAL
- en: 'It also gives clarity that allows the following common questions to be answered:'
  prefs: []
  type: TYPE_NORMAL
- en: What's running in production?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Which tickets have been resolved with the last release?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Which change might be responsible for service degradations?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The questions for all those answers may even be presented in a friendly dashboard.
    Naturally, the dashboard itself can be stored in Git as well.
  prefs: []
  type: TYPE_NORMAL
- en: GitOps tools
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The GitOps space is a new and growing one. There are already tools that can
    be considered stable and mature. Here are some of the most popular ones.
  prefs: []
  type: TYPE_NORMAL
- en: FluxCD
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: FluxCD is an opinionated GitOps operator for Kubernetes. Selected integrations
    provide core functionality. It uses Helm charts and Kustomize to describe the
    resources.
  prefs: []
  type: TYPE_NORMAL
- en: Its integration with Prometheus adds observability to the deployment process.
    To help with maintenance, FluxCD features a CLI.
  prefs: []
  type: TYPE_NORMAL
- en: ArgoCD
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Unlike FluxCD, it offers a broader choice of tools to use. This might be useful
    if you're already using Jsonnet or Ksonnet for your configuration. Like FluxCD,
    it integrates with Prometheus and features a CLI.
  prefs: []
  type: TYPE_NORMAL
- en: At the time of writing this book, ArgoCD is a more popular solution than FluxCD.
  prefs: []
  type: TYPE_NORMAL
- en: Jenkins X
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Contrary to what the name might suggest, Jenkins X doesn't have much in common
    with the well-known Jenkins CI system. It is backed by the same company, but the
    entire concepts of Jenkins and Jenkins X are totally different.
  prefs: []
  type: TYPE_NORMAL
- en: While the other two tools are purposefully small and self-contained, Jenkins
    X is a complex solution with many integrations and a broader scope. It supports
    the triggering of custom build tasks, making it look like a bridge between a classic
    CI/CD system and GitOps.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Congratulations on reaching the end of the chapter! Using modern C++ is not
    limited to understanding the recently added language features. Your applications
    will run in production. As an architect, it's also your choice to make sure the
    runtime environment matches requirements. In the few previous chapters, we described
    some popular trends in distributed applications. We hope this knowledge will help
    you decide which one is the best fit for your product.
  prefs: []
  type: TYPE_NORMAL
- en: Going cloud-native brings a lot of benefits and can automate a good chunk of
    your workflow. Switching custom-made tools to industry standards makes your software
    more resilient and easier to update. In this chapter, we have covered the pros,
    cons, and use cases of popular cloud-native solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Some, such as distributed tracing with Jaeger, bring immediate benefits to most
    projects. Others, such as Istio or Kubernetes, perform best in large-scale operations.
    After reading this chapter, you should have sufficient knowledge to decide whether
    introducing cloud-native design into your application is worth the cost.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What's the difference between running your applications in your cloud and making
    them cloud-native?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How can you run cloud-native applications on-premises?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What's the minimal highly available cluster size for Kubernetes?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Which Kubernetes object represents a microservice that allows network connections?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why is logging not sufficient in distributed systems?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does a service mesh help with building secure systems?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does GitOps increase productivity?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What's the standard CNCF project for monitoring?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Mastering Kubernetes*: [https://www.packtpub.com/product/mastering-kubernetes-third-edition/9781839211256](https://www.packtpub.com/product/mastering-kubernetes-third-edition/9781839211256)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Mastering Distributed Tracing*: [https://www.packtpub.com/product/mastering-distributed-tracing/9781788628464](https://www.packtpub.com/product/mastering-distributed-tracing/9781788628464)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Mastering Service Mesh*: [https://www.packtpub.com/product/mastering-service-mesh/9781789615791](https://www.packtpub.com/product/mastering-service-mesh/9781789615791)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
