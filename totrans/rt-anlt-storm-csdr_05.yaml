- en: Chapter 5. Storm High Availability and Failover
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter takes you to the next level in your journey through Storm, where
    we get you acquainted with the integration of Storm with other necessary components
    in the ecosystem. We will cover the concepts of high availability and reliability,
    practically.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter is the next step in understanding the clustered mode setup of Storm
    and its associated components. We will understand the various configurations in
    Storm and Zookeeper and the concept behind them.
  prefs: []
  type: TYPE_NORMAL
- en: 'The topics that will be covered in this chapter are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Setting up RabbitMQ (single instance and clustered mode)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Developing the AMQP spout to integrate Storm and RabbitMQ
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a RabbitMQ feeder component
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building high availability for RabbitMQ and the Storm cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Storm schedulers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you will be able to set up and understand RabbitMQ
    and integrate Storm with RabbitMQ. Also, you will be able to test high availability
    and guaranteed processing of the Storm cluster.
  prefs: []
  type: TYPE_NORMAL
- en: An overview of RabbitMQ
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The punch line that goes for RabbitMQ is *Messaging that just works*.
  prefs: []
  type: TYPE_NORMAL
- en: RabbitMQ is one of the most widely used implementations of the AMQP messaging
    protocol that provides a platform for message receipt and delivery. This in-memory
    queue also has the capacity to hold and retain messages till they are consumed
    by a consumer. This flexible brokering system is very easy to use and works on
    most of the operating systems such as windows, UNIX, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'RabbitMQ is an implementation of the **Advanced Message Queuing Protocol**
    (**AMQP**). As depicted in the following figure, the vital components of RabbitMQ
    are **exchange** and **Queue**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![An overview of RabbitMQ](img/00039.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The publisher and the consumer are two essential actors; the former generates
    the messages and publishes them to the exchange, which in turn (depending upon
    its type) publishes the message from the publisher to the queue and from the queue
    to the consumer, who picks up the message.
  prefs: []
  type: TYPE_NORMAL
- en: The point to note is that here the publisher interacts with the exchange and
    not the queue. There are various kinds of exchanges that RabbitMQ supports such
    as direct, fanout, topic, and so on. The task of the exchange is to route the
    message to one or more queues depending upon the type of exchange and the routing
    key associated with the message. So if it's a direct exchange, the message will
    be delivered to one queue bound to the exchange with the routing key matching
    the one in the message. If it's a fanout exchange, then the message is delivered
    to all the queues bound to the exchange, and the routing is totally ignored.
  prefs: []
  type: TYPE_NORMAL
- en: Installing the RabbitMQ cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: RabbitMQ is a messaging broker—an intermediary for messaging. It gives your
    applications a common platform to send and receive messages, and your messages
    a safe place to live until they are received.
  prefs: []
  type: TYPE_NORMAL
- en: Prerequisites for the setup of RabbitMQ
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Make sure you have taken care of the fact that short names are also included
    in the `/etc/hosts` file as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Short names in `/etc/hosts` are mandatory because in a RabbitMQ cluster, the
    internode communication happens using these short names.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, we have two machines in our cluster with the following mentioned
    IPs and hostnames; this information is used by the RabbitMQ daemons while starting
    the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'If the short names are not set, you will see this error: **System NOT running
    to use fully qualified hostnames**.'
  prefs: []
  type: TYPE_NORMAL
- en: Setting up a RabbitMQ server
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Ubuntu ships with RabbitMQ but it''s often not the latest version. The latest
    version can be retrieved from RabbitMQ''s Debian repository. The following shell
    script should be run for the RabbitMQ installation on Ubuntu:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Testing the RabbitMQ server
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following steps will get you the commands that are to be executed on the
    Ubuntu terminal to start the RabbitMQ server and test it. They are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Start the RabbitMQ server by running the following command on the shell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![Testing the RabbitMQ server](img/00040.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Check the server status by running the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![Testing the RabbitMQ server](img/00041.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'On each RabbitMQ instance, to enable the RabbitMQ management console, execute
    the following command and restart the RabbitMQ server running on that instance,
    by using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'To enable the RabbitMQ plugins, navigate to `/usr/lib/rabbitmq/bin` and execute
    the following command on both nodes and restart them:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Startup, shutdown, and error logs are created under the `/var/log/rabbitmq`
    directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Creating a RabbitMQ cluster
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Here are the steps that you need to execute to set up a two (or more) node
    RabbitMQ cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Considering `rmq-flc-1` and `rmq-flc-2` are the short hostnames of the two
    instances, we will start standalone RabbitMQ servers on both instances using the
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'On `rmq-flc-2`, we will stop the RabbitMQ application, reset the node, join
    the cluster, and restart the RabbitMQ application using the following commands
    (all this is being done while the RabbitMQ server is up and running on `rmq-flc-1`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Check the cluster status by running the following command on any of the machines:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The following output should be seen:![Creating a RabbitMQ cluster](img/00042.jpeg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The cluster is set up successfully.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The cluster can be accessed at `http:/` `/<hostip>:15672` (username: `guest`,
    password: `guest`), if the UI is enabled.'
  prefs: []
  type: TYPE_NORMAL
- en: Enabling the RabbitMQ UI
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Perform the following steps to enable the RabbitMQ UI:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Execute the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding command will result in the following output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Repeat the preceding steps on all nodes of the cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Restart each node using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Access the UI using the `http:``//<hostip>:15672` link. The default username
    and password is `guest`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Creating mirror queues for high availability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we talk about a special kind of queues that guarantee high
    availability over the RabbitMQ default queues. By default, the queues that we
    create are located on a single node based on the order in which they are declared,
    and this can become the single point of failure. Let's look at an example. I have
    a cluster of two RabbitMQ nodes, `rabbit1` and `rabbit2`, and I declare one exchange
    over my cluster, say, `myrabbitxchange`. Let's say by the order of execution,
    the queue is created in `rabbit1`. Now if `rabbit1` goes down, then the queue
    is gone and the clients will not be able to publish to it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus to avoid situations, we need highly available queues; they are called
    mirrored queues, which are replicated on all the nodes in the cluster. Mirrored
    queues have one master and multiple slaves, the oldest one is the master and if
    it''s not available, the oldest amongst the available nodes becomes the master.
    Messages are published to all slaves. This enhances the availability but doesn''t
    distribute the load. To create the mirror queues, use the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Mirroring can be enabled by adding a policy using the web UI. Go to the **Admin**
    tab and select **Policies** and click on **Add policy**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Specify policy **Name**, **Pattern**, **Definition**, and click on **Add Policy**,
    as shown in the following screenshot:![Creating mirror queues for high availability](img/00043.jpeg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Integrating Storm with RabbitMQ
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have installed Storm, the next step will be to integrate RabbitMQ
    with Storm, for which we will have to create a custom spout called the RabbitMQ
    spout. This spout will read the messages from the specified queue; thus, it will
    furnish the role of a consumer, and then push these messages to a downstream topology.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is how the spout code will look:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'AMQP Maven dependency that will be required to be introduced in the project
    `pom.xml`, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Creating a RabbitMQ feeder component
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we have installed the RabbitMQ cluster, all we need is to develop
    a publisher component that will publish the messages to RabbitMQ. This will be
    a simple Java component that will mimic the live feed to RabbitMQ. The basic code
    snippet for this is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Wiring the topology for the AMQP spout
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now we have the clustered queue setup ready, the AMQP spout in place, and the
    feeder component in place; let's put the last and final piece in place, that's
    the overall integration of the Storm topology.
  prefs: []
  type: TYPE_NORMAL
- en: Let's use our `WordCount` topology again and instead of `RandomSentenceSpout`
    we will use `AMQPRecvSpout`, which we designed in the previous section, *Integrating
    Storm with RabbitMQ*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code chunk needs to be modified:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Building high availability of components
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now we are at an opportune juncture to look for high availability of various
    components in the cluster. We will do this as a series of exercises wherein we
    assume that each component is installed in the clustered mode and more than one
    instance of it exists in the ecosystem.
  prefs: []
  type: TYPE_NORMAL
- en: 'The high availability of RabbitMQ can be checked only after you have a mirrored
    queue in place. Let''s assume:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We have two nodes in the RabbitMQ cluster: node1 and node2'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MyExchange` is the name of the exchange that is created for the purpose of
    this exercise'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MyQueue` is a mirrored queue that is created for this exercise'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Next, we will just run the `fixedEmitter` code we created in the *Creating
    a RabbitMQ feeder component* section. Now perform the Litmus test:'
  prefs: []
  type: TYPE_NORMAL
- en: Let's assume the queue `MyQueue` has 100 messages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now bring down node2 (this means, one node on the cluster is down)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All the 100 messages will be retained and will be visible on the console; node1
    fills in when there is an absence of node2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This behavior ensures that services are not disrupted even if a node in the
    cluster goes down.
  prefs: []
  type: TYPE_NORMAL
- en: High availability of the Storm cluster
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now let''s see the demonstration of a failover or high availability in Storm.
    The Storm framework is built in such a way that it can continue to execute as
    long as:'
  prefs: []
  type: TYPE_NORMAL
- en: It has the required number of Zookeeper connections
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It has the required number of workers on one or more supervisors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So what do the preceding statements actually mean? Well, let''s understand
    this with an example. Let''s say I am executing the `WordCount` topology on a
    Storm cluster. This cluster has the following configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: There are two Storm supervisors with four workers on each Storm supervisor,
    so a total eight workers in the cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are three Zookeeper nodes (max connections 30), so in total 30*2*3=180
    connections
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A topology is allocated with three workers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s assume when we submit this topology onto the cluster, the tasks and
    processes are spawned as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![High availability of the Storm cluster](img/00044.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding figure depicts the cluster diagrammatically and the gray workers
    are the ones that are allocated to the topology. Now we are all set to try out
    the high availability test for Storm and Zookeeper. The tests for Storm and Zookeeper
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Test 1** (all components are up and the topology is running): Kill the Nimbus
    node after the topology is submitted; you will notice that the topology will continue
    to execute normally.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Test 2** (all components are up and the topology is running): Kill one Zookeeper
    node and you will notice that the topology will continue to execute normally,
    because two of the other available Zookeepers have sufficient resources in terms
    of connections that can keep the Storm cluster up and running.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Test 3** (all components are up and the topology is running): Kill two Zookeeper
    nodes and you will notice that the topology will continue to execute normally,
    because one of the other two available Zookeepers have sufficient resources in
    terms of connections that they can keep the Storm cluster up and running.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Test 4** (all components are up and the topology is running): Kill supervisor
    2; now we have one of the gray workers on this node. So when this node goes down,
    the gray worker dies, and then because the second supervisor is not available
    it''s spawned again, this time on supervisor 1\. So all workers of the topology
    will be executing on one single supervisor now, but the system will continue to
    perform with limited resources but will not fail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guaranteed processing of the Storm cluster
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The next topic to discuss in this section is to see *Storm's guaranteed message
    processing in action*. We discussed this concept in previous chapters, but to
    understand it practically, I didn't go into depth because I wanted to introduce
    you all to the AMQP spout first. Now let's go back to the example we discussed
    in [Chapter 2](part0020_split_000.html#page "Chapter 2. Getting Started with Your
    First Topology"), *Getting Started with Your First Topology*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now as depicted in the following figure, the dash arrow flow shows that the
    events that fail to process are re-queued to the queue:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Guaranteed processing of the Storm cluster](img/00045.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Now let's tweak our `wordCount` topology a bit where we had added `AMQPRecvSpout`
    to fail the events, and see where they actually show up. Let's assume I used `FixedEmitter`
    to emit 10 events into the queue. Now I tweak my `wordCount` bolt and induce artificial
    sleep for five minutes in the execute method, so that every event is held there
    for 300 seconds (using `Thread.sleep(300)`). This will lead to its timeout as
    the default event timeout is 60 seconds.
  prefs: []
  type: TYPE_NORMAL
- en: Now when you run the topology, you will be able to see the events being re-queued
    back to RabbitMQ using the UI.
  prefs: []
  type: TYPE_NORMAL
- en: The Storm isolation scheduler
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Storm isolation scheduler was released in Storm Version 0.8.2\. This was
    a very handy feature that is very actively being used ever since its release,
    in the case of the shared Storm cluster. Let's understand its working and capability
    through an example; say, we have a four supervisor node Storm cluster with four
    slots each, so in total I have 16 slots. Now I want to employ three Storm topologies
    here, say, Topo1, Topo2, and Topo3; each has four workers allocated to it.
  prefs: []
  type: TYPE_NORMAL
- en: 'So by probable default, the scheduling behavior of the Storm distribution will
    be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|   | Supervisor 1 | Supervisor 2 | Supervisor 3 | Supervisor 4 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Topo1** | Worker 1 | Worker 2 | Worker 3 | Worker 4 |'
  prefs: []
  type: TYPE_TB
- en: '| **Topo2** | Worker 2 | Worker 1 | Worker 1 | Worker 1 |'
  prefs: []
  type: TYPE_TB
- en: '| **Topo3** | Worker 3 | Worker 3 | Worker 2 | Worker 2 |'
  prefs: []
  type: TYPE_TB
- en: Storm will respect load distribution and will spawn one worker of each topology
    on each node.
  prefs: []
  type: TYPE_NORMAL
- en: Now let's tweak the scenario a bit and introduce a requirement that Topo1 is
    a very resource-intensive topology. (I want to dedicate one supervisor entirely
    to this one so that I save on network hops.) This could be attained by the use
    of the isolation scheduler.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will have to make the following entry in the `storm.yaml` file of each Storm
    node in the cluster (Nimbus and supervisor):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The cluster is required to be restarted for this setting to take effect. This
    setting means that we have dedicated two supervisor nodes to Topo1 and it will
    be no longer be shared with other topologies being submitted to the cluster. This
    will also ensure a viable solution to multitenancy problems encountered in production.
  prefs: []
  type: TYPE_NORMAL
- en: 'The other two supervisors will be shared amongst Topo2 and Topo3\. The probable
    distribution will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|   | Supervisor 1 | Supervisor 2 | Supervisor 3 | Supervisor 4 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Topo1** | Worker 1Worker 2 | Worker 1Worker 2 |   |   |'
  prefs: []
  type: TYPE_TB
- en: '| **Topo2** |   |   | Worker 1Worker 2 | Worker 1Worker 2 |'
  prefs: []
  type: TYPE_TB
- en: '| **Topo3** |   |   | Worker 3Worker 4 | Worker 3Worker 4 |'
  prefs: []
  type: TYPE_TB
- en: So, as evident from the preceding table, Topo1 will be isolated to Supervisor1
    and 2 while Top2 and Topo3 will share the remaining eight slots on the Supervisor3
    and 4.
  prefs: []
  type: TYPE_NORMAL
- en: Quiz time
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Q.1 State whether the following sentences are true or false:'
  prefs: []
  type: TYPE_NORMAL
- en: AMQP is a STOMP protocol.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: RabbitMQ is not fail-safe.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: An AMQP client is required to publish to RabbitMQ.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A mirrored queue can recover from the failure of nodes in a cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Q.2 Fill in the blanks:'
  prefs: []
  type: TYPE_NORMAL
- en: _______________ is the exchange where messages are delivered based on the routing
    key.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: _______________ is the exchange where messages are broadcasted.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The ___________ is an implementation of the Storm spout on the AMQP consumer
    protocol.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Q.3 Execute the `WordCount` topology on a three node Storm cluster (one nimbus
    and two supervisor nodes) clubbed with a two node RabbitMQ cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: Try out various failure scenarios mentioned in the *Building high availability
    of components* section
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Induce an artificial delay in message processing to calibrate the guaranteed
    processing of the Storm topology
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you have understood the RabbitMQ implementation of the AMQP
    protocol. We completed the cluster setup and integrated the output of the Storm
    topology with the queues. We also explored and practically tested the scenarios
    of high availability and reliability for both RabbitMQ and Storm. We closed the
    chapter by touching upon the Storm schedulers. In the next chapter, we will get
    acquainted with Storm persistence using Cassandra.
  prefs: []
  type: TYPE_NORMAL
