- en: Chapter 8. Integration with Hadoop
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Executing our first sample MapReduce job using the mongo-hadoop connector
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Writing our first Hadoop MapReduce job
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running MapReduce jobs on Hadoop using streaming
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running a MapReduce job on Amazon EMR
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hadoop is a well-known open source software to process large datasets. It also
    has an API for the MapReduce programming model, which is widely used. Nearly all
    the big data solutions have some sort of support to integrate them with Hadoop
    in order to use its MapReduce framework. MongoDB has a connector as well that
    integrates with Hadoop and lets us write MapReduce jobs using the Hadoop MapReduce
    API, process the data residing in the MongoDB/MongoDB dumps, and write the result
    to the MongoDB/MongoDB dump files. In this chapter, we will look at some recipes
    about the basic MongoDB and Hadoop integration.
  prefs: []
  type: TYPE_NORMAL
- en: Executing our first sample MapReduce job using the mongo-hadoop connector
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will see how to build the mongo-hadoop connector from the
    source and set up Hadoop just for the purpose of running the examples in the standalone
    mode. The connector is the backbone that runs Hadoop MapReduce jobs on Hadoop
    using the data in Mongo.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are various distributions of Hadoop; however, we will use Apache Hadoop
    ([http://hadoop.apache.org/](http://hadoop.apache.org/)). The installation will
    be done on Ubuntu Linux. Apache Hadoop always runs on the Linux environment for
    production, and Windows is not tested for production systems. For development
    purposes, Windows can be used. If you are a Windows user, I would recommend that
    you install a virtualization environment such as VirtualBox ([https://www.virtualbox.org/](https://www.virtualbox.org/)),
    set up a Linux environment, and then install Hadoop on it. Setting up VirtualBox
    and Linux on it is not shown in this recipe, but this is not a tedious task. The
    prerequisite for this recipe is a machine with the Linux operating system on it
    and an Internet connection. The version that we will set up here is 2.4.0 of Apache
    Hadoop. At the time of writing of this book, the latest version of Apache Hadoop,
    which is supported by the mongo-hadoop connector, is 2.4.0.
  prefs: []
  type: TYPE_NORMAL
- en: A Git client is needed to clone the repository of the mongo-hadoop connector
    to the local filesystem. Refer to [http://git-scm.com/book/en/Getting-Started-Installing-Git](http://git-scm.com/book/en/Getting-Started-Installing-Git)
    to install Git.
  prefs: []
  type: TYPE_NORMAL
- en: You will also need MongoDB to be installed on your operating system. Refer to
    [http://docs.mongodb.org/manual/installation/](http://docs.mongodb.org/manual/installation/)
    and install it accordingly. Start the `mongod` instance listening to port `27017`.
    It is not expected for you to be an expert in Hadoop but some familiarity with
    it will be helpful. Knowing the concept of MapReduce is important and knowing
    the Hadoop MapReduce API will be an advantage. In this recipe, we will explain
    what is needed to get the work done. You can get more details on Hadoop and its
    MapReduce API from other sources. The wiki page at [http://en.wikipedia.org/wiki/MapReduce](http://en.wikipedia.org/wiki/MapReduce)
    gives some good information about the MapReduce programming.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will first install Java, Hadoop, and the required packages. We will start
    with installing JDK on the operating system. Type the following on the command
    prompt of the operating system:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'If the program doesn''t execute and you are told about various packages that
    contain javac and program, then we need to install Java as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This is all we need to do to install Java.
  prefs: []
  type: TYPE_NORMAL
- en: Download the current version of Hadoop from [http://www.apache.org/dyn/closer.cgi/hadoop/common/](http://www.apache.org/dyn/closer.cgi/hadoop/common/)
    and download version 2.4.0 (or the latest mongo-hadoop connector support).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'After the `.tar.gz` file is downloaded, execute the following on the command
    prompt:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Open the `etc/hadoop/hadoop-env.sh` file and replace export `JAVA_HOME = ${JAVA_HOME}`
    with export `JAVA_HOME = /usr/lib/jvm/default-java`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now get the mongo-hadoop connector code from GitHub on our local filesystem.
    Note that you don''t need a GitHub account to clone a repository. Clone the Git
    project from the operating system command prompt as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a soft link—the Hadoop installation directory is the same as the one
    that we extracted in step 3:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'For example, if Hadoop is extracted/installed in the home directory, then this
    is the command to be executed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: By default, the mongo-hadoop connector will look for a Hadoop distribution under
    the `~/hadoop-binaries` folder. So, even if the Hadoop archive is extracted elsewhere,
    we can create a soft link to it. Once this link has been created, we should have
    the Hadoop binaries in the `~/hadoop-binaries/hadoop-2.4.0/bin` path.
  prefs: []
  type: TYPE_NORMAL
- en: We will now build the mongo-hadoop connector from the source for the Apache
    Hadoop version 2.4.0\. The build-by-default builds for the latest version, so
    as of now, the `-Phadoop_version` parameter can be left out as 2.4 is the latest.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This build process will take some time to get completed.
  prefs: []
  type: TYPE_NORMAL
- en: Once the build completes successfully, we will be ready to execute our first
    MapReduce job. We will do this using a `treasuryYield` sample provided with the
    mongo-hadoop connector project. The first activity is to import the data to a
    collection in Mongo.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Assuming that the `mongod` instance is up and running and listening to port
    `27017` for connections and the current directory is the root of the mongo-hadoop
    connector code base, execute the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the import action is successful, we are left with copying two jar files
    to the `lib` directory. Execute the following in the operating system shell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The JAR built for the mongo-hadoop core to be copied was named as shown in the
    preceding section for the trunk version of the code and built for Hadoop-2.4.0\.
    Change the name of the JAR accordingly when you build it yourself for a different
    version of the connector and Hadoop. The Mongo driver can be the latest version.
    Version 2.12.0 is the latest version at the time of writing of this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, execute the following command on the command prompt of the operating system
    shell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The output should print out a lot of things; however, the following line in
    the output tells us that the map reduce job is successful:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Connect the `mongod` instance running on localhost from the mongo client and
    execute a find on the following collection:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Installing Hadoop is not a trivial task and we don't need to get into this to
    try our samples for the hadoop-mongo connector. To learn about Hadoop, its installation,
    and other things, there are dedicated books and articles available. For the purpose
    of this chapter, we will simply download the archive and extract and run the MapReduce
    jobs in the standalone mode. This is the quickest way to get going with Hadoop.
    All the steps up to step 6 are needed to install Hadoop. In the next couple of
    steps, we will clone the mongo-hadoop connector recipe. You can also download
    a stable version for your version of Hadoop at [https://github.com/mongodb/mongo-hadoop/releases](https://github.com/mongodb/mongo-hadoop/releases)
    if you prefer not to build from the source. We then build the connector for our
    version of Hadoop (2.4.0) till step 13\. Step 14 onward is what we will do to
    run the actual MapReduce job to work on the data in MongoDB. We imported the data
    to the `yield_historical.in` collection, which will be used as an input for the
    MapReduce job. Go ahead and query the collection in the mongo shell using the
    `mongo_hadoop` database to see a document. Don't worry if you don't understand
    the contents; we want to see what we intend to do with this data in this example.
  prefs: []
  type: TYPE_NORMAL
- en: The next step was to invoke the MapReduce operation on the data. The Hadoop
    command was executed giving one jar's path, (`examples/treasury_yield/build/libs/treasury_yield-1.2.1-SNAPSHOT-hadoop_2.4.jar`).
    This is the jar that contains the classes implementing the sample MapReduce operation
    for the treasury yield. The `com.mongodb.hadoop.examples.treasury.TreasuryYieldXMLConfig`
    class in this JAR file is the Bootstrap class containing the main method. We will
    visit this class soon. There are lots of configurations supported by the connector.
    The complete list of configurations can be found at [https://github.com/mongodb/mongo-hadoop/](https://github.com/mongodb/mongo-hadoop/).
    For now, we will just remember that `mongo.input.uri` and `mongo.output.uri` are
    the collections for input and output for the map reduce operations.
  prefs: []
  type: TYPE_NORMAL
- en: With the project cloned, you can now import it to any Java IDE of your choice.
    We are particularly interested in the project at `/examples/treasury_yield` and
    core present in the root of the cloned repository.
  prefs: []
  type: TYPE_NORMAL
- en: Let's look at the `com.mongodb.hadoop.examples.treasury.TreasuryYieldXMLConfig`
    class. This is the entry point for the MapReduce method and has a main method
    in it. To write MapReduce jobs for mongo using the mongo-hadoop connector, the
    main class always has to extend from `com.mongodb.hadoop.util.MongoTool`. This
    class implements the `org.apache.hadoop.Tool` interface, which has the run method
    and is implemented for us by the `MongoTool` class. All that the main method needs
    to do is execute this class using the `org.apache.hadoop.util.ToolRunner` class
    by invoking its static `run` method passing the instance of our main class (which
    is an instance of `Tool`).
  prefs: []
  type: TYPE_NORMAL
- en: 'There is a static block that loads some configurations from two XML files,
    `hadoop-local.xml` and `mongo-defaults.xml`. The format of these files (or any
    XML file) is as follows. The root node of the file is the configuration node with
    multiple property nodes under it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The property values that make sense in this context are all those that we mentioned
    in the URL provided earlier. We instantiate `com.mongodb.hadoop.MongoConfig` wrapping
    an instance of `org.apache.hadoop.conf.Configuration` in the constructor of the
    bootstrap class, `TreasuryYieldXmlConfig`. The `MongoConfig` class provides sensible
    defaults, which are enough to satisfy majority of the use cases. Some of the most
    important things that we need to set in the `MongoConfig` instance are the output
    and input format, `mapper` and `reducer` classes, output key and value of the
    mapper, and output key and value of the reducer. The input format and output format
    will always be the `com.mongodb.hadoop.MongoInputFormat` and `com.mongodb.hadoop.MongoOutputFormat`
    classes, which are provided by the mongo-hadoop connector library. For the mapper
    and reducer output key and value, we have any of the `org.apache.hadoop.io.Writable`
    implementations. Refer to the Hadoop documentation for different types of the
    Writable implementations in the `org.apache.hadoop.io` package. Apart from these,
    the mongo-hadoop connector also provides us with some implementations in the `com.mongodb.hadoop.io`
    package. For the treasury yield example, we used the `BSONWritable` instance.
    These configurable values can either be provided in the XML file that we saw earlier
    or be programmatically set. Finally, we have the option to provide them as `vm`
    arguments that we did for `mongo.input.uri` and `mongo.output.uri`. These parameters
    can be provided either in the XML or invoked directly from the code on the `MongoConfig`
    instance; the two methods are `setInputURI` and `setOutputURI`, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now look at the `mapper` and `reducer` class implementations. We will
    copy the important portion of the class here in order to analyze. Refer to the
    cloned project for the entire implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Our mapper extends the `org.apache.hadoop.mapreduce.Mapper` class. The four
    generic parameters are the key class, type of the input value, type of the output
    key, and output value. The body of the map method reads the `_id` value from the
    input document, which is the date, and extracts the year out of it. Then, it gets
    the double value from the document for the `bc10Year` field and simply writes
    to the context key-value pair where key is the year and value of the double to
    the context key value pair. The implementation here doesn't rely on the value
    of the `pKey` parameter passed, which can be used as the key instead of hardcoding
    the `_id` value in the implementation. This value is basically the same field
    that would be set using the `mongo.input.key` property in the XML or the `MongoConfig.setInputKey`
    method. If none is set, `_id` is the default value.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at the reducer implementation (with the logging statements removed):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'This class extends from `org.apache.hadoop.mapreduce.Reducer` and has four
    generic parameters: the input key, input value, output key, and output value.
    The input to the reducer is the output from the mapper, and thus, if you notice
    carefully, the type of the first two generic parameters is the same as the last
    two generic parameters of the mapper that we saw earlier. The third and fourth
    parameters are the type of the key and value emitted from the reduce. The type
    of the value is `BSONDocument` and thus we have `BSONWritable` as the type.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We now have the reduce method that has two parameters: the first one is the
    key, which is the same as the key emitted from the map function, and the second
    parameter is `java.lang.Iterable` of the values emitted for the same key. This
    is how standard map reduce functions work. For instance, if the map function gave
    the following key value pairs, (1950, 10), (1960, 20), (1950, 20), (1950, 30),
    then reduce will be invoked with two unique keys, 1950 and 1960, and the values
    for the key 1950 will be `Iterable` with (10, 20, 30), whereas that of 1960 will
    be `Iterable` of a single element (20). The reducer''s reduce function simply
    iterates though `Iterable` of the doubles, finds the sum and count of these numbers,
    and writes one key value pair where the key is the same as the incoming key and
    the output value is `BasicBSONObject` with the sum, count, and average for the
    computed values.'
  prefs: []
  type: TYPE_NORMAL
- en: There are some good samples including the Enron dataset in the examples of the
    cloned mongo-hadoop connector. If you would like to play around a bit, I would
    recommend that you take a look at these example projects and run them.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: What we saw here is a readymade sample that we executed. There is nothing like
    writing one MapReduce job ourselves to clear our understanding. In the next recipe,
    we will write one sample MapReduce job using the Hadoop API in Java and see it
    in action.
  prefs: []
  type: TYPE_NORMAL
- en: See also…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you''re wondering what the `Writable` interface is all about and why you
    should not use plain old serialization instead, then refer to this URL that gives
    the explanation by the creator of Hadoop himself: [http://www.mail-archive.com/hadoop-user@lucene.apache.org/msg00378.html](http://www.mail-archive.com/hadoop-user@lucene.apache.org/msg00378.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Writing our first Hadoop MapReduce job
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will write our first MapReduce job using the Hadoop MapReduce
    API and run it using the mongo-hadoop connector getting the data from MongoDB.
    Refer to the *Executing MapReduce in Mongo using a Java client* recipe in [Chapter
    3](ch03.html "Chapter 3. Programming Language Drivers"), *Programming Language
    Drivers* to see how MapReduce is implemented using a Java client, test data creation,
    and problem statement.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Refer to the previous *Executing our first sample MapReduce job using the mongo-hadoop
    connector* recipe to set up the mongo-hadoop connector. The prerequisites of this
    recipe and the *Executing MapReduce in Mongo using a Java client* recipe from
    [Chapter 3](ch03.html "Chapter 3. Programming Language Drivers"), *Programming
    Language Drivers* are all that we need for this recipe. This is a maven project
    and thus maven needs to be set up and installed. Refer to the *Connecting to the
    Single node from a Java client* recipe in [Chapter 1](ch01.html "Chapter 1. Installing
    and Starting the Server"), *Installing and Starting the Server* where we provided
    the steps to set up maven in Windows; this project is built on Ubuntu Linux and
    the following is the command that you need to execute in the operating system
    shell to get maven:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have a Java `mongo-hadoop-mapreduce-test` project, which can be downloaded
    from the Packt website. The project is targeted to achieve the same use case that
    we achieved in the recipes in [Chapter 3](ch03.html "Chapter 3. Programming Language
    Drivers"), *Programming Language Drivers* where we used MongoDB's MapReduce framework.
    We had invoked that MapReduce job using the Python and Java client on previous
    occasions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'On the command prompt with the current directory in the root of the project,
    where the `pom.xml` file is present, execute the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The JAR file, `mongo-hadoop-mapreduce-test-1.0.jar`, will be built and kept
    in the target directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'With the assumption that the CSV file is already imported to the `postalCodes`
    collection, execute the following command with the current directory still in
    the root of the `mongo-hadoop-mapreduce-test` project that we just built:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the MapReduce job is completed, open the mongo shell by typing the following
    on the operating system command prompt and execute the following query in the
    shell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Compare the output to the one that we got earlier when we executed the MapReduce
    jobs using mongo's map reduce framework (in [Chapter 3](ch03.html "Chapter 3. Programming
    Language Drivers"), *Programming Language Drivers*).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have kept the classes very simple and with the bare minimum things that
    we need. We just have three classes in our project: `TopStateMapReduceEntrypoint`,
    `TopStateReducer`, and `TopStatesMapper`, all in the same `com.packtpub.mongo.cookbook`
    package. The mapper''s `map` function just writes a key value pair to the context,
    where the key is the name of the state and value is an integer value, one. The
    following is the code snippet from the `mapper` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'What the reducer gets is the same key that is the list of the states and Iterable
    of integer value, one. All that we do is write the same name of the state and
    sum of the iterables to the context. Now, as there is no size method in Iterable
    that can give the count in constant time, we are left with adding up all the ones
    that we get in linear time. The following is the code in the reducer method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'We write the text string that is the key and value that is the JSON document
    containing the count to the context. The mongo-hadoop connector is then responsible
    for writing the `postalCodesHadoopmrOut` document to the output collection that
    we have, with the `_id` field the same as the key emitted. Thus, when we execute
    the following, we get the top five states with the most number of cities in our
    database:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, the main method of the main entry point class is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: All we do is wrap the `org.apache.hadoop.conf.Configuration` object with the
    `com.mongodb.hadoop.MongoConfig` instance to set various properties and then submit
    the MapReduce job for execution using ToolRunner.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We executed a simple MapReduce job on Hadoop using the Hadoop API, sourcing
    the data from MongoDB, and writing the data to the MongoDB collection. What if
    we want to write the `map` and `reduce` functions in a different language? Fortunately,
    this is possible using a concept called Hadoop streaming where `stdout` is used
    as a means to communicate between the program and Hadoop MapReduce framework.
    In the next recipe, we will demonstrate how to use Python to implement the same
    use case that we did in this recipe using Hadoop streaming.
  prefs: []
  type: TYPE_NORMAL
- en: Running MapReduce jobs on Hadoop using streaming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In our previous recipe, we implemented a simple MapReduce job using the Java
    API of Hadoop. The use case was the same as what we did in the recipes in [Chapter
    3](ch03.html "Chapter 3. Programming Language Drivers"), *Programming Language
    Drivers* where we implemented MapReduce using the Mongo client APIs in Python
    and Java. In this recipe, we will use Hadoop streaming to implement MapReduce
    jobs.
  prefs: []
  type: TYPE_NORMAL
- en: The concept of streaming works on communication using `stdin` and `stdout`.
    You can get more information on Hadoop streaming and how it works at [http://hadoop.apache.org/docs/r1.2.1/streaming.html](http://hadoop.apache.org/docs/r1.2.1/streaming.html).
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Refer to the *Executing our first sample MapReduce job using the mongo-hadoop
    connector* recipe in this chapter to see how to set up Hadoop for development
    purposes and build the mongo-hadoop project using Gradle. As far as the Python
    libraries are concerned, we will be installing the required library from the source;
    however, you can use `pip` (Python's package manager) to set up if you do not
    wish to build from the source. We will also see how to set up pymongo-hadoop using
    `pip`.
  prefs: []
  type: TYPE_NORMAL
- en: Refer to recipe *Connecting to a single node using a Python client*, in [Chapter
    1](ch01.html "Chapter 1. Installing and Starting the Server"), *Installing and
    Starting the Server* on how to install PyMongo for your host operating system.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will first build pymongo–hadoop from the source. With the project cloned
    to the local filesystem, execute the following in the root of the cloned project:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: After you enter the password, the setup will continue to be installed on pymongo-hadoop
    on your machine.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This is all we need to do to build pymongo-hadoop from the source. However,
    if you had chosen not to build from the source, you can execute the following
    command in the operating system shell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'After installing pymongo-hadoop in either way, we will now implement our `mapper`
    and `reducer` function in Python. The `mapper` function is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Now for the `reducer` function, which will look like the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The environment variables, `$HADOOP_HOME` and `$HADOOP_CONNECTOR_HOME`, should
    point to the base directory of Hadoop and the mongo-hadoop connector project,
    respectively. Now, we will invoke the `MapReduce` function using the following
    command in the operating system shell. The code available with the book on the
    Packt website has the `mapper`, `reduce` Python script, and shell script that
    will be used to invoke the `mapper` and `reducer` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The `mapper.py` and `reducer.py` files are present in the current directory
    when executing this command.
  prefs: []
  type: TYPE_NORMAL
- en: 'On executing the command, which should take some time for the successful execution
    of the MapReduce job, open the mongo shell by typing the following command on
    the operating system command prompt and execute the following query from the shell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Compare the output to the one that we got earlier when we executed the MapReduce
    jobs using mongo's MapReduce framework in [Chapter 3](ch03.html "Chapter 3. Programming
    Language Drivers"), *Programming Language Drivers*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's look at steps 5 and 6 where we write the `mapper` and `reducer` functions.
    We define a `map` function that accepts a list of all the documents. We iterate
    through these and yield documents, where the `_id` field is the name of the key
    and the count value field has a value of one. There will be the same number of
    documents yielded as the total number of input documents.
  prefs: []
  type: TYPE_NORMAL
- en: We instantiate `BSONMapper` finally, which accepts the `mapper` function as
    the parameter. The function returns a generator object, which is then used by
    this `BSONMapper` class to feed the value to the MapReduce framework. All we need
    to remember is that the `mapper` function needs to return a generator (which is
    returned as we call yield in the loop) and then instantiate the `BSONMapper` class,
    which is provided to us by the `pymongo_hadoop` module. For the intrigued, you
    can choose to look at the source code under the project cloned on our local filesystem
    in the `streaming/language_support/python/pymongo_hadoop/mapper.py` file and see
    what it does. It is a small and simple-to-understand piece of code.
  prefs: []
  type: TYPE_NORMAL
- en: For the `reducer` function, we get the key and list of documents for this key
    as the value. The key is the same as the value of the `_id` field emitted from
    the document in the `map` function. We simply return a new document here with
    `_id` as the name of the state and count is the number of documents for this state.
    Remember that we return a document and not emit one as we did in map. Finally,
    we instantiate `BSONReducer` and pass the `reducer` function. The source code
    under the project cloned on our local filesystem in the `streaming/language_support/python/pymongo_hadoop/reducer.py`
    file has the implementation of the `BSONReducer` class.
  prefs: []
  type: TYPE_NORMAL
- en: 'We finally invoked the command in the shell to initiate the MapReduce job that
    uses streaming. A few things to note here are that we need two JAR files: one
    in `share/hadoop/tools/lib` of the Hadoop distribution and one in the mongo-hadoop
    connector, which is present in the `streaming/build/libs/` directory. The input
    and output formats are `com.mongodb.hadoop.mapred.MongoInputFormat` and `com.mongodb.hadoop.mapred.MongoOutputFormat`,
    respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: As we saw earlier, `sysout` and `sysin` forms the backbone of streaming. So,
    basically, we need to encode our BSON objects to write to `sysout`, and then,
    we should be able to read `sysin` to convert the content to the BSON objects again.
    For this purpose, the mongo-hadoop connector provides us with two framework classes,
    `com.mongodb.hadoop.streaming.io.MongoInputWriter` and `com.mongodb.hadoop.streaming.io.MongoOutputReader`
    to encode and decode from and to the BSON objects. These classes extend from `org.apache.hadoop.streaming.io.InputWriter`
    and `org.apache.hadoop.streaming.io.OutputReader`, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: The value of the `stream.io.identifier.resolver.class` property is given as
    `com.mongodb.hadoop.streaming.io.MongoIdentifierResolver`. This class extends
    from `org.apache.hadoop.streaming.io.IdentifierResolver` and gives us a chance
    to register our implementations of `org.apache.hadoop.streaming.io.InputWriter`
    and `org.apache.hadoop.streaming.io.OutputReader` with the framework. We also
    register the output key and output value class using our custom `IdentifierResolver`.
    Just remember to use this resolver always in case you are using streaming with
    the mongo-hadoop connector.
  prefs: []
  type: TYPE_NORMAL
- en: We finally execute the `mapper` and `reducer` python functions, which we discussed
    earlier. An important thing to remember is that do not print out logs to `sysout`
    from the `mapper` and `reducer` functions. The `sysout` and `sysin` mapper and
    reducer are the means of communication, and writing logs to it can yield undesirable
    behavior. As we can see in the example, write either to standard error (`stderr`)
    or a log file.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When using a multiline command in Unix, you continue the command on the next
    line using `\`. However, remember not to have spaces after `\`.
  prefs: []
  type: TYPE_NORMAL
- en: Running a MapReduce job on Amazon EMR
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This recipe involves running the MapReduce job on the cloud using AWS. You
    will need an AWS account in order to proceed. Register with AWS at [http://aws.amazon.com/](http://aws.amazon.com/).
    We will see how to run a MapReduce job on the cloud using **Amazon Elastic Map
    Reduce** (**Amazon EMR**). Amazon EMR is a managed MapReduce service provided
    by Amazon on the cloud. Refer to [https://aws.amazon.com/elasticmapreduce/](https://aws.amazon.com/elasticmapreduce/)
    for more details. Amazon EMR consumes data, binaries/JARs, and so on from AWS
    S3 bucket, processes them and writes the results back to S3 bucket. **Amazon Simple
    Storage Service** (**Amazon S3**) is another service by AWS for data storage on
    the cloud. Refer to [http://aws.amazon.com/s3/](http://aws.amazon.com/s3/) for
    more details on Amazon S3\. Though we will use the mongo-hadoop connector, an
    interesting fact is that we won''t require a MongoDB instance to be up and running.
    We will use the MongoDB data dump stored in an S3 bucket for our data analysis.
    The MapReduce program will run on the input BSON dump and generate the result
    BSON dump in the output bucket. The logs of the MapReduce program will be written
    to another bucket dedicated for logs. The following figure gives us an idea of
    how our setup would look at a high level:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Running a MapReduce job on Amazon EMR](img/B04831_08_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will use the same Java sample as we did in the *Writing our first Hadoop
    MapReduce job* recipe for this recipe. To know more about the `mapper` and `reducer`
    class implementation, you can refer to the *How It works* section of the same
    recipe. We have a `mongo-hadoop-emr-test` project available with the code that
    can be downloaded from the Packt website, which is used to create a MapReduce
    job on the cloud using the AWS EMR APIs. To simplify things, we will upload just
    one JAR to the S3 bucket to execute the MapReduce job. This JAR will be assembled
    using a BAT file for Windows and a shell script on Unix-based operating systems.
    The `mongo-hadoop-emr-test` Java project has the `mongo-hadoop-emr-binaries` subdirectory
    containing the necessary binaries along with the scripts to assemble them in one
    JAR.
  prefs: []
  type: TYPE_NORMAL
- en: The assembled `mongo-hadoop-emr-assembly.jar` file is also provided in the subdirectory.
    Running the `.bat` or `.sh` file will delete this JAR and regenerate the assembled
    JAR, which is not mandatory. The already provided assembled JAR is good enough
    and will work just fine. The Java project contains subdirectory data with a `postalCodes.bson`
    file in it. This is the BSON dump generated out of the database containing the
    `postalCodes` collection. The `mongodump` utility provided with the mongo distribution
    is used to extract this dump.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first step of this exercise is to create a bucket on S3\. You can choose
    to use an existing bucket; however, for this recipe, I am creating a `com.packtpub.mongo.cookbook.emr-in`
    bucket. Remember that the name of the bucket has to be unique across all the S3
    buckets and you will not be able to create a bucket with this very name. You will
    have to create one with a different name and use it in place of `com.packtpub.mongo.cookbook.emr-in`
    that is used in this recipe.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Do not create bucket names with an underscore (`_`); instead, use a hyphen (`-`).
    The bucket creation with an underscore will not fail; however, the MapReduce job
    later will fail as it doesn't accept underscores in the bucket names.
  prefs: []
  type: TYPE_NORMAL
- en: We will upload the assembled JAR files and a `.bson` file for the data to the
    newly created (or existing) S3 bucket. To upload the files, we will use the AWS
    web console. Click on the **Upload** button and select the assembled JAR file
    and the `postalCodes.bson` file to be uploaded to the S3 bucket. After uploading,
    the contents of the bucket should look as follows:![How to do it…](img/B04831_08_01.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The following steps are to initiate the EMR job from the AWS console without
    writing a single line of code. We will also see how to initiate this using AWS
    Java SDK. Follow steps 4 to 9 if you are looking to initiate the EMR job from
    the AWS console. Follow steps 10 and 11 to start the EMR job using the Java SDK.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will first initiate a MapReduce job from the AWS console. Visit [https://console.aws.amazon.com/elasticmapreduce/](https://console.aws.amazon.com/elasticmapreduce/)
    and click on the **Create Cluster** button. In the **Cluster Configuration** screen,
    enter the details as shown in the image, except for the logging bucket, which
    you need to select as your bucket to which the logs need to be written. You can
    also click on the folder icon next to the textbox for the bucket name and select
    the bucket present for your account to be used as the logging bucket.![How to
    do it…](img/B04831_08_07.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The termination protection option is set to **No** as this is a test instance.
    In case of any error, we would rather want the instances to terminate in order
    to avoid keeping them running and incurring charges.
  prefs: []
  type: TYPE_NORMAL
- en: In the **Software Configuration** section, select the **Hadoop version** as
    **2.4.0** and **AMI version** as **3.1.0 (hadoop 2.4.0)**. Remove the additional
    applications by clicking on the cross next to their names, as shown in the following
    image:![How to do it…](img/B04831_08_08.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the **Hardware Configuration** section, select the **EC2 instance type**
    as **m1.medium**. This is the minimum that we need to select for the Hadoop version
    2.4.0\. The number of instances for the slave and task instances is zero. The
    following image shows the configuration that is selected:![How to do it…](img/B04831_08_09.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the **Security and Access** section, leave all the default values. We also
    have no need for a **Bootstrap Action**, so leave this as well.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The final step is to set up **Steps** for the MapReduce job. In the **Add step**
    drop down, select the **Custom JAR** option, and then select the **Auto-terminate**
    option as **Yes**, as shown in the following image:![How to do it…](img/B04831_08_10.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now click on the **Configure** and **Add** button and enter the details.
  prefs: []
  type: TYPE_NORMAL
- en: The value of the **JAR S3 Location** is given as `s3://com.packtpub.mongo.cookbook.emr-in/mongo-hadoop-emr-assembly.jar`.
    This is the location in my input bucket; you need to change the input bucket as
    per your own input bucket. The name of the JAR file would be same.
  prefs: []
  type: TYPE_NORMAL
- en: 'Enter the following arguments in the **Arguments** text area; the name of the
    main class is the first in the list:'
  prefs: []
  type: TYPE_NORMAL
- en: '`com.packtpub.mongo.cookbook.TopStateMapReduceEntrypoint`'
  prefs: []
  type: TYPE_NORMAL
- en: '`-Dmongo.job.input.format=com.mongodb.hadoop.BSONFileInputFormat`'
  prefs: []
  type: TYPE_NORMAL
- en: '`-Dmongo.job.mapper=com.packtpub.mongo.cookbook.TopStatesMapper`'
  prefs: []
  type: TYPE_NORMAL
- en: '`-Dmongo.job.reducer=com.packtpub.mongo.cookbook.TopStateReducer`'
  prefs: []
  type: TYPE_NORMAL
- en: '`-Dmongo.job.output=org.apache.hadoop.io.Text`'
  prefs: []
  type: TYPE_NORMAL
- en: '`-Dmongo.job.output.value=org.apache.hadoop.io.IntWritable`'
  prefs: []
  type: TYPE_NORMAL
- en: '`-Dmongo.job.output.value=org.apache.hadoop.io.IntWritable`'
  prefs: []
  type: TYPE_NORMAL
- en: '`-Dmongo.job.output.format=com.mongodb.hadoop.BSONFileOutputFormat`'
  prefs: []
  type: TYPE_NORMAL
- en: '`-Dmapred.input.dir=s3://com.packtpub.mongo.cookbook.emr-in/postalCodes.bson`'
  prefs: []
  type: TYPE_NORMAL
- en: '`-Dmapred.output.dir=s3://com.packtpub.mongo.cookbook.emr-out/`'
  prefs: []
  type: TYPE_NORMAL
- en: The value of the final two arguments contains the input and output bucket used
    for my MapReduce sample; this value will change according to your own input and
    output buckets. The value of Action on failure would be Terminate. The following
    image shows the values filled in; click on **Save** after all these details have
    been entered:![How to do it…](img/B04831_08_11.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now click on the **Create Cluster** button. This will take some time to provision
    and start the cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the following few steps, we will create a MapReduce job on EMR using the
    AWS Java API. Import the `EMRTest` project provided with the code samples to your
    favorite IDE. Once imported, open the `com.packtpub.mongo.cookbook.AWSElasticMapReduceEntrypoint`
    class.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There are five constants that need to be changed in the class. They are the
    Input, Output, and Log bucket that you will use for your example and the AWS access
    and secret key. The access key and secret key act as the username and password
    when you use AWS SDK. Change these values accordingly and run the program. On
    successful execution, it should give you a job ID for the newly initiated job.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Irrespective of how you initiated the EMR job, visit the EMR console at [https://console.aws.amazon.com/elasticmapreduce/](https://console.aws.amazon.com/elasticmapreduce/)
    to see the status of your submitted ID. The Job ID that you can see in the second
    column of your initiated job will be same as the job ID printed to the console
    when you executed the Java program (if you initiated using the Java program).
    Click on the name of the job initiated, which should direct you to the job details
    page. The hardware provisioning will take some time, and then finally, your map
    reduce step will run. Once the job has been completed, the status of the job should
    look as follows on the Job details screen:![How to do it…](img/B04831_08_04.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'When expanded, the **Steps** section should look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it…](img/B04831_08_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Click on the stderr link below the Log files section to view all the logs' output
    for the MapReduce job.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that the MapReduce job is complete, our next step is to see the results
    of it. Visit the S3 console at [https://console.aws.amazon.com/s3](https://console.aws.amazon.com/s3)
    and visit the output bucket. In my case, the following is the content of the out
    bucket:![How to do it…](img/B04831_08_06.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `part-r-0000.bson` file is of our interest. This file contains the results
    of our MapReduce job.
  prefs: []
  type: TYPE_NORMAL
- en: 'Download the file to your local filesystem and import to a running mongo instance
    locally, using the mongorestore utility. Note that the restore utility for the
    following command expects a mongod instance to be up and running and listening
    to port `27017` with the `part-r-0000.bson` file in the current directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, connect to the `mongod` instance using the mongo shell and execute the
    following query:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'We will see the following results for the query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: This is the expected result for the top five results. If we compare the results
    that we got in *Executing MapReduce in Mongo using a Java client* from [Chapter
    3](ch03.html "Chapter 3. Programming Language Drivers"), *Programming Language
    Drivers* using Mongo's MapReduce framework and the *Writing our first Hadoop MapReduce
    job* recipe in this chapter, we can see that the results are identical.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Amazon EMR is a managed Hadoop service that takes care of the hardware provisioning
    and keeps you away from the hassle of setting up your own cluster. The concepts
    related to our MapReduce program have already been covered in the *Writing our
    first Hadoop MapReduce job* recipe and there is nothing more to mention. One thing
    that we did was to assemble the JARs that we need in one big fat JAR to execute
    our MapReduce job. This approach is okay for our small MapReduce job; in case
    of larger jobs where a lot of third-party JARs are needed, we will have to go
    for an approach where we will add the JARs to the `lib` directory of the Hadoop
    installation and execute in the same way as we did in our MapReduce job that we
    executed locally. Another thing that we did differently from our local setup was
    not to use a `mongid` instance to source the data and write the data to, but instead,
    we used the BSON dump files from the mongo database as an input and wrote the
    output to the BSON files. The output dump will then be imported to a mongo database
    locally and the results will be analyzed. It is pretty common to have the data
    dumps uploaded to S3 buckets, and running analytics jobs on this data that has
    been uploaded to S3 on the cloud using cloud infrastructure is a good option.
    The data accessed from the buckets by the EMR cluster need not have public access
    as the EMR job runs using our account's credentials; we are good to access our
    own buckets to read and write data/logs.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After trying out this simple MapReduce job, it is highly recommended that you
    get to know about the Amazon EMR service and all its features. The developer's
    guide for EMR can be found at [http://docs.aws.amazon.com/ElasticMapReduce/latest/DeveloperGuide/](http://docs.aws.amazon.com/ElasticMapReduce/latest/DeveloperGuide/).
  prefs: []
  type: TYPE_NORMAL
- en: There is a sample MapReduce job in the Enron dataset given as part of the mongo-hadoop
    connector's examples. It can be found at [https://github.com/mongodb/mongo-hadoop/tree/master/examples/elastic-mapreduce](https://github.com/mongodb/mongo-hadoop/tree/master/examples/elastic-mapreduce).
    You can choose to implement this example as well on Amazon EMR as per the given
    instructions.
  prefs: []
  type: TYPE_NORMAL
