- en: Defining GlusterFS Storage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Every day, applications require faster storage that can sustain thousands of
    concurrent I/O requests. GlusterFS is a highly-scalable, redundancy filesystem
    that can deliver high-performance I/O to many clients simultaneously. We will
    define the core concept of a cluster and then introduce how GlusterFS plays an
    important role.
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding chapter, we went through the different aspects of designing
    solutions to provide high availability and performance to applications that have
    many requirements. In this chapter, we'll go through solving a very specific problem,
    that is, storage.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the core concept of a cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The reason for choosing GlusterFS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explaining **software-defined storage** (**SDS**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring the differences between file, object, and block storage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explaining the need for high performance and highly available storage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter will focus on defining GlusterFS. You can refer to the project's
    home page at [https://github.com/gluster/glusterfs](https://github.com/gluster/glusterfs) or
    [https://www.gluster.org/](https://www.gluster.org/).
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, the project's documentation can be found at [https://docs.gluster.org/en/latest/](https://docs.gluster.org/en/latest/).
  prefs: []
  type: TYPE_NORMAL
- en: What is a cluster?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can leverage the many advantages of SDS, which allows for easy scalability
    and enhanced fault tolerance. GlusterFS is a piece of software that can create
    highly scalable storage clusters while providing maximum performance.
  prefs: []
  type: TYPE_NORMAL
- en: Before we go through how we can solve this specific need, we first need to define
    what a cluster is, why it exists, and what problems a cluster might be able to
    solve.
  prefs: []
  type: TYPE_NORMAL
- en: Computing a cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Put simply, a cluster is a set of computers (often called nodes) that work in
    tandem on the same workload and can distribute loads across all available members
    of the cluster to increase performance, while, at the same time, allowing for
    self-healing and availability. Note that the term **server** wasn't used before
    as, in reality, any computer can be added to a cluster. Made from a simple Raspberry
    Pi to multiple CPU servers, clusters can be made from a small two-node configuration
    to thousands of nodes in a data center.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of a cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d1b1a48f-f1dc-40ea-971b-5bc322251318.png)'
  prefs: []
  type: TYPE_IMG
- en: Technically speaking, clustering allows workloads to scale performance by adding
    servers of the same kind with similar resource characteristics. Ideally, a cluster
    will have homogeneous hardware to avoid problems where nodes have different performance
    characteristics and, at the same time, make maintenance reasonably identical—this
    means hardware with the same CPU family, memory configuration, and software. The
    idea of adding nodes to a cluster allows you to compute workloads to decrease
    their processing time. Depending on the application, compute times can sometimes
    even decrease linearly.
  prefs: []
  type: TYPE_NORMAL
- en: To further understand the concept of a cluster, imagine that you have an application
    that takes historical financial data. The application then receives such data
    and creates a forecast based on the stored information. On a single node, the
    forecast process (processes on a cluster are typically named jobs) takes roughly
    six days to complete, as we're dealing with several **terabytes** (**TB**) of
    data. Adding an extra node with the same characteristics decreases the processing
    time to four days. Adding a third node further decreases the time it takes to
    complete to three days.
  prefs: []
  type: TYPE_NORMAL
- en: Note that while we added three times the number of compute resources, the compute
    time only decreased by approximately half. Some applications can scale performance
    linearly, while others don't have the same scalability, requiring more and more
    resources for fewer gains, up to the point of diminishing returns. Adding more
    resources to obtain minimal time gain is not cost-effective.
  prefs: []
  type: TYPE_NORMAL
- en: 'With all this in mind, we can point out several characteristics that define
    a cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: It can help reduce processing times by adding compute resources
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It can scale both vertically and horizontally
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It can be redundant, that is, if one node fails, others should take the workload
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It can allow for increased resources to be available for applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is a single pool of resources rather than individual servers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It has no single point of failure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Storage clusters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have an understanding of how to compute a cluster, let's move on
    to another application of clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of aggregating compute resources to decrease processing times, a storage
    cluster's main functionality is to aggregate available space to provide maximum
    space utilization while, at the same time, providing some form of redundancy.
    With the increased need for storing large amounts of data comes the need of being
    able to do it at a lower cost, while still maintaining increased data availability.
    Storage clusters help to solve this problem by allowing single monolithic storage
    nodes to work together as a large pool of available storage space. Thus, it allows
    storage solutions to reach the petascale mark without the need to deploy specialized
    proprietary hardware.
  prefs: []
  type: TYPE_NORMAL
- en: For example, say that we have a single node with 500 TB of available space and
    we need to achieve the 1-**Petabyte** (**PB**) mark while providing redundancy.
    This individual node becomes a single point of failure because, if it goes down,
    then there's no way the data can be accessed. Additionally, we've reached the
    maximum **hard disk drive** (**HDD**) capacity available. In other words, we can't
    scale horizontally.
  prefs: []
  type: TYPE_NORMAL
- en: To solve this problem, we can add two more nodes with the same configuration,
    as the already existing one provides a total of 1 PB of available space. Now,
    let's do some math here, 500 TB times 3 should be approximately 1.5 PB, correct?
    The answer is most definitely yes. However, since we need to provide high availability
    to this solution, the third node acts as a backup, making the solution tolerate
    a single-node failure without interrupting the client's communication. This capability
    to allow node failures is all thanks to the power of SDS and storage clusters,
    such as GlusterFS, which we'll explore next.
  prefs: []
  type: TYPE_NORMAL
- en: What is GlusterFS?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: GlusterFS is an open source project by Gluster, which was acquired by Red Hat,
    Inc. in 2011\. This acquisition does not mean that you have to acquire a Red Hat
    subscription or pay Red Hat to use it since, as previously mentioned, it is an
    open source project; therefore, you can freely install it, look at its source
    code, and even contribute to the project. Although Red Hat offers paid solutions
    based on GlusterFS, we will talk about the **open source software** (**OSS**)
    and project itself in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram is the number of **Contributors** and **Commits** in
    the Gluster project:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a6c3ccf4-03a9-4fef-b25f-b5ec04db0748.png)'
  prefs: []
  type: TYPE_IMG
- en: To understand GlusterFS, we must understand how it differs from traditional
    storage. To do this, we need to understand the concepts behind SDS, including
    what GlusterFS is.
  prefs: []
  type: TYPE_NORMAL
- en: 'Traditional storage is an industry-standard storage array with proprietary
    software in it that is bound to the hardware vendor. All this restricts you to
    the following set of rules,set by your storage provider:'
  prefs: []
  type: TYPE_NORMAL
- en: Scalability limitations
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Hardware compatibility limitations
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Client-operating system limitations
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Configuration limitations
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Vendor lock-in
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: SDS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With SDS, many, if not all, of the preceding limitations are gone, since it
    provides impressive scalability by not depending on any hardware. You can fundamentally
    take an industry-standard server from any vendor that contains the storage you
    require and add it to your storage pool. By only doing this one simple step, you
    already overcome four of the preceding limitations.
  prefs: []
  type: TYPE_NORMAL
- en: Cost reduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The example from *SDS* section, highly reduces the **operating expense** (**OPEX**) costs,
    as you do not have to buy additional highly-priced expansion shelves for an existing
    vendor storage array that can take weeks to arrive and be installed. You can quickly
    grab a server that you have stored in the corner of your data center, and use
    it to provide storage space for your existing applications. This process is called
    plugin scalability and is present in most of the open source SDS projects out
    there. In theory, the sky is the limit when it comes to scalability with SDS.
  prefs: []
  type: TYPE_NORMAL
- en: Scalability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: SDS scales when you add new servers to your storage pools and also increases
    the resilience of your storage cluster. Depending on what configuration you have,
    data is spread across multiple member nodes providing additional high availability
    by mirroring or creating parity for your data.
  prefs: []
  type: TYPE_NORMAL
- en: Control
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You also need to understand that SDS does not create space out of nothing, nor
    does it separate the concept of storage from hardware—such as hard drives, **solid
    state drives** (**SSD**), or any hardware device that is designed to store information.
    These hardware devices will always be where the actual data is stored. SDS adds
    a logical layer that allows you to control where and how you store this data.
    It leverages this with its most fundamental components, that is, with an **application
    programming interface** (**API**) that allows you to manage and maintain your
    storage cluster and logical volumes, which provide the storage capacity to your
    other servers, applications, and even monitoring agents that self-heal the cluster
    in the event of degradation.
  prefs: []
  type: TYPE_NORMAL
- en: The market is moving toward SDS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: SDS is the future, and this is where the storage industry is moving. In fact,
    it is predicted that, in the next few years, approximately 70% of all current
    storage arrays will be available as software-only solutions or **virtual storage
    appliances** (**VSAs**). Traditional **network-attached storage** (**NAS**) solutions
    are 30% more expensive than current SDS implementations, and mid-range disk arrays
    are even more costly. Taking all this into account alongside the fact that data
    consumption is growing approximately 40% in enterprise every year, with a cost
    decline of only 25%, you can see why we are moving toward an SDS world in the
    near future.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the number of applications that are running public, private, and hybrid
    clouds, consumer and business data consumption is growing exponentially and ceaselessly.
    This data is usually mission-critical and requires a high level of resiliency.
    The following is a list of some of these applications:'
  prefs: []
  type: TYPE_NORMAL
- en: E-commerce and online storefronts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Financial applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enterprise resource planning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Health care
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Big data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Customer relationship management
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When companies store this type of data (called **bulk data**), they not only
    need to archive it, but they also need to access it, and with the lowest latency
    possible. Imagine a scenario where you are sent to take X-rays during your doctor's
    appointment, and when you arrive, they tell you that you have to wait for a week
    to get your scans because they have no storage space available to save your images.
    Naturally, this scenario will not happen because every hospital has a highly efficient
    procurement process, where they can predict usage based on their storage consumption
    and decide when to start the purchase and installation of new hardware—but you
    get the idea. It is much faster and more efficient to install a POSIX-standard
    server into your SDS layer and be ready to go.
  prefs: []
  type: TYPE_NORMAL
- en: Massive storage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many other companies also require data lakes as secondary storage, mainly to
    store data in its raw form for analysis, real-time analytics, machine learning,
    and more. SDS is excellent for this type of storage, mainly because the maintenance
    required is minimal and also for the economic reasons that we discussed previously.
  prefs: []
  type: TYPE_NORMAL
- en: We have been talking mainly about how economic and scalable SDS is, but it is
    also important to mention the high flexibility that it brings to the table. SDS
    can be used for everything from archiving data and storing reach media to providing
    storage for **virtual machines** (**VMs**), as an endpoint for object storage
    in your private cloud, and even in containers. It can be deployed on any of the
    previously mentioned infrastructures. It can run on your public cloud of choice,
    in your current on-premises virtual infrastructure, and even in a Docker container
    or Kubernetes pod. In fact, it's so flexible that you can even integrate Kubernetes
    with GlusterFS using a RESTful management interface called *heketi* that dynamically
    provisions volumes every time you require persistent volumes for your pods.
  prefs: []
  type: TYPE_NORMAL
- en: Block, file, and object storage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have gone through why SDS is the future of next-generation workloads,
    it is time to dig a little deeper into the types of storage that we can achieve
    using SDS.
  prefs: []
  type: TYPE_NORMAL
- en: Traditional **storage area network** (**SAN**) and NAS solutions more commonly
    serve storage using protocols such as **internet small computer systems interface** (**iSCSI**),
    **fibre channel** (**FC**), **fibre** **channel over ethernet** (**FCoE**), **network
    file system** (**NFS**), and **server message block** (**SMB**)/**common internet
    file system** (**CIFS**). However, because we are moving more toward the cloud,
    our storage needs change and this is where object storage comes into play. We
    will explore what object storage is and how it compares to block and file storage.
    GlusterFS is also a file storage solution, but it has block and object storage
    capabilities that can be configured further down the line.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram displays block, file, and object storage:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/21326729-1160-4c53-b272-9745456b4f9a.png)'
  prefs: []
  type: TYPE_IMG
- en: Block storage, file storage, and object storage work in very different ways
    when it comes to how the client stores data in them—causing their use cases to
    be completely different.
  prefs: []
  type: TYPE_NORMAL
- en: Block storage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A SAN is where block storage is mainly utilized, using protocols such as FC, or iSCSI,
    which are essentially mappings of the **Small Computer System Interface** (**SCSI**) protocol
    over FC and TCP/IP, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'A typical FC SAN looks like the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/37d1c291-1ddc-4951-ad4e-035dda119178.png)'
  prefs: []
  type: TYPE_IMG
- en: 'A typical iSCSI SAN looks like the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/45a4ac04-e210-4c3b-bb83-57316dabb29a.png)'
  prefs: []
  type: TYPE_IMG
- en: Data is stored in logical block addresses. When retrieving data, the application
    usually says—*I want X number of blocks from address XXYYZZZZ*. This process tends
    to be very fast (less than a millisecond), making this type of storage very low
    on latency, a very transactional-oriented type of storage form, and ideal for
    random access. However, it also has its disadvantages when it comes to sharing
    across multiple systems. This is because block storage usually presents itself
    in its raw form, and you require a filesystem on top of it that can support multiple
    writes across different systems without corruption—in other words, a clustered
    filesystem.
  prefs: []
  type: TYPE_NORMAL
- en: This type of storage also has some downsides when it comes to high availability
    or disaster recovery; because it is presented in its raw form, the storage controllers
    and managers are, therefore, not aware of how this storage is being used. So,
    when it comes to replicate its data to a recovery point, it only takes blocks
    into account, and some filesystems are terrible at reclaiming or zeroing blocks,
    which leads to unused blocks being replicated as well, thus leading to deficient
    storage utilization.
  prefs: []
  type: TYPE_NORMAL
- en: Because of its advantages and low latency, block storage is perfect for structured
    databases, random read/write operations, and to store multiple VM images that
    query disks with hundreds, if not thousands, of I/O requests. For this, clustered
    filesystems are designed to support multiple reads and writes from different hosts.
  prefs: []
  type: TYPE_NORMAL
- en: However, due to its advantages and disadvantages, block storage requires quite
    a lot of care and feeding—you need to take care of the filesystem and partitioning
    that you are going to put on top of your block devices. Additionally, you have
    to make sure that the filesystem is kept consistent and secure, with correct permissions,
    and without corruption across all the systems that are accessing it. VMs have
    other filesystems stored in their virtual disks that also add another layer of
    complexity—data can be written to the VM's filesystem and into the hypervisor's
    filesystem. Both filesystems have files that come and go, and they need to be
    adequately zeroed for blocks to be reclaimed in a thinly provisioned replication
    scenario, and, as we mentioned before, most storage arrays are not aware of the
    actual data being written to them.
  prefs: []
  type: TYPE_NORMAL
- en: File storage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: On the other hand, file storage or NAS is far more straightforward. You don't
    have to worry about partitioning, or about selecting and formatting a filesystem
    that suits your multi-host environment.
  prefs: []
  type: TYPE_NORMAL
- en: NAS is usually NFS or SMB/CIFS protocols, which are mainly used for storing
    data in shared folders as unstructured data. These protocols are not very good
    at scaling or meeting the high media demands that we face in the cloud, such as
    social media serving and creating/uploading thousands of images or videos each
    day. This is where object storage saves the day, but we will be covering object
    storage later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: File storage, as the name suggests, works at the file level of storage when
    you perform a request to NAS; you are requesting a file or a piece of a file from
    the filesystem, not a series of logical addresses. With NAS, this process is abstracted
    from the host (where the storage is mounted), and your storage array or SDS is
    in charge of accessing the disks on the backend and retrieving the file that you
    are requesting. File storage also comes with native features, such as file-locking,
    user and group integration (when we are talking about OSS, we are talking about
    NFS mainly here), security, and encryption.
  prefs: []
  type: TYPE_NORMAL
- en: Even though NAS abstracts and makes things simple for the client, it also has
    its downsides, as NAS relies heavily, if not entirely, on the network. It also
    has an additional filesystem layer with much higher latency than block storage.
    Many factors can cause latency or increase **round-trip time** (**RTT**). You
    need to consider things such as how many hops your NAS is away from your clients,
    TCP window scaling, or having no jumbo frames enabled on devices accessing your
    file shares. Also, all these factors not only affect latency but are key players
    when it comes to the throughput of your NAS solution, which is where file storage
    excels the most.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram demonstrates how versatile file storage sharing is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e1d8ff10-cd79-433d-9875-70b65d8ec7c1.png)'
  prefs: []
  type: TYPE_IMG
- en: Object storage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Object storage is entirely different from NAS (file storage) and SAN (block
    storage). Although data is still accessed through the network, the way that data
    is retrieved is uniquely different. You will not access files through a filesystem,
    but through RESTful APIs using HTTP methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'Objects are stored in a flat namespace, which can store millions or billions
    of them; this is the key to its high scalability, as it is not restrained by the
    number of nodes as it is in regular filesystems, such as XFS and EXT4\. It is
    important to know that the namespaces can have partitions (often called buckets),
    but they cannot be nested as regular folders in a filesystem because the namespace
    is flat:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d81a09ae-c01d-46d8-aa33-f29999148167.png)'
  prefs: []
  type: TYPE_IMG
- en: When comparing object storage with traditional storage, the self-parking versus
    valet-parking analogy is often used. Why is this similar? Well, because, in traditional
    filesystems, when you store your file you store it in a folder or directory, and
    it is your responsibility to know where that file was stored, just like parking
    a car in a parking spot—you need to remember the number and floor of where you
    left your car. With object storage, on the other hand, when you upload your data
    or put a file in a bucket, you are granted a unique identifier that you can later
    use to retrieve it; you don't need to remember where it was stored. Just like
    a valet, who will go and get the car for you, you simply need to give them the
    ticket you received when you left your car.
  prefs: []
  type: TYPE_NORMAL
- en: Continuing with the valet-parking reference, you usually give your valet information
    about the car they need to get to you, not because they need it, but because they
    can identify your car better in this way—for instance, the color, plate number,
    or model of the car will help them a lot. With object storage, the process is
    the same. Each object has its own metadata, its unique ID, and the file itself,
    which are all part of the stored object.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows what comprises an object in object storage:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9055589c-e121-4f79-b61c-773602d62904.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As we have mentioned several times, object storage is accessed through RESTful
    APIs. So, in theory, any device that supports HTTP protocols can access your object
    storage buckets via HTTP methods such as `PUT` or `GET`. This sounds insecure,
    but, in fact, most software-defined object storage has some type of authentication
    method, and you require an authentication token in order to retrieve or upload
    files. A simple request using the Linux `curl` tool may look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we can see how multiple distinct devices can connect to object storage
    buckets in the cloud through the HTTP protocol:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/08e53f1f-9861-4a8b-b7f1-4f566628aefa.png)'
  prefs: []
  type: TYPE_IMG
- en: Why choose GlusterFS?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we understand the core concepts of SDS, storage clusters, and the differences
    between block, file, and object storage, we can go through some of the reasons
    why enterprise customers choose GlusterFS for their storage needs.
  prefs: []
  type: TYPE_NORMAL
- en: As previously stated, GlusterFS is an SDS, that is, a layer that sits on top
    of traditional local storage mount points, allowing the aggregation of storage
    space between multiple nodes into a single storage entity or a storage cluster.
    GlusterFS can run on shelf-commodity hardware to private, public, or hybrid clouds.
    Although its primary usage is file storage (NAS), several plugins allow it to
    be used as a backend for block storage through the gluster-block plugin and for
    object storage with the gluster-swift plugin.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the main features that define GlusterFS are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Commodity hardware
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can be deployed on a private, public, or hybrid cloud
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No single point of failure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scalability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Asynchronous geo-replication
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Self-healing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Flexibility
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GlusterFS features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's go through each one of these features to understand why GlusterFS is so
    attractive to enterprise customers.
  prefs: []
  type: TYPE_NORMAL
- en: Commodity hardware – GlusterFS runs on pretty much anything
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: From **Advanced RISC Machines** (**ARM**) on a Raspberry Pi to any variety of
    x86 hardware, Gluster merely requires local storage used as a brick, which lays
    the foundation storage for volumes. There is no need for dedicated hardware or
    specialized storage controllers.
  prefs: []
  type: TYPE_NORMAL
- en: In its most basic configuration, a single disk formatted as XFS can be used
    with a single node. While not the best configuration, it allows for further growth
    by adding more bricks or more nodes.
  prefs: []
  type: TYPE_NORMAL
- en: GlusterFS can be deployed on private, public, or hybrid clouds
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: From a container image to a full VM dedicated to GlusterFS, one of the main
    points of interest for cloud customers is that since GlusterFS is merely software,
    it can be deployed on private, public, or hybrid clouds. Because there is no vendor,
    locking volumes that span different cloud providers is entirely possible. Allowing
    for multi-cloud provider volumes with high availability setups is done so that
    when one cloud provider has problems, the volume traffic can be moved to an entirely
    different provider with minimal-to-no downtime, depending on the configuration.
  prefs: []
  type: TYPE_NORMAL
- en: No single point of failure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Depending on the volume configuration, data is distributed across multiple nodes
    in the cluster, removing a single point of failure, as no `head`or `master`node
    controls the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Scalability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: GlusterFS allows for the smooth scaling of resources by vertically adding new
    bricks, or by horizontally adding new nodes to the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: All this can be done online while the cluster serves data, without any disruption
    to the client's communication.
  prefs: []
  type: TYPE_NORMAL
- en: Asynchronous geo-replication
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: GlusterFS takes the no-single-point-of-failure concept, which provides geo-replication,
    allowing data to be asynchronously replicated to clusters in entirely different
    geophysical data centers.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows geo-replication across multiple sites:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6a4c90ba-0cb9-499b-928a-86e6aeaff77e.png)'
  prefs: []
  type: TYPE_IMG
- en: Performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since data is distributed across multiple nodes, we can also have multiple clients
    accessing the cluster at the same time. This process of accessing data from multiple
    sources simultaneously is called parallelism, and GlusterFS allows for increased
    performance by directing clients to different nodes. Additionally, performance
    can be increased by adding bricks or nodes—effectively, by scaling horizontally
    or vertically.
  prefs: []
  type: TYPE_NORMAL
- en: Self-healing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the case of unexpected downtime, the remaining nodes can still serve traffic.
    If new data is added to the cluster while one of the nodes is down, this data
    needs to be synchronized once the node is brought back up.
  prefs: []
  type: TYPE_NORMAL
- en: GlusterFS will automatically self-heal these new files once they're accessed,
    triggering a self-heal operation between the nodes and copying the missing data.
    This is transparent to the users and clients.
  prefs: []
  type: TYPE_NORMAL
- en: Flexibility
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: GlusterFS can be deployed on-premises on already existing hardware or existing
    virtual infrastructure, on the cloud as a VM, or as a container. There is no lock-in
    as to how it needs to be deployed, and customers can decide what suits their needs
    best.
  prefs: []
  type: TYPE_NORMAL
- en: Remote direct memory access (RDMA)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: RDMA allows for ultra-low latency and extremely high-performance network communication
    between the Gluster server and the Gluster clients. GlusterFS can leverage RDMA
    for **high-performance computing** (**HPC**) applications and highly-concurrent
    workloads.
  prefs: []
  type: TYPE_NORMAL
- en: Gluster volume types
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Having gained an understanding of the core features of GlusterFS, we can now
    define the different types of volumes that GlusterFS provides. This will help
    in the next chapters as we dive into the actual design of a GlusterFS solution.
  prefs: []
  type: TYPE_NORMAL
- en: GlusterFS provides the flexibility of choosing the type of volume that best
    suits the needs of the workload; for example, for a high-availability requirement,
    we can use replicated volume. This type of volume replicates the data between
    two or more nodes, resulting in exact copies of each of the nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s quickly list the types of volumes that are available, and later, we''ll
    discuss each of their advantages and disadvantages:'
  prefs: []
  type: TYPE_NORMAL
- en: Distributed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Replicated
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distributed replicated
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dispersed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distributed dispersed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distributed
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As the name implies, data is distributed across the bricks in the volume and
    across the nodes. This type of volume allows for a seamless and low-cost increase
    in available space. The main drawback is that there is no data redundancy since
    files are allocated between bricks that could be on the same node or different
    nodes. It is mainly used for high-storage-capacity and concurrency applications.
  prefs: []
  type: TYPE_NORMAL
- en: Think of this volume type as **just a bunch of disks** (**JBOD**) or a linear **logical
    volume manager** (**LVM**) where space is just aggregated without any stripping
    or parity.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows a distributed volume:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/54bad647-9417-49c4-ae05-6402334472b3.png)'
  prefs: []
  type: TYPE_IMG
- en: Replicated
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In a replicated volume, data is copied across bricks on different nodes. Expanding
    a replicated volume requires the same number of replicas to be added. For example,
    if I have a volume with two replicas and I want to expand it, I require a total
    of four replicas.
  prefs: []
  type: TYPE_NORMAL
- en: A replicated volume can be compared to a RAID1, where data is mirrored between
    all available nodes. One of its shortcomings is that scalability is relatively
    limited. On the other hand, its main characteristic is high availability, as data
    is served even in the event of unexpected downtime.
  prefs: []
  type: TYPE_NORMAL
- en: With this type of volume, mechanisms to avoid split-brain situations must be
    implemented. A split-brain occurs when new data is written to the volume, and
    different sets of nodes are allowed to process writes separately. Server quorum
    is such a mechanism, as it allows for a tiebreaker to exist.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows a replicated volume:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f74ac36a-4b5e-4784-8cb0-db549f1313fc.png)'
  prefs: []
  type: TYPE_IMG
- en: Distributed replicated
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A distributed replicated volume is similar to a replicated volume, with the
    main difference being that replicated volumes are distributed. To explain this,
    consider having two separate replicated volumes, each with 10 TB of space. When
    both are distributed, the volume ends up with a total of 20 TB of space.
  prefs: []
  type: TYPE_NORMAL
- en: This type of volume is mainly used when both high availability and redundancy
    are needed, as the cluster can tolerate node failures.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows a distributed replicated volume:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1c01cd67-92b1-4cc1-8a91-93d2597d0327.png)'
  prefs: []
  type: TYPE_IMG
- en: Dispersed
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Dispersed volumes take the best of both distributed and replicated volumes by
    stripping the data across all available bricks and, at the same time, allowing
    redundancy. Bricks should be of the same size, as the volume suspends all writes
    once the smallest brick becomes full. For example, imagine a dispersed volume
    such as a RAID 5 or 6, where data is stripped and parity is created, allowing
    data to be reconstructed from the parity. While the analogy helps us to understand
    this type of volume, the actual process is entirely different as it uses erasure
    codes where data is broken into fragments. Dispersed volumes provide the right
    balance of performance, space, and high availability.
  prefs: []
  type: TYPE_NORMAL
- en: Distributed dispersed
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In a distributed dispersed volume, data is distributed across volumes of a dispersed
    type. Redundancy is provided at the dispersed volume level, having similar advantages
    to a distributed replicated volume.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine a JBOD on top of two RAID 5 arrays—growing this type of volume requires
    an additional dispersed volume. While not necessarily the same size, ideally,
    it should maintain the same characteristics to avoid complications.
  prefs: []
  type: TYPE_NORMAL
- en: The need for highly redundant storage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With an increase in the available space for applications comes an increased
    demand on the storage. Applications may require access to their information all
    of the time without any disruption that could cause the entire business continuity
    to be at risk. No company wants to have to deal with an outage, let alone an interruption
    in the central infrastructure that leads to money being lost, customers not being
    served, and users not being able to log in to their accounts because of bad decisions.
  prefs: []
  type: TYPE_NORMAL
- en: Let's consider storing data on a traditional monolithic storage array—doing
    this can cause significant risks as everything is in a single place. A single
    massive storage array containing all of the company's information signifies an
    operational risk as the array is predisposed to fail. Every single type of hardware—no
    matter how good—fails at some point.
  prefs: []
  type: TYPE_NORMAL
- en: Monolithic arrays tend to handle failures by providing some form of redundancy
    through the use of traditional RAID methods used on the disk level. While this
    is good for small local storage that serves a couple of hundred users, this might
    not be a good idea when we reach the petascale and storage space and active concurrent
    users increase drastically. In specific scenarios, a RAID recovery can cause the
    entire storage system to go down or degrade performance to the point that the
    application doesn't work as expected. Additionally, with increased disk sizes
    and single-disk performance being the same over the past couple of years, recovering
    a single disk now takes a more substantial amount of time; rebuilding 1 TB disks
    is not the same as rebuilding 10 TB disks.
  prefs: []
  type: TYPE_NORMAL
- en: Storage clusters, such as GlusterFS, handle redundancy differently by providing
    methods that best fit the workload. For example, when using a replicated volume,
    data is mirrored from one node to another. If a node goes down, then traffic is
    seamlessly directed to the remaining nodes, being utterly transparent to the users.
    Once the problematic node is serviced, it can be quickly put back into the cluster,
    where it will go through self-healing of the data. In comparison to traditional
    storage, a storage cluster removes the single point of failure by distributing
    data to multiple members of the clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Having increased availability means that we can reach the application service-level
    agreements and maintain the desired uptime.
  prefs: []
  type: TYPE_NORMAL
- en: Disaster recovery
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There's no escaping from it—disasters happen, whether it's natural or human
    error. What counts is how well-prepared we are for them, and how fast and efficiently
    we can recover.
  prefs: []
  type: TYPE_NORMAL
- en: 'Implementing disaster recovery protocols is of utmost importance for business
    continuity. There are two terms that we need to understand before proceeding:
    **recovery time objective** (**RTO**) and **recovery point objective** (**RPO**).
    Let''s take a quick glance at each. RTO is the time it takes to recover from a
    failure or event that causes a disruption. Put simply, it refers to how fast we
    can get the application back up. RPO, on the other hand, refers to how far the
    data can go back in time without affecting business continuity, that is, how much
    data you can lose.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The concept of RPO looks something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/13880b4a-7116-4947-bb98-e1a8bbeb6aa9.png)'
  prefs: []
  type: TYPE_IMG
- en: RTO
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As previously stated, this is the amount of time it takes to recover a functionality
    after a failure. Depending on the complexity of the solution, RTO might take a
    considerable amount of time.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the business requirements, RTO might be as short as a couple of
    hours. This is where designing a highly redundant solution comes into play—by
    decreasing the amount of time that is required to be operational again.
  prefs: []
  type: TYPE_NORMAL
- en: RPO
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is the amount of time data that can be lost and still go back to a recovery
    point, in other words, this is how often recovery points are taken; in the case
    of backups, how often a backup is taken (it could be hourly, daily, or weekly),
    and in the case of a storage cluster, how often changes are replicated.
  prefs: []
  type: TYPE_NORMAL
- en: One thing to take into consideration is the speed at which changes can be replicated,
    as we want changes to be replicated almost immediately; however, due to bandwidth
    constraints, the real time replication is not possible most of the time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, an essential factor to consider is how data is replicated. Generally,
    there are two types of replication: synchronous and asynchronous.'
  prefs: []
  type: TYPE_NORMAL
- en: Synchronous replication
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Synchronous replication means that data is replicated immediately after it is
    written. This is useful for minimizing RPO, as there is no wait or drift between
    the data from one node to another. A GlusterFS-replicated volume provides this
    kind of replication. Bandwidth should also be considered, as changes need to be
    committed immediately.
  prefs: []
  type: TYPE_NORMAL
- en: Asynchronous replication
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Asynchronous replication means that the data is replicated into fragments of
    time, for example, every 10 minutes. During set up, the RPO is chosen based on
    several factors, including the business need and the bandwidth that is available.
  prefs: []
  type: TYPE_NORMAL
- en: Bandwidth is the primary consideration; this is because, depending on the size
    of the changes, the real time replication might not fit in the RPO window, requiring
    a more considerable replication time and directly affecting RPO times. If unlimited
    bandwidth is available, synchronous replication should be chosen.
  prefs: []
  type: TYPE_NORMAL
- en: In hindsight, we, as IT architects, spend a significant amount of time trying
    to figure out how to make our systems more resilient. Indeed, successfully decreasing
    RTO and RPO times can mark the difference between a partially thought out solution
    and an entirely architected design.
  prefs: []
  type: TYPE_NORMAL
- en: The need for high performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With more and more users accessing the same resources, response times get slower
    and applications start taking longer to process. The performance of traditional
    storage has not changed in the last couple of years—a single HDD yields about
    150 MB/s with response times of several milliseconds. With the introduction of
    flash media and protocols such as **non-volatile memory express** (**NVMe**),
    a single SSD can easily achieve gigabytes-per-second and sub-millisecond response
    times; SDS can leverage these new technologies to provide increased performance
    and significantly reduce response times.
  prefs: []
  type: TYPE_NORMAL
- en: Enterprise storage is designed to handle multiple concurrent requests for hundreds
    of clients who are trying to get their data as fast as possible, but when the
    performance limits are reached, traditional monolithic storage starts slowing
    down, causing applications to fail as requests are not completed in time. Increasing
    the performance of this type of storage comes at a high price and, in most cases,
    it can't be done while the storage is still serving data.
  prefs: []
  type: TYPE_NORMAL
- en: The need for increased performance comes from the increased load in storage
    servers; with the explosion in data consumption, users are storing much more information
    and require it much faster than before.
  prefs: []
  type: TYPE_NORMAL
- en: Applications also require data to be delivered to them as quickly as possible;
    for example, consider the stock market, where data is requested multiple times
    a second by thousands of users. At the same time, another thousand users are continuously
    writing new data. If a single transaction is not committed in time, people will
    not be able to make the correct decision when buying or selling stocks because
    the wrong information is displayed.
  prefs: []
  type: TYPE_NORMAL
- en: The previous problem is something that architects have to face when designing
    a solution that can deliver the expected performance that is necessary for the
    application to work as expected. Taking the right amount of time to size storage
    solutions correctly makes the entire process flow smoother with less back and
    forth between design and implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Storage systems, such as GlusterFS, can serve thousands of concurrent users
    simultaneously without a significant decrease in performance, as data is spread
    across multiple nodes in the cluster. This approach is considerably better than
    accessing a single storage location, such as with traditional arrays.
  prefs: []
  type: TYPE_NORMAL
- en: Parallel I/O
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I/O refers to the process of requesting and writing data to a storage system.
    The process is done through I/O streams, where data is requested one block, file,
    or object at a time.
  prefs: []
  type: TYPE_NORMAL
- en: Parallel I/O refers to the process where multiple streams perform operations
    concurrently on the same storage system. This increases performance and reduces
    access times, as various files or blocks are read or written at the same time.
  prefs: []
  type: TYPE_NORMAL
- en: In comparison, serial I/O is the process of performing a single stream of I/O,
    which could lead to reduced performance and increased latency or access times.
  prefs: []
  type: TYPE_NORMAL
- en: Storage clusters, such as GlusterFS, take advantage of parallel I/O, since data
    is spread through multiple nodes, allowing for numerous clients to access data
    at the same time without any drop in latency or throughput.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we went through the core concepts of what a cluster is and
    defined it as a set of computers called nodes working together in the same type
    of workload. A compute cluster's primary function is to perform tasks that run
    CPU-intensive workloads, which are designed to reduce processing time. A storage
    cluster's function is to aggregate available storage resources into a single storage
    space that simplifies management and allows you to efficiently reach the petascale
    or go beyond the 1-PB available space. Then, we explored how SDS is changing the
    way that data is stored and how GlusterFS is one of the projects that is leading
    this change. SDS allows for the simplified management of storage resources, while
    at the same time adding features that were impossible with traditional monolithic
    storage arrays.
  prefs: []
  type: TYPE_NORMAL
- en: To further understand how applications interact with storage, we defined the
    core differences between block, file, and object storage. Primarily, block storage
    deals with logical blocks of data in a storage device, file storage works by reading
    or writing actual files from a storage space, and object storage provides metadata
    to each object for further interaction. With these concepts of different interactions
    with storage in mind, we went on to point out the characteristics of GlusterFS
    that make it attractive for enterprise customers and how these features tie into
    what SDS stands for.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we delved into the main reasons why high availability and high performance
    are a must for every storage design and how performing parallel, or serial, I/O
    can affect application performance.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will dive into the actual process of architecting a
    GlusterFS storage cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: How can I optimize my storage performance?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What type of workload is GlusterFS better suited for?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Which cloud providers offer object storage?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What types of storage does GlusterFS offer?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Does Red Hat own GlusterFS?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Do I have to pay to use GlusterFS?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Does Gluster offer disaster recovery or replication?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Ceph Cookbook – Second Edition* by Vikhyat Umrao and Michael Hackett: [https://prod.packtpub.com/in/virtualization-and-cloud/ceph-cookbook-second-edition](https://prod.packtpub.com/in/virtualization-and-cloud/ceph-cookbook-second-edition)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Mastering Ceph* by Nick Fisk: [https://prod.packtpub.com/in/big-data-and-business-intelligence/mastering-ceph](https://prod.packtpub.com/in/big-data-and-business-intelligence/mastering-ceph)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Learning Ceph – Second Edition *by Anthony D''Atri and Vaibhav Bhembre: [https://prod.packtpub.com/in/virtualization-and-cloud/learning-ceph-second-edition](https://prod.packtpub.com/in/virtualization-and-cloud/learning-ceph-second-edition)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
