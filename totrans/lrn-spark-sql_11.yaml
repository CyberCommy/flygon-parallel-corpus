- en: Tuning Spark SQL Components for Performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will focus on the performance tuning aspects of Spark SQL-based
    components. The Spark SQL Catalyst optimizer is central to the efficient execution
    of many, if not all, Spark applications, including **ML Pipelines**, **Structured
    Streaming**, and **GraphFrames**-based applications. We will first explain the
    key foundational aspects regarding serialization/deserialization using encoders and
    the logical and physical plans associated with query executions, and then present
    the details of the **cost-based optimization** (**CBO**) feature released in Spark
    2.2\. Additionally, we will present some tips and tricks that developers can use
    to improve the performance of their applications throughout the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'More specifically, in this chapter, you will learn the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Basic concepts essential to understanding performance tuning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding Spark internals that drives performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding cost-based optimizations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the performance impact of enabling whole-stage code generation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing performance tuning in Spark SQL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Spark computations are typically in-memory and can be bottlenecked by the resources
    in the cluster: CPU, network bandwidth, or memory. In addition, although the data
    fits in memory, network bandwidth may be challenging.'
  prefs: []
  type: TYPE_NORMAL
- en: Tuning Spark applications is a necessary step to reduce both the number and
    size of data transfer over the network and/or reduce the overall memory footprint
    of the computations.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will focus our attention on Spark SQL Catalyst because it
    is key to deriving benefits from a whole set of application components.
  prefs: []
  type: TYPE_NORMAL
- en: 'Spark SQL is at the heart of major enhancements made to Spark recently, including
    **ML Pipelines**, **Structured Streaming**, and **GraphFrames**. The following
    figure illustrates the key role **Spark SQL** plays between the **Spark Core**
    and the higher-level APIs built on top of it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00292.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In the next several sections, we will cover the fundamental understanding required
    for tuning Spark SQL applications. We will start with the **DataFrame/Dataset**
    APIs.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding DataFrame/Dataset APIs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A **Dataset** is a strongly typed collection of domain-specific objects that
    can be transformed parallelly, using functional or relational operations. Each
    Dataset also has a view called a **DataFrame**, which is not strongly typed and
    is essentially a Dataset of row objects.
  prefs: []
  type: TYPE_NORMAL
- en: Spark SQL applies structured views to the data from different source systems
    stored using different data formats. Structured APIs, such as the DataFrame/Dataset
    API, allows developers to use a high-level API to write their programs. These
    APIs allow them to focus on the "what" rather than the "how" of the data processing
    required.
  prefs: []
  type: TYPE_NORMAL
- en: Even though applying a structure can limit what can be expressed, in practice,
    structured APIs can accommodate the vast majority of computations required in
    application development. Also, it is these very limitations (imposed by structured
    APIs) that present several of the main optimization opportunities.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will explore encoders and their role in efficient serialization
    and deserialization.
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing data serialization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **Encoder** is the fundamental concept in the **serialization** and **deserialization**
    (**SerDe**) framework in Spark SQL 2.0\. Spark SQL uses the SerDe framework for
    I/O resulting in greater time and space efficiencies. Datasets use a specialized
    encoder to serialize the objects for processing or transmitting over the network
    instead of using Java serialization or Kryo.
  prefs: []
  type: TYPE_NORMAL
- en: Encoders are required to support domain objects efficiently. These encoders
    map the domain object type, `T`, to Spark's internal type system, and `Encoder
    [T]` is used to convert objects or primitives of type `T` to and from Spark SQL's
    internal binary row format representation (using Catalyst expressions and code
    generation). The resulting binary structure often has a much lower memory footprint
    and is optimized for efficiency in data processing (for example, in a columnar
    format).
  prefs: []
  type: TYPE_NORMAL
- en: Efficient serialization is key to achieving good performance in distributed
    applications. Formats that are slow to serialize objects will significantly impact
    the performance. Often, this will be the first thing you tune to optimize a Spark
    application.
  prefs: []
  type: TYPE_NORMAL
- en: Encoders are highly optimized and use runtime code generation to build custom
    bytecode for serialization and deserialization. Additionally, they use a format
    that allows Spark to perform many operations, such as filtering and sorting, without
    requiring to be deserialized back to an object. As encoders know the schema of
    the records, they can offer significantly faster serialization and deserialization
    (compared to the default Java or Kryo serializers).
  prefs: []
  type: TYPE_NORMAL
- en: In addition to speed, the resulting serialized size of encoder output can also
    be significantly smaller, thereby reducing the cost of network transfers. Furthermore,
    the serialized data is already in the Tungsten binary format, which means that
    many operations can be executed in place, without needing to materialize the object.
    Spark has built-in support for automatically generating encoders for primitive
    types, such as String and Integer, and also case classes.
  prefs: []
  type: TYPE_NORMAL
- en: Here, we present an example of creating a custom encoder for the Bid records
    from [Chapter 1](part0022.html#KVCC0-e9cbc07f866e437b8aa14e841622275c)*, *Getting
    Started with Spark SQL**. Note that the encoders for most common types are automatically
    provided by importing `spark.implicits._`, and the default encoders are already
    imported in Spark shell.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s import all the classes we need for the code in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will define a `case` class for our domain object for `Bid` records
    in the input Dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will create an `Encoder` object using the `case` class from the preceding
    step, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The schema can be accessed using the schema property, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We use the implementation of `ExpressionEncoder` (the only implementation of
    an encoder trait available in Spark SQL 2), as illustrated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Here are the serializer and the deserializer parts of the encoder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will read in our input Dataset, as demonstrated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We will then display a `Bid` record, as follows, from our newly created DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'For convenience, we can use the record from the preceding step to create a
    new record as in the `Dataset[Bid]`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We will then serialize the record to the internal representation, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Spark uses `InternalRows` internally for I/O. So, we deserialize the bytes
    to a JVM object, that is, a `Scala` object, as follows. However, we need to import
    `Dsl` expressions, and explicitly specify `DslSymbol`, as there are competing
    implicits in the Spark shell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we retrieve the serialized `Bid` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We can verify that the two objects are the same, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: In the next section, we will shift our focus to Spark SQL's Catalyst optimizations.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Catalyst optimizations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We briefly explored the Catalyst optimizer in [Chapter 1](part0022.html#KVCC0-e9cbc07f866e437b8aa14e841622275c)*,
    *Getting Started with Spark SQL**. Basically, Catalyst has an internal representation
    of the user's program, called the **query plan**. A set of transformations is
    executed on the initial query plan to yield the optimized query plan. Finally,
    through Spark SQL's code generation mechanism, the optimized query plan gets converted
    to a DAG of RDDs, ready for execution. At its core, the Catalyst optimizer defines
    the abstractions of users' programs as trees and also the transformations from
    one tree to another.
  prefs: []
  type: TYPE_NORMAL
- en: In order to take advantage of optimization opportunities, we need an optimizer
    that automatically finds the most efficient plan to execute data operations (specified
    in the user's program). In the context of this chapter, Spark SQL's Catalyst optimizer
    acts as the interface between the user's high-level programming constructs and
    the low-level execution plans.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the Dataset/DataFrame API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A Dataset or a DataFrame is typically created as a result of reading from a data
    source or the execution of a query. Internally, queries are represented by trees
    of operators, for example, logical and physical trees. Internally, a Dataset represents
    a logical plan that describes the computation required to produce the data. When
    an action is invoked, Spark's query optimizer optimizes the logical plan and generates
    a physical plan for efficient execution in a parallel and distributed manner.
  prefs: []
  type: TYPE_NORMAL
- en: A query plan is used describe a data operation such as aggregate, join, or filter,
    to generate a new Dataset using different kinds of input Datasets.
  prefs: []
  type: TYPE_NORMAL
- en: The first kind of query plan is the logical plan, and it describes the computation
    required on the Datasets without specifically defining the mechanism of conducting
    the actual computation. It gives us an abstraction of the user's program and allows
    us to freely transform the query plan, without worrying about the execution details.
  prefs: []
  type: TYPE_NORMAL
- en: A query plan is a part of Catalyst that models a tree of relational operators,
    that is, a structured query. A query plan has a `statePrefix` that is used when
    displaying a plan with `!` to indicate an invalid plan, and `'` to indicate an
    unresolved plan. A query plan is invalid if there are missing input attributes
    and children subnodes are non-empty, and it is unresolved if the column names
    have not been verified and column types have not been looked up in the catalog.
  prefs: []
  type: TYPE_NORMAL
- en: As a part of the optimizations, the Catalyst optimizer applies various rules
    to manipulate these trees in phases. We can use the explain function to explore
    the logical as well as the optimized physical plan.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we will present a simple example of three Datasets and display their optimization
    plans using the `explain()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Analyzed logical plans result from applying the Analyzer''s check rules on
    the initial parsed plan. Analyzer is a logical query plan Analyzer in Spark SQL
    that semantically validates and transforms an unresolved logical plan to an analyzed
    logical plan (using logical evaluation rules):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Enable `TRACE` or `DEBUG` logging levels for the respective session-specific
    loggers to see what happens inside the Analyzer. For example, add the following
    line to `conf/log4j` properties:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The Analyzer is a Rule Executor that defines the logical plan evaluation rules
    for resolving and modifying the same. It resolves unresolved relations and functions
    using the session catalog. The optimization rules for fixed points and the one-pass
    rules (the once strategy) in the batch are also defined here.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the logical plan optimization phase, the following set of actions is executed:'
  prefs: []
  type: TYPE_NORMAL
- en: Rules convert logical plans into semantically equivalent ones for better performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Heuristic rules are applied to push down predicated columns, remove unreferenced
    columns, and so on
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Earlier rules enable the application of later rules; for example, merge query
    blocks enable global join reorder
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`SparkPlan` is the Catalyst query plan for physical operators that are used
    to build the physical query plan. Upon execution, the physical operators produce
    RDDs of rows. The available logical plan optimizations can be extended and additional
    rules can be registered as experimental methods.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Understanding Catalyst transformations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will explore Catalyst transformations in detail. Transformations
    in Spark are pure functions, that is, a tree is not mutated during the transformation
    (instead, a new one is produced). In Catalyst, there are two kinds of transformations:'
  prefs: []
  type: TYPE_NORMAL
- en: In the first type, the transformation does not change the type of the tree.
    Using this transformation, we can transform an expression to another expression,
    a logical plan to another logical plan, or a physical plan to another physical
    plan.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second type of transformation changes a tree from one kind of tree to another.
    For example, this type of transformation is used to change a logical plan to a
    physical plan.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A function (associated with a given tree) is used to implement a single rule.
    For example, in expressions, this can be used for constant folding optimization.
    A transformation is defined as a partial function. (Recall that a partial function
    is a function that is defined for a subset of its possible arguments.) Typically,
    case statements figure out whether a rule is triggered or not; for example, the
    predicate filter is pushed below the `JOIN` node as it reduces the input size
    of `JOIN`; this is called the **predicate pushdown**. Similarly, a projection
    is performed only for the required columns used in the query. This way, we can
    avoid reading unnecessary data.
  prefs: []
  type: TYPE_NORMAL
- en: Often, we need to combine different types of transformation rule. A Rule Executor
    is used to combine multiple rules. It transforms a tree to another tree of the
    same type by applying many rules (defined in batches).
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two approaches used for applying rules:'
  prefs: []
  type: TYPE_NORMAL
- en: In the first approach, we apply the rule repeatedly until the tree does not
    change any more (called the fixed point)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the second type, we apply all the rules in a batch, only once (the once strategy)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Next, we look at the second type of transformation, in which we change from
    one tree to another kind of tree: more specifically, how Spark transforms a logical
    plan to a physical plan. A logical plan is transformed to a physical plan by applying
    a set of strategies. Primarily, a pattern matching approach is taken for these
    transformations. For example, a strategy converts the logical project node to
    a physical project node, a logical Filter node to a physical Filter node, and
    so on. A strategy may not be able to convert everything, so mechanisms are built-in
    to trigger other strategies at specific points in the code (for example, the `planLater`
    method).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The optimization process comprises of three steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Analysis (Rule Executor): This transforms an unresolved logical plan to a resolved
    logical plan. The unresolved to resolved state uses the Catalog to find where
    Datasets and columns come from and the types of column.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Logical Optimization (Rule Executor): This transforms a resolved logical plan
    to an optimized logical plan.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Physical Planning (Strategies+Rule Executor): This consists of two phases:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Transforms an optimized logical plan to a physical plan.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rule Executor is used to adjust the physical plan to make it ready for execution.
    This includes how we shuffle the data and how we partition it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As shown in the following example, an expression represents a new value, and
    it is computed based on its input values, for example, adding a constant to each
    element within a column, such as `1 + t1.normal`. Similarly, an attribute is a
    column in a Dataset (for example, `t1.id`) or a column generated by a specific
    data operation, for example, v.
  prefs: []
  type: TYPE_NORMAL
- en: The output has a list of attributes generated by this logical plan, for example,
    id and v. The logical plan also has a set of invariants about the rows generated
    by this plan, for example, `t2.id > 5000000`. Finally, we have statistics, the
    size of the plan in rows/bytes, per column stats, for example, min, max, and the
    number of distinct values, and the number of null values.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second kind of query plan is the physical plan, and it describes the required
    computations on Datasets with specific definitions on how to conduct the computations.
    A physical plan is actually executable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'All the plans for the preceding query are displayed in the following code block.
    Note our annotations in the parsed logical plan, reflecting parts of the original
    SQL query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: You can use Catalyst's API to customize Spark to roll out your own planner rules.
  prefs: []
  type: TYPE_NORMAL
- en: For more details on the Spark SQL Catalyst optimizer, refer to [https://spark-summit.org/2017/events/a-deep-dive-into-spark-sqls-catalyst-optimizer/](https://spark-summit.org/2017/events/a-deep-dive-into-spark-sqls-catalyst-optimizer/).
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing Spark application execution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will present the key details of the SparkUI interface, which
    is indispensable for tuning tasks. There are several approaches to monitoring
    Spark applications, for example, using web UIs, metrics, and external instrumentation.
    The information displayed includes a list of scheduler stages and tasks, a summary
    of RDD sizes and memory usage, environmental information, and information about
    the running executors.
  prefs: []
  type: TYPE_NORMAL
- en: 'This interface can be accessed by simply opening `http://<driver-node>:4040`
    (`http://localhost:4040`) in a web browser. Additional `SparkContexts` running
    on the same host bind to successive ports: 4041, 4042, and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: For a more detailed coverage of monitoring and instrumentation in Spark, refer
    to [https://spark.apache.org/docs/latest/monitoring.html](https://spark.apache.org/docs/latest/monitoring.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'We will explore Spark SQL execution visually using two examples. First, we
    create the two sets of Datasets. The difference between the first set (`t1`, `t2`,
    and `t3`) and the second set (`t4`, `t5`, and `t6`) of `Dataset[Long]` is the
    size:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'We will execute the following `JOIN` query against two sets of Datasets to
    visualize the Spark jobs information in the SparkUI dashboard:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot displays the event timeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00293.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The generated **DAG Visualization** with the stages and the shuffles (**Exchange**)
    is shown next:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00294.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'A summary of the job, including execution duration, successful tasks, and the
    total number of tasks, and so on, is displayed here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00295.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Click on the SQL tab to see the detailed execution flow, as demonstrated:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00296.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, we will run the same queries on the set of larger Datasets. Note that
    the `BroadcastHashJoin` in the first example now changes to `SortMergeJoin` due
    to the increased size of the input Datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The execution DAG is shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00297.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The job execution summary is, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00298.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The SQL execution details are shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00299.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In addition to displaying in the UI, metrics are also available as JSON data.
    This gives developers a good way to create new visualizations and monitoring tools
    for Spark. The REST endpoints are mounted at `/api/v1`; for example, they would
    typically be accessible at `http://localhost:4040/api/v1`. These endpoints have
    been strongly versioned to make it easier to develop applications using them.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring Spark application execution metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spark has a configurable metrics system based on the `Dropwizard Metrics` library.
    This allows users to report Spark metrics to a variety of sinks, including `HTTP`,
    `JMX`, and `CSV` files. Spark's metrics corresponding to Spark components include
    the Spark standalone master process, applications within the master that report
    on various applications, a Spark standalone worker process, Spark executor, the
    Spark driver process, and the Spark shuffle service.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next series of screenshots contain details, including summary metrics and
    the aggregated metrics by executors for one of the stages of the JOIN query against
    the larger Datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00300.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The summary metrics for the completed tasks is, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00301.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The aggregated metrics by Executor is, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00302.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Using external tools for performance tuning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: External monitoring tools are often used to profile the performance of Spark
    jobs in large-sized `Spark clusters`. For example, Ganglia can provide an insight
    into overall cluster utilization and resource bottlenecks. Additionally, the `OS
    profiling` tools and `JVM` utilities can provide fine-grained profiling on individual
    nodes and for working with `JVM` internals, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: For more details on visualizing Spark application execution, refer to [https://databricks.com/blog/2015/06/22/understanding-your-spark-application-through-visualization.html](https://databricks.com/blog/2015/06/22/understanding-your-spark-application-through-visualization.html).
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will shift our focus to the new cost-based optimizer
    released in Spark 2.2.
  prefs: []
  type: TYPE_NORMAL
- en: Cost-based optimizer in Apache Spark 2.2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In Spark, the optimizer''s goal is to minimize end-to-end query response time.
    It is based on two key ideas:'
  prefs: []
  type: TYPE_NORMAL
- en: Pruning unnecessary data as early as possible, for example, filter pushdown
    and column pruning.
  prefs: []
  type: TYPE_NORMAL
- en: Minimizing per-operator cost, for example, broadcast versus shuffle and optimal
    join order.
  prefs: []
  type: TYPE_NORMAL
- en: Till Spark 2.1, Catalyst was essentially a rule-based optimizer. Most Spark
    SQL optimizer rules are heuristic rules: `PushDownPredicate`, `ColumnPruning`,
    `ConstantFolding`, and so on. They do not consider the cost of each operator or
    selectivity when estimating `JOIN` relation sizes. Therefore, the `JOIN` order
    is mostly decided by its position in `SQL queries` and the physical join implementation
    is decided based on heuristics. This can lead to suboptimal plans being generated.
    However, if the cardinalities are known in advance, more efficient queries can
    be obtained. The goal of the CBO optimizer is to do exactly that, automatically.
  prefs: []
  type: TYPE_NORMAL
- en: Huawei implemented the CBO in Spark SQL initially; after they open sourced their
    work, many other contributors, including Databricks, worked to finish its first
    version. The CBO-related changes to Spark SQL, specifically the major entry points
    into Spark SQL's data structure and workflow, have been designed and implemented
    in a non-intrusive manner.
  prefs: []
  type: TYPE_NORMAL
- en: A configuration parameter, `spark.sql.cbo`, can be used to enable/disable this
    feature. Currently (in Spark 2.2), the default value is false.
  prefs: []
  type: TYPE_NORMAL
- en: For more details, refer to Huawei's design document available at [https://issues.apache.org/jira/browse/SPARK-16026](https://issues.apache.org/jira/browse/SPARK-16026).
  prefs: []
  type: TYPE_NORMAL
- en: Spark SQL's Catalyst optimizer has many rule-based optimization techniques implemented,
    for example, predicate pushdown to reduce the number of the qualifying records
    before a join operation is performed, and project pruning to reduce the number
    of the participating columns before further processing. However, without detailed
    column statistical information on data distribution, it is difficult to accurately
    estimate the filter factor and cardinality, and thus the output size of a database
    operator. With inaccurate and/or misleading statistics, the optimizer can end
    up choosing suboptimal query execution plans.
  prefs: []
  type: TYPE_NORMAL
- en: In order to improve the quality of query execution plans, the Spark SQL optimizer
    has been enhanced with detailed statistical information. A better estimate of
    the number of output records and the output size (for each database operator)
    helps the optimizer choose a better query plan. The CBO implementation collects,
    infers, and propagates `table / column` statistics on `source / intermediate`
    data. The query tree is annotated with these statistics. Furthermore, it also
    calculates the cost of each operator in terms of the number of output rows, the
    output size, and so on. Based on these cost calculations, it picks the most optimal
    query execution plan.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the CBO statistics collection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `Statistics` class is the key data structure holding statistics information.
    This data structure is referenced when we execute statistics collection SQL statements
    to save information into the system catalog. This data structure is also referenced
    when we fetch statistics information from the system catalog to optimize a query
    plan.
  prefs: []
  type: TYPE_NORMAL
- en: 'CBO relies on detailed statistic to optimize a query execution plan. The following
    SQL statement can be used to collect `table-level` statistics, such as the number
    of rows, number of files (or HDFS data blocks), and table size (in bytes). It
    collects `table-level` statistics and saves them in the `meta-store`. Before 2.2,
    we only had the table size and not the number of rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, the following SQL statement can be used to collect column level
    statistics for the specified columns. The collected information includes the maximal
    column value, minimal column value, number of distinct values, number of null
    values, and so on. It collects column level statistics and saves them in the `meta-store`.
    Typically, it is executed only for columns in the `WHERE` and the `GROUP BY` clauses:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The given SQL statement displays the metadata, including table level statistics
    of a table in an extended format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The `customers` table is created in a later section of this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The following SQL statement can be used to display statistics in the optimized
    logical plan:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Statistics collection functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Statistics are collected using a set of functions, for example, the row count
    is actually obtained by running a SQL statement, such as `select count(1) from
    table_name`. Using SQL statement to get row count is fast as we are leveraging
    Spark SQL's execution parallelism. Similarly, the `analyzeColumns` function gets
    the basic statistics information for a given column. The basic statistics, such
    as `max`, `min`, and `number-of-distinct-values`, are also obtained by running
    SQL statements.
  prefs: []
  type: TYPE_NORMAL
- en: Filter operator
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A filter condition is the predicate expression specified in the `WHERE` clause
    of a SQL select statement. The predicate expression can be quite complex when
    we evaluate the overall filter factor.
  prefs: []
  type: TYPE_NORMAL
- en: There are several operators for which the filter cardinality estimation is executed,
    for example, between the `AND`, `OR`, and `NOT` logical expressions, and also
    for logical expressions such as `=`, `<`, `<=`, `>`, `>=`, and `in`.
  prefs: []
  type: TYPE_NORMAL
- en: For the filter operator, our goal is to compute the filter to find out the portion
    of the previous (or child) operator's output after applying the filter condition.
    A filter factor is a double number between `0.0` and `1.0`. The number of output
    rows for the filter operator is basically the number of its `child node's` output
    times the filter factor. Its output size is its `child node's` output size times
    the filter factor.
  prefs: []
  type: TYPE_NORMAL
- en: Join operator
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we compute the cardinality of a two-table join output, we should already
    have the output cardinalities of its `child nodes` on both sides. The cardinality
    of each join side is no longer the number of records in the original join table.
    Rather, it is the number of qualified records after applying all execution operators
    before this join operator.
  prefs: []
  type: TYPE_NORMAL
- en: If a user collects the `join column` statistics, then we know the number of
    distinct values for each `join column`. Since we also know the number of records
    on the join relation, we can tell whether or not `join column` is a unique key.
    We can compute the ratio of a number of distinct values on `join column` over
    the number of records in join relation. If the ratio is close to `1.0` (say greater
    than `0.95`), then we can assume that the `join column` is unique. Therefore,
    we can precisely determine the number of records per distinct value if a `join
    column` is unique.
  prefs: []
  type: TYPE_NORMAL
- en: Build side selection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The CBO can select a good physical strategy for an execution operator. For example,
    CBO can choose the `build side` selection for a `hash join` operation. For two-way
    hash joins, we need to choose one operand as `build side` and the other as `probe
    side`. The approach chooses the lower-cost child as the `build side` of `hash
    join`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before Spark 2.2, the build side was selected based on original table sizes.
    For the following Join query example, the earlier approach would have selected
    `BuildRight`. However, with CBO, the `build side` is selected based on the estimated
    cost of various operators before the join. Here, `BuildLeft` would have been selected.
    It can also decide whether or not a broadcast join should be performed. Additionally,
    the execution sequence of the database operators for a given query can be rearranged.
    `cbo` can choose the best plan among multiple candidate plans for a given query.
    The goal is to select the candidate plan with the lowest cost:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: In the next section, we will explore CBO optimization in multi-way joins.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding multi-way JOIN ordering optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Spark SQL optimizer''s heuristics rules can transform a `SELECT` statement
    into a query plan with the following characteristics:'
  prefs: []
  type: TYPE_NORMAL
- en: The filter operator and project operator are pushed down below the join operator,
    that is, both the filter and project operators are executed before the join operator
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Without subquery block, the join operator is pushed down below the aggregate
    operator for a select statement, that is, a join operator is usually executed
    before the aggregate operator
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With this observation, the biggest benefit we can get from CBO is multi-way
    join ordering optimization. Using a dynamic programming technique, we try to get
    the globally optimal join order for a multi-way join query.
  prefs: []
  type: TYPE_NORMAL
- en: For more details on multi-way join reordering in Spark 2.2, refer to [https://spark-summit.org/2017/events/cost-based-optimizer-in-apache-spark-22/](https://spark-summit.org/2017/events/cost-based-optimizer-in-apache-spark-22/).
  prefs: []
  type: TYPE_NORMAL
- en: Clearly, the join cost is the dominant factor in choosing the best join order.
    The cost formula is dependent on the implementation of the Spark SQL execution
    engine.
  prefs: []
  type: TYPE_NORMAL
- en: 'The join cost formula in Spark is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*weight * cardinality + size * (1 - weight)*'
  prefs: []
  type: TYPE_NORMAL
- en: The weight in the formula is a tuning parameter configured via the `spark.sql.cbo.joinReorder.card.weight`
    parameter (the default value is `0.7`). The cost of a plan is the sum of the costs
    of all intermediate tables. Note that the current cost formula is very coarse
    and subsequent versions of Spark are expected to have a more fine-grained formula.
  prefs: []
  type: TYPE_NORMAL
- en: For more details on reordering the joins using a dynamic programming algorithm,
    refer to the paper by Selinger et al, at [http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.129.5879&rep=rep1&type=pdf](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.129.5879&rep=rep1&type=pdf).
  prefs: []
  type: TYPE_NORMAL
- en: First, we put all the items (basic joined nodes) into level 1, then we build
    all two-way joins at level 2 from plans at level 1 (single items), then build
    all three-way joins from plans at previous levels (two-way joins and single items),
    then four-way joins and so on, until we have built all n-way joins, and pick the
    best plan among them at each stage.
  prefs: []
  type: TYPE_NORMAL
- en: When building m-way joins, we only keep the best plan (with the lowest cost)
    for the same set of m items. For example, for three-way joins, we keep only the
    best plan for items `{A, B, C}` among plans `(A J B) J C`, `(A J C) J B`, and
    `(B J C) J A`.
  prefs: []
  type: TYPE_NORMAL
- en: One drawback of this algorithm is the assumption that a lowest cost plan can
    only be generated among the lowest cost plans from its previous levels. In addition,
    because the decision to choose a sorted-merge join, which preserves the order
    of its input, versus other join methods is done in the query planner phase, we
    do not have this information to make a good decision in the optimizer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we present an extended example of a multi-way join with the `cbo` and
    `joinReorder` parameters switched off and switched on to demonstrate the speed
    improvements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'We define a function benchmark to measure the execution time of our queries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'In the first example, as shown, we switch off the `cbo` and `joinReorder` parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output on the command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00303.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'In the next example, we switch on `cbo` but keep the `joinReorder` parameter
    disabled:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output on the command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00304.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Note the slight improvement in the execution time of the query with the `cbo`
    parameter enabled.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the final example, we switch on both the `cbo` and `joinReorder` parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output on the command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00305.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Note the substantial improvement in the execution time of the query with both
    the parameters enabled.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will examine performance improvements achieved for various
    `JOINs` using whole-stage code generation.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding performance improvements using whole-stage code generation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we first present a high-level overview of whole-stage code
    generation in Spark SQL, followed by a set of examples to show improvements in
    various `JOINs` using Catalyst's code generation feature.
  prefs: []
  type: TYPE_NORMAL
- en: 'After we have an optimized query plan, it needs to be converted to a DAG of
    RDDs for execution on the cluster. We use this example to explain the basic concepts
    of Spark SQL whole-stage code generation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding optimized logical plan can be viewed as a sequence of **Scan**,
    **Filter**, **Project**, and **Aggregate** operations, as shown in the following
    figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00306.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Traditional databases will typically execute the preceding query based on the
    Volcano Iterator Model, in which each operator implements an iterator interface,
    and consumes records from its input operator and outputs records to the operator
    that is sequenced after it. This model makes it easy to add new operators independent
    of their interactions with other operators. It also promotes composability of
    operators. However, the Volcano Model is inefficient because it involves execution
    of many virtual function calls, for example, three calls are executed for each
    record in the `Aggregate` function. Additionally, it requires extensive memory
    accesses (due to reads/writes in each operator as per the iterator interface).
    It is also challenging to leverage modern CPU features, such as pipelining, prefetching,
    and branch prediction, on the Volcano model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of generating iterator code for each operator, Spark SQL tries to generate
    a single function for the set of operators in the SQL statement. For example,
    the pseudo-code for the preceding query might look something like the following.
    Here, the `for` loop reads over all the rows (Scan operation), the if-condition
    roughly corresponds to the Filter condition, and the aggregate is essentially
    the count:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Note that the simple code mentioned has no virtual function calls and the count
    variable getting incremented is available in the CPU registers. This code is easily
    understood by compilers and therefore modern hardware can be leveraged to speed
    up queries like this one.
  prefs: []
  type: TYPE_NORMAL
- en: The key ideas underlying whole-stage code generation include the fusing together
    of operators, the identification of chains of operators (stages), and the compilation
    of each stage into a single function. This results in code generation that mimics
    hand-written optimized code for the execution of the query.
  prefs: []
  type: TYPE_NORMAL
- en: For more details on compiling query plans on modern hardware, refer to [http://www.vldb.org/pvldb/vol4/p539-neumann.pdf](http://www.vldb.org/pvldb/vol4/p539-neumann.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use `EXPLAIN CODEGEN` to explore the code generated for a query, as
    shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Here, we present a series of `JOIN` examples with whole-stage code generation
    switched off and subsequently switched on to see the significant impact on execution
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: The examples in this section have been taken from the `JoinBenchmark.scala`
    class available at [https://github.com/apache/spark/blob/master/sql/core/src/test/scala/org/apache/spark/sql/execution/benchmark/JoinBenchmark.scala](https://github.com/apache/spark/blob/master/sql/core/src/test/scala/org/apache/spark/sql/execution/benchmark/JoinBenchmark.scala).
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, we present the details of obtaining the execution
    times for the JOIN operations with long values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'For the following set of examples, we present only the essentials for obtaining
    their execution times with and without whole-stage code generation. Refer to the
    preceding example and follow the same sequence of steps to replicate the following
    examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: As an exercise, use the examples in this section to explore their logical and
    physical plans, and also view and understand their execution using SparkUI.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several Spark SQL parameter settings used in tuning tasks. `SQLConf`
    is an internal key-value configuration store for parameters and hints used in
    Spark SQL. In order to print all the current values of these parameters, use the
    following statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also use the following statement to list the extended set of all the
    defined configuration parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we presented the foundational concepts related to tuning a
    Spark application, including data serialization using encoders. We also covered
    the key aspects of the cost-based optimizer introduced in Spark 2.2 to optimize
    Spark SQL execution automatically. Finally, we presented some examples of `JOIN`
    operations, and the improvements in execution times as a result of using whole-stage
    code generation.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will explore application architectures that leverage
    Spark modules and Spark SQL in real-world applications. We will also describe
    the deployment of some of the main processing models being used for batch processing,
    streaming applications, and machine learning pipelines.
  prefs: []
  type: TYPE_NORMAL
