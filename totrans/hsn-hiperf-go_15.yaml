- en: Profiling Go Code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Profiling is a practice that can be used to measure the resources utilized
    in a computer system. Profiling is often done to understand the CPU or memory
    utilization within a program in order to optimize for execution time, size, or
    reliability. Alongside profiling, in this chapter, we are going to learn the following:'
  prefs: []
  type: TYPE_NORMAL
- en: How to profile requests in Go with `pprof`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to compare multiple profiles
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to read the resulting profiles and flame graphs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing profiling will help you to deduce where you can make improvements
    within your function and how much time individual pieces take within your function
    call with respect to the overall system.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding profiling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Profiling Go code is one of the best ways to determine where the bottlenecks
    are within your code base. We have physical limitations to our computer systems
    (CPU clock speed, memory size/speed, I/O read/write speeds, and network throughput,
    to give a few examples), but we can often optimize our programs to more efficiently
    utilize our physical hardware. After a profile of a computer program is taken
    with a profiler, a report is created. This report, often called a profile, can
    tell you information about the program that you ran. There are many reasons why
    you might want to understand the CPU and memory utilization of your program. A
    couple of examples are listed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**CPU profiling reasons:**'
  prefs: []
  type: TYPE_NORMAL
- en: Check performance improvements in new releases of software
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Validate how much CPU is being utilized for each task
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Limit CPU utilization in order to save money
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding where latency comes from
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Memory profiling reasons:**'
  prefs: []
  type: TYPE_NORMAL
- en: Incorrect usage of global variables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goroutines that don't complete
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Incorrect reflection usage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Large string allocation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will next talk about exploring instrumentation methodologies.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring instrumentation methodologies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `pprof` tool has many different methodologies for incorporating profiling
    into your code. The Go language creators wanted to make sure that it was simple
    and effective in implementing the profiling necessary to write performant programs.
    We can implement profiling in many stages of Go software development—namely, engineering,
    the creation of new functions, testing, and production.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to remember that profiling does add a small performance penalty,
    as more metrics are being collected on a continuous basis in your running binaries.
    Many companies (Google included) feel that this trade-off is acceptable. Adding
    an additional 5% overhead for CPU and memory profiling is worth the cost in order
    to consistently write performant code.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing profiling with go test
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can create both CPU and memory profiles using the `go test` command. This
    can be useful if you'd like to compare the outputs from multiple test runs. These
    outputs will often be stored in long-term storage for comparison over a longer
    date range. To execute CPU and memory profiles for a test, execute the `go test
    -cpuprofile /tmp/cpu.prof -memprofile /tmp/mem.prof -bench` command.
  prefs: []
  type: TYPE_NORMAL
- en: This will create two output files, `cpu.prof` and `mem.prof`, which will both
    be stored in the `/tmp/` folder. These resulting profiles can be analyzed using
    the techniques in the *Analyzing profiles* section later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Manually instrumenting profiling in code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If there is a particular place in your code that you'd like to profile specifically,
    you can implement profiling directly around that code. This can be especially
    useful if you only want to profile a smaller segment of your code, if you want
    the `pprof` output to be smaller and more concise, or if you don't want to add
    additional overhead to known expensive parts of your code by implementing profiling
    around them. There are separate methodologies for performing CPU and memory profiling
    around different segments of a code base.
  prefs: []
  type: TYPE_NORMAL
- en: 'Profiling CPU utilization for a specific chunk of code would look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Profiling memory utilization for a specific chunk of code would look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Hopefully, we won't have to implement individual segments of code if we design
    effectively, iterate with impact, and implement our profiling using the idioms
    from the next section, but it is nice to know that this is always a potential
    option for profiling code and retrieving meaningful output.
  prefs: []
  type: TYPE_NORMAL
- en: Profiling running service code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The most commonly utilized method for implementing profiling in Go code is enabling
    the profiler in your HTTP handler function. This can be useful for debugging live
    production systems. Being able to profile your production system in real time
    lets you make decisions based on real, live production data, rather than your
    local development environment.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, errors only occur when the order of magnitude of data for a particular
    system reaches a specific scale. A method or function that can handle 1,000 data
    points effectively might not be able to handle 1,000,000 data points effectively
    on the underlying hardware that the function or method is running on. This is
    especially important while running on hardware that changes. Whether you are running
    on Kubernetes with noisy neighbors, a new piece of physical hardware with unknown
    specs, or with a new version of a piece of code or third-party library, understanding
    the performance impact of changes is paramount in creating reliability and resilience.
  prefs: []
  type: TYPE_NORMAL
- en: Being able to receive the data from a production system, where your end users
    and the order of magnitude of their data may be larger than what you use locally,
    can help you to make performance improvements impacting the end user that you
    might not have ever spotted when iterating locally.
  prefs: []
  type: TYPE_NORMAL
- en: If we'd like to implement the `pprof` library in our HTTP handler, we can use
    the `net/http/pprof` library. This can be done by importing `_ "net/http/pprof"` into
    your main package.
  prefs: []
  type: TYPE_NORMAL
- en: 'Your HTTP handler will then have HTTP handlers registered for your profiling.
    Make sure that you aren''t performing this action on a publicly exposed HTTP server;
    having the breakdown of your program''s profile would expose some serious security
    vulnerabilities. The index of the `pprof` package displays paths that become available
    to you when you utilize this package. The following is a screenshot of the index
    of the `pprof` tool:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f6f83adb-86ca-49aa-83a5-38ed0d43d3dc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can take a look at the exposed HTTP `pprof` paths and their descriptions.
    The paths and related descriptions can be found in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Name** | **HTTP path** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| `allocs` | `/debug/pprof/allocs` | Memory allocation information. |'
  prefs: []
  type: TYPE_TB
- en: '| `block` | `/debug/pprof/block` | Information on where the goroutines block
    waits. This typically happens on synchronization primitives. |'
  prefs: []
  type: TYPE_TB
- en: '| `cmdline` | `/debug/pprof/cmdline` | Values of the invocation of the command
    line of our binary. |'
  prefs: []
  type: TYPE_TB
- en: '| `goroutine` | `/debug/pprof/goroutine` | Stack traces of goroutines that
    are currently running. |'
  prefs: []
  type: TYPE_TB
- en: '| `heap` | `/debug/pprof/heap` | Memory allocations sampling (for monitoring
    memory usage and leaks). |'
  prefs: []
  type: TYPE_TB
- en: '| `mutex` | `/debug/pprof/mutex` | Contended mutex stack traces. |'
  prefs: []
  type: TYPE_TB
- en: '| `profile` | `/debug/pprof/profile` | The CPU profile. |'
  prefs: []
  type: TYPE_TB
- en: '| `symbol` | `/debug/pprof/symbol` | Request program counters. |'
  prefs: []
  type: TYPE_TB
- en: '| `threadcreate` | `/debug/pprof/threadcreate` | OS thread creation stack traces.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `trace` | `/debug/pprof/trace` | Current program trace. This will be discussed
    in depth in [Chapter 13](ec12b9e7-c528-45c2-b0b8-dea297659b3e.xhtml), *Tracing
    Go Code*. |'
  prefs: []
  type: TYPE_TB
- en: In the next section, we will discuss CPU profiling.
  prefs: []
  type: TYPE_NORMAL
- en: Briefing on CPU profiling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s perform some example profiling on a simple Go program in order to understand
    how the profiler works. We will create a sample program with a couple of sleep
    parameters in order to see the timings for different function calls:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we instantiate our package and add all of our imports:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, in our `main` function, we have an HTTP handler that has two sleep functions
    that are called as part of the handler:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Our `sleep` function just sleeps for a particular millisecond duration and
    prints the resulting output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: When we run our program, we see the output `go run httpProfiling.go`. To generate
    a profile from this particular code, we need to call `curl -s "localhost:1234/debug/pprof/profile?seconds=10"
    > out.dump`. This will run a profile for 10 seconds and return the results to
    a file named `out.dump`. By default, the `pprof` tool will run for 30 seconds
    and return the binary to `STDOUT`. We want to make sure that we limit the time
    of this test to something that is reasonable for the test duration, and we need
    to redirect the output in order to be able to capture something meaningful to
    look at in our profiling tools.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we generate a test load for our function. We can use Apache Bench to accomplish
    this task, generating 5,000 requests with a concurrency of 10; we set this up
    using `ab -n 5000 -c 10 http://localhost:1234/`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once we get the output from this test, we can take a look at our `out.dump`
    file, `go tool pprof out.dump`. This will take you into the profiler. This is
    a slight variant of the C++ profiler `pprof`. This tool has quite a bit of functionality.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We can use the `topN` command to look at the top *N* samples that are contained
    in the profile we generated, as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/2559344c-6b24-4454-88e5-f3b963485d80.png)'
  prefs: []
  type: TYPE_IMG
- en: 'While the profiler is being executed, Go stops the program roughly 100 times
    per second. During this time, it records the program counters on the goroutine''s
    stack. We can also use the cumulative flag `(-cum)` in order to sort by the cumulative
    values that we have in our current profile sampling:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e0b8c731-8d0f-4b85-ae22-87ef49d77237.png)'
  prefs: []
  type: TYPE_IMG
- en: We also have the ability to display a visual representation of the trace in
    graph form. After we make sure that the `graphviz` package is installed (it should
    be included in your package manager, or it can be downloaded from [http://www.graphviz.org/](http://www.graphviz.org/)
    by simply typing the `web` command)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This will give us a visual representation of the profile that we generated
    from within our program:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f4b4289d-63a6-459f-817e-fd642cceee4c.png)'
  prefs: []
  type: TYPE_IMG
- en: The red boxes in the profile are the code paths that are the most impactful
    to the request flow. We can take a look at these boxes, and, as we'd expect, we
    can see that a good portion of our sample program takes time in sleeping and writing
    responses back to the client. We can take a look at the specific functions in
    this same web format by passing the name of the function that we want to see a
    web graph for. For example, if we want to see a detailed view of our `sleep` function,
    we can just type the `(pprof) web sleep` command.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll then get an `SVG` with the image focused on the sleep call:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/66241c91-a271-45ea-8575-4766a652208d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'After we get this breakdown, we may want to take a look into what the sleep
    function is actually performing. We can use the `list` command in `pprof` in order
    to get the output that profiles the invocation of the `sleep` command and its
    subsequent calls. The following screenshot shows this; the code is shortened for
    the sake of brevity:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/89c2dac7-abe2-414f-9738-993f76fafc9a.png)'
  prefs: []
  type: TYPE_IMG
- en: Being able to break down the work we are doing by profiling into segmentable
    chunks can tell us a lot about the direction we need to take our development in
    from a utilization perspective.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will see what memory profiling is.
  prefs: []
  type: TYPE_NORMAL
- en: Briefing on memory profiling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can perform similar actions to the CPU testing that we did in the previous
    section with memory. Let's take a look at another method to handle profiling,
    using the testing functionality. Let's use an example that we created back in
    [Chapter 2](7bacdf42-9455-4499-a70a-c50c9a6c4e26.xhtml), *Data Structures and
    Algorithms—*the `o-logn` function. We can use the benchmark that we have already
    created for this particular function and add some memory profiling to this particular
    test. We can execute the `go test -memprofile=heap.dump -bench` command.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will see a similar output to what we saw in [Chapter 2](7bacdf42-9455-4499-a70a-c50c9a6c4e26.xhtml), *Data
    Structures and Algorithms*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6e4f4592-7126-4df9-9b54-48cc7363664b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The only difference is that now we''ll have the heap profile from this test.
    If we view it with the profiler, we''ll see data about the heap usage rather than
    the CPU usage. We''ll also be able to see the memory allocation for each of our
    functions in that program. The following diagram illustrates this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0addac7e-6218-4608-8628-cad13ac28e26.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This is helpful, as it enables us to see the generated heap sizes for each
    of the parts of this code. We can also take a look at the top cumulative memory
    allocations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/df629a1d-7502-4fba-88f8-41e8e3219e28.png)'
  prefs: []
  type: TYPE_IMG
- en: As our programs grow more complex, being able to understand the state of the
    memory utilization becomes more and more important. In the next section, we will
    discuss how to extend our profiling capabilities with upstream `pprof`.
  prefs: []
  type: TYPE_NORMAL
- en: Extended capabilities with upstream pprof
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If we want to be able to use additional functionality by default, we can use
    the upstream `pprof` binary in order to extend the views we have with our profiles:'
  prefs: []
  type: TYPE_NORMAL
- en: We can retrieve this with an invocation of `go get github.com/google/pprof`. The `pprof` tool has
    a couple different invocation methods. We can use the report generation method
    to generate a file in the requested format (currently supported are the `.dot`,
    `.svg`, `.web`, `.png`, `.jpg`, `.gif`, and `.pdf` formats). We can also use the
    interactive terminal format in a similar way to what we did in the previous sections
    about CPU and memory profiling. The last and most commonly used method is using
    the HTTP server. This method involves hosting an HTTP server that includes much
    of the pertinent output in an easily digestible format.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Once we have retrieved the binary via `go get`, we can invoke it with a web
    interface, looking at an output that we generated previously: `pprof -http=:1234
    profile.dump`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We can then visit the newly available UI and see the features and functionality
    that were not built into the default `pprof` tool. A couple of the key highlights
    available from this tool are as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A regex searchable form field to help with searching for necessary profiling
    elements
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A drop-down view menu for easy viewing of the different profiling tools available
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A sample dropdown to display the samples from the profile
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A refine filter for hiding/showing different parts of the request flow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Having all of these tools at our disposal for profiling helps to make the profiling
    process more streamlined. If we want to take a look at the time that is taken
    to run anything with a `fmt` in the name of the call, we can use the sample view
    with the regex filter, and it''ll highlight the `fmt` calls, as we can see in
    the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dd1b454e-ebb1-4ded-aaa4-a57165b00f4a.png)'
  prefs: []
  type: TYPE_IMG
- en: Being able to filter according to these values can be helpful in narrowing the
    scope of your ill-performing function.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing multiple profiles
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One of the really nice features of profiling is that you can compare profiles
    with one another. If we have two separate measurements from the same program,
    we can determine whether or not the change we made is having a positive impact
    on the system. Let''s augment our HTTP sleep timing function a little bit:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s add a few extra imports:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we''ll augment our handler to take a query string parameter for `time`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll leave our sleep function exactly the same:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have this extra functionality, we can take multiple profiles with
    different timings just by passing a query parameter to our HTTP handler:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We can run our new timed profiling tool:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'In another Terminal, we can start our profiling tool:'
  prefs:
  - PREF_OL
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then make many requests for our new resource:'
  prefs:
  - PREF_OL
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then gather a second profile:'
  prefs:
  - PREF_OL
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we make a second request for our new resource, generating a second profile:'
  prefs:
  - PREF_OL
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We now have two separate profiles available, which are stored in `5-millisecond-profile.dump`
    and `10-millisecond-profile.dump`. We can compare these using the same tools as
    before, setting a base profile and a secondary profile. The following screenshot
    illustrates this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/fa125aa7-f40e-4fcc-b0f1-f7de7862cb9b.png)'
  prefs: []
  type: TYPE_IMG
- en: Comparing profiles allows us to understand how changes impact our systems.
  prefs: []
  type: TYPE_NORMAL
- en: Let's move on to flame graphs in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Interpreting flame graphs within pprof
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the most helpful/useful tools in the upstream `pprof` package is the
    flame graph. A flame graph is a fixed-rate sampling visualization that can help
    to determine hot codepaths in a profile. As your programs get more and more complex,
    the profiles become larger and larger. It will often become difficult to know
    exactly what codepath is eating up the most CPU, or, as I often like to call it,
    *the long pole in the tent*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Flame graphs were originally developed by Brendan Gregg at Netflix to solve
    a MySQL CPU utilization problem. The advent of this visualization has helped many
    programmers and system administrators determine what the source of latency is
    in their program. The `pprof` binary produces an icicle-style (flames pointing
    downward) flame graph. In a flame graph, we have data visualized in a specific
    frame:'
  prefs: []
  type: TYPE_NORMAL
- en: The *x* axis is the collection of all of the samples from our request
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The y axis shows the number of frames that are on the stack, also often known
    as the stack depth
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The width of the box shows the total amount of CPU time a particular function
    call used
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These three things visualized together helps to determine which part of the
    program introduces the most latency. You can visit the flame graph section of
    the `pprof` profile at `http://localhost:8080/ui/flamegraph`. The following image
    shows an example of such a flame graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8be26f48-4577-485d-999c-888ddc547d29.png)'
  prefs: []
  type: TYPE_IMG
- en: If we look at our `bubbleSort` example from [Chapter 2](7bacdf42-9455-4499-a70a-c50c9a6c4e26.xhtml),
    *Data Structures and Algorithms*, we can see the breakdown of the different bits
    that take up CPU time in our tests. In the interactive web mode, we can hover
    over each of these samples and validate their duration and percentage execution
    time.
  prefs: []
  type: TYPE_NORMAL
- en: In the upcoming section, we will see how to detect memory leaks in Go.
  prefs: []
  type: TYPE_NORMAL
- en: Detecting memory leaks in Go
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As discussed in the *Memory object allocation* section of [Chapter 8](8b95d93a-6bc1-4307-bd75-0e317898321a.xhtml),
    *Memory Management in Go*, we have a myriad of tools at our disposal to view the
    current memory statistics for our currently executing program. In this chapter,
    we will also learn about profiling using the pprof tool. One of the more common
    Go memory leaks is the unbounded creation of goroutines. This happens frequently
    when you overload an unbuffered channel or you have an abstraction with a lot
    of concurrency spawning new goroutines that don't finish. Goroutines have a very
    small footprint and systems can often spawn a very large number of them, but they
    eventually have an upper bound that becomes taxing to find when trying to troubleshoot
    your program in a production environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, we are going to look at an unbuffered channel that
    has a leaky abstraction:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by initializing our package and importing our necessary dependencies:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'In our main function, we handle HTTP listening to and serving the `leakyAbstraction`
    function.  We are serving this over HTTP in order to make it simple to see the
    number of goroutines grow:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'In our `leakyAbstraction` function, we first initialize an unbuffered string
    channel.  We then endlessly iterate through a for loop, writing the number of
    goroutines to the HTTP response writer and writing the result of our `wait()`
    function to the channel:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Our `wait()` function sleeps for five microseconds and returns a string:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'These functions together will spawn goroutines until the runtime is no longer
    able to do so and dies.  We can test this by running our server by executing the
    following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'After the server is running, in a separate Terminal window, we can make a request
    to our server with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The `curl` command will print the number of goroutines generated until the
    server is killed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e971d565-e6a6-4825-980c-71da2d27ecba.png)'
  prefs: []
  type: TYPE_IMG
- en: Please note that this request may take a while depending on the specifications
    of your system.  This is okay—it illustrates the number of goroutines that your
    program has available for use.
  prefs: []
  type: TYPE_NORMAL
- en: Using the techniques we learn in this chapter, we will be able to further debug
    memory issues like this one with pprof, but understanding the underlying problems
    will help us to avoid memory issues.
  prefs: []
  type: TYPE_NORMAL
- en: 'This example was written in order to explicitly show a memory leak, but if
    we wanted to make this executable not leak goroutines, we''d have to fix two things:'
  prefs: []
  type: TYPE_NORMAL
- en: Our unbounded for loop should most likely have a bound
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We could add a buffered channel in order to make sure we have the ability to
    process all the spawned goroutines that come in through the channel
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have learned about profiles—what profiles are and how to
    generate profiles using `pprof`. You also learned how to analyze profiles using
    different methodologies, how to compare profiles, and how to read flame graphs
    for performance. Being able to perform this action in a production environment
    will help you to maintain stability, improve performance, and give your end user
    a better end user experience. In the next chapter, we will discuss another methodology
    of analyzing code—tracing.
  prefs: []
  type: TYPE_NORMAL
