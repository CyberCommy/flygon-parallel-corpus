- en: Scraping - Code of Conduct
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover:'
  prefs: []
  type: TYPE_NORMAL
- en: Scraping legality and scraping politely
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Respecting robots.txt
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Crawling using the sitemap
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Crawling with delays
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using identifiable user agents
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting the number of concurrent requests per domain
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using auto throttling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Caching responses
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While you can technically scrape any website, it is important to know whether
    scraping is legal or not. We will discuss scraping legal concerns, explore general
    rules of thumb, and see best practices to scrape politely and minimize potential
    damage to the target websites.
  prefs: []
  type: TYPE_NORMAL
- en: Scraping legality and scraping politely
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There's no real code in this recipe.  It's simply an exposition of some of the
    concepts related to the legal issues involved in scraping.  I'm not a lawyer,
    so don't take anything I write here as legal advice.  I'll just point out a few
    things you need to be concerned with when using a scraper.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The legality of scraping breaks down into two issues:'
  prefs: []
  type: TYPE_NORMAL
- en: Ownership of content
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Denial of service
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fundamentally, anything posted on the web is open for reading.  Every time you
    load a page, any page, your browser downloads that content from the web server
    and visually presents it to you. So in a sense, you and your browser are already
    scraping anything you look at on the web.  And by the nature of the web, because
    someone is posting content publicly on the web, they are inherently asking you
    to take that information, but often only for specific purposes.
  prefs: []
  type: TYPE_NORMAL
- en: The big issue comes with creating automated tools that directly look for and
    make copies of *things* on the internet, with a *thing* being either data, images,
    videos, or music - essentially things that are created by others and represent
    something that has value to the creator, or owners. These items may create issues
    when explicitly making a copy of the item for your own personal use, and are much
    more likely to create issues when making a copy and using that copy for your or
    others' gain.
  prefs: []
  type: TYPE_NORMAL
- en: Videos, books, music, and images are some of the obvious items of concern over
    the legality of making copies either for personal or commercial use. In general,
    if you scrape content such as this from open sites, such as those that do not
    require authorized access or require payment for access to the content, then you
    are fine.  There are also *fair use* rules that allow the reuse of content in
    certain situations, such as small amounts of document sharing in a classroom scenario,
    where knowledge that is published for people to learn is shared and there is no
    real economic impact.
  prefs: []
  type: TYPE_NORMAL
- en: Scraping of *data* from websites is often a much fuzzier problem.  By data I
    mean information that is provided as a service.  A good example, from my experience,
    is energy prices that are published to a provider's website.  These are often
    provided as a convenience to customers, but not for you to scrape freely and use
    the data for your own commercial analytics service. That data can often be used
    without concern if you are just collecting it for a non-public database or you
    are only using for your own use, then it is likely fine.  But if you use that
    database to drive your own website and share that content under your own name,
    then you might want to watch out.
  prefs: []
  type: TYPE_NORMAL
- en: The point is, check out the disclaimers / terms of service on the site for what
    you can do with that information. It should be documented, but if it is not, then
    that does not mean that you are in the clear to go crazy. Always be careful and
    use common sense, as you are taking other peoples content for you own purposes.
  prefs: []
  type: TYPE_NORMAL
- en: The other concern, which I lump into a concept known as denial of service, relates
    to the actual process of collecting information and how often you do it. The process
    of manually reading content on a site differs significantly to writing automated
    bots that relentlessly badger web servers for content. Taken to an extreme, this
    access frequency could be so significant that it denies other legitimate users
    access to the content, hence denying them service. It can also increase costs
    for the hosters of the content by increasing their cost for bandwidth, or even
    electrical costs for running the servers.
  prefs: []
  type: TYPE_NORMAL
- en: A well managed website will identify these types of repeated and frequent access
    and shut them down using tools such as web application firewalls with rules to
    block your access based on IP address, headers, and cookies.  In other cases,
    these may be identified and your ISP contacted to get you to stop doing these
    tasks. Remember, you are never truly anonymous, and smart hosters can figure out
    who you are, exactly what you accessed, and when you accessed it.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So how do you go about being a good scraper?  There are several factors to
    this that we will cover in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: You can start with respecting the `robots.txt` file
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Don't crawl every link you find on a site, just those given in a site map
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Throttle your requests, so as do as Han Solo said to Chewbacca: Fly Casual;
    or, don''t look like you are repeatedly taking content by Crawling Casual'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identify yourself so that you are known to the site
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Respecting robots.txt
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Many sites want to be crawled. It is inherent in the nature of the beast: Web
    hosters put content on their sites to be seen by humans. But it is also important
    that other computers see the content. A great example is search engine optimization
    (SEO). SEO is a process where you actually design your site to be crawled by spiders
    such as Google, so you are actually encouraging scraping. But at the same time,
    a publisher may only want specific parts of their site crawled, and to tell crawlers
    to keep their spiders off of certain portions of the site, either it is not for
    sharing, or not important enough to be crawled and wast the web server resources.'
  prefs: []
  type: TYPE_NORMAL
- en: The rules of what you are and are not allowed to crawl are usually contained
    in a file that is on most sites known as `robots.txt`. The `robots.txt` is a human
    readable but parsable file, which can be used to identify the places you are allowed,
    and not allowed, to scrape.
  prefs: []
  type: TYPE_NORMAL
- en: 'The format of the `robots.txt` file is unfortunately not standard and anyone
    can make their own modifications, but there is very strong consensus on the format.
    A `robots.txt` file is normally found at the root URL of the site. To demonstrate
    a`robots.txt` file, the following code contains excerpts of the one provided by
    Amazon at [http://amazon.com/robots.txt](http://amazon.com/robots.txt).  I''ve
    edited it down to just show the important concepts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'It can be seen that there are three main elements in the file:'
  prefs: []
  type: TYPE_NORMAL
- en: A user agent declaration for which the following lines, until the end of file
    or next user agent statement, are to be applied
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A set of URLs that are allowed to be crawled
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A set of URLs are prohibited from being crawled
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The syntax is actually quite simple, and Python libraries exist to help us implement
    the rules contained within `robots.txt`. We will be using the `reppy` library
    to facilitate honoring `robots.txt`.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's examine how to demonstrate using `robots.txt` with the reppy library. For
    more information on reppy, see its GitHub page at [https://github.com/seomoz/reppy](https://github.com/seomoz/reppy).
  prefs: []
  type: TYPE_NORMAL
- en: '`reppy` can be installed like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'However, I found that on my Mac I got an error during installation, and it
    required the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: General information/searching on Google for a `robots.txt` Python parsing library
    will generally guide you toward using the robotparser library. This library is
    available for Python 2.x.  For Python 3, it has been moved into the `urllib` library. 
    However, I have found that this library reports incorrect values in specific scenarios.
    I'll point that out in our example.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To run the recipe, execute the code in  `05/01_sitemap.py`.  The script will
    examine whether several URLs are allowed to be crawled on amazon.com.  When running
    it, you will see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: How it works
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The script begins by importing `reppy.robots`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The code then uses `Robots` to fetch the `robots.txt` for amazon.com.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the content that was fetched, the script checks several URLs for accessibility:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The results of this code is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The call to `robots.allowed` is given the URL and the user agent. It returns
    `True` or `False` based upon whether the URL is allowed to be crawled. In this
    case, the results where True, False, True and False for the specified URLs. Let's
    examine how.
  prefs: []
  type: TYPE_NORMAL
- en: 'The / URL has no entry in `robots.txt`, so it is allowed by default.  But in
    the file under the * user agent group are the following two lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '/gp/dmusic is not allowed, so False is returned. /gp/dmusic/promotions/PrimeMusic
    is explicitly allowed. If the Allowed: entry was not specified, then the Disallow:
    /gp/dmusic/ line would also disallow any further paths down from /gp/dmusic/. 
    This essentially says that any URLs starting with /gp/dmusic/ are disallowed,
    except that you are allowed to crawl /gp/dmusic/promotions/PrimeMusic.'
  prefs: []
  type: TYPE_NORMAL
- en: Here is where there is a difference when using the `robotparser` library. `robotparser`
    reports that `/gp/dmusic/promotions/PrimeMusic` is disallowed. The library does
    not handle this type of scenario correctly, as it stops scanning `robots.txt`
    at the first match, and does not continue further into the file to look for any
    overrides of this kind.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First, for detailed information on `robots.txt`, see [https://developers.google.com/search/reference/robots_txt](https://developers.google.com/search/reference/robots_txt).
  prefs: []
  type: TYPE_NORMAL
- en: Note that not all sites have a `robots.txt`, and its absence does not imply
    you have free rights to crawl all the content.
  prefs: []
  type: TYPE_NORMAL
- en: Also, a `robots.txt` file may contain information on where to find the sitemap(s)
    for the website. We examine these sitemaps in the next recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Scrapy can also read `robots.txt` and find sitemaps for you.
  prefs: []
  type: TYPE_NORMAL
- en: Crawling using the sitemap
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A sitemap is a protocol that allows a webmaster to inform search engines about
    URLs on a website that are available for crawling.  A webmaster would want to
    use this as they actually want their information to be crawled by a search engine.
    The webmaster wants to make that content available for you to find, at least through
    search engines. But you can also use this information to your advantage.
  prefs: []
  type: TYPE_NORMAL
- en: 'A sitemap lists the URLs on a site, and allows a webmasters to specify additional
    information about each URL:'
  prefs: []
  type: TYPE_NORMAL
- en: When it was last updated
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How often the content changes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How important the URL is in relation to others
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sitemaps are useful on websites where:'
  prefs: []
  type: TYPE_NORMAL
- en: Some areas of the website are not available through the browsable interface;
    that is, you cannot reach those pages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ajax, Silverlight, or Flash content is used but not normally processed by search
    engines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The site is very large and there is a chance for the web crawlers to overlook
    some of the new or recently updated content
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When websites have a huge number of pages that are isolated or not well linked
    together
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When a website has few external links
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A sitemap file has the following structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Each URL in the site will be represented with a `<url></url>` tag, with all
    those tags wrapped in an outer `<urlset></urlset>` tag. There will always a `<loc></loc>`
    tag specifying the URL. The other three tags are optional.
  prefs: []
  type: TYPE_NORMAL
- en: 'Sitemaps files can be incredibly large, so they are often broken into multiple
    files and then referenced by a single sitemap index file. This file has the following
    format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: In most cases, the `sitemap.xml` file is found at the root of the domain.  As
    an example, for nasa.gov it is[ https://www.nasa.gov/sitemap.xml](https://www.nasa.gov/sitemap.xml). 
    But note that this is not a standard, and different sites may have the map, or
    maps, at different locations.
  prefs: []
  type: TYPE_NORMAL
- en: 'A sitemap for a particular website may also be located within the site''s `robots.txt`
    file. As an example, the `robots.txt` file for microsoft.com ends with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Therefore, to get microsoft.com's sitemaps, we would first need to read the
    `robots.txt` file and extract that information.
  prefs: []
  type: TYPE_NORMAL
- en: Let's now look at parsing a sitemap.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Everything you need is in the `05/02_sitemap.py` script, along with the `sitemap.py`
    file in then same folder. The `sitemap.py` file implements a basic sitemap parser
    that we will use in the main script. For the purposes of this example, we will
    get the sitemap data for nasa.gov.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First execute the `05/02_sitemap.py` file. Make sure that the associated `sitemap.py`
    file is in the same directory or your path. When running, after a few seconds
    you will get output similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The program found 35,511 URLs throughout all of the nasa.gov sitemaps! The code
    only printed the first 10 as this would have been quite a bit of output. Using
    this info to initialize a crawl of all of these URLs will definitely take quite
    a long time!
  prefs: []
  type: TYPE_NORMAL
- en: But this is also the beauty of the sitemap. Many, if not all, of these results
    have a `lastmod` tag that tells you when the content at the end of that associated
    URL was last modified. If you are implementing a polite crawler of nasa.gov, you
    would want to keep these URLs and their timestamp in a database, and then before
    crawling that URL check to see if the content has actually changed, and don't
    crawl if it hasn't.
  prefs: []
  type: TYPE_NORMAL
- en: Now let's see how this actually worked.
  prefs: []
  type: TYPE_NORMAL
- en: How it works
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The recipe works as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The script starts by calling `get_sitemap()`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'This is given a URL to the sitemap.xml file (or any other file - non-gzipped).
    The implementation simply gets the content at the URL and returns it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The bulk of the work is done by passing that content to `parse_sitemap()`.
    In the case of nasa.gov, this sitemap contains the following content, a sitemap
    index file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '`process_sitemap()` starts with a call to `process_sitemap()`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'This function starts by calling `process_sitemap()`, which returns a list of
    Python dictionary objects with `loc`, `lastmod`, `changeFreq`, and priority key
    value pairs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: This is performed by parsing the sitemap using `BeautifulSoup` and `lxml`. The  `loc`
    property is always `set`, and `lastmod`, `changeFreq` and priority are set if
    there is an an associated XML tag. The .tag property itself just notes whether
    this content was retrieved from a `<sitemap>` tag or a `<url>` tag (`<loc>` tags
    can be on either).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`parse_sitemap()` then continues with processing those results one by one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Each item is examined. If it is from a sitemap index file (the URL ends in .xml
    and the .tag is the sitemap), then we need to read that .xml file and parse its
    content, whose results are placed into our list of items to process. In this example,
    four sitemap files are identified, and each of these are read, processed, parsed,
    and their URLs added to the result.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To demonstrate some of this content, the following are the first few lines
    of sitemap-1.xml:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Overall, this one sitemap has 11,006 lines, so roughly 11,000 URLs!  And in
    total, as was reported, there are 35,511 URLs  across all three sitemaps.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sitemap files may also be zipped, and end in a .gz extension. This is because
    it likely contains many URLs and the compression will save a lot of space. While
    the code we used does not process gzip sitemap files, it is easy to add this using
    functions in the gzip library.
  prefs: []
  type: TYPE_NORMAL
- en: 'Scrapy also provides a facility for starting crawls using the sitemap. One
    of these is a specialization of the Spider class, SitemapSpider. This class has
    the smarts to parse the sitemap for you, and then start following the URLs. To
    demonstrate, the script `05/03_sitemap_scrapy.py` will start the crawl at the
    nasa.gov top-level sitemap index:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'When running this, there will be a ton of output, as the spider is going to
    start crawling all 30000+ URLs.  Early in the output, you will see output such
    as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Scrapy has found all of the sitemaps and read in their content. Soon afterwards,
    you will start to see a number of redirections and notifications that certain
    pages are being parsed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Crawling with delays
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Fast scraping is considered a bad practice. Continuously pounding a website
    for pages can burn up CPU and bandwidth, and a robust site will identify you doing
    this and block your IP. And if you are unlucky, you might get a nasty letter for
    violating terms of service!
  prefs: []
  type: TYPE_NORMAL
- en: The technique of delaying requests in your crawler depends upon how your crawler
    is implemented. If you are using Scrapy, then you can set a parameter that informs
    the crawler how long to wait between requests. In a simple crawler just sequentially
    processing URLs in a list, you can insert a thread.sleep statement.
  prefs: []
  type: TYPE_NORMAL
- en: Things can get more complicated if you have implemented a distributed cluster
    of crawlers that spread the load of page requests, such as using a message queue
    with competing consumers. That can have a number of different solutions, which
    are beyond the scope provided in this context.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will examine using Scrapy with delays. The sample is in `o5/04_scrape_with_delay.py`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Scrapy by default imposes a delay of 0 seconds between page requests. That is,
    it does not wait between requests by default.
  prefs: []
  type: TYPE_NORMAL
- en: 'This can be controlled using the `DOWNLOAD_DELAY` setting.  To demonstrate,
    let''s run the script from the command line:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: This crawls all of the pages at blog.scrapinghub.com, and reports the total
    time to perform the crawl. `LOG_LEVEL=WARNING` removes most logging output and
    just gives out the output from print statements. This used the default wait between
    pages of 0 and resulted in a crawl roughly seven seconds in length.
  prefs: []
  type: TYPE_NORMAL
- en: 'The wait between pages can be set using the `DOWNLOAD_DELAY` setting. The following
    delays for five seconds between page requests:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: By default, this does not actually wait 5 seconds. It will wait `DOWNLOAD_DELAY`
    seconds, but by a random factor between 0.5 and 1.5 times `DOWNLOAD_DELAY`. Why
    do this? This makes your crawler look "less robotic." You can turn this off by
    using the `RANDOMIZED_DOWNLOAD_DELAY=False` setting.
  prefs: []
  type: TYPE_NORMAL
- en: How it works
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This crawler is implemented as a Scrapy spider. The class definition begins
    with declaring the spider name and the start URL:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The parse method looks for CSS 'div.prev-post > a', and follows those links.
  prefs: []
  type: TYPE_NORMAL
- en: 'The scraper also defines a close method, which is called by Scrapy when the
    crawl is complete:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: This accesses the spiders crawler stats object, retrieves the start and finish
    time for the spider, and reports the difference to the user.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The script also defines code for when executing the script directly with Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: This begins by creating a CrawlerProcess object. This object can be passed a
    dictionary representing the settings and values to configure the crawl with. This
    defaults to a five-second delay, without randomization, and an output level of
    DEBUG.
  prefs: []
  type: TYPE_NORMAL
- en: Using identifiable user agents
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What happens if you violate the terms of service and get flagged by the website
    owner? How can you help the site owners in contacting you, so that they can nicely
    ask you to back off to what they consider a reasonable level of scraping?
  prefs: []
  type: TYPE_NORMAL
- en: 'What you can do to facilitate this is add info about yourself in the User-Agent
    header of the requests. We have seen an example of this in `robots.txt` files,
    such as from amazon.com. In their `robots.txt` is an explicit statement of a user
    agent for Google: GoogleBot.'
  prefs: []
  type: TYPE_NORMAL
- en: During scraping, you can embed your own information within the User-Agent header
    of the HTTP requests. To be polite, you can enter something such as 'MyCompany-MyCrawler
    (mybot@mycompany.com)'. The remote server, if tagging you in violation, will definitely
    be capturing this information, and if provided like this, it gives them a convenient
    means of contacting your instead of just shutting you down.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Setting the user agent differs depending upon what tools you use. Ultimately,
    it is just ensuring that the User-Agent header is set to a string that you specify.
    When using a browser, this is normally set by the browser to identity the browser
    and the operating system. But you can put anything you want into this header.
    When using requests, it is very straightforward:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'When using Scrapy, it is as simple as configuring a setting:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: How it works
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Outgoing HTTP requests have a number of different headers. These ensure that
    the User-Agent header is set to this value for all requests made of the target
    web server.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While it is possible to set any content you want in the User-Agent header, some
    web servers will inspect the User-Agent header and make decisions on how to respond
    based upon the content. A common example of this is using the header to identify
    mobile devices to provide a mobile presentation.
  prefs: []
  type: TYPE_NORMAL
- en: But some sites also only allow access to content to specific User-Agent values.
    Setting your own value could have the effect of having the web server not respond
    or return other errors, such as unauthorized. So when you use this technique,
    make sure to check it will work.
  prefs: []
  type: TYPE_NORMAL
- en: Setting the number of concurrent requests per domain
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is generally inefficient to crawl a site one URL at a time. Therefore, there
    is normally a number of simultaneous page requests made to the target site at
    any given time. Normally, the remote web server can quite effectively handle multiple
    simultaneous requests, and on your end you are just waiting for data to come back
    in for each, so concurrency generally works well for your scraper.
  prefs: []
  type: TYPE_NORMAL
- en: But this is also a pattern that smart websites can identify and flag as suspicious
    activity. And there are practical limits on both your crawler's end and the website.
    The more concurrent requests that are made, the more memory, CPU, network connections,
    and network bandwidth is required on both sides. These have costs involved, and
    there are practical limits on these values too.
  prefs: []
  type: TYPE_NORMAL
- en: So it is generally a good practice to set a limit on the number of requests
    that you will simultaneously make to any web server.
  prefs: []
  type: TYPE_NORMAL
- en: How it works
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are number of techniques that can be used to control concurrency levels,
    and the process can often be quite complicated with controlling multiple requests
    and threads of execution. We won't discuss here how this is done at the thread
    level and only mention the construct built into Scrapy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Scrapy is inherently concurrent in its requests.  By default, Scrapy will dispatch
    at most eight simultaneous requests to any given domain. You can change this using
    the `CONCURRENT_REQUESTS_PER_DOMAIN` setting. The following sets the value to
    1 concurrent request:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Using auto throttling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Fairly closely tied to controlling the maximum level of concurrency is the concept
    of throttling. Websites vary in their ability to handle requests, both across
    multiple websites and on a single website at different times. During periods of
    slower response times, it makes sense to lighten up of the number of requests
    during that time. This can be a tedious process to monitor and adjust by hand.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately for us, scrapy also provides an ability to do this via an extension
    named `AutoThrottle`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'AutoThrottle can easily be configured using the `AUTOTHROTTLE_TARGET_CONCURRENCY`
    setting:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: How it works
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: scrapy tracks the latency on each request. Using that information, it can adjust
    the delay between requests to a specific domain so that there are no more than
    `AUTOTHROTTLE_TARGET_CONCURRENCY` requests simultaneously active for that domain,
    and that the requests are evenly distributed in any given time span.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are lot of options for controlling throttling. You can get an overview
    of them at [https://doc.scrapy.org/en/latest/topics/autothrottle.html?&_ga=2.54316072.1404351387.1507758575-507079265.1505263737#settings.](https://doc.scrapy.org/en/latest/topics/autothrottle.html?&_ga=2.54316072.1404351387.1507758575-507079265.1505263737#settings)
  prefs: []
  type: TYPE_NORMAL
- en: Using an HTTP cache for development
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The development of a web crawler is a process of exploration, and one that will
    iterate through various refinements to retrieve the requested information. During
    the development process, you will often be hitting remote servers, and the same
    URLs on those servers, over and over. This is not polite. Fortunately, scrapy
    also comes to the rescue by providing caching middleware that is specifically designed
    to help in this situation.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Scrapy will cache requests using a middleware module named HttpCacheMiddleware.
    Enabling it is as simple as configuring the `HTTPCACHE_ENABLED` setting to `True`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: How it works
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The implementation of HTTP caching is simple, yet complex at the same time.
    The `HttpCacheMiddleware` provided by Scrapy has a plethora of configuration options
    based upon your needs. Ultimately, it comes down to storing each URL and its content
    in a store along with an associated duration for cache expiration. If a second
    request is made for a URL within the expiration interval, then the local copy
    will be retrieved instead of making a remote request. If the time has expired,
    then the contents are fetched from the web server, stored in the cache, and a
    new expiration time set.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are many options for configuration scrapy caching, including means of
    storing content (file system, DBM, or LevelDB), cache policies, and how Http Cache-Control
    directives from the server are handled. To explore these options, check out the
    following URL: [https://doc.scrapy.org/en/latest/topics/downloader-middleware.html?_ga=2.50242598.1404351387.1507758575-507079265.1505263737#dummy-policy-default.](https://doc.scrapy.org/en/latest/topics/downloader-middleware.html?_ga=2.50242598.1404351387.1507758575-507079265.1505263737#dummy-policy-default.)
  prefs: []
  type: TYPE_NORMAL
