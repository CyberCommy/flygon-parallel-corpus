- en: Chapter 10. Maximizing Performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With concepts relating to deploying and launching our application behind us,
    we'll lock in high-performance tactics within Go and related third-party packages
    in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: As your web service or API grows, performance issues may come to the fore. One
    sign of a successful web service is a need for more and more horsepower behind
    your stack; however, reducing this need through programmatic best practices is
    an even better approach than simply providing more processing power to your application.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''ll look at:'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing middleware to reduce redundancy in our code and pave the way for
    some performance features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Designing caching strategies to keep content fresh and provide it as quickly
    as possible
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with disk-based caching
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with memory caching
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rate-limiting our API through middleware
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Google's SPDY protocol initiative
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you should know how to build your own middleware
    into your social network (or any other web service) to bring in additional features
    that introduce performance speedups.
  prefs: []
  type: TYPE_NORMAL
- en: Using middleware to reduce cruft
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When working with the Web in Go, the built-in approaches to routing and using
    handlers don't always lend themselves to very clean methods for middleware out
    of the box.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, although we have a very simple `UsersRetrieve()` method, if we
    want to prevent consumers from getting to that point or run something before it,
    we will need to include these calls or parameters multiple times in our code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'And an other call is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Middleware allows us to more cleanly direct the internal patterns of our application,
    as we can apply checks against rate limits and authentication as given in the
    preceding code. We can also bypass calls if we have some external signal that
    tells us that the application should be temporarily offline without stopping the
    application completely.
  prefs: []
  type: TYPE_NORMAL
- en: Considering the possibilities, let's think about useful ways in which we can
    utilize middleware in our application.
  prefs: []
  type: TYPE_NORMAL
- en: 'The best way to approach this is to find places where we''ve inserted a lot
    of needless code through duplication. An easy place to start is our authentication
    steps that exist as a potential block in a lot of sections of code in our `api.go`
    file. Refer to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We call the `CheckLogin()` function multiple times throughout the application,
    so we can offload this to middleware to reduce the cruft and duplicate code throughout.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another method is the access control header setting that allows or denies requests
    based on the permitted domains. We use this for a few things, particularly for
    our server-side requests that are bound to CORS rules:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This too can be handled by middleware as it doesn't require any customization
    that is based on request type. On any request in which we wish to set the permitted
    domains, we can move this code into middleware.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, this represents good code design, but it can sometimes be tricky without
    custom middleware handlers.
  prefs: []
  type: TYPE_NORMAL
- en: 'One popular approach to middleware is chaining, which works something like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This is extremely common within the world of Node.js, where the `next()`, `then()`,
    and `use()` functions pepper the code liberally. And it's possible to do this
    within Go as well.
  prefs: []
  type: TYPE_NORMAL
- en: There are two primary approaches to this. The first is by wrapping handlers
    within handlers. This is generally considered to be ugly and is not preferred.
  prefs: []
  type: TYPE_NORMAL
- en: Dealing with wrapped handler functions that return to their parent can be a
    nightmare to parse.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, let''s instead look at the second approach: chaining. There are a number
    of frameworks that include middleware chaining, but introducing a heavy framework
    simply for the purpose of middleware chaining is unnecessary. Let''s look at how
    we can do this directly within a Go server:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: As mentioned earlier, there are a couple of places in our code and most server-based
    applications where middleware would be very helpful. Later in this chapter, we'll
    look at moving our authentication model(s) into middleware to reduce the amount
    of repetitious calls that we make within our handlers.
  prefs: []
  type: TYPE_NORMAL
- en: However, for performance's sake, another function for a middleware of this kind
    can be used as a blocking mechanism for cache lookups. If we want to bypass potential
    bottlenecks in our `GET` requests, we can put a caching layer between the request
    and the response.
  prefs: []
  type: TYPE_NORMAL
- en: We're using a relational database, which is one of the most common sources of
    web-based bottlenecks; so, in situations where stale or infrequently changing
    content is acceptable, placing the resulting queries behind such a barrier can
    drastically improve our API's overall performance.
  prefs: []
  type: TYPE_NORMAL
- en: Given that we have two primary types of requests that can benefit from middleware
    in different ways, we should spec how we'll approach the middleware strategy for
    various requests.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram is a model of how we can architect middleware. It can
    serve as a basic guide for where to implement specific middleware handlers for
    certain types of API calls:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Using middleware to reduce cruft](img/1304OS_10_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: All requests should be subject to some degree of rate-limiting, even if certain
    requests have much higher limits than others. So, the `GET`, `PUT`, `POST`, and
    `DELETE` requests will run through at least one piece of middleware on every request.
  prefs: []
  type: TYPE_NORMAL
- en: Any requests with other verbs (for example, `OPTIONS`) should bypass this.
  prefs: []
  type: TYPE_NORMAL
- en: The `GET` requests should be subject to caching, which we also described as
    making the data they return amenable to some degree of staleness.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, PUT, POST, and DELETE requests obviously cannot be cached,
    as this will either force our responses to be inaccurate or it will lead to duplicate
    attempts to create or remove data.
  prefs: []
  type: TYPE_NORMAL
- en: Let's start with the `GET` requests and look at two related ways in which we
    can bypass a bottleneck when it is possible to deliver server-cached results instead
    of hitting our relational database.
  prefs: []
  type: TYPE_NORMAL
- en: Caching requests
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are, of course, more than one or two methods for inducing caching across
    the lifetime of any given request. We'll explore a few of them in this section
    to introduce the highest level of nonredundant caching.
  prefs: []
  type: TYPE_NORMAL
- en: There is client-side caching at a script or a browser level that is ostensibly
    bound to the rules that are sent to it from the server side. By this, we mean
    yielding to HTTP response headers such as `Cache-Control`, `Expires`, `If-None-Match`,
    `If-Modified-Since`, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: These are the simplest forms of cache control that you can enforce, and they
    are also pretty important as part of a RESTful design. However, they're also a
    bit brittle as they do not allow any enforcement of those directives and clients
    that can readily dismiss them.
  prefs: []
  type: TYPE_NORMAL
- en: Next, there is proxy-based caching—typically third-party applications that either
    serve a cached version of any given request or pass-through to the originating
    server application. We looked at a precursor to this when we talked about using
    Apache or Nginx in front of our API.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, there is server-level caching at the application level. This is typically
    done in lieu of proxy caching because the two tend to operate on the same rule
    sets. In most cases, appealing to a standalone proxy cache is the wisest option,
    but there are times when those solutions are unable to accommodate specific edge
    cases.
  prefs: []
  type: TYPE_NORMAL
- en: There's also some merit in designing these from scratch to better understand
    caching strategies for your proxy cache. Let's briefly look at building server-side
    application caching for our social network in both disk-based and memory-based
    ways, and see how we can utilize this experience to better define caching rules
    at the proxy level.
  prefs: []
  type: TYPE_NORMAL
- en: Simple disk-based caching
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Not all that long ago, the way most developers handled caching requests was
    typically through disk-based caching at the application level.
  prefs: []
  type: TYPE_NORMAL
- en: In this approach, some parameters were set around the caching mechanisms and
    qualifiers of any given request. Then, the results of the request were saved to
    a string and then to a lock file. Finally, the lock file was renamed. The process
    was pretty steady although it was archaic and worked well enough to be reliable.
  prefs: []
  type: TYPE_NORMAL
- en: There were a number of downsides that were somewhat insurmountable at the time
    in the early days of the Web.
  prefs: []
  type: TYPE_NORMAL
- en: Note that disks, particularly mechanical magnetic disks, have been notoriously
    and comparatively slow for storage and access, and they are bound to cause a lot
    of issues with filesystems and OS operations with regard to lookups, finds, and
    sorting.
  prefs: []
  type: TYPE_NORMAL
- en: Distributed systems also pose an obvious challenge where a shared cache is necessary
    to ensure consistency across balanced requests. If server A updates its local
    cache and the next request returns a cache hit from server B, you can see varying
    results depending on the server. Using a network file server may reduce this,
    but it introduces some issues with permissions and network latency.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, nothing is simpler than saving a version of a request to
    a file. That, along with disk-based caching's long history in other sectors of
    programming, made it a natural early choice.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, it's not entirely fair to suggest that disk-based caching's days are
    over. Faster drives, often SSDs, have reopened the potential for using non-ephemeral
    storage for quick access.
  prefs: []
  type: TYPE_NORMAL
- en: Let's take a quick look at how we can design a disk-based cache middleware solution
    for our API to reduce load and bottlenecks in heavy traffic.
  prefs: []
  type: TYPE_NORMAL
- en: The first consideration to take into account is what to cache. We would never
    want to allow the `PUT`, `POST`, and `DELETE` requests to cache for obvious reasons,
    as we don't want duplication of data nor erroneous responses to `DELETE` or `POST`
    requests that indicate that a resource has been created or deleted when in fact
    it hasn't.
  prefs: []
  type: TYPE_NORMAL
- en: So, we know that we're only caching the `GET` requests or listings of data.
    This is the only data we have that can be "outdated" in the sense that we can
    accept some staleness without making major changes in the way the application
    operates.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start with our most basic request, `/api/users`, which returns a list
    of users in our system, and introduce some middleware for caching to a disk. Let''s
    set it up as a skeleton to explain how we evaluate:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Our `CacheItem` struct is the only real element in the package. It consists
    of either a valid cache hit (and information about the cached element including
    the last modification time, contents, and so on) or a cache miss. A cache miss
    will return to our API that either the item does not exist or has surpassed the
    time-to-live (TTL). In this case, the `diskcache` package will then set the cache
    to file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is where we''ll do this. If a request has no cached response or the cache
    is invalid, we''ll need to get the results back so that we can save it. This makes
    the middleware part a little trickier, but we''ll show you how to handle this
    shortly. The following `GetCache()` function looks into our cache directory and
    either finds and returns a cache item (whether valid or not) or produces a false
    value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The following `Evaluate()` function will be our primary point of entry, passing
    to `GetCache()` and possibly `SetCache()` later, if we need to create or recreate
    our cache entry:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: In this structure, we'll utilize a context (so that we can delineate between
    request types), the resulting value (for saving), and an open-ended variadic of
    strings that we can use as qualifiers for our cache entry. By this, we mean the
    parameters that force a unique cache file to be produced. Let's say we designate
    `page` and `search` as two such qualifiers. Page 1 requests will be different
    than page 2 requests and they will be cached separately. Page 1 requests for a
    search for Nathan will be different from page 1 requests for a search for Bob,
    and so on.
  prefs: []
  type: TYPE_NORMAL
- en: This point is very strict for hard files because we need to name (and look up)
    our cache files in a reliable and consistent way, but it's also important when
    we save caches in a datastore.
  prefs: []
  type: TYPE_NORMAL
- en: With all of this in mind, let's examine how we will discern a cacheable entry
  prefs: []
  type: TYPE_NORMAL
- en: Enabling filtering
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Presently our API does not accept any specific parameters against any of our
    `GET` requests, which return lists of entities or specific details about an entity.
    Examples here include a list of users, a list of status updates, or a list of
    relationships.
  prefs: []
  type: TYPE_NORMAL
- en: You may note that our `UsersRetrieve()` handler presently returns the next page
    in response to a `start` value and a `limit` value. Right now this is hard-coded
    at a start value of `0` and a limit value of `10`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition, we have a `Pragma: no-cache` header that is being set. We obviously
    don''t want that. So, to prepare for caching, let''s add a couple of additional
    fields that clients can use to find particular users they''re looking for by attributes.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first is a start and a limit, which dictates a pagination of sorts. What
    we now have is this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s make this responsive to the request first by accepting a start:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Now, we can accept a start value as well as a limit value. Note that we also
    put a cap on the number of results that we'll return. Any results that are more
    than 50 will be ignored and a maximum of 50 results will be returned.
  prefs: []
  type: TYPE_NORMAL
- en: Transforming a disk cache into middleware
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now we''ll take the skeleton of `diskcache`, turn it into a middleware call,
    and begin to speed up our `GET` requests:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'This obviously represents a strict location for cache files, but it can also
    be branched into subdirectories that are based on a context, for example, our
    API endpoints in this case. So, `/api/users` in a `GET` request would map to `/var/www/cache/users/get/.`
    This reduces the volume of data in a single directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Our generic `CacheItem` struct consists of the file''s name, its physical location,
    the age in seconds, and the contents, as mentioned in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Our `IsValid()` method first determines whether the file exists and is readable,
    if it's older than the `MaxAge` variable. If it cannot be read or if it's too
    old, then we return false, which tells our `Evaluate()` entry point to create
    the file. Otherwise, we return true, which directs the `Evaluate()` function to
    perform a read of the existing cache file.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: In our imports section, you may note that the `sync` package is called; `SetCache()`
    should, in production at least, utilize a mutex to induce locking on file operations.
    We use `Lock()` and `Unlock()` (in a defer) to handle this.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Note that our filename here is generated by joining the parameters in the `qu`
    variadic parameter. If we want to fine-tune this, we will need to sort the parameters
    alphabetically and this will prevent cache misses if the parameters are supplied
    in a different order.
  prefs: []
  type: TYPE_NORMAL
- en: Since we control the originating call, that's low-risk. However, since this
    is built as a shared library, it's important that the behavior should be fairly
    consistent.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'We can test this pretty simply using a tiny example that just writes files
    by value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: If we run this, then change the value of `Here is a value ...`, and run it again
    within 60 seconds, we'll get our cached value. This shows that our diskcache package
    saves and returns values without hitting what could otherwise be a backend bottleneck.
  prefs: []
  type: TYPE_NORMAL
- en: So, let's now put this in front of our `UsersRetrieve()` handler with some optional
    parameters. By setting our cache by `page` and `search` as cacheable parameters,
    we'll mitigate any load-based impact on our database.
  prefs: []
  type: TYPE_NORMAL
- en: Caching in distributed memory
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Similar to disk-based caching, we're bound to a single entity key with simple
    in-memory caching although this is still a useful alternative to disk caching.
  prefs: []
  type: TYPE_NORMAL
- en: Replacing the disk with something like Memcache(d) will allow us to have very
    fast retrieval, but will provide us with no benefit in terms of keys. In addition,
    the potential for large amounts of duplication means that our memory storage that
    is generally smaller than physical storage might become an issue.
  prefs: []
  type: TYPE_NORMAL
- en: However, there are a number of ways to sneak into in-memory or distributed memory
    caching. We won't be showing you that drop-in replacement, but through a segue
    with one NoSQL solution, you can easily translate two types of caching into a
    strict, memory-only caching option.
  prefs: []
  type: TYPE_NORMAL
- en: Using NoSQL as a cache store
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unlike Memcache(d), with a datastore or a database we have the ability to do
    more complex lookups based on non-chained parameters.
  prefs: []
  type: TYPE_NORMAL
- en: For example, in our `diskcache` package, we chain together parameters such as
    `page` and `search` in such a way that our key (in this case a filename) is something
    like `getusers_1_nathan.cache`.
  prefs: []
  type: TYPE_NORMAL
- en: It is essential that these keys are generated in a consistent and reliable way
    for lookup since any change results in a cache miss instead of an expected hit,
    and we will need to rebuild our cached request, which will completely eliminate
    the intended benefit.
  prefs: []
  type: TYPE_NORMAL
- en: For databases, we can do very high-detail column lookups for cache requests,
    but, given the nature of relational databases, this is not a good solution. After
    all, we built the caching layer very specifically to avoid hitting common bottlenecks
    such as a RDBMS.
  prefs: []
  type: TYPE_NORMAL
- en: For the sake of an example, we'll again utilize MongoDB as a way to compile
    and lookup our cache files with high throughput and availability and with the
    extra flexibility that is afforded to parameter-dependent queries.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, we'll add a basic document with just a page, search, contents,
    and a modified field. The last field will serve as our timestamp for analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Despite `page` being a seemingly obvious integer field, we'll create it as a
    string in MongoDB to avoid type conversion when we do queries.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: For obvious reasons, we'll call this `memorycache` instead of memcache to avoid
    any potential confusion.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: We've supplanted any OS and disk-based packages with the MongoDB ones. The BSON
    package is also included as part of making specific `Find()` requests.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In a production environment, when looking for a key-value store or a memory
    store for such intents, one should be mindful of the locking mechanisms of the
    solution and their impact on your read/write operations.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: It's worth noting here that MongoDB has a time-to-live concept for data expiration.
    This might remove the necessity to manually expire content but it may not be available
    in alternative store platforms.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Note the literal identifiers in the `CacheRecord` struct; these allow us to
    generate MongoDB IDs automatically. Without this, MongoDB will complain about
    duplicate indices on `_id_`. The following `IsValid()` function literally returns
    information about a file in our `diskcache` package. In a `memorycache` version,
    we will only return one piece of information, whether or not a record exists within
    the requested age.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Note also that we're not deleting old records. This may be the logical next
    step to keep cache records snappy.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Whether or not we find a record, we insert a new one in the preceding code.
    This gives us the most recent record when we do a lookup and it also allows us
    to have some sense of revision control in a way. You can also update the record
    to eschew revision control.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: This operates in much the same way as `diskcache` except that, instead of a
    list of unstructured parameter names, we provide key/value pairs in the `param`
    hash map.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, the usage changes a little bit. Here''s an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: When we run this, we'll set our content in the datastore and this will last
    for 60 seconds before it becomes invalid and recreates the cache contents in a
    second row.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a cache as middleware
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To place this cache in the middleware chain for all of our `GET` requests, we
    can implement the strategy that we outlined above and add a caching middleware
    element.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using our example from earlier, we can implement this at the front of the chain
    using our `middleware()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'This allows us to execute a `DiskCache()` handler before the `UsersRetrieve()`
    function. However, we''ll also want to save our response if we don''t have a valid
    cache, so we''ll also call `DiskCacheSave()` at the end. The `DiskCache()` middleware
    handler will block the chain if it receives a valid cache. Here''s how that works:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: If we get `check.Cached` as true, we simply serve the contents. Otherwise, we
    continue on.
  prefs: []
  type: TYPE_NORMAL
- en: 'One minor modification to our primary function is necessary to transfer the
    contents to our next function right before writing the function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'And then, `DiskCacheSave()` can essentially be a duplicate of `DiskCache`,
    except that it will set the actual contents from the `http.Request` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Using a frontend caching proxy in front of Go
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another tool in our toolbox is utilizing front-end caching proxies (as we did
    in [Chapter 7](ch07.html "Chapter 7. Working with Other Web Technologies"), *Working
    with Other Web Technologies*) as our request-facing cache layer.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to traditional web servers such as Apache and Nginx, we can also
    employ services that are intended almost exclusively for caching, either in place
    of, in front of, or in parallel with the said servers.
  prefs: []
  type: TYPE_NORMAL
- en: Without going too deeply into this approach, we can replicate some of this functionality
    with better performance from outside the application. We'd be remiss if we didn't
    at least broach this. Tools such as Nginx, Varnish, Squid, and Apache have built-in
    caching for reverse-proxy servers.
  prefs: []
  type: TYPE_NORMAL
- en: For production-level deployments, these tools are probably more mature and better
    suited for handling this level of caching.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can find more information on Nginx and reverse proxy caching at [http://nginx.com/resources/admin-guide/caching/](http://nginx.com/resources/admin-guide/caching/).
  prefs: []
  type: TYPE_NORMAL
- en: Varnish and Squid are both built primarily for caching at this level as well.
    More detail on Varnish and Squid can be found at [https://www.varnish-cache.org/](https://www.varnish-cache.org/)
    and [http://www.squid-cache.org/](http://www.squid-cache.org/) respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Rate limiting in Go
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introducing caching to our API is probably the simplest way to demonstrate effective
    middleware strategy. We're able to now mitigate the risk of heavy traffic and
    move toward
  prefs: []
  type: TYPE_NORMAL
- en: One particularly useful place for this kind of middleware functionality in a
    web service is rate limiting.
  prefs: []
  type: TYPE_NORMAL
- en: Rate limiting exists in APIS with high traffic to allow consumers to use the
    application without potentially abusing it. Abuse in this case can just mean excessive
    access that can impact performance, or it can mean creating a deterrent for large-scale
    data acquisition.
  prefs: []
  type: TYPE_NORMAL
- en: Often, people will utilize APIs to create local indices of entire data collections,
    effectively spidering a site through an API. With most applications, you'll want
    to prevent this kind of access.
  prefs: []
  type: TYPE_NORMAL
- en: In either case, it makes sense to impose some rate limiting on certain requests
    within our application. And, importantly, we'll want this to be flexible enough
    so that we can do it with varying limits depending on the request time.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can do this using a number of factors, but the two most common methods are
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Through the corresponding API user credentials
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Through the IP address of the request
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In theory, we can also introduce rate limits on the underlying user by making
    a request per proxy. In a real-world scenario, this would reduce the risk of a
    third-party application being penalized for its user's usage.
  prefs: []
  type: TYPE_NORMAL
- en: The important factor is that we discover rate-limit-exceeded notations before
    delving into more complex calls, as we want to break the middleware chain at precisely
    that point if the rate limit has been exceeded.
  prefs: []
  type: TYPE_NORMAL
- en: For this rate-limiting middleware example, we'll again use MongoDB as a request
    store and a limit based on a calendar day from midnight to midnight. In other
    words, our limit per user resets every day at 12:01 a.m.
  prefs: []
  type: TYPE_NORMAL
- en: Storing actual requests is just one approach. We can also read from web server
    logs or store them in memory. However, the most lightweight approach is keeping
    them in a datastore.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'This is simply our MongoDB host or hosts. Here, we have a struct with boundaries
    for the beginning and end of a calendar day:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: The following `CreateDateBounds()` function calculates today's date and then
    adds `86400` seconds to the returned `Unix()` value (effectively 1 day).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: With the following `RegisterRequest()` function, we're simply logging another
    request to the API. Here again, we're only binding the request to the IP, adding
    an authentication key, user ID, or
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code is a simple, standard initialization setup, except for the
    `createDateBounds()` function, which simply sets the start and end of our lookup:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The following `CheckRequest()` function acts as the coordinating function for
    the entire process; it determines whether any given request exceeds the daily
    limit and returns the `Valid` status property:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Implementing rate limiting as middleware
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unlike the cache system, turning the rate limiter into middleware is much easier.
    Either the IP address is rate-limited, or it's not and we move on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s an example for updating users:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'And then, we can introduce a `RateLimit()` middleware call:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: This allows us to block the middleware chain if our `ratelimit.CheckRequest()`
    call fails and prevents any more processing-intensive parts of our API from being
    called.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing SPDY
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If there's one thing you can say about Google's vast ecosystem of products,
    platforms, and languages, it's that there's a perpetual, consistent focus on one
    thing that spans all of them—a need for speed.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We briefly mentioned the SPDY pseudo-protocol in [Chapter 7](ch07.html "Chapter 7. Working
    with Other Web Technologies"), *Working with Other Web Technologies*. You can
    read more about SPDY from its whitepaper at [http://www.chromium.org/spdy/spdy-whitepaper](http://www.chromium.org/spdy/spdy-whitepaper).
  prefs: []
  type: TYPE_NORMAL
- en: As Google (the search engine) quickly scaled from being a student project to
    the most popular site on Earth to the de facto way people find anything anywhere,
    scaling the product and its underlying infrastructure became key.
  prefs: []
  type: TYPE_NORMAL
- en: And, if you think about it, this search engine is heavily dependent on sites
    being available; if the sites are fast, Google's spiders and indexers will be
    faster and the results will be more current.
  prefs: []
  type: TYPE_NORMAL
- en: Much of this is behind Google's *Let's Make the Web Faster* campaign, which
    aims to help developers on both the backend and frontend by being cognizant of
    and pushing toward speed as the primary consideration.
  prefs: []
  type: TYPE_NORMAL
- en: Google is also behind the SPDY pseudo-protocol that augments HTTP and operates
    as a stopgap set of improvements, many of which are finding their way into the
    standardization of HTTP/2.
  prefs: []
  type: TYPE_NORMAL
- en: There are a lot of SPDY implementations that are written for Go, and SPDY seems
    to be a particularly popular project to embrace as it's not yet supported directly
    in Go. Most implementations are interchangeable drop-in replacements for `http`
    in `net/http`. In most practical cases, you can get these benefits by simply leaving
    SPDY to a reverse proxy such as HAProxy or Nginx.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Here are a few SPDY implementations that implement both secure and nonsecure
    connections and that are worth checking out and comparing:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `spdy.go` file from Solomon Hykes: [https://github.com/shykes/spdy-go](https://github.com/shykes/spdy-go)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `spdy` file from Jamie Hall: [https://github.com/SlyMarbo](https://github.com/SlyMarbo)'
  prefs: []
  type: TYPE_NORMAL
- en: We'll first look at `spdy.go` from the preceding list. Switching our `ListenAndServe`
    function is the easiest first step, and this approach to implement SPDY is fairly
    common.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s how to use `spdy.go` as a drop-in replacement in our `api.go` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Pretty simple, huh? Some SPDY implementations make serving pages through the
    SPDY protocol in lieu of `HTTP/HTTP` semantically indistinguishable.
  prefs: []
  type: TYPE_NORMAL
- en: For some Go developers, this counts as an idiomatic approach. For others, the
    protocols are different enough that having separate semantics is logical. The
    choice here depends on your preference.
  prefs: []
  type: TYPE_NORMAL
- en: However, there are a few other considerations to take into account. First, SPDY
    introduces some additional features that we can utilize. Some of these are baked-in
    like header compression.
  prefs: []
  type: TYPE_NORMAL
- en: Detecting SPDY support
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For most clients, detecting SPDY is not something that you need to worry about
    too much, as SPDY support relies on TLS/SSL support.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we worked at a few concepts that are important to highly-performant
    APIs. These primarily included rate limiting and disk and memory caching that
    were executed through the use of custom-written middleware.
  prefs: []
  type: TYPE_NORMAL
- en: Utilizing the examples within this chapter, you can implement any number of
    middleware-reliant services to keep your code clean and introduce better security,
    faster response times, and more features.
  prefs: []
  type: TYPE_NORMAL
- en: In the next and final chapter, we'll focus on security-specific concepts that
    should lock in additional concerns with rate limits, denial-of-service detection,
    and mitigating and preventing attempts at code and SQL injections.
  prefs: []
  type: TYPE_NORMAL
