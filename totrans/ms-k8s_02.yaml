- en: Creating Kubernetes Clusters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we learned what Kubernetes is all about, how it is
    designed, what concepts it supports, how to use its runtime engines, and how it
    fits within the CI/CD pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'Creating a Kubernetes cluster is a non-trivial task. There are many options
    and tools to select from, and there are many factors to consider. In this chapter,
    we''ll roll up our sleeves and build some Kubernetes clusters. We will also discuss
    and evaluate tools such as Minikube, kubeadm, kube-spray, bootkube, and stackube.
    We will also look into deployment environments, such as local, cloud, and bare
    metal. The topics we will cover are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Creating a single-node cluster with Minikube
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a multi-node cluster using kubeadm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating clusters in the cloud
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating bare-metal clusters from scratch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reviewing other options for creating Kubernetes clusters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At the end of this chapter, you will have a solid understanding of the various
    options to create Kubernetes clusters and knowledge of the best-of-breed tools
    to support the creation of Kubernetes clusters; you will also build a couple of
    clusters, both single-node and multi-node.
  prefs: []
  type: TYPE_NORMAL
- en: A quick single-node cluster with Minikube
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will create a single-node cluster on Windows. The reason
    we will use Windows is that Minikube and single-node clusters are most useful
    for local developer machines. While Kubernetes is typically deployed on Linux
    in production, many developers work on Windows PCs or Macs. That said, there aren''t
    too many differences if you do want to install Minikube on Linux:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/60646c29-b79f-4add-9880-40725cee6321.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are some prerequisites to install before you can create the cluster itself.
    These include VirtualBox, the `kubectl` command-line interface for Kubernetes,
    and, of course, Minikube itself. Here is a list of the latest versions at the
    time of writing:'
  prefs: []
  type: TYPE_NORMAL
- en: '**VirtualBox**: [https://www.virtualbox.org/wiki/Downloads](https://www.virtualbox.org/wiki/Downloads)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kubectl**: [https://kubernetes.io/docs/tasks/tools/install-kubectl/](https://kubernetes.io/docs/tasks/tools/install-kubectl/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Minikube**: [https://kubernetes.io/docs/tasks/tools/install-minikube/](https://kubernetes.io/docs/tasks/tools/install-minikube/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On Windows
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Install VirtualBox and make sure kubectl and Minikube are on your path. I personally
    just throw all the command-line programs I use into `c:\windows`. You may prefer
    another approach. I use the excellent ConEMU to manage multiple consoles, terminals,
    and SSH sessions. It works with `cmd.exe`, PowerShell, PuTTY, Cygwin, msys, and
    Git-Bash. It doesn't get much better than that on Windows.
  prefs: []
  type: TYPE_NORMAL
- en: With Windows 10 Pro, you have the option to use the Hyper-V hypervisor. This
    is technically a better solution than VirtualBox, but it requires the Pro version
    of Windows and is completely Windows-specific. When using VirtualBox, these instructions
    are universal and will be easy to adapt to other versions of Windows, or other
    operating systems altogether. If you have Hyper-V enabled, you must disable it
    because VirtualBox can't co-exist with Hyper-V.
  prefs: []
  type: TYPE_NORMAL
- en: 'I recommend using PowerShell in administrator mode. You can add the following
    alias and function to your PowerShell profile:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: On macOS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can add aliases to your `.bashrc` file (similar to the PowerShell alias
    and function on Windows):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Now I can use `k` and `mk` and type less. The flags to Minikube in the `mk`
    function provide better logging that way, and direct the output to the console,
    as well as to the files (similar to tee).
  prefs: []
  type: TYPE_NORMAL
- en: 'Type `mk version` to verify that Minikube is correctly installed and functioning:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Type `k version` to verify that kubectl is correctly installed and functioning:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Don't worry about the error on the last line. There is no cluster running, so
    kubectl can't connect to anything. That's expected.
  prefs: []
  type: TYPE_NORMAL
- en: You can explore the available commands and flags for both Minikube and kubectl.
    I will not go over each and every one, only the commands I use.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Minikube tool supports multiple versions of Kubernetes. At the time of
    writing, this is the list of supported versions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: I will go with 1.10.0, the latest stable release. Let's create the cluster by
    using the `start` command and specifying v1.10.0 as the version.
  prefs: []
  type: TYPE_NORMAL
- en: 'This can take a while as Minikube may need to download an image and then set
    up the local cluster. Just let it run. Here is the expected output (on Mac):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s review what Minikube does by following the output. You''ll need to do
    a lot of this when creating a cluster from scratch:'
  prefs: []
  type: TYPE_NORMAL
- en: Start a VirtualBox VM
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create certificates for the local machine and the VM
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Download images
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set up networking between the local machine and the VM
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the local Kubernetes cluster on the VM
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Configure the cluster
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Start all the Kubernetes control plane components
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Configure kubectl to talk to the cluster
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Troubleshooting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If something goes wrong during the process, try to follow the error messages.
    You can add the `--alsologtostderr` flag to get detailed error info from the console.
    Everything Minikube does is organized neatly under `~/.minikube`. Here is the
    directory structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Checking out the cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have a cluster up and running, let's peek inside.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s `ssh` into the VM:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Great! That works. The weird symbols are ASCII art for `minikube`. Now, let's
    start using `kubectl`, because it is the Swiss Army knife of Kubernetes and will
    be useful for all clusters (including federated clusters).
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover many of the `kubectl` commands on our journey. First, let''s
    check the cluster status using `cluster-info`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The Kubernetes master is running at `https://192.168.99.101:8443`
  prefs: []
  type: TYPE_NORMAL
- en: KubeDNS is running at `https://192.168.99.1010:8443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy`
  prefs: []
  type: TYPE_NORMAL
- en: To further debug and diagnose cluster problems, use `kubectl cluster-info dump`.
    You can see that the master is running properly. To see a much more detailed view
    of all the objects in the cluster as a JSON type, use `k cluster-info dump`. The
    output can be a little daunting, so let's use more specific commands to explore
    the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s check out the nodes in the cluster using `get nodes`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: So, we have one node called `minikube`. To get a lot of information about it,
    type `k describe node minikube`. The output is verbose; I'll let you try it yourself.
  prefs: []
  type: TYPE_NORMAL
- en: Doing work
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have a nice empty cluster up and running (well, not completely empty, as
    the DNS service and dashboard run as pods in the `kube-system` namespace). It''s
    time to run some pods. Let''s use the `echo` server as an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Kubernetes created a deployment and we have a pod running. Note the `echo`
    prefix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'To expose our pod as a service, type the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Exposing the service as a `NodePort` type means that it is exposed to the host
    on a port, but it is not the `8080` port we ran the pod on. Ports get mapped in
    the cluster. To access the service, we need the cluster IP and exposed port:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can access the `echo` service, which returns a lot of information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Congratulations! You just created a local Kubernetes cluster and deployed a
    service.
  prefs: []
  type: TYPE_NORMAL
- en: Examining the cluster with the dashboard
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes has a very nice web interface, which is deployed, of course, as a
    service in a pod. The dashboard is well designed, and provides a high-level overview
    of your cluster, and also drills down into individual resources, viewing logs,
    editing resource files, and more. It is the perfect weapon when you want to manually
    check out your cluster. To launch it, type `minikube dashboard`.
  prefs: []
  type: TYPE_NORMAL
- en: Minikube will open a browser window with the dashboard UI. Note that on Windows,
    Microsoft Edge can't display the dashboard. I had to run it myself on a different
    browser.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the workloads view, which displays Deployments, Replica Sets, Replication
    Controllers, and Pods:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/cc5421c1-7fb6-4997-993a-5b8f1e0f30dc.png)'
  prefs: []
  type: TYPE_IMG
- en: It can also display Daemon Sets, Stateful Sets, and Jobs, but we don't have
    any in this cluster.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we created a local, single-node Kubernetes cluster on Windows,
    explored it a little bit using `kubectl`, deployed a service, and played with
    the web UI. In the next section, we'll move on to a multi-node cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a multinode cluster using kubeadm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, I'll introduce you to `kubeadm`, the recommended tool for creating
    Kubernetes clusters on all environments. It is still under active development,
    but it is the way to go because it is part of Kubernetes, and will always embody
    best practices. To make it accessible for the entire cluster, we will base it
    on VMs. This section is for readers who want  a hands-on experience of deploying
    a multi-node cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Setting expectations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before embarking on this journey, I want to make it clear that it might *not*
    be a smooth ride. `kubeadm` has a difficult task: It has to follow the evolution
    of Kubernetes itself, which is a moving target. As a result, it is not always
    stable. When I wrote the first edition of *Mastering Kubernetes*, I had to dig
    deep and hunt for various workarounds to make it work. Guess what? I had to do
    the same thing for the second edition. Be prepared to make some adjustments and
    ask around. If you want a more streamlined solution, I will discuss some very
    good options later.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubeadm operates on preprovisioned hardware (physical or virtual). Before we
    create the Kubernetes cluster, we need to prepare a few VMs and install basic
    software, such as `docker`, `kubelet`, `kubeadm`, and `kubectl` (which is only
    needed on the master).
  prefs: []
  type: TYPE_NORMAL
- en: Preparing a cluster of vagrant VMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following vagrant file will create a cluster of four VMs called `n1`, `n2`,
    `n3`, and `n4`. Type `vagrant up` to get the cluster up and running. It is based
    on Bento/Ubuntu versions 16.04 and not Ubuntu/Xenial, which suffers from various
    issues:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Installing the required software
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'I like Ansible a lot for configuration management. I installed it on the `n4`
    VM (running Ubuntu 16.04). From now on I''ll use `n4` as my control machine, which
    means we''re operating in a Linux environment. I could use Ansible directly on
    my Mac, but since Ansible doesn''t run on Windows, I prefer a more universal approach:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'I used version 2.5.0\. You should be fine with the latest version:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The `sshpass` program I installed will help `ansible` connect to all the vagrant
    VMs with the built-in vagrant user. This is important only for a local VM-based
    multi-node cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'I created a directory called `ansible` and put three files in it: `hosts`,
    `vars.yml`, and `playbook.yml`.'
  prefs: []
  type: TYPE_NORMAL
- en: The host file
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `host` file is the inventory file that tells the `ansible` directory what
    hosts to operate on. The hosts must be SSH-accessible from the controller machine.
    The following are the three VMs that the cluster will be installed on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The vars.yml file
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `vars.yml` file just keeps a list of the packages I want to install on
    each node. `vim`, `htop`, and `tmux` are my favorite packages to install on each
    machine I need to manage. The others are required by Kubernetes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The playbook.yml file
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `playbook.yml` file is the file you run to install the packages on all
    hosts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Since some of the packages are from the Kubernetes APT repository, I need to
    add it, along with the Google signing key:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Connect to `n4`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'You may need to `ssh` once to each of the `n1`, `n2`, and `n3` nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'A more permanent solution is to add a file called `~/.ansible.cfg` that contains
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the playbook from `n4` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: If you run into connection failure, try again. The Kubernetes APT repository
    is sometimes slow to respond. You need to do this just once per node.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It's time to create the cluster itself. We'll initialize the master on the first
    VM, then set up networking and add the rest of the VMs as nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Initializing the master
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s initialize the master on `n1` (`192.168.77.10`). It is critical to use
    the `--apiserver-advertise-address` flag in case of a vagrant VM-based cloud:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'In Kubernetes 1.10.1, this results in the following error message:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The reason is that the required cri-tools are not installed by default. We
    are dealing with the cutting edge of Kubernetes here. I created an additional
    playbook to install Go and cri-tools, turned off the swap, and fixed the hostname
    of the vagrant VMs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Remember to run it on `n4` again to update all the nodes in the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is some of the output of a successful launch of Kubernetes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'There will be a lot more information that you must write down to join other
    nodes to the cluster later. To start using your cluster, you need to run the following
    as a regular user:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: You can now join any number of machines by running a command on each node as
    the root. Use the command returned from `kubeadm init cmmand:sudo kubeadm join
    --token << token>> --discovery-token-ca-cert-hash <<discvery token>> --skip-prflight-cheks`.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the pod network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The networking of the cluster is the big-ticket item. The pods need to be able
    to talk to each other. That requires a pod network add-on. There are several options
    for this. Clusters generated by `kubeadm`, require a CNI-based add-on. I chose
    to use the Weave Net add-on, which supports the Network Policy resource. Your
    can choose whatever you like.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following commands on the master VM:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'To verify, use the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: The last pod is our `weave-net-fl7wn`, which is what we're looking for, as well
    as the `kube-dns pod`. Both are running. All is well!
  prefs: []
  type: TYPE_NORMAL
- en: Adding the worker nodes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now we can add worker nodes to the cluster using the token we got earlier.
    On each node, run the following command (don''t forget `sudo`) with the tokens
    you got when initializing Kubernetes on the master node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'At the time of writing (using Kubernetes 1.10) some preflight checks fail,
    but this is a false negative. Everything is actually fine, and you can skip those
    preflight checks by adding `--ignore-preflight-errors=all`. I hope that when you
    read the book, these wrinkles will be ironed out. You should see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'This node has joined the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Run `kubectl get nodes` on the master to see this node join the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: This might not work for some combinations because of an issue with CNI plugin
    initialization.
  prefs: []
  type: TYPE_NORMAL
- en: Creating clusters in the cloud (GCP, AWS, and Azure)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Creating clusters locally is fun, and important during development and when
    trying to troubleshoot problems locally. But in the end, Kubernetes is designed
    for cloud-native applications (applications that run in the cloud). Kubernetes
    doesn't want to be aware of individual cloud environments because that doesn't
    scale. Instead, Kubernetes has the concept of a cloud-provider interface. Every
    cloud provider can implement this interface and then host Kubernetes. Note that,
    as of version 1.5, Kubernetes still maintains implementations for many cloud providers
    in its tree, but in the future, they will be refactored out.
  prefs: []
  type: TYPE_NORMAL
- en: The cloud-provider interface
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The cloud-provider interface is a collection of Go data types and interfaces.
    It is defined in a file called `cloud.go`, available at [http://bit.ly/2fq4NbW](http://bit.ly/2fq4NbW).
    Here is the main interface:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: This is very clear. Kubernetes operates in terms of instances, `Zones`, `Clusters`,
    and `Routes`, and also requires access to a load balancer and provider name. The
    main interface is primarily a gateway. Most methods return other interfaces.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, the `Clusters` interface is very simple:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: The `ListClusters()` method returns cluster names. The `Master()` method returns
    the IP address or DNS name of the master node.
  prefs: []
  type: TYPE_NORMAL
- en: The other interfaces are not much more complicated. The entire file is 214 lines
    long (at the time of writing) and includes a lot of comments. The take-home point
    is that it is not too complicated to implement a Kubernetes provider if your cloud
    utilizes those basic concepts.
  prefs: []
  type: TYPE_NORMAL
- en: Google Cloud Platform (GCP)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **Google Cloud Platform** (**GCP**) supports Kubernetes out of the box.
    The so-called **Google Kubernetes Engine** (**GKE**) is a container management
    solution built on Kubernetes. You don't need to install Kubernetes on GCP, and
    you can use the Google Cloud API to create Kubernetes clusters and provision them.
    The fact that Kubernetes is a built-in part of the GCP means it will always be
    well integrated and well tested, and you don't have to worry about changes to
    the underlying platform breaking the cloud-provider interface.
  prefs: []
  type: TYPE_NORMAL
- en: All in all, if you plan to base your system on Kubernetes and you don't have
    any existing code on other cloud platforms, then GCP is a solid choice.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Web Services (AWS)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Amazon Web Services** (**AWS**) has its own container-management service
    called ECS, but it is not based on Kubernetes. You can run Kubernetes on AWS very
    well. It is a supported provider, and there is a lot of documentation on how to
    set it up. While you could provision some VMs yourself and use `kubeadm`, I recommend
    using the **Kubernetes operations** (**Kops**) project. Kops is a Kubernetes project
    available on GitHub ([http://bit.ly/2ft5KA5](http://bit.ly/2ft5KA5)). It is not
    part of Kubernetes itself, but it is developed and maintained by the Kubernetes
    developers.'
  prefs: []
  type: TYPE_NORMAL
- en: 'It supports the following features:'
  prefs: []
  type: TYPE_NORMAL
- en: Automated Kubernetes cluster CRUD for the cloud (AWS)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Highly-available (HA) Kubernetes clusters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It uses a state-sync model for dry-run and automatic idempotency
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Custom support for `kubectl` add-ons
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kops can generate Terraform configuration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is based on a simple meta-model defined in a directory tree
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Easy command-line syntax
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Community support
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To create a cluster, you need to do some minimal DNS configuration through
    `route53`, set up a S3 bucket to store the cluster configuration, and then run
    a single command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: The complete instructions can be found at [http://bit.ly/2f7r6EK](http://bit.ly/2f7r6EK).
  prefs: []
  type: TYPE_NORMAL
- en: 'At the end of 2017, AWS joined the CNCF and announced two big projects regarding
    Kubernetes: Its own Kubernetes-based container orchestration solution (EKS) and
    a container-on-demand solution (Fargate).'
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Elastic Container Service for Kubernetes (EKS)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Amazon Elastic Container Service for Kubernetes** is a fully managed and
    highly available Kubernetes solution. It has three masters running in three AZs.
    EKS also takes care of upgrades and patching. The great thing about EKS is that
    it runs a stock Kubernetes without any changes. This means you can use all the
    standard plugins and tools developed by the community. It also opens the door
    to convenient cluster federation with other cloud providers and/or your own on-premises
    Kubernetes clusters.'
  prefs: []
  type: TYPE_NORMAL
- en: EKS provides deep integration with AWS infrastructure. IAM authentication is
    integrated with Kubernetes **role-based access control** (**RBAC**).
  prefs: []
  type: TYPE_NORMAL
- en: You can also use `PrivateLink` if you want to access your Kubernetes masters
    directly from your own Amazon VPC. With `PrivateLink`, your Kubernetes masters
    and the Amazon EKS service endpoint appear as elastic network interfaces with
    private IP addresses in your Amazon VPC.
  prefs: []
  type: TYPE_NORMAL
- en: Another important piece of the puzzle is a special CNI plugin that lets your
    Kubernetes components talk to each other using AWS networking.
  prefs: []
  type: TYPE_NORMAL
- en: Fargate
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Fargate** lets you run containers directly without worrying about provisioning
    hardware. It eliminates a huge part of the operational complexity at the cost
    of losing some control. When using Fargate, you package your application into
    a container, specify CPU and memory requirements, and define networking and IAM
    policies, and you''re off to the races. Fargate can run on top of ECS and EKS.
    It is a very interesting member of the serverless camp, although it''s not directly
    related to Kubernetes.'
  prefs: []
  type: TYPE_NORMAL
- en: Azure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Azure** used to have its own container management service. You could use
    the Mesos-based DC/OS or Docker Swarm to manage them, but you could also use Kubernetes,
    of course. You could also provision the cluster yourself (for example, using Azure''s
    desired-state configuration) then create the Kubernetes cluster using `kubeadm`.
    The recommended approach used to be to use yet another non-core Kubernetes project
    called `kubernetes-anywhere` ([http://bit.ly/2eCS7Ps](http://bit.ly/2eCS7Ps)).
    The goal of `kubernetes-anywhere` is to provide a cross-platform way to create
    clusters in a cloud environment (at least for GCP, AWS, and Azure).'
  prefs: []
  type: TYPE_NORMAL
- en: The process is pretty painless. You need to have Docker, `make`, and `kubectl`
    installed, and of course, your Azure subscription ID. Then, you clone the `kubernetes-anywhere`
    repository, run a couple of `make` commands, and your cluster is good to go.
  prefs: []
  type: TYPE_NORMAL
- en: The complete instructions to create an Azure cluster are at [http://bit.ly/2d56WdA](http://bit.ly/2d56WdA).
  prefs: []
  type: TYPE_NORMAL
- en: However, in the second half of 2017, Azure jumped on the Kubernetes bandwagon
    too and introduced AKS-Azure Container Service. It is similar to Amazon EKS, although
    it's a little further ahead in its implementation.
  prefs: []
  type: TYPE_NORMAL
- en: AKS provides a REST API, as well as a CLI, to manage your Kubernetes cluster,
    but you can use `kubectl` and any other Kubernetes tooling directly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some of the benefits of using AKS:'
  prefs: []
  type: TYPE_NORMAL
- en: Automated Kubernetes version upgrades and patching
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Easy cluster scaling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Self-healing hosted control plane (masters)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cost savings—pay only for running agent pool nodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, we covered the cloud-provider interface and looked at the various
    recommended ways to create Kubernetes clusters on various cloud providers. The
    scene is still young and the tools evolving quickly. I believe convergence will
    happen soon. Tools and projects such as `kubeadm`, `kops`, `Kargo`, and `kubernetes-anywhere`
    will eventually merge and provide a uniform and easy way to bootstrap Kubernetes
    clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Alibaba Cloud
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Chinese **Alibaba** Cloud is an up-and-comer on the cloud platform scene.
    It mimics AWS pretty closely, although its English documentation leaves a lot
    to be desired. I deployed a production application on Ali Cloud, but not one that
    used Kubernetes clusters. There seems to be official support for Kubernetes on
    Ali Cloud, but the documentation is in Chinese. I found one forum post in English
    that details how to deploy a Kubernetes cluster on Ali Cloud at [https://www.alibabacloud.com/forum/read-830](https://www.alibabacloud.com/forum/read-830).
  prefs: []
  type: TYPE_NORMAL
- en: Creating a bare-metal cluster from scratch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we looked at running Kubernetes on cloud providers.
    This is the dominant deployment story for Kubernetes, but there are strong use
    cases for running Kubernetes on bare metal. I don't focus here on hosted versus
    on-premises; this is yet another dimension. If you already manage a lot of servers
    on-premises, you are in the best position to decide.
  prefs: []
  type: TYPE_NORMAL
- en: Use cases for bare metal
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Bare-metal clusters are a beast especially if you manage them yourself. There
    are companies that provide commercial support for bare-metal Kubernetes clusters,
    such as Platform 9, but the offerings are not mature yet. A solid open-source
    option is Kubespray, which can deploy industrial-strength Kubernetes clusters
    on bare metal, AWS, GCE, Azure, and OpenStack.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some use cases where it makes sense:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Budget concerns**: If you already manage large-scale bare clusters, it may
    be much cheaper to run Kubernetes clusters on your physical infrastructure'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Low network latency**: If you must have low latency between your nodes, then
    the VM overhead might be too much'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Regulatory requirements**: If you must comply with regulations, you may not
    be allowed to use cloud providers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**You want total control over hardware**: Cloud providers give you many options,
    but you may have particular needs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When should you consider creating a bare-metal cluster?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The complexities of creating a cluster from scratch are significant. A Kubernetes
    cluster is not a trivial beast. There is a lot of documentation on the web about
    how to set up bare-metal clusters, but as the whole ecosystem moves forward, many
    of these guides get out of date quickly.
  prefs: []
  type: TYPE_NORMAL
- en: You should consider going down this route if you have the operational capability
    to take the time to debug problems at every level of the stack. Most of the problems
    will probably be networking-related, but filesystems and storage drivers can bite
    you too, as well as general incompatibilities and version mismatches between components,
    such as Kubernetes itself, Docker (or rkt, if you brave it), Docker images, your
    OS, your OS kernel, and the various add-ons and tools you use.
  prefs: []
  type: TYPE_NORMAL
- en: The process
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There is a lot to do. Here is a list of some of the concerns you''ll have to
    address:'
  prefs: []
  type: TYPE_NORMAL
- en: Implementing your own cloud provider's interface or sidestepping it
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choosing a networking model and how to implement it (using a CNI plugin or directly
    compiling)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Whether or not to use a network policy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Select images for system components
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Security models and SSL certificates
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Admin credentials
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Templates for components such as an API server, replication controller, and
    scheduler
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cluster services such as DNS, logging, monitoring, and GUI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I recommend reading the guide at the Kubernetes site ([http://bit.ly/1ToR9EC](http://bit.ly/1ToR9EC))
    to get a deeper understanding of what it takes to create a cluster from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: Using virtual private cloud infrastructure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If your use case falls under the bare-metal use cases, but you don't have the
    necessary skilled manpower or the inclination to deal with the infrastructure
    challenges of bare metal, you have the option of using a private cloud such as
    OpenStack (for example, with stackube). If you want to aim a little higher in
    the abstraction ladder, then Mirantis offers a cloud platform built on top of
    OpenStack and Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we considered the option of building a bare-metal cluster Kubernetes
    cluster. We looked into the use cases that require it and highlighted the challenges
    and difficulties.
  prefs: []
  type: TYPE_NORMAL
- en: Bootkube
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Bootkube** is very interesting too. It can launch self-hosted Kubernetes
    clusters. Self-hosted means that most of the cluster components run as regular
    pods and can be managed, monitored, and upgraded using the same tools and processes
    you use for your containerized applications. There are significant benefits to
    this approach, which simplifies the development and operation of Kubernetes clusters.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we got into some hands-on cluster creation. We created a single-node
    cluster using Minikube and a multi-node cluster using `kubeadm`. Then we looked
    at the many options to create Kubernetes clusters using cloud providers. Finally,
    we touched on the complexities of creating Kubernetes clusters on bare metal.
    The current state of affairs is very dynamic. The basic components are changing
    rapidly, the tooling is still young, and there are different options for each
    environment. It's not completely trivial to set up a Kubernetes cluster, but with
    some effort and attention to detail, you can get it done quickly.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will explore the important topics of monitoring, logging,
    and troubleshooting. Once your cluster is up and running and you start deploying
    workloads, you need to make sure that it runs properly and satisfies requirements.
    This requires ongoing attention and responding to various failures that happen
    in the real world.
  prefs: []
  type: TYPE_NORMAL
