- en: Chapter 4. Updates and Gradual Rollouts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter will expand upon the core concepts, which show the reader how to
    roll out updates and test new features of their application with minimal disruption
    to uptime. It will cover the basics of doing application updates, gradual rollouts,
    and A/B testing. In addition, we will look at scaling the Kubernetes cluster itself.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will discuss the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Application scaling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rolling updates
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A/B testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scaling up your cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Example set up
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we start exploring the various capabilities built into Kubernetes for
    scaling and updates, we will need a new example environment. We are going to use
    a variation of our previous container image with a blue background (refer to Figure
    4.2 for a comparison). We have the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 4-1*: `pod-scaling-controller.yaml`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 4-2*: `pod-scaling-service.yaml`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create these services with the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Scaling up
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Over time, as you run your applications in the Kubernetes cluster, you will
    find that some applications need more resources, whereas others can manage with
    fewer resources. Instead of removing the entire RC (and associated pods), we want
    a more seamless way to scale our application up and down.
  prefs: []
  type: TYPE_NORMAL
- en: Thankfully, Kubernetes includes a `scale` command, which is suited specifically
    to this purpose. In our new example, we have only one replica running. You can
    check this with a `get pods` command.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s try scaling that up to three with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: If all goes well, you'll simply see the word **scaled** on the output of your
    terminal window.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Optionally, you can specify the `--current-replicas` flag as a verification
    step. The scaling will only occur if the actual number of replicas currently running
    matches this count.
  prefs: []
  type: TYPE_NORMAL
- en: After listing our pods once again, we should now see three pods running with
    a name similar to `node-js-scale-``**XXXXX**`, where the `X`s are a random string.
  prefs: []
  type: TYPE_NORMAL
- en: You can also use the `scale` command to reduce the number of replicas. In either
    case, the `scale` command adds or removes the necessary pod replicas, and the
    service automatically updates and balances across new or remaining replicas.
  prefs: []
  type: TYPE_NORMAL
- en: Smooth updates
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The scaling of our application up and down as our resource demands change is
    useful for many production scenarios, but what about simple application updates?
    Any production system will have code updates, patches, and feature additions.
    These could be occurring monthly, weekly, or even daily. Making sure that we have
    a reliable way to push out these changes without interruption to our users is
    a paramount consideration.
  prefs: []
  type: TYPE_NORMAL
- en: Once again, we benefit from the years of experience the Kubernetes system is
    built on. There is a built-in support for rolling updates with the 1.0 version.
    The `rolling-update` command allows us to update entire RCs or just the underlying
    Docker image used by each replica. We can also specify an update interval, which
    will allow us to update one pod at a time and wait until proceeding to the next.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take our scaling example and perform a rolling update to the 0.2 version
    of our container image. We will use an update interval of 2 minutes, so we can
    watch the process as it happens in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: You should see some text about creating a new RC named `node-js-scale-XXXXX`,
    where the `X`s will be a random string of numbers and letters. In addition, you
    will see the beginning of a loop that is starting one replica of the new version
    and removing one from the existing RC. This process will continue until the new
    RC has the full count of replicas running.
  prefs: []
  type: TYPE_NORMAL
- en: If we want to follow along in real time, we can open another terminal window
    and use the `get pods` command, along with a label filter, to see what's happening.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This command will filter for pods with `node-js-scale` in the name. If you run
    this after issuing the `rolling-update` command, you should see several pods running
    as it creates new versions and removes the old ones one by one.
  prefs: []
  type: TYPE_NORMAL
- en: 'The full output of the previous `rolling-update` command should look something
    like Figure 4.1, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Smooth updates](../images/00045.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.1\. The scaling output
  prefs: []
  type: TYPE_NORMAL
- en: As we can see here, Kubernetes is first creating a new RC named `node-js-scale-10ea08ff9a118ac6a93f85547ed28f6`.
    K8s then loops through one by one. Creating a new pod in the new controller and
    removing one from the old. This continues until the new controller has the full
    replica count and the old one is at zero. After this, the old controller is deleted
    and the new one is renamed to the original controller name.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you run a `get pods` command now, you''ll note that the pods still all have
    a longer name. Alternatively, we could have specified the name of a new controller
    in the command, and Kubernetes will create a new RC and pods using that name.
    Once again, the controller of the old name simply disappears after updating is
    complete. I recommend specifying a new name for the updated controller to avoid
    confusion in your pod naming down the line. The same `update` command with this
    method would look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Using the static external IP address from the service we created in the first
    section, we can open the service in a browser. We should see our standard container
    information page. However, you'll note that the title now says **Pod Scaling v0.2**
    and the background is light yellow.
  prefs: []
  type: TYPE_NORMAL
- en: '![Smooth updates](../images/00046.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.2\. v0.1 and v0.2 (side by side)
  prefs: []
  type: TYPE_NORMAL
- en: It's worth noting that during the entire update process, we've only been looking
    at pods and RCs. We didn't do anything with our service, but the service is still
    running fine and now directing to the new version of our pods. This is because
    our service is using label selectors for membership. Because both our old and
    new replicas use the same labels, the service has no problem using the new pods
    to service requests. The updates are done on the pods one by one, so it's seamless
    for the users of the service.
  prefs: []
  type: TYPE_NORMAL
- en: Testing, releases, and cutovers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The rolling update feature can work well for a simple blue-green deployment
    scenario. However, in a real-world blue-green deployment with a stack of multiple
    applications, there can be a variety of interdependencies that require in-depth
    testing. The `update-period` command allows us to add a `timeout` flag where some
    testing can be done, but this will not always be satisfactory for testing purposes.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, you may want partial changes to persist for a longer time and all
    the way up to the load balancer or service level. For example, you wish to A/B
    test a new user interface feature with a portion of your users. Another example
    is running a canary release (a replica in this case) of your application on new
    infrastructure like a newly added cluster node.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at an A/B testing example. For this example, we will need
    to create a new service that uses `sessionAffinity`. We will set the affinity
    to `ClientIP`, which will allow us to forward clients to the same backend pod.
    This is a key if we want a portion of our users to see one version while others
    see another:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 4-3*: `pod-AB-service.yaml`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create this service as usual with the `create` command as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'This will create a service that will point to our pods running both version
    0.2 and 0.3 of the application. Next, we will create the two RCs which create
    two replicas of the application. One set will have version 0.2 of the application,
    and the other will have version 0.3, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 4-4*: `pod-A-controller.yaml`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 4-5:* `pod-B-controller.yaml`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that we have the same service label, so these replicas will also be added
    to the service pool based on this selector. We also have `livenessProbe` and `readinessProbe`
    defined to make sure that our new version is working as expected. Again, use the
    `create` command to spin up the controller:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Now we have a service balancing to both versions of our app. In a true A/B test,
    we would now want to start collecting metrics on the visit to each version. Again,
    we have the `sessionAffinity` set to `ClientIP`, so all requests will go to the
    same pod. Some users will see v0.2, and some will see v0.3.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Because we have `sessionAffinity` turned on, your test will likely show the
    same version every time. This is expected, and you would need to attempt a connection
    from multiple IP addresses to see both user experiences with each version.
  prefs: []
  type: TYPE_NORMAL
- en: Since the versions are each on their own pod, one can easily separate logging
    and even add a logging container to the pod definition for a sidecar logging pattern.
    For brevity, we will not cover that setup in this book, but we will look at some
    of the logging tools in [Chapter 6](part0046_split_000.html#1BRPS1-22fbdd9ef660435ca6bcc0309f05b1b7
    "Chapter 6. Monitoring and Logging"), *Monitoring and Logging*.
  prefs: []
  type: TYPE_NORMAL
- en: We can start to see how this process would be useful for a canary release or
    a manual blue-green deployment. We can also see how easy it is to launch a new
    version and slowly transition over to the new release.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at a basic transition quickly. It''s really as simple as a few
    `scale` commands, which are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Use the `get pods` command combined with `–l` filter in between `scale` commands
    to watch the transition as it happens.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we have fully transitioned over to version 0.3 (`node-js-scale-b`). All
    users will now see the version 0.3 of the site. We have four replicas of version
    0.3 and 0 of 0.2\. If you run a `get rc` command, you will notice that we still
    have a RC for 0.2 (`node-js-scale-a`). As a final cleanup, we can remove that
    controller completely as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the newly released version 1.1, K8s has a new "Horizontal Pod Autoscaler"
    construct which allows you to automatically scale pods based on CPU utilization.
  prefs: []
  type: TYPE_NORMAL
- en: Growing your cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All these techniques are great for the scaling of the application, but what
    about the cluster itself. At some point, you will pack the nodes full and need
    more resources to schedule new pods for your workloads.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When you create your cluster, you can customize the starting number of (minions)
    nodes with the `NUM_MINIONS` environment variable. By default, it is set to `4`.
    The following example shows how to set it to `5` before running `kube-up.sh`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Bear in mind that changing this after the cluster is started will have no effect.
    You would need to tear down the cluster and create it once again. Thus, this section
    will show you how to add nodes to an existing cluster without rebuilding it.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling up the cluster on GCE
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Scaling up your cluster on GCE is actually quite easy. The existing plumbing
    uses managed instance groups in GCE, which allow you to easily add more machines
    of a standard configuration to the group via an instance template.
  prefs: []
  type: TYPE_NORMAL
- en: You can see this template easily in the GCE console. First, open the console;
    by default, this should open your default project console. If you are using another
    project for your Kuberenetes cluster, simply select it from the project dropdown
    at the top of the page.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the side panel under **Compute** and then **Compute Engine**, select **Instance
    templates**. You should see a template titled **kuberenetes-minion-template**.
    Note that the name could vary slightly if you''ve customized your cluster naming
    settings. Click on that template to see the details. Refer to the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Scaling up the cluster on GCE](../images/00047.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.3\. The GCE Instance template for minions
  prefs: []
  type: TYPE_NORMAL
- en: You'll see a number of settings, but the meat of the template is under **Custom**
    metadata. Here, you will see a number of environment variables and also a startup
    script that is run after a new machine instance is created. These are the core
    components that allow us to create new machines and have them automatically added
    to the available cluster nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Because the template for new machines is already created, it is very simple
    to scale out our cluster in GCE. Simply go to the **Instance groups** located
    right above the **Instance templates** link on the side panel. Again, you should
    see a group titled **kubernetes-minion-group** or something similar. Click on
    that group to see the details, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Scaling up the cluster on GCE](../images/00048.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.4\. The GCE Instance group for minions
  prefs: []
  type: TYPE_NORMAL
- en: You'll see a page with a CPU metrics graph and four instances listed here. By
    default, the cluster creates four nodes. We can modify this group by clicking
    the **Edit group** button at the top of the page.
  prefs: []
  type: TYPE_NORMAL
- en: '![Scaling up the cluster on GCE](../images/00049.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.5\. The GCE Instance group edit page
  prefs: []
  type: TYPE_NORMAL
- en: You should see **kubernetes-minion-template** selected in **Instance template**
    that we reviewed a moment ago. You'll also see an **Autoscaling** setting, which
    is **Off** by default and an instance count of `4`. Simply, increment this to
    `5` and click on **Save**. You'll be taken back to the group details page and
    see a pop-up dialog showing the pending changes.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a few minutes, you''ll have a new instance listed on the details page. We
    can test that this is ready by using the `get nodes` command from the command
    line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Autoscaling and scaling down
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the preceding example, we left autoscaling turned off. However, there may
    be some cases where you want to automatically scale your cluster up and down.
    Turning on autoscaling will allow you to choose a metric to monitor and scale
    on. A minimum and maximum number of instances can be defined as well as a cool
    down period between actions. For more information on autoscaling in GCE, refer
    to the link [https://cloud.google.com/compute/docs/autoscaler/?hl=en_US#scaling_based_on_cpu_utilization](https://cloud.google.com/compute/docs/autoscaler/?hl=en_US#scaling_based_on_cpu_utilization).
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**A word of caution on autoscaling and scale down in general**'
  prefs: []
  type: TYPE_NORMAL
- en: First, if we repeat the earlier process and decrease the countdown to four,
    GCE will remove one node. However, it will not necessarily be the node you just
    added. The good news is that pods will be rescheduled on the remaining nodes.
    However, it can only reschedule where resources are available. If you are close
    to full capacity and shut down a node, there is a good chance that some pods will
    not have a place to be rescheduled. In addition, this is not a live migration,
    so any application state will be lost in the transition. The bottom line is that
    you should carefully consider the implications before scaling down or implementing
    an autoscaling scheme.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling up the cluster on AWS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The AWS provider code also makes it very easy to scale up your cluster. Similar
    to GCE, the AWS setup uses autoscaling groups to create the default four minion
    nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'This can also be easily modified using the CLI or the web console. In the console,
    from the EC2 page, simply go to the **Auto Scaling Groups** section at the bottom
    of the menu on the left. You should see a name similar to **kubernetes-minion-group**.
    Select that group and you will see details as shown in Figure 4.6:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Scaling up the cluster on AWS](../images/00050.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.6\. Kubernetes minion autoscaling details
  prefs: []
  type: TYPE_NORMAL
- en: We can scale this group up easily by clicking **Edit**. Then, change the **Desired**,
    **Min**, and **Max** values to `5` and click on **Save**. In a few minutes, you'll
    have the fifth node available. You can once again check this using the `get nodes`
    command.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling down is the same process, but remember that we discussed the same considerations
    in the previous *Scaling the cluster on GCE* section. Workloads could get abandoned
    or at the very least unexpectedly restarted.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling manually
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For other providers, creating new minions may not be an automated process. Depending
    on your provider, you'll need to perform various manual steps. It can be helpful
    to look at the provider-specific scripts under the `cluster` directory.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We should now be a bit more comfortable with the basics of application scaling
    in Kubernetes. We also looked at the built-in functions in order to roll updates
    as well a manual process for testing and slowly integrating updates. Finally,
    we took a look at scaling the nodes of our underlying cluster and increasing overall
    capacity for our Kubernetes resources.
  prefs: []
  type: TYPE_NORMAL
