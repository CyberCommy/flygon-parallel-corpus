- en: '*Chapter 2*: Performance Measurements'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Whether writing a new high-performance program or optimizing an existing one,
    one of the first tasks set before you will be to define the performance of the
    code in its current state. Your success will be measured by how much you can improve
    its performance. Both of these statements imply the existence of a performance
    metric, something that can be measured and quantified. One of the more interesting
    outcomes of the last chapter was the discovery that there isn''t even a single
    definition of performance that fits every need: what you measure when you want
    to quantify performance depends on the nature of the problem you''re working on.'
  prefs: []
  type: TYPE_NORMAL
- en: But there is much more to the measurements than simply defining the goals and
    confirming success. Every step of your performance optimization, whether of existing
    code or new code you're just writing, should be guided and informed by measurements.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first rule of performance is *Never guess about performance*, and it is
    worth dedicating the first section of this chapter to the goal of convincing you
    to take this rule to heart without questions or doubts. After shattering your
    faith in your intuition, we have to give you something else to stand on instead:
    the tools and approaches for measuring and learning about performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Why performance measurements are essential
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why all performance-related decisions must be driven by measurements and data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to measure the performance of real programs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is benchmarking, profiling, and micro-benchmarking of programs, and how
    to use them to measure performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First of all, you will need a C++ compiler. All examples in this chapter were
    compiled on a Linux system using GCC or Clang compilers. All major Linux distributions
    have GCC as a part of their regular install; newer versions may be available in
    the distribution's repositories. The Clang compiler is available through the LLVM
    project, [http://llvm.org/](http://llvm.org/), although several Linux distributions
    also maintain their own repositories. On Windows, Microsoft Visual Studio is the
    most common compiler, but both GCC and Clang are available as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Second, you will need a program profiling tool. In this chapter, we will use
    the Linux "perf" profiler. Again, it comes installed (or is available for installation)
    on most Linux distributions. The documentation can be found here: [https://perf.wiki.kernel.org/index.php/Main_Page](https://perf.wiki.kernel.org/index.php/Main_Page).'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will also demonstrate the use of another profiler, the CPU profiler from
    the set of Google Performance tools (GperfTools) found here: [https://github.com/gperftools/gperftools](https://github.com/gperftools/gperftools)
    (again, your Linux distribution may have it available for installation through
    its repositories).'
  prefs: []
  type: TYPE_NORMAL
- en: There are many other profiling tools available, both free and commercial. They
    all present fundamentally the same information, but in different ways and with
    many different analysis options. By following the examples in this chapter, you
    can learn what to expect of the profiling tool and what are the possible limitations;
    the specifics of each tool you use will have to be mastered on your own.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we will use a micro-benchmarking tool. In this chapter, we use the
    Google Benchmark library found at [https://github.com/google/benchmark](https://github.com/google/benchmark).
    You will, most likely, have to download and install it yourself: even if it comes
    installed with your Linux distribution, it is likely to be outdated. Follow the
    installation instructions on the web page.'
  prefs: []
  type: TYPE_NORMAL
- en: With all the necessary tools installed, we are ready to do our first experiment
    in performance measurements.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for the chapter can be found here: [https://github.com/PacktPublishing/The-Art-of-Writing-Efficient-Programs/tree/master/Chapter02](https://github.com/PacktPublishing/The-Art-of-Writing-Efficient-Programs/tree/master/Chapter02)'
  prefs: []
  type: TYPE_NORMAL
- en: Performance measurements by example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will have time to learn about each of the performance analysis tools in more
    detail in the rest of this chapter, but in this section, we will do a quick end-to-end
    example and analyze the performance of a simple program. This will show you what
    the typical performance analysis flow looks like and how different tools are used.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is also a hidden agenda: by the end of this section, you will come to
    believe that you should never guess about performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Any real-world program that you may have to analyze and optimize is likely
    to be large enough to take many pages in this book, so we will use a simplified
    example. This program sorts substrings in a very long string: suppose we have
    a string `S`, such as `"abcdcba"` (this is not so long; our actual strings will
    have millions of characters). We can have a substring starting from any character
    in this string, for example, the substring `S0` starts with the offset 0 and,
    therefore, has the value `"abcdcba"`. The substring `S2` starts with offset 2
    and has the value `"cdcba"`, and the substring `S5` has the value `"ba"`. If we
    were to sort these substrings in decreasing order using the regular string comparison,
    the order of the substrings would be `S2`, then `S5`, then `S0` (in order of the
    first characters, `''c''`, `''b''`, and `''a''`, respectively).'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use the STL sort algorithm, `std::sort`, to sort the substrings if we
    represent them with a character pointer: swapping two substrings now involves
    just swapping the pointers while the underlying string remains unchanged. Here
    is our example program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that, in order for this example to compile, we need to include the appropriate
    header files and write the `using` declarations for the names we shorten:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: In the subsequent examples, we will omit the common header files and the `using`
    declarations for common names such as `cout` or `vector`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The example defines a string that is used as the underlying data for the substrings
    to be sorted and for the vector of substrings (character pointers), but we have
    not yet shown how the data itself is created. Then, the substrings are sorted
    using `std::sort` with a custom comparison function: a lambda expression that
    calls the comparison function itself, `compare()`. We use the lambda expression
    to adapt the interface of the `compare()` function, which takes two pointers and
    the maximum string length, to the interface expected by `std::sort` (just two
    pointers). This is known as the Adapter Pattern.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In our case, the lambda expression has the second role: in addition to calling
    the comparison function, it also counts the number of comparison calls. Since
    we are interested in the performance of the sort, this information may be useful
    if we want to compare different sorting algorithms (we are not going to do this
    now, but this is a technique you may find useful in your own performance optimization
    efforts).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The comparison function itself is only declared in this example, but not defined.
    Its definition is in a separate file and reads as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'It is a straightforward comparison of two strings: it returns true if the first
    string is greater than the second one and false otherwise. We could have just
    as easily defined the function in the same file as the code itself and avoided
    the need for the extra file, but even with this small example, we are trying to
    reproduce the behavior of a real-world program that will likely call many functions
    scattered across many different files. Therefore, we have the comparison function
    in its own file, which we call `compare.C` in this chapter, and the rest of the
    example is in one file, `example.C`.'
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, we use the C++ high-resolution timers from the `chrono` library to measure
    how long it took to sort the substrings.
  prefs: []
  type: TYPE_NORMAL
- en: The only thing that is missing in our example is the actual data for the string.
    The substring sort is a fairly common task in many real applications, and each
    has its own way of acquiring the data. In our artificial example, the data will
    have to be equally artificial. We can, for example, generate a random string.
    On the other hand, in many practical applications of substring sort, there is
    one character that occurs in the string much more often than any other.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can simulate this type of data as well by filling the string with a single
    character and then randomly changing a few of them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The size of the string `L` and the number of substrings `N` are chosen to have
    reasonable run times on the machine that was used to run these tests (if you want
    to repeat the examples, you may have to adjust the numbers up or down depending
    on the speed of your processor).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now our example is ready to be compiled and executed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.1'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.1_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.1
  prefs: []
  type: TYPE_NORMAL
- en: The results you will get depend on the compiler you use, the computer you run
    on, and, of course, on the data corpus.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have our first performance measurement, the first question you may
    ask is, how do we optimize it? This is not the first question you should be asking,
    though. The real first question should be, *do we need to optimize?* To answer
    that, you need to have the targets and goals for performance, as well as the data
    on the relative performance of the other parts of this program; for example, if
    the actual string is generated from a simulation that takes ten hours, the one
    hundred seconds it takes to sort it is hardly worth noticing. Of course, we are
    still dealing with the artificial example, and we won't get very far in this chapter
    unless we assume that, yes, we have to improve performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, are we ready to talk about how to optimize it? Again, not so fast: the
    question now should be, **what do we optimize?** Or, more generally, where does
    the program spend the most time? Even in this simple example, it could be the
    sort itself or the comparison function. We do not have access to the source code
    of the sort (unless we want to hack the standard library, anyway), but we could
    insert the timer calls into the comparison function.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Unfortunately, this is unlikely to yield good results: each comparison is pretty
    fast, timer calls themselves take time, and calling the timer every time the function
    is called will significantly change the very results we''re trying to measure.
    In a real-world program, such instrumentation with timers is often not practical
    anyway. You would have to insert timers into hundreds of functions if you didn''t
    know where the time is spent (and how would you know that without any measurements?).
    This is where the profiler tools come in.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will learn much more about the profiler tools in the next section. For now,
    suffice it to say that the following command line will compile and execute the
    program and collect its runtime profile using the Google profiler from the GperfTools
    package:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.2'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.2_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.2
  prefs: []
  type: TYPE_NORMAL
- en: The profile data is collected in the file `prof.data`, as given by the `CPUPROFILE`
    environment variable. You may have noticed that the program took longer to run
    this time. This is an almost unavoidable side effect of performance profiling.
    We will come back to it in the next section. The relative performance of the different
    parts of the program should still be correct, assuming the profiler itself is
    working correctly.
  prefs: []
  type: TYPE_NORMAL
- en: 'The last line of the output tells us that the profiler has collected some data
    for us, now we need to display it in a readable format. For the data collected
    by the Google profiler, the user interface tool is `google-pprof` (often installed
    as simply `pprof`), and the simplest invocation of it just lists every function
    in the program, along with the fraction of the time spent in that function (the
    second column):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.3'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.3_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.3
  prefs: []
  type: TYPE_NORMAL
- en: The profiler shows that almost all the time is spent in the comparison function
    `compare()` and that the sort hardly takes any time at all (the second line is
    one of the functions called by `std::sort` and should be considered a part of
    the time spent in the sort but outside of the comparison). Note that for any practical
    profiling, we would need more than the 50 samples collected here. The number of
    samples depends on how long the program runs, and, to get reliable data, you need
    to accumulate at least a few dozen samples in every function you want to measure.
    In our case, the result is so glaringly obvious that we can proceed with just
    the samples we collected.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since the substring comparison function takes 98% of the total run time, we
    have only two ways to improve the performance: we can make this function faster,
    or we can call it fewer times (many people forget the second possibility and go
    straight for the first one). The second approach would require the use of a different
    sort algorithm and is, therefore, outside of the scope of this book. Here we will
    focus on the first option. Let us again review the code for the comparison function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This is just a few lines of code, and we should be able to understand and predict
    everything about its behavior. There is the check for comparing a substring to
    itself, which is definitely faster than actually doing the comparison character
    by character, so, unless we are sure that the function is never called with identical
    values for both pointers, this line stays.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then there is a loop (the body of the loop is comparing the characters one
    at a time), which we have to do because we do not know which character might be
    different. The loop itself runs until we find a difference or until we compare
    the maximum possible number of characters. It is easy to see that the latter condition
    cannot possibly happen: the string is null-terminated, so, even if all characters
    in both substrings are the same, sooner or later we will reach the end of the
    shorter substring, compare the null character at its end with a non-null character
    in the other substring, and the shorter substring will be considered the lesser
    of the two.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The only case where we could potentially read past the end of the string is
    when both substrings start at the same location, but we check for that at the
    very beginning of the function. This is great: we have found some unnecessary
    work that we were doing, so we can optimize the code and get rid of one comparison
    operation per loop iteration. Considering that there aren''t many other operations
    in the loop body, this ought to be significant.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The change in the code is simple enough: we can just remove the comparison
    (we also do not need to pass the length into the comparison function anymore):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Fewer parameters, fewer operations, less code all around. Let''s run the program
    and see how much run time this optimization saved us:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.4'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.4_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.4
  prefs: []
  type: TYPE_NORMAL
- en: To say that this didn't go according to plan would be a major understatement.
    The original code took 98 milliseconds to solve the same problem (*Figure 2.1*).
    The "optimized" code takes 210 milliseconds, despite doing less work (note that
    not all compilers exhibit this particular performance anomaly on this example,
    but we're using a real production compiler; there is no trickery here, this could
    happen to you too).
  prefs: []
  type: TYPE_NORMAL
- en: 'To wrap up this example, which is actually a much-condensed example from a
    real-life program, I will tell you that while we were trying to optimize this
    fragment of code, another programmer was working in a different part of the code
    and also needed a substring comparison function. When the separately developed
    pieces of code were put together, only one version of this function was kept,
    and it happens to be the one we did not write; the other programmer wrote almost
    the same code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Examine this code fragment and the one right before it and see if you can spot
    the difference.
  prefs: []
  type: TYPE_NORMAL
- en: 'The only difference is the type of the loop variable: earlier, we used `unsigned
    int`, and we were not wrong: the index starts from 0 and advances; we do not expect
    any negative numbers. The last code fragment uses `int`, unnecessarily giving
    up half of the range of the possible index values.'
  prefs: []
  type: TYPE_NORMAL
- en: 'After this code consolidation, we can run our benchmark again, this time with
    the new comparison function. The result is, again, unexpected:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.5'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.5_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.5
  prefs: []
  type: TYPE_NORMAL
- en: The latest version takes 74 milliseconds, faster than our original version (98
    milliseconds, Fig 2.1) and much faster than the almost identical second version
    (210 milliseconds, Fig 2.2).
  prefs: []
  type: TYPE_NORMAL
- en: 'For the explanation of this particular mystery, you will have to wait until
    the next chapter. The goal of this section was to convince you to never guess
    about performance: the "obvious" optimization – doing the exact same computation
    with less code – backfired spectacularly, and the trivial change that should not
    have mattered at all – using signed integers instead of unsigned in a function
    where all values are non-negative anyway – turned out to be an effective optimization.'
  prefs: []
  type: TYPE_NORMAL
- en: If the performance results can be so counter-intuitive even in this very simple
    example, then the only way to make good decisions about performance has to be
    the measurement-driven approach. In the rest of this chapter, we will see some
    of the most common tools used to collect performance measurements, learn how to
    use them, and how to interpret their results.
  prefs: []
  type: TYPE_NORMAL
- en: Performance benchmarking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The easiest way to collect information about the performance of a program is
    to run it and measure how long it takes. Of course, we need more data than that
    to make any useful optimizations: it would be nice to know which parts of the
    program make it take that long, so we don''t waste our own time optimizing the
    code that may be very inefficient but also takes very little time and thus does
    not contribute to the bottom line.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We already saw a simple example of that when we added a timer to our sample
    program: now we know how long the sort itself takes. That is, in a nutshell, the
    whole idea of benchmarking. The rest is elbow grease, instrumenting the code with
    timers, collecting the information, and reporting it in a useful format. Let us
    see what tools we have for that, starting with the timers provided by the language
    itself.'
  prefs: []
  type: TYPE_NORMAL
- en: C++ chrono timers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'C++ has some facilities that can be used to collect timing information in its
    chrono library. You can measure the time that elapsed between any two points in
    the program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We should point out that the C++ chrono clocks measure real time (often called
    wall-clock time). Usually, this is what you want to measure. However, a more detailed
    analysis often requires measuring the CPU time, which is the time that is passing
    only when the CPU is working and stands still when the CPU is idle. In a single-threaded
    program, the CPU time cannot be greater than the real time; if the program is
    compute-intensive, the two times are ideally the same, this means that the CPU
    was fully loaded. On the other hand, a user interface program spends most of the
    time waiting for the user and idling the CPU; here, we want the CPU time to be
    as low as possible: it is a sign that the program is efficient and uses as few
    CPU resources as possible to service the user''s requests. For that, we have to
    go beyond what is available in C++17.'
  prefs: []
  type: TYPE_NORMAL
- en: High-resolution timers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To measure the CPU time, we have to use OS-specific system calls; on Linux
    and other POSIX-compliant systems, we can use the `clock_gettime()` call to access
    the hardware high-resolution timers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The function returns the current time in its second argument; `tv_sec` is the
    number of seconds since some point in the past, and `tv_nsec` is the number of
    nanoseconds since the last whole second. The origin of time does not really matter
    since we always measure time intervals; however, take care to subtract seconds
    first and only then add nanoseconds, otherwise, you lose significant digits of
    the result by subtracting two large numbers.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several hardware timers we can use in the previous code, one of which
    is selected by the value of the `clock_id` variable. One of these timers is the
    same system or real-time clock we have used already. Its ID is `CLOCK_REALTIME`.
    The other two timers of interest to us are the two CPU timers: `CLOCK_PROCESS_CPUTIME_ID`  is
    a timer that measures the CPU time used by the current program, and `CLOCK_THREAD_CPUTIME_ID`
    is a similar timer, but it measures only the time used by the calling thread.'
  prefs: []
  type: TYPE_NORMAL
- en: 'When benchmarking the code, it is often helpful to report the measurements
    from more than one timer. In the simplest case of a single-threaded program that
    is doing uninterrupted computations, all three timers should return the same result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Here the "CPU-intensive work" is some kind of computation, and all three times
    should be almost identical. You can observe this in a simple experiment with any
    kind of computation. The values of the times will depend on the speed of the computer,
    but, that aside, the result should look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: If the reported CPU time does not match the real time, it is likely that the
    machine is overloaded (many other processes are competing for the CPU resources),
    or the program is running out of memory (if the program uses more memory than
    the physical memory on the machine, it will have to use the much slower disk swap,
    and the CPUs can't do any work while the program is waiting for the memory to
    be paged in from disk).
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, if the program does not compute much but instead, waits
    on user input, or receives the data from the network, or does some other work
    that does not take many CPU resources, we will see a very different result. The
    simplest way to observe this behavior is by calling the `sleep()` function instead
    of the computation we used earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we will, hopefully, see that a sleeping program uses very little CPU:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The same should be true for a program that is blocked on a socket or a file
    or is waiting for a user action.
  prefs: []
  type: TYPE_NORMAL
- en: 'So far, we have not seen any difference between the two CPU timers, and you
    will not see any unless your program uses threads. We can make our compute-heavy
    program do the same work but use a separate thread for it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The total amount of computations remains the same, and there is still only
    one thread doing the work, so we do not expect any changes to the real time or
    the process-wide CPU time. However, the thread that is calling the timers is now
    idle; all it does is wait on the future returned by `std::async` until the work
    is done. This waiting is very similar to the `sleep()` function in the previous
    example, and we can see it in the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Now the real time and the process-wide CPU time look like those from the "heavy
    computing" example, but the thread-specific CPU time is low, like in the "sleeping"
    example. That is because the overall program is doing heavy computing, but the
    thread that calls the timers is indeed mostly sleeping.
  prefs: []
  type: TYPE_NORMAL
- en: 'Most of the time, if we are going to use threads for computing, the goal is
    to do more computations faster, so we will use several threads and spread the
    work between them. Let us modify the preceding example to compute also on the
    main thread:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Now both threads are doing computations, so the CPU time used by the program
    passes at a double rate compared to the real time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'This is pretty good: we have done 1 second worth of computations in only 0.53
    seconds of real time. Ideally, this would have been 0.5 seconds, but in reality,
    there is some overhead for launching threads and waiting for them. Also, one of
    the two threads might have taken slightly longer to do the work, then the other
    thread was idle some of the time.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Benchmarking a program is a powerful way to collect performance data. Simply
    by observing the time it takes to execute a function or handle an event, we can
    learn a lot about the performance of the code. For compute-intensive code, we
    can see whether the program is indeed doing computations non-stop or is waiting
    on something. For multi-threaded programs, we can measure how effective the concurrency
    is and what the overhead is. But we are not just limited to collecting execution
    times: we can also report any counts and values we deem relevant: how many times
    a function was called, how long the average string we sort is, anything we need
    to help us interpret the measurements.'
  prefs: []
  type: TYPE_NORMAL
- en: 'However, this flexibility comes at a price: with benchmarking, we can answer
    almost any question about the performance of the program that we want to ask.
    But we have to ask the question first: we report only what we decided to measure.
    If we want to know how long a certain function takes, we have to add the timers
    to it; if they aren''t there, we will learn nothing until we rewrite the code
    and rerun the benchmark. On the other hand, it would not do to sprinkle timers
    everywhere in the code: these function calls are fairly expensive, so using too
    many can both slow down your program and distort the performance measurements.
    With experience and good coding discipline, you can learn to instrument the code
    you write in advance, so at least its major sections can be benchmarked easily.'
  prefs: []
  type: TYPE_NORMAL
- en: 'But what should you do if you have no idea where to start? What if you have
    inherited a code base that was not instrumented for any benchmarking? Or, maybe,
    you isolated your performance bottleneck to a large section of code, but there
    are no more timers inside of it? One approach is to continue instrumenting the
    code until you have enough data to analyze the problem. But this brute-force approach
    is slow, so you will want some guidance on where to focus your efforts. This is
    where profiling comes in: it lets you collect performance data for a program that
    wasn''t instrumented, by hand, for easy benchmarking. We will learn about profiling
    in the next section.'
  prefs: []
  type: TYPE_NORMAL
- en: Performance profiling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The next set of performance analysis tools that we are going to learn about
    is the profiling tools, or profilers. We have already seen a profiler in use:
    in the last section, we used it to identify the function that was taking the majority
    of the computation time. This is exactly what profilers are used for, to find
    "hot" functions and code fragments, that is, the lines of code where the program
    spends the most time.'
  prefs: []
  type: TYPE_NORMAL
- en: There are many different profiling tools available, both commercial and open
    source. In this section, we are going to examine two profilers that are popular
    on Linux systems. The goal is not to make you an expert on a particular tool but
    to give you an idea of what to expect from the profiler you choose to use and
    how to interpret its results.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let us point out that there are several different types of profilers:'
  prefs: []
  type: TYPE_NORMAL
- en: Some profilers execute the code under an interpreter or a virtual machine and
    observe where it spends the time. The main downside of these profilers is that
    they make the program run much slower than the code compiled directly to machine
    instructions, at least for languages like C++ that are so compiled and do not
    normally run under a virtual machine.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Other profilers require that the code is instrumented with special instructions
    during compilation or linking. These instructions provide additional information
    to the profiler, for example, so that they can notify the data collection engine
    when a function is called or a loop begins and ends. These profilers are faster
    than the ones of the previous type but still slower than the native execution.
    They also require a special compilation of the code and rely on the assumption
    that the instrumented code has the same performance as the original code, at least
    relatively, if not absolutely.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Most modern profilers use the hardware event counters that are present on all
    modern CPUs. These are special hardware registers that can be used to track certain
    hardware events. An example of a hardware event is executing an instruction. You
    can see how this can be useful for profiling: the processor will do the work of
    counting instructions for us without any additional instrumentation or any overhead.
    All we need to do is to read the values of the counter registers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Unfortunately, useful profiling is a bit more complicated than simply counting
    instructions. We need to know how much time was spent in each function and even
    in each line of code. This can be done if the profiler reads the instruction count
    before and after executing each function (or each loop, each line of code, and
    so on). This is why some profilers use a hybrid approach: they instrument the
    code to mark the points of interest but use the hardware performance counters
    for the actual measurements.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Other profilers rely on time-based sampling: they interrupt the program at
    a certain interval, say, once per 10 milliseconds, and record the values of the
    performance counters as well as the current location of the program (the instruction
    that is about to be executed). If, say, 90% of all samples were taken during a
    call to the `compare()` function, we can assume that the program spends 90% of
    the time doing string comparisons. The accuracy of this approach depends on the
    number of samples taken and the interval between the samples.'
  prefs: []
  type: TYPE_NORMAL
- en: The more often we sample the execution of the program, the more data we collect,
    but the greater the overhead is as well. Hardware-based profilers can, in some
    cases, have no adverse effect on the runtime of the program at all if the sampling
    is done not too often.
  prefs: []
  type: TYPE_NORMAL
- en: The perf profiler
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first profiler tool we are going to learn in this section is the Linux `perf`
    profiler. This is one of the most popular profilers on Linux simply because it
    comes installed with most distributions. This profiler uses hardware performance
    counters and time-based sampling; it does not require any instrumentation of the
    code.
  prefs: []
  type: TYPE_NORMAL
- en: 'The simplest way to run this profiler is to collect the counter values for
    the entire program; this is done using the `perf stat` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.6'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.6_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.6
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in *Figure 2.6*, the compilation does not require any special
    options or tools. The program is executed by the profiler, and the `stat` option
    tells the profiler to display the counts accumulated in the hardware performance
    counters during the entire run of the program. In this case, our program ran for
    158 milliseconds (consistent with the time printed by the program itself) and
    executed over 1.3 billion instructions. There are several other counters shown,
    such as "page-faults" and "branches." What are these counters, and what other
    counters can we see?
  prefs: []
  type: TYPE_NORMAL
- en: 'As it turns out, modern CPUs can collect statistics on many different types
    of events, but only a few types at a time; in the preceding example, eight counters
    were reported, so we can assume that this CPU has eight independent counters.
    However, each of these counters can be assigned to count one of many event types.
    The profiler itself can list all the events that are known to it and can be counted:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.7'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.7_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.7
  prefs: []
  type: TYPE_NORMAL
- en: 'The list in *Figure 2.7* is incomplete (the printout continues for many more
    lines), and the exact counters available vary from one CPU to another (and, if
    you use a virtual machine, on the type and configuration of the hypervisor). The
    results collected by our profiling run in *Figure 2.6* are simply the default
    set of counters, but we can select other counters for profiling:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.8'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.8_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.8
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 2.8*, we measure CPU cycles and instructions, as well as branches,
    branch misses, cache references, and cache misses. A detailed explanation of these
    counters and the events they monitor will be presented in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Briefly, the cycle time is the inverse of the CPU frequency, so a 3GHz CPU can
    run 3 billion cycles per second. By the way, most CPUs can run at variable speeds,
    which complicates the measurements. Thus, for accurate profiling and benchmarking,
    it is recommended to disable the power saving mode and other features that can
    cause the CPU clock to vary. The instruction counter measures the number of processor
    instructions that were executed; as you can see, the CPU executes, on average,
    almost four instructions per cycle.
  prefs: []
  type: TYPE_NORMAL
- en: 'The "branches" are the conditional instructions: every `if` statement and every
    `for` loop with a condition generates at least one of these instructions. Branch
    misses will be explained in detail in the next chapter; for now, we can just say
    that it is an expensive and undesirable event, from the performance point of view.'
  prefs: []
  type: TYPE_NORMAL
- en: The "cache references" count how many times the CPU needed to fetch something
    from memory. Most of the time, "something" is a piece of data, such as a character
    in the string. Depending on the state of the processor and memory, this fetch
    can be very fast or very slow; the latter is counted as a "cache miss" ("slow"
    is a relative concept; relative to the processor speed of 3 GHz, 1 microsecond
    is a very long time). The memory hierarchy will be explained in a later chapter;
    again, a cache miss is an expensive event.
  prefs: []
  type: TYPE_NORMAL
- en: Armed with an understanding of how the CPUs and the memory work, you will be
    able to use such measurements to gauge the overall efficiency of your program
    and determine what kinds of factors are limiting its performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'So far, we have seen only whole-program measurements. The measurements in *Figure
    2.8* may tell us what is holding back the performance of our code: for example,
    if we accept for now that "cache misses" are bad for performance, we can deduce
    that the main problem in this code is its inefficient memory access (one out of
    ten memory accesses is slow). However, this type of data does not tell us which
    parts of the code are responsible for poor performance. For that, we need to collect
    the data not just before and after but also during the program execution. Let
    us see how to do that with `perf`.'
  prefs: []
  type: TYPE_NORMAL
- en: Detailed profiling with perf
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `perf` profiler combines the hardware counters with time interval-based
    sampling to record the profile of the running program. For each sample, it records
    the position of the program counter (the address of the instruction to be executed)
    and the values of the performance counters that we are monitoring. After the run,
    the data is analyzed; the functions and code lines with the most samples are responsible
    for the majority of the execution time.
  prefs: []
  type: TYPE_NORMAL
- en: 'The data collection run of the profiler is no more difficult than the overall
    measurement run. Note that, at run time, the instruction addresses are collected;
    to convert these to the line numbers in the original source code, the program
    must be compiled with debug information. If you are used to the two compilation
    modes, "optimized" and "debug non-optimized," this combination of compiler options
    may come as a surprise: both debug and optimization are enabled. The reason for
    the latter is that we need to profile the same code that will run in production,
    otherwise, the data is mostly meaningless. With this in mind, we can compile our
    code for profiling and run the profiler using the `perf record` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.9'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.9_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.9
  prefs: []
  type: TYPE_NORMAL
- en: 'Just like `perf stat`, we could have specified a counter or a set of counters
    to monitor but, this time, we accept the default counter. We haven''t specified
    how often the samples are taken; again, there is a default for that, but we could
    also specify it explicitly: for example, `perf record -c 1000` records 1000 samples
    per second.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The program runs, produces its regular output, as well as the messages from
    the profiler. The last one tells us that the profiling samples have been captured
    in the file named `perf.data` (again, this is the default that can be changed).
    To visualize the data from this file, we need to use the profile analysis tool,
    which is also a part of the same perftools suite, specifically, the `perf report`
    command. Running this command will launch this screen:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.10'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.10_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.10
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the profiling summary, a breakdown of the execution time by function.
    From here, we can drill down into any function and see which lines contributed
    the most to the execution time:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.11'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.11_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.11
  prefs: []
  type: TYPE_NORMAL
- en: 'The numbers on the left in *Figure 2.11* are the percentages of the execution
    time spent at each line. So, what exactly does the "line" tell us? *Figure 2.11*
    illustrates one of the more frequent difficulties in analyzing such profiles.
    It shows both the source code and the assembly instructions produced from it;
    the execution time counters are, naturally, associated with every hardware instruction
    (that is what the CPU executes, so that''s the only thing it can count). The correspondence
    between the compiled code and the source is established by the profiler using
    the debugging information embedded by the compiler. Unfortunately, this correspondence
    is not exact, and the reason for this is optimization. The compiler performs a
    wide range of optimizations, all of which end up rearranging the code and changing
    the way the computations are done. You can see the results even in this very simple
    example: why does the source code line'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: appear twice? There is only one such line in the original source code. The reason
    is that the instructions generated from this line are not all in the same place;
    the optimizer reordered them with the instructions originating from other lines.
    So the profiler shows this line near both machine instructions that were originally
    generated from it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Even without looking at the assembler, we can see that the time is spent comparing
    the characters, as well as running the loop itself; these two source lines account
    for most of the time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'To get the most out of the profile, it helps to understand at least the basics
    of the assembly language of the platform we''re working on (X86 CPUs, in our case).
    The profiler also has some helpful tools that facilitate the analysis. For example,
    by placing the cursor on the `jne` (jump if not equal) instruction, we can see
    where the jump would take us, as well as the condition associated with the jump:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.12'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.12_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.12
  prefs: []
  type: TYPE_NORMAL
- en: This looks like a jump back to repeat the last few lines of code, so the `cmp`
    (compare) instruction above the jump must be the comparison of the loop, `i1 <
    l`. Together, the jump and the comparison account for 18% of the execution time,
    so our earlier attention to the seemingly unnecessary comparison operation appears
    justified.
  prefs: []
  type: TYPE_NORMAL
- en: The perf profiler has many more options and capabilities for analyzing, filtering,
    and aggregating the results, all of which you can learn from its documentation.
    There are also several GUI frontends for this profiler. Next, we are going to
    take a quick look at another profiler, the one from Google Performance tools.
  prefs: []
  type: TYPE_NORMAL
- en: The Google Performance profiler
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Google CPU profiler also uses hardware performance counters. It also requires
    link-time instrumentation of the code (but no compile-time instrumentation). To
    prepare the code for profiling, you have to link it with the profiler library:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.13'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.13_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.13
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Figure 2.13*, the library is specified by the command-line option `–lprofiler`.
    Unlike perf, this profiler does not need any special tools to invoke the program;
    the necessary code is already linked into the executable. The instrumented executable
    does not automatically start profiling itself. We have to activate the profiling
    by setting the environment variable `CPUPROFILE` to the filename of the file where
    we want to store the results. Other options are also controlled through the environment
    variables instead of command-line options, for example, the variable `CPUPROFILE_FREQUENCY`
    sets the number of samples per second:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.14'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.14_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.14
  prefs: []
  type: TYPE_NORMAL
- en: 'Again, we see the output from the program itself and from the profiler, and
    we get the profile data file that we must analyze. The profiler has both the interactive
    and the batch mode; the interactive mode is a simple text user interface:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.15'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.15_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.15
  prefs: []
  type: TYPE_NORMAL
- en: 'Simply running `google-pprof` (often installed as just `pprof`) with the names
    of the executable and the profile as arguments brings up the command prompt. From
    here, we can, for example, get the summary of all functions annotated with percentages
    of the execution time. We can further analyze the program performance at the source
    code level:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.16'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.16_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.16
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, this profiler takes a slightly different approach and does
    not immediately dump us, neck-deep, into machine code (although annotated assembly
    can also be produced). This apparent simplicity is somewhat deceptive, though:
    the caveats we described earlier still apply, the optimizing compiler still does
    its transformations on the code.'
  prefs: []
  type: TYPE_NORMAL
- en: Different profilers have somewhat different strengths and weaknesses, owing
    to the different approaches taken by their authors. Without turning this chapter
    into a profiler manual, we will show in the rest of this section some of the more
    common problems you may encounter when collecting and analyzing the profile.
  prefs: []
  type: TYPE_NORMAL
- en: Profiling with call graphs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'So far, our simple example has avoided one problem that, in reality, happens
    in every program. When we discovered that the comparison function is responsible
    for the majority of the execution time, we immediately knew which part of the
    program is responsible: there was only one line that calls this function.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Most real-life programs are not so simple: after all, one of the main reasons
    we write functions is to facilitate code reuse. It stands to reason that many
    functions will be called from multiple locations, some many times and others just
    a few times, often with very different parameters. Simply knowing which function
    takes a lot of time is not enough: we also need to know in which context it happens
    (after all, the most effective optimization may be to call the expensive function
    less often).'
  prefs: []
  type: TYPE_NORMAL
- en: 'What we need is a profile that does not just tell us how much time is spent
    in each function and on each line of code, but also how much time is spent in
    each call chain. These profilers usually present this information using the call
    graphs: graphs where callers and callees are nodes and calls are edges.'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we have to modify our example so we can call some function from more
    than one location. Let us start by making two `sort` calls:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The calls differ only in the comparison functions; in our case, the first comparison
    function is the same as before, and the second one produces the opposite order.
    The two functions have the same loop over substring characters as our old comparison
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Both functions use the same common function to compare each character:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'This isn''t, of course, how you would do it in a real program: if you really
    wanted to avoid the code duplication caused by repeating the loop, you would write
    a single function parametrized by the character comparison operator. However,
    we do not want to deviate too far from the example we started with, and we want
    to keep the code simple so we can explain the results one complication at a time.'
  prefs: []
  type: TYPE_NORMAL
- en: Now we are ready to produce a call graph that will show us how the cost of the
    character comparison is split between the two calls to sort. Both profilers we
    have used can produce call graphs; in this section, we will use the Google profiler.
    For this profiler, data collection already included the call chain information;
    we just haven't tried to visualize it so far.
  prefs: []
  type: TYPE_NORMAL
- en: 'We compile the code and run the profiler exactly as we did it earlier (for
    simplicity, we put each function in its own source file):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.17'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.17_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.17
  prefs: []
  type: TYPE_NORMAL
- en: 'The profiler can show the call graph in several different formats (Postscript,
    GIF, PDF, and so on). For example, to generate the PDF output, we would run this
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The information we''re interested in right now is at the bottom of the call
    graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.18'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.18_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.18
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in *Figure 2.18*, the `compare()` function, which accounts for
    58.6% of the total execution time, has two callers. Of the two, the `compare1()`
    function makes slightly more calls than the `compare2()` function; the former
    accounts for 27.6% of the execution time (or 59.8% if you include the time spent
    in its share of calls to `compare()`) and the latter is responsible for 13.8%
    of the time by itself, or 40.2% in total.
  prefs: []
  type: TYPE_NORMAL
- en: 'The basic call graphs are often enough to identify the problem call chains
    and select areas of the program for further exploration. Profiling tools also
    have more advanced reporting capabilities, such as the filtering of function names,
    aggregation of results, and so on. Mastering the features of your chosen tool
    can be the difference between knowledge and guesswork: interpreting performance
    profiles can be tricky and frustrating, and there are many reasons for it: some
    arise from tool limitations, but others are more fundamental. In the next section,
    we will talk about one of the latter reasons: for the measurements to be relevant,
    they must be done on fully optimized code.'
  prefs: []
  type: TYPE_NORMAL
- en: Optimization and inlining
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have already seen how compiler optimization muddies the waters when it comes
    to interpreting performance profiles: all profiling is done, at the end of the
    day, on the compiled machine code, while we see the program in its source form.
    The relation between these two forms is obscured by compiler optimizations. One
    of the most aggressive optimizations, in terms of rearranging the source code,
    is compile-time inlining of function calls.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The inlining requires that the source of the function be visible at the call
    site, so, in order to show you how this looks, we have to combine the entire source
    code in one file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Now the compiler can, and probably will, generate the machine code for the
    comparison right where it is used by the sort, instead of calling the external
    function. Such inlining is a potent optimization tool; it happens quite often
    and not just with functions from the same file. Much more often, inlining affects
    header-only functions (functions whose entire implementation is in the header
    file). For example, in the preceding code, the call to `std::sort`, which looks
    like a function call, is almost certainly inlined because `std::sort` is a template
    function: its entire body is in the header files.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us see how the profiler tools we used earlier deal with the inlined code.
    Running the Google profiler for annotated source lines produces this report:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.19'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.19_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.19
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, the profiler knows that the `compare()` function was inlined
    but still shows its original name. The lines in the source code correspond to
    the location where the code for the function is written, not where it is called,
    for example, line 23 is this line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The perf profiler, on the other hand, does not show inline functions as easily:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.20'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.20_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.20
  prefs: []
  type: TYPE_NORMAL
- en: 'Here we can see that the time appears to be spent in the sort code and the
    main program itself. Examining the annotated source, however, shows us that the
    code that was generated from the `compare()` function''s source is still responsible
    for the absolute majority of the execution time:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.21'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.21_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.21
  prefs: []
  type: TYPE_NORMAL
- en: There is, unfortunately, no easy way to undo the effects of the optimizations
    on the performance profiles. Inlining, code reordering, and other transformations
    turn detailed performance analysis into a skill that develops with practice. Perforce,
    some practical suggestions for the effective use of profiling are now in order.
  prefs: []
  type: TYPE_NORMAL
- en: Practical profiling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'It may be tempting to think of profiling as the ultimate solution to all your
    performance measurement needs: run the whole program under a profiler, collect
    all the data, and get the complete analysis of everything that is going on in
    the code. Unfortunately, it rarely works out this way. Sometimes, the tool limitations
    get in the way. Often, the complexity of the information contained in the large
    amounts of data is simply too overwhelming. How, then, should you use profiling
    effectively?'
  prefs: []
  type: TYPE_NORMAL
- en: 'The recommended approach is to collect high-level information first, then refine
    it. A coarse profile that breaks down the execution time between large modules
    may be a good place to start. On the other hand, you may have that information
    already if the modules are instrumented for benchmarking and have timers bracketing
    all major execution steps. If you don''t have such instrumentation, the initial
    profile offers good suggestions for what these steps are, so consider adding the
    benchmarking instrumentation now, so you have them next time: you don''t really
    expect to solve all your performance problems once and for all, do you?'
  prefs: []
  type: TYPE_NORMAL
- en: 'With the benchmarking results and the coarse profile, you will likely encounter
    one of several scenarios. If you are very lucky, the profile will point to some
    low-hanging fruit, like a function that takes 99% of the time doing a sort of
    a list. Yes, it happens: nobody expected the list to be longer than ten elements
    when the code was first written, and so it was for a while, and then everyone
    forgot about that code until it showed up as the long pole on the profile.'
  prefs: []
  type: TYPE_NORMAL
- en: 'More likely, the profile will lead you to some large functions or modules.
    Now you have to iterate, create tests that focus on the interesting parts of the
    program, and profile a smaller portion of the code in more detail. Some amount
    of benchmarking data can also be very helpful in interpreting the profiles: while
    the profile will tell you how much time was spent in a given function or a loop,
    it won''t count loop iterations or trace through if-else conditions. Note that
    most profilers can count function calls, so a good modular code is easier to profile
    than a huge monolithic mess.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As you collect and refine the profiles, the data will guide your attention
    toward the performance-critical areas of the code. It is also the point where
    you can fall into a common error: as you are focused on the code that is too slow,
    you may jump to optimizing it without considering the bigger picture. For example,
    the profile shows that a particular loop spends most time in memory allocation.
    Before you decide that you need a more efficient memory allocator, consider whether
    you actually need to allocate and deallocate memory on every iteration of the
    loop. The best way to make slow code faster is often to call it less often. This
    may require a different algorithm or just a more efficient implementation.'
  prefs: []
  type: TYPE_NORMAL
- en: Just as often, you will discover that there is a computation you must do, it
    is the performance-critical part of the code, and the only way to speed up the
    program is to make this code faster. Now you have to try different ideas for optimizing
    it and see what works best. You can do it live in the program itself, but often
    this is a wasteful approach that significantly reduces your productivity. Ideally,
    you want to quickly experiment with different implementations or even different
    algorithms for a particular problem. It is here that you can take advantage of
    the third method for collecting performance data, micro-benchmarking.
  prefs: []
  type: TYPE_NORMAL
- en: Micro-benchmarking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By the end of the previous section, we figured out where our program spends
    most of its execution time. We were also surprised when our "obvious" and "foolproof"
    optimization backfired and made the program run slower, not faster. It is clear
    now that we have to investigate the performance-critical function in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: 'We already have the tools for that: the overall program is exercising this
    code, and we have ways to measure its performance. But we''re not really interested
    in the rest of the program anymore, at least not until we solve the performance
    issues we already identified.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Working with a large program to optimize just a few lines of code has the following
    two major drawbacks:'
  prefs: []
  type: TYPE_NORMAL
- en: First of all, even though the few lines are identified as performance-critical,
    it doesn't mean the rest of the program takes no time at all (in our demo example,
    it does, but recall that this example is supposed to represent the entire large
    program you're working on). You may be waiting for hours before the large program
    gets to the interesting point, either because the entire job is that long or because
    the performance-critical function is called only under certain conditions, like
    a particular request coming over the net.
  prefs: []
  type: TYPE_NORMAL
- en: 'Second, working with a large program just takes more time: the compile and
    link times are longer, your work may be interacting with code changes made by
    other programmers, even editing takes longer because all the extra code is distracting.
    The bottom line, at this point, is we are interested in just one function, so
    we would like to be able to call this function and measure the results. This is
    where micro-benchmarking comes in.'
  prefs: []
  type: TYPE_NORMAL
- en: Basics of micro-benchmarking
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In a nutshell, micro-benchmarking is just a way to do what we just said we
    want to do: run a small chunk of code and measure its performance. In our case,
    it''s just one function, but it could be a more complex code fragment, too. What''s
    important is that this code fragment could be invoked easily with the right starting
    conditions: for a function, it''s just the arguments, but for a larger fragment,
    a more complex internal state may have to be recreated.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In our case, we know exactly what arguments we need to call the string comparison
    function with – we constructed the arguments ourselves. The second thing we need
    is to measure the execution time; we have already seen the timers that can be
    used for this purpose. With this in mind, we can write a very simple benchmark
    that calls several variants of our string comparison function and reports the
    results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: In this program, we test only two of the comparison functions, both without
    the end of loop condition, one with an `int` index and the other with an `unsigned
    int` index. Also, we will not be repeating the `#include` and `using` statements
    in the subsequent listings. The input data is just a long string filled with the
    same character from start to end, so the substring comparison will run all the
    way to the end of the string. We can, of course, benchmark on any data we need,
    but let's start with the simplest case.
  prefs: []
  type: TYPE_NORMAL
- en: 'The program looks like it will do exactly what we need… at least until we run
    it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.22'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.22_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.22
  prefs: []
  type: TYPE_NORMAL
- en: 'Zero time, either way. What went wrong? Perhaps, the execution time for a single
    function call is simply too fast to be measured? This is not a bad guess, and
    we can address this problem easily: if one call is too short, we just need to
    make more calls:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'We can increase the number of iterations `NI` until we get some results, right?
    Not so fast:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.23'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.23_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.23
  prefs: []
  type: TYPE_NORMAL
- en: 'Too fast, actually, but why? Let us step through the program in the debugger
    and see what it actually did:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.24'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.24_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.24
  prefs: []
  type: TYPE_NORMAL
- en: We set the breakpoint in `main`, so the program is paused as soon as it launches,
    then we execute the program line by line… except, that's not all the lines we
    have written! Where is the rest of the code? We can guess that the compiler is
    to blame, but why? We need to learn more about compiler optimizations.
  prefs: []
  type: TYPE_NORMAL
- en: Micro-benchmarking and compiler optimizations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To understand this mystery of the missing code, we have to take a fresh look
    at what the missing code actually does. It creates some strings, calls the comparison
    functions, and … there is no "and." Nothing else happens. Other than watching
    the code scroll by in the debugger, how would you know, just by running this program,
    if this code was executed? You cannot. The compiler has arrived at the same conclusion,
    way ahead of us. Since the programmer cannot tell the difference between executing
    and not executing a part of the code, the compiler has optimized it out. But wait,
    you say, the programmer *can* tell the difference: it takes much less time to
    do nothing than to do something. And here we come to a very important concept
    from the C++ standard that is critical to the understanding of compiler optimizations:
    the observable behavior.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The standard says that the compiler can make whatever changes it wants to the
    program as long as the effect of these changes does not alter the observable behavior.
    The standard is also very specific about what constitutes the observable behavior:'
  prefs: []
  type: TYPE_NORMAL
- en: Accesses (reads and writes) to volatile objects occur strictly according to
    the semantics of the expressions in which they occur. In particular, they are
    not reordered with respect to other volatile accesses on the same thread.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At program termination, data written to files is exactly as if the program was
    executed as written.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Prompting text that is sent to interactive devices will be shown before the
    program waits for input. More generally, input and output operations cannot be
    omitted or rearranged.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'There are a few exceptions to the preceding rules, none of which apply to our
    program. The compiler must follow the *as-if* rule: the optimized program should
    show the same observable behavior as if it was executed exactly as written, line
    for line. Now note what is not included in the preceding list: running the program
    under debugger does not constitute observable behavior. Neither does execution
    time, otherwise, no program could be optimized to make it faster.'
  prefs: []
  type: TYPE_NORMAL
- en: 'With this new understanding, let us take another look at the benchmark code:
    the results of the string comparison do not affect the observable behavior in
    any way, so the entire computation can be done or omitted at the compiler''s discretion.
    This observation also gives us a way to fix this problem: we have to make sure
    that the result of the computation affects the observable behavior. One way to
    do it is to take advantage of the volatile semantics described previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Now the result of every call to the comparison functions is written into a
    volatile variable, and, according to the standard, these values must be correct
    and written in the right order. The compiler now has no choice but to call our
    comparison functions and get the results. The way these results are computed can
    still be optimized as long as the result itself does not change. This is exactly
    what we want: we want the compiler to generate the best code for the comparison
    functions, hopefully, the same code it generates in the real program. We just
    don''t want it to drop these functions altogether. Running this benchmark shows
    that we have finally achieved our goal, the code is definitely running:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.25'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.25_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.25
  prefs: []
  type: TYPE_NORMAL
- en: The first value is the runtime of the `compare1()` function, which uses `int`
    indices, and it is indeed slightly faster than the `unsigned int` version (but
    don't put too much faith into these results just yet).
  prefs: []
  type: TYPE_NORMAL
- en: 'The second option for entangling our computations with some observable behavior
    is to simply print out the results. However, this can get a bit tricky. Consider
    the straightforward attempt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that the variable `sink` is no longer volatile, but instead, we write
    out its final value. This does not work as well as you might expect:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.26'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.26_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.26
  prefs: []
  type: TYPE_NORMAL
- en: 'The execution time of the function `compare2()` is in the same ballpark as
    before, but `compare1()` appears to be much faster now. Of course, by now, we
    know enough to understand that this "improvement" is illusory: the compiler simply
    figured out that the result of the first call is overwritten by the second call
    and, therefore, does not affect the observable behavior.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This brings up an interesting question: why didn''t the compiler figure out
    that the second iteration of the loop gives the same result as the first one and
    optimized away every call to the comparison functions except the first one, for
    each function? It could have, if the optimizer were advanced enough, and then
    we would have to do more to get around it: generally, compiling the functions
    as separate compilation units is enough to prevent any such optimizations, although
    some compilers are capable of whole-program optimizations, so you may have to
    turn them off when running micro-benchmarks.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note also that our two benchmark runs have produced somewhat different values
    even for the execution time of the function that wasn''t optimized away. If you
    run the program again, you will get yet another value, also somewhere in the same
    range, but slightly different. This isn''t good enough: we need more than just
    ballpark figures. We could run the benchmark several times, figure out how many
    repetitions we need, and compute the average time, but we don''t have to do it
    manually. We don''t have to write code to do this either, because such code has
    already been written and is available as one of several micro-benchmarking tools.
    We are going to learn about one such tool now.'
  prefs: []
  type: TYPE_NORMAL
- en: Google Benchmark
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Writing a micro-benchmark involves a lot of boilerplate code, mostly for measuring
    time and accumulating results. Furthermore, this code is critical for the accuracy
    of the measurements. There are several good-quality micro-benchmark libraries
    available. In this book, we use the Google Benchmark library. The instructions
    for downloading and installing the library can be found in the *Technical requirements*
    section. In this section, we will describe how to use the library and interpret
    the results.
  prefs: []
  type: TYPE_NORMAL
- en: 'To use the Google Benchmark library, we have to write a small program that
    will prepare the inputs and execute the code we want to benchmark. This is a basic
    Google Benchmark program for measuring the performance of one of our string comparison
    functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Every Google benchmark program must include the header for the library, `benchmark/benchmark.h`,
    plus, of course, any other headers needed to compile the code we want to measure
    (they are omitted in the preceding listing). The program itself consists of a
    number of benchmark "fixtures," each one is just a function with a specific signature:
    it takes one parameter, `benchmark::State`, by reference, and returns nothing.
    The parameter is an object provided by the Google Benchmark library to interface
    with the library itself.'
  prefs: []
  type: TYPE_NORMAL
- en: We need one fixture for each code fragment, such as a function that we want
    to benchmark. The first thing we do in each benchmark fixture is to set up the
    data we need to use as inputs for the code we want to run. More generally, we
    can say that we need to recreate the initial state of this code to represent what
    it would be in the real program. In our case, the input is the string, so we need
    to allocate and initialize the string. We can hardcode the size of the string
    into the benchmark, but there is also a way to pass arguments into a benchmark
    fixture. Our fixture uses one argument, the string length, which is an integer
    accessed as `state.range(0)`. It is possible to pass arguments of other types,
    please refer to the documentation of the Google Benchmark library for details.
  prefs: []
  type: TYPE_NORMAL
- en: 'The entire setup is free in the sense of the benchmark measurements: we do
    not measure the time it takes to prepare the data. The code whose execution time
    is measured goes into the body of the benchmarking loop, `for (auto _ : state)
    { … }`. In the older examples, you can find this loop written as `while (state.KeepRunning())
    { … }`, which does the same thing but slightly less efficiently. The library measures
    the time it takes to do each iteration and decides how many iterations it wants
    to do to accumulate enough measurements to reduce the random noise that is inevitable
    in measuring the run time of a small fragment of code. Only the run time of the
    code inside the benchmarking loop is measured.'
  prefs: []
  type: TYPE_NORMAL
- en: The loop exits when the measurement is accurate enough (or a certain time limit
    is reached). After the loop, we usually have some code to clean up the data that
    was initialized earlier, although in our case, this cleanup is handled by the
    destructor of the `std::unique_ptr` object. We can also make calls on the state
    object to affect what results are reported by the benchmark. The library always
    reports the average time it takes to run one iteration of the loop, but sometimes
    it is more convenient to express the program speed in some other way. For our
    string comparison, one option is to report the number of characters per second
    processed by the code. We can do it by calling `state.SetItemsProcessed()` with
    the number of characters we processed during the entire run, `N` characters per
    iteration (or `2*N` if you want to count both substrings; *items* can count whatever
    you define as a unit of processing).
  prefs: []
  type: TYPE_NORMAL
- en: 'Nothing is going to happen just because we defined a benchmark fixture, we
    need to register it with the library. This is done using the `BENCHMARK` macro;
    the argument of the macro is the name of the function. By the way, there is nothing
    special about that name, it can be any valid C++ identifier; that ours begins
    with `BM_` is merely a naming convention we follow in this book. The `BENCHMARK`
    macro is also where you will specify any arguments you want to pass to the benchmark
    fixture. The arguments and other options affecting the benchmark are passed using
    the overloaded arrow operator, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: This line registers the benchmark fixture `BM_loop_int` with one argument, `1<<20`,
    that can be retrieved inside the fixture by calling `state.range(0)`. We will
    see more examples of different arguments throughout this book, and even more can
    be found in the library documentation.
  prefs: []
  type: TYPE_NORMAL
- en: You will also notice that there is no `main()` in the preceding code listing;
    instead, there is another macro, `BENCHMARK_MAIN()`. The `main()` is not written
    by us but provided by the Google Benchmark library, and it does all the necessary
    work of setting up the benchmarking environment, registering the benchmarks, and
    executing them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us return for a moment to the code we want to measure and examine it more
    closely:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The `benchmark::DoNotOptimize(…)` wrapper function plays a role similar to
    the `volatile` sink we have used before: it ensures that the compiler does not
    optimize away the entire call to `compare_int()`. Note that it does not actually
    turn off any optimizations; in particular, the code inside the parentheses is
    optimized as usual, which is what we want. All it does is tells the compiler that
    the result of the expression, in our case, the return value of the comparison
    function, should be considered "used" as if it was printed out and cannot be simply
    discarded.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We are now ready to compile and run our first micro-benchmark:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.27'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.27_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.27
  prefs: []
  type: TYPE_NORMAL
- en: The compile line now has to list the path to the Google Benchmark `include`
    files and the library; several additional libraries are needed by the Google Benchmark
    library `libbenchmark.a`. Once invoked, the benchmark program prints some information
    about the system we are running on, then it executes every fixture that was registered,
    with all their arguments. We get one line of output for every benchmark fixture
    and a set of arguments; the report includes the average real time and the average
    CPU time of a single execution of the body of the benchmark loop, how many times
    the loop was executed, and any other statistics we have attached to the report
    (in our case, the number of characters per second processed by the comparison,
    over 2G characters per second).
  prefs: []
  type: TYPE_NORMAL
- en: 'How much do these numbers vary from run to run? The benchmark library can calculate
    that for us if we enable the statistics collection with the right command-line
    arguments. For example, to repeat the benchmark ten times and report the results,
    we would run the benchmark like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.28'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.28_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.28
  prefs: []
  type: TYPE_NORMAL
- en: It looks like the measurements are pretty accurate; the standard deviation is
    quite small. Now we can benchmark the different variants of the substring comparison
    function against each other and figure out which one is the fastest. But before
    we do that, I have to let you in on a big secret.
  prefs: []
  type: TYPE_NORMAL
- en: Micro-benchmarks are lies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You will discover it soon enough as you start running more and more micro-benchmarks.
    At first, the results make sense, you're making good optimizations, and everything
    looks great. Then you make some small change and get a very different result.
    You go back to investigate, and now the same tests you already ran give very different
    numbers. Eventually, you come up with two almost identical tests that show completely
    opposite results, and you realize that you just can't trust micro-benchmarks.
    It will destroy your faith in micro-benchmarks, and the only thing I can do about
    it is to destroy it now, in a controlled manner, while we can still salvage something
    from the wreckage.
  prefs: []
  type: TYPE_NORMAL
- en: The fundamental problem with micro-benchmarks and any other detailed performance
    measurements is that they strongly depend on the context. As you read through
    the rest of the book, you will understand more and more that the performance behavior
    of modern computers is very complex. The results do not just depend on what the
    code is doing, but also on what the rest of the system is doing at the same time,
    on what it was doing earlier, and on the path the execution took through the code
    before it got to the point of interest. None of these things are replicated in
    a micro-benchmark.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead, the benchmark has its own context. The authors of the benchmarking
    libraries are not ignorant of this problem, and they try to counter it the best
    they can. For example, unseen to you, the Google Benchmark library does a *burn-in*
    on every test: the first few iterations may have very different performance characteristics
    from the rest of the run, so the library ignores the initial measurements until
    the results "settle." But this also defines a particular context, probably different
    from the real program where every call to the function is repeated only once (on
    the other hand, sometimes we do end up calling the same function with the same
    arguments many times throughout the run of the program, so that could be a different
    context).'
  prefs: []
  type: TYPE_NORMAL
- en: There is nothing you can do to faithfully reproduce the real environment of
    a large program in every detail before running the benchmark. But some details
    are more important than others. In particular, the greatest source of contextual
    differences, by far, is the compiler, or, more specifically, the optimizations
    it does on a real program versus the micro-benchmark. We have already seen how
    the compiler stubbornly tries to figure out that the entire micro-benchmark is
    basically a very slow way of doing nothing useful (or at least nothing observable),
    and replace it with a much faster way of doing the same. The `DoNotOptimize` wrapper
    we used earlier gets us around some of the problems caused by the compiler optimizations.
  prefs: []
  type: TYPE_NORMAL
- en: However, there is still the possibility that the compiler may, for example,
    figure out that every call to the function returns the same result. Also, because
    the function definition is in the same file as the call site, the compiler can
    inline the entire function and use any information it can gather about the arguments
    to optimize the function code. Such optimizations would not be available in the
    general case when the function is called from another compilation unit.
  prefs: []
  type: TYPE_NORMAL
- en: 'To represent the real situation more accurately in our micro-benchmark, we
    can move the comparison function into its own file and compile it separately.
    Now we have one file (compilation unit) with just the benchmark fixtures:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'We can compile the files separately and link them together (any full-program
    optimizations must be turned off). Now we have a reasonable expectation that the
    compiler is not generating some special reduced version of the substring comparison
    because of what it figured out about the arguments we use in our benchmark. With
    this simple precaution alone, the results are much more consistent with what we
    observed when we profiled the entire program:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.29'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.29_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.29
  prefs: []
  type: TYPE_NORMAL
- en: The initial version of the code used the `unsigned int` index and a boundary
    condition in the loop (the last line); simply dropping that boundary condition
    check as entirely unnecessary results in a surprising performance degradation
    (the middle line); finally, changing the index to a `signed int` recovers the
    lost performance and even improves it (the first line).
  prefs: []
  type: TYPE_NORMAL
- en: Compiling the code fragments separately is usually enough to avoid any unwanted
    optimizations. Less commonly, you may find that the compiler does different optimizations
    to a particular chunk of code depending on what else is in the same file. This
    could be simply a bug in the compiler, but it can also be a result of some heuristic
    that is, in the experience of the compiler writers, more often right than not.
    If you observe that the results depend on some code that is not executed at all,
    only compiled, this may be the reason. One solution is to use the compilation
    unit from the real program and just call the function that you want to benchmark.
    Of course, you will have to satisfy compilation and link dependencies, so here
    is yet another reason to write modular code and minimize dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: 'The other source of the context is the state of the computer itself. Obviously,
    if the entire program ran out of memory and is cycling pages in and out of swap,
    your small memory benchmark will not be representative of the real problem; on
    the other hand, the problem now is not in the "slow" code, the problem is that
    too much memory is consumed elsewhere. However, more subtle versions of this context
    dependency exist and may affect the benchmarks. A tell-tale sign of this situation
    is usually this: the results depend on the order in which the tests are executed
    (in the micro-benchmark, it is the order of the `BENCHMARK` macros). If reordering
    the tests or running just a subset of tests gives different results, there is
    some sort of dependency between them. It could be a code dependency, often as
    straightforward as data accumulation in some global data structure. Or it could
    be a subtle dependency on the hardware state. Those are much harder to figure
    out, but you will learn about some situations that lead to such dependencies later
    in this book.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, there is a major source of context dependency that is entirely in
    your hands (which does not necessarily make it easy to avoid, but at least possible).
    It is the dependency on the state of your program. We already had to deal with
    the most obvious aspect of such dependency: the inputs to the code we want to
    benchmark. Sometimes, the inputs are known or can be reconstructed. Often, the
    performance problem happens only for certain kinds of inputs, and we don''t know
    what is so special about them until we analyze the performance of the code with
    these specific inputs, which is exactly what we were trying to do with the micro-benchmark
    in the first place. In such cases, it is often the easiest to capture the inputs
    from the real run of the real program, store them in a file, and use them to recreate
    the state of the code we''re measuring. This input could be as simple as a collection
    of data or as complex as a sequence of events that need to be recorded and "played
    back" to an event handler to reproduce the desired behavior.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The more complex the state we need to reconstruct is, the harder it is to reproduce
    the performance behavior of the real program in a partial benchmark. Note that
    this problem somewhat resembles the problem of writing unit tests: they, too,
    are much harder to write if the program cannot be broken up into smaller units
    with a simpler state. Once again, we see the advantages of a well-designed software
    system: a codebase with good unit test coverage is usually much easier to micro-benchmark,
    piece by piece.'
  prefs: []
  type: TYPE_NORMAL
- en: As you were warned when we started this section, it is meant to partially restore
    your faith in micro-benchmarks. They can be a useful tool, as we will see many
    times in this book. They can also lead you astray, sometimes very far. You now
    understand some of the reasons why and are better prepared to try to recover the
    useful bits of information from the results, rather than giving up on small-scale
    benchmarking altogether.
  prefs: []
  type: TYPE_NORMAL
- en: None of the tools we have presented in this chapter is a solution to every problem;
    they are not meant to be. You can achieve the best results by using these tools
    to collect information in various ways, so they complement each other.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, you have learned perhaps the single most important lesson
    in the entire book: it makes no sense to talk, or even think, about performance
    without referring to specific measurements. The rest is largely craftsmanship:
    we presented several ways to measure performance, starting from the whole program
    and drilling down to a single line of code.'
  prefs: []
  type: TYPE_NORMAL
- en: A large high-performance project will see every tool and method you learned
    about in this chapter used more than once. Coarse measurements – benchmarking
    and profiling the entire program or large parts of it – point to the areas of
    the code that require further investigation. Additional rounds of benchmarking
    or the collection of a more detailed profile usually follow. Eventually, you will
    identify the parts of the code that require optimization, and the question becomes,
    *"how do I do this faster?"* At this point, you can use a micro-benchmark or another
    small-scale benchmark to experiment with the code you're optimizing. You may even
    discover that you don't understand as much as you thought about this code and
    need a more detailed analysis of its performance; don't forget that you can profile
    micro-benchmarks!
  prefs: []
  type: TYPE_NORMAL
- en: 'Eventually, you will have a new version of the performance-critical code that
    looks favorable in small benchmarks. Still, do not assume anything: now you have
    to measure the performance of the complete program with your optimizations or
    enhancements. Sometimes, these measurements will confirm your understanding of
    the problem and validate its solution. At other times, you will discover that
    the problem is not what you thought it was, and the optimization, while beneficial
    by itself, does not have the desired effect on the overall program (it may even
    make things worse). You now have a new data point, you can compare the profiles
    of the old and new solutions and look for the answers in the differences this
    comparison reveals.'
  prefs: []
  type: TYPE_NORMAL
- en: The development and optimization of high-performance programs is almost never
    a linear, step-by-step process. Instead, it has many iterations of going from
    a high-level overview to low-level detailed work and back. In this process, there
    is a role for your intuition; just make sure always to test and confirm your expectations
    because, when it comes to performance, nothing is truly obvious.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, we will see the solution to the mystery we encountered
    earlier: removing unnecessary code makes the program slower. In order to do this,
    we have to understand how to use the CPU efficiently for maximum performance,
    and the entire next chapter is dedicated to that.'
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Why are performance measurements necessary?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why do we need so many different ways to measure performance?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the advantages and limitations of manual benchmarking?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How is profiling used to measure performance?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the uses of small-scale benchmarking, including micro-benchmarks?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
