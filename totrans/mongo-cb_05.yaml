- en: Chapter 5. Advanced Operations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Atomic find and modify operations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing atomic counters in Mongo
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing server-side scripts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating and tailing a capped collection cursors in MongoDB
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Converting a normal collection to capped collection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Storing binary data in Mongo
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Storing large data in Mongo using GridFS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Storing data to GridFS from Java client
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Storing data to GridFS from Python client
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing triggers in Mongo using oplog
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Flat plane (2D) geospatial queries in Mongo using geospatial indexes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spherical indexes and GeoJSON compliant data in Mongo
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing full text search in Mongo
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integrating MongoDB for full text search with Elasticsearch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 2](ch02.html "Chapter 2. Command-line Operations and Indexes"),
    *Command-line Operations and Indexes*, we saw how to perform basic operations
    from the shell to query, update, and insert documents, and also saw different
    types of indexes and index creation. In this chapter, we will see some of the
    advanced features of Mongo, such as GridFS, Geospatial Indexes, and Full text
    search. Other recipes we will see include an introduction and use of capped collections
    and implementing server-side scripts in MongoDB.
  prefs: []
  type: TYPE_NORMAL
- en: Atomic find and modify operations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 2](ch02.html "Chapter 2. Command-line Operations and Indexes"),
    *Command-line Operations and Indexes*, we had some recipes that explained various
    CRUD operations we perform in MongoDB. There was one concept that we didn't cover
    and it is atomically find and modify documents. Modification consists of both
    update and delete operations. In this recipe, we will go through the basics of
    MongoDB's `findAndModify` operation. In the next recipe, we will use this method
    to implement a counter.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Look at the recipe *Installing single node MongoDB* in [Chapter 1](ch01.html
    "Chapter 1. Installing and Starting the Server"), *Installing and Starting the
    Server* and start a single instance of MongoDB. That is the only prerequisite
    for this recipe. Start a mongo shell and connect to the started server.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will test a document in `atomicOperationsTest` collection. Execute the following
    from the shell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Execute the following from the mongo shell and observe the output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We will execute another one this time but with slightly different parameters;
    observe the output for this operation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We will execute another update this time that would upsert the document as
    follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, query the collection once as follows and see the documents present:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We will finally execute the delete as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If we perform find and update operations independently by first finding the
    document and then updating it in MongoDB, the results might not be as expected.
    There might be an interleaving update between the find and the update operations,
    which may have changed the document state. In some of the specific use cases,
    like implementing atomic counters, this is not acceptable and thus we need a way
    to atomically find, update, and return a document. The returned value is either
    the one before the update is applied or after the update is applied and is decided
    by the invoking client.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have executed the steps in the preceding section, let's see what
    we actually did and what all these fields in the JSON document passed as the parameter
    to the `findAndModify` operation mean. Starting with step 3, we gave a document
    as a parameter to the `findAndModify` function that contains the fields `query`,
    `update`, and `new`.
  prefs: []
  type: TYPE_NORMAL
- en: The `query` field specifies the search parameters that would be used to find
    the document and the `update` field contains the modifications that need to be
    applied. The third field, new, if set to `true`, tells MongoDB to return the updated
    document.
  prefs: []
  type: TYPE_NORMAL
- en: In step 4, we actually added a new field to the document passed as a parameter
    called **fields** that is used to select a limited set of fields from the result
    document returned. Also, the value of the field new is `true`, which tells that
    we want the updated document that is, the one after the update operation is executed
    and not the one before.
  prefs: []
  type: TYPE_NORMAL
- en: In step 5 contains a new field called `upsert`, which upserts (update + insert)
    the document. That is, if the document with the given query is found, it is updated
    else a new one is created and updated. If the document didn't exist and an upsert
    happened, having the value of the parameter `new` as `false` will return `null`.
    This is because there was nothing present before the update operation was executed.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, in step 7, instead of the `update` field, we used the `remove` field
    with the value `true` indicating that the document is to be removed. Also, the
    value of the new field is `false`, which means that we expect the document that
    got deleted.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An interesting use case of atomic `FindandModify` operations is developing an
    atomic counter in Mongo. In our next recipe, we will see how to implement this
    use case.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing atomic counters in Mongo
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Atomic counters are a necessity for a large number of use cases. Mongo doesn't
    have a built in feature for atomic counters; nevertheless, it can be easily implemented
    using some of its cool offerings. In fact, with the help of previously described
    `findAndModify()` command, implementing is quite simple. Refer to the previous
    recipe *Atomic find and modify operations* to know what atomic find and modify
    operations are in Mongo.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Look at the recipe *Installing single node MongoDB* in [Chapter 1](ch01.html
    "Chapter 1. Installing and Starting the Server"), *Installing and Starting the
    Server* and start a single instance of Mongo. That is the only prerequisite for
    this recipe. Start a mongo shell and connect to the started server.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Execute the following piece of code from the mongo shell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Now from the shell invoke the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The function is as simple as a `findAndModify` operation on a collection used
    to store all the counters. The counter identifier is the `_id` field of the document
    stored and the value of the counter is stored in the field `count`. The document
    passed to the `findAndModify` operations accepts the query, which uniquely identifies
    the document storing the current count—a query using the `_id` field. The update
    operation is an `$inc` operation that will increment the value of the `count`
    field by 1\. But what if the document doesn't exist? This will happen on the first
    invocation of the counter. To take care of this scenario, we will set the `upsert`
    flag to `true`. The value of `count` will always start with 1 and there is no
    way it would accept any user-defined start number for the sequence or a custom
    increment step. To address such requirements, we will have to specifically add
    a document with the initialized values to the counters collection. Finally, we
    are interested in the state of the counter after the value is incremented; hence,
    we set the value of the field `new` as `true`.
  prefs: []
  type: TYPE_NORMAL
- en: 'On invoking this method thrice (as we did), we should see the following in
    the collection counters. Simply execute the following query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Using this small function, we now have implemented atomic counters in Mongo.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can store such common code on a Mongo server that would be available for
    execution in other functions. Look at the recipe *Implementing* *server-side scripts*
    to see how we can store JavaScript functions on the Mongo server. This allows
    us even to invoke this function from other programming language clients.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing server-side scripts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will see how to write server stored JavaScript similar to
    stored procedures in relational databases. This is a common use case where other
    pieces of code require access to these common functions and we have them in one
    central place. To demonstrate server-side scripts, the function will simply add
    two numbers.
  prefs: []
  type: TYPE_NORMAL
- en: There are two parts to this recipe. First, we see how to load the scripts from
    the collections on the client-side JavaScript shell and secondly, we will see
    how to execute these functions on the server.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The documentation specifically mentions that it is not recommended to use server-side
    scripts. Security is one concern though if the data is not properly audited and
    hence we need to be careful with what functions are defined. Since Mongo 2.4,
    the server-side JavaScript engine is V8, which can execute multiple threads in
    parallel as opposed to the engine prior to version 2.4 of Mongo, which executes
    only one thread at a time.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Look at the recipe *Installing single node MongoDB* in [Chapter 1](ch01.html
    "Chapter 1. Installing and Starting the Server"), *Installing and Starting the
    Server* and start a single instance of Mongo. That is the only prerequisite for
    this recipe. Start a mongo shell and connect to the started server.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Create a new function called `add` and save it to the collection `db.system.js`
    as follows. The current database should be test:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that this function is defined, load all the functions as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, invoke `add` and see if it works:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now use this function and execute this on the server-side instead:
    Execute the following from the shell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Execute the following steps (you can execute the preceding command):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The collection `system.js` is a special MongoDB collection used to store JavaScript
    code. We add a new server-side JavaScript using the `save` function in this collection.
    The `save` function is just a convenience function that inserts the document if
    it is not present or updates an existing one. The objective is to add a new document
    to this collection which you may add even using `insert` or `upsert`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The secret lies in the method `loadServerScripts`. Let''s look at the code
    of this method: `this.system.js.find().forEach(function(u){eval(u._id + " = "
    + u.value);});`'
  prefs: []
  type: TYPE_NORMAL
- en: It evaluates a JavaScript using the `eval` function and assigns the function
    defined in the `value` attribute of the document to a variable named with the
    name given in the `_id` field of the document for each document present in the
    collection `system.js`.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, if the following document is present in the collection `system.js`,
    `{ _id : ''add'', value : function(num1, num2) {return num1 + num2}}`, then the
    function given in the `value` field of the document will be assigned to the variable
    named as `add` in the current shell. The value `add` is given in the `_id` field
    of the document.'
  prefs: []
  type: TYPE_NORMAL
- en: These scripts do not really execute on the server but their definition is stored
    on the server in a collection. The JavaScript method `loadServerScripts`, just
    instantiates some variables in the current shell and make those functions available
    for invocation. It is the JavaScript interpreter of the shell that executes these
    functions and not the server. The collection `system.js` is defined in the scope
    of the database. Once loaded, these act as JavaScript functions defined in the
    shell and hence the functions are available throughout the scope of the shell
    irrespective of the database currently active.
  prefs: []
  type: TYPE_NORMAL
- en: As far as security is concerned, if the shell is connected to the server with
    security enabled, then the user invoking `loadServerScripts` must have privileges
    to read the collections in the database. For more details on enabling security
    and various roles a user can have, refer to the recipe *Setting up users in Mongo*
    in [Chapter 4](ch04.html "Chapter 4. Administration"), *Administration*. As we
    saw earlier, the function `loadServerScripts` reads data from the collection `system.js`
    and if the user doesn't have privileges to read from the collection, the function
    invocation will fail. Apart from that, the functions executed from the shell after
    being loaded should have appropriate privileges. For instance, if a function inserts/updates
    in any collection, the user should have read and write privileges on that particular
    collection accessed from the function.
  prefs: []
  type: TYPE_NORMAL
- en: Executing scripts on the server is perhaps what one would expect to be server-side
    script as opposed to executing in the shell connected. In this case, the functions
    are evaluated on the server's JavaScript engine and the security checks are more
    stringent as long running functions can hold locks, having detrimental effects
    on the performance. The wrapper to invoke the execution of a JavaScript code on
    the server-side is the `db.eval` function accepting the code to evaluate on the
    server-side along with the parameters if any.
  prefs: []
  type: TYPE_NORMAL
- en: Before evaluating the function, the write operation takes a global lock; this
    can be skipped if the parameter `nolock` is used. For instance, the preceding
    `add` function can be invoked as follows instead of calling `db.eval` and achieving
    the same results. We additionally provided the `nolock` field to instruct the
    server not to acquire the global lock before evaluating the function. If this
    function were to perform write operations on a collection, then the `nolock` field
    is ignored.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'If security is enabled on the server, the invoking user needs to have the following
    four roles: `userAdminAnyDatabase`, `dbAdminAnyDatabase`, `readWriteAnyDatabase`,
    and `clusterAdmin` (on the admin database) to successfully invoke the `db.eval`
    function.'
  prefs: []
  type: TYPE_NORMAL
- en: Programming languages do provide a way for invocation of such server-side scripts
    using the `eval` function. For instance, in Java API, the class `com.mongodb.DB`
    has the method `eval` to invoke server-side JavaScript code. Such server-side
    executions are highly useful when we want to avoid unnecessary network traffic
    for the data and get the result to the clients. However, too much logic on the
    database server can quickly make things difficult to maintain and affect the performance
    of the server badly.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As of MongoDB 3.0.3, the `db.eval()` method is being deprecated and it is advised
    that users do not rely on this method but instead use client-side scripts. See
    [https://jira.mongodb.org/browse/SERVER-17453](https://jira.mongodb.org/browse/SERVER-17453)
    for more details.
  prefs: []
  type: TYPE_NORMAL
- en: Creating and tailing a capped collection cursors in MongoDB
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Capped collections are fixed size collections where documents are added towards
    the end of the collection, similar to a queue. As capped collection have a fixed
    size, older documents are removed if the limit is reached.
  prefs: []
  type: TYPE_NORMAL
- en: They are naturally sorted by the order of the insertion and any retrieval needed
    on them required ordered by time can be retrieved using the `$natural` sort order.
    This makes document retrieval very fast.
  prefs: []
  type: TYPE_NORMAL
- en: The following figure gives a pictorial representation of a capped collection
    of a size which is good enough to hold up to three documents of equal size (which
    is too small for any practical use, but good for understanding). As we can see
    in the image, the collection is similar to a circular queue where the oldest document
    is replaced by the newly added document should the collection become full. The
    tailable cursors are special types of cursors that tail the collection similar
    to a tail command in Unix. These cursors iterate through the collection similar
    to a normal cursors do, but additionally wait for data to be available in the
    collection if it is not available. We will see capped collections and tailable
    cursors in detail in this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: '![Creating and tailing a capped collection cursors in MongoDB](img/B04831_05_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Look at the recipe *Installing single node MongoDB* recipe in [Chapter 1](ch01.html
    "Chapter 1. Installing and Starting the Server"), *Installing and Starting the
    Server* and start a single instance of Mongo. That is the only prerequisite for
    this recipe. Start a MongoDB shell and connect to the started server.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are two parts to this recipe: in the first part, we will create a capped
    collection called `testCapped` and try performing some basic operations on it.
    Next, we will be creating a tailable cursor on this capped collection.'
  prefs: []
  type: TYPE_NORMAL
- en: Drop the collection if one already exists with this name.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Now create a capped collection as follows. Note the size given here is the
    size in bytes allocated for the collection and not the number of documents it
    contains:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now insert 100 documents in the capped collection as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Now query the collection as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Try to remove the data from the collection as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: We will now create and demonstrate a tailable cursor. It is recommended that
    you type/copy the following pieces of code into a text editor and keep it handy
    for execution.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To insert data in a collection, we will be using the following fragment of
    code. Execute this piece of code in the shell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'To tail a capped collection, we use the following piece of code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Open a shell and connect to the running mongod process. This will be the second
    shell opened and connected to the server. Copy and paste the code mentioned in
    step 8 in this shell and execute it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Observe how the records inserted are shown as they are inserted into the capped
    collection.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will create a capped collection explicitly using the `createCollection` function.
    This is the only way a capped collection is created. There are two parameters
    to the `createCollection` function. The first one is the name of the collection
    and the second is a JSON document that contains the two fields, `capped` and `size`,
    which are used to inform the user that the collection is capped or not and the
    size of the collection in bytes respectively. An additional field `max` can be
    provided to specify the maximum number of documents in the collection. The field
    size is required even if the `max` field is specified. We then insert and query
    the documents. When we try to remove the documents from the collection, we would
    see an error that removal is not permitted from the capped collection. It allows
    the documents to be deleted only when new documents are added and there isn't
    space available to accommodate them.
  prefs: []
  type: TYPE_NORMAL
- en: What we see next is a tailable cursor we created. We start two shells and one
    of them is a normal insertion of documents with an interval of 1 second between
    subsequent insertions. In the second shell, we create a cursor and iterate through
    it and print the documents that we get from the cursor onto the shell. The additional
    options we added to the cursor make the difference though. There are two options
    added, `DBQuery.Option.tailable` and `DBQuery.Option.awaitData`. These options
    are for instructing that the cursor is tailable, rather than normal, where the
    last position is marked and we can resume where we left off, and secondly to wait
    for more data for some time rather than returning immediately when no data is
    available and when we reach towards the end of the cursor, respectively. The `awaitData`
    option can be used with tailable cursors only. The combination of these two options
    gives us a feel similar to the tail command in Unix filesystem.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a list of available options, visit the following page: [http://docs.mongodb.org/manual/reference/method/cursor.addOption/](http://docs.mongodb.org/manual/reference/method/cursor.addOption/).'
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the next recipe, we will see how to convert a normal collection to a capped
    collection.
  prefs: []
  type: TYPE_NORMAL
- en: Converting a normal collection to a capped collection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This recipe will demonstrate the process of converting a normal collection to
    a capped collection.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Look at the recipe *Installing single node MongoDB* in [Chapter 1](ch01.html
    "Chapter 1. Installing and Starting the Server"), *Installing and Starting the
    Server* and start a single instance of Mongo. That is the only prerequisite for
    this recipe. Start a mongo shell and connect to the started server.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Execute the following to ensure you are in the `test` database:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a normal collection as follows. We will be adding 100 documents to it,
    type/copy the following code snippet on to the mongo shell and execute it. The
    command is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Query the collection as follows to confirm it contains the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, query the collection `system.namespaces` as follows and note the result
    document:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Execute the following command to convert the collection to capped collection:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Query the collection to take a look at the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Query the collection `system.namespaces` as follows and note the result document:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We created a normal collection with 100 documents and then tried to convert
    it to a capped collection with 100 bytes size. The command has the following JSON
    document passed to the `runCommand` function, `{convertToCapped : <name of normal
    collection>, size: <size in bytes of the capped collection>}`. This command creates
    a capped collection with the mentioned size and loads the documents in natural
    ordering from the normal collection to the target capped collection. If the size
    of the capped collection reaches the limit mentioned, the old documents are removed
    in the FIFO order making space for new documents. Once this is done, the created
    capped collection is renamed. Executing a find on the capped collection confirms
    that not all 100 documents originally present in the normal collection are present
    in the capped collection. A query on the `system.namespaces` collection before
    and after the execution of the `convertToCapped` command shows the change in the
    `collection` attributes. Note that, this operation acquires a global write lock
    blocking all read and write operations in this database. Also, any indexes present
    on the original collection are not created for the capped collection, upon conversion.'
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Oplog is an important collection used for replication in MongoDB and is a capped
    collection. For more information on replication and oplogs, refer to the recipe
    *Understanding and analyzing oplogs* in [Chapter 4](ch04.html "Chapter 4. Administration"),
    *Administration*. In a recipe later in this chapter, we will use this oplog to
    implement a feature similar to after insert/update/delete trigger of a relational
    database.
  prefs: []
  type: TYPE_NORMAL
- en: Storing binary data in Mongo
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we saw how to store text values, dates, and numbers fields in a document.
    Binary content also needs to be stored at times in the database. Consider cases
    where users would need to store files in a database. In relational databases,
    the BLOB data type is most commonly used to address this requirement. MongoDB
    also supports binary contents to be stored in a document in the collection. The
    catch is that the total size of the document shouldn't exceed 16 MB, which is
    the upper limit of the document size as of the writing this book. In this recipe,
    we will store a small image file into Mongo's document and also retrieve it later.
    If the content you wish to store in MongoDB collections is greater than 16 MB,
    then MongoDB offers an out of the box solution called **GridFS**. We will see
    how to use GridFS in another recipe later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Look at the recipe *Installing single node MongoDB* in [Chapter 1](ch01.html
    "Chapter 1. Installing and Starting the Server"), *Installing and Starting the
    Server* and start a single instance of MongoDB. Also, the program to write binary
    content to the document is written in Java. Refer to the recipes *Executing query
    and insert operations using a Java client*, *Implementing aggregation in Mongo
    using a Java client* and *Executing MapReduce in Mongo using a Java client* in
    [Chapter 3](ch03.html "Chapter 3. Programming Language Drivers"), *Programming
    Language Drivers*, for more details on Java drivers. Open a mongo shell and connect
    to the local MongoDB instance listening to port `27017`. For this recipe, we will
    be using the project `mongo-cookbook-bindata`. This project is available in the
    source code bundle downloadable from Packt site. The folder needs to be extracted
    on the local filesystem. Open a command line shell and go to the root of the project
    extracted. It should be the directory where the file `pom.xml` is found.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'On the operating system shell with the `pom.xml` present in the current directory
    of the `mongo-cookbook-bindata` project, execute the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Observe the output; the execution should be successful.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Switch to mongo shell that is connected to the local instance and execute the
    following query:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Scroll through the document and take a note of the fields in the document.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If we scroll through the large document printed out, we see that the fields
    are `fileName`, `size`, and `data`. The first two fields are of type string and
    number respectively, which we populated on document creation and hold the name
    of the file we provide and the size in bytes. The data field is a field of BSON
    type BinData, where we see the data encoded in Base64 format.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following lines of code show how we populated the DBObject that we added
    to the collection:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: As we see above, two fields `fileName` and `size` are used to store the name
    of the file and the size of the file and are of type string and number respectively.
    The field data is added to the `DBObject` as a byte array, it gets stored automatically
    as the BSON type BinData in the document.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: What we saw in this recipe is straightforward as long as the document size is
    less than 16 MB. If the size of the files stored exceeds this value, we have to
    resort to solutions like GridFS, which is explained in next recipe *Storing large
    data in Mongo using GridFS*.
  prefs: []
  type: TYPE_NORMAL
- en: Storing large data in Mongo using GridFS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A document size in MongoDB can be up to 16 MB. But does that mean we cannot
    store data more than 16 MB in size? There are cases where you prefer to store
    videos and audio files in database rather than in a filesystem for a number of
    advantages such as a few of them are storing metadata along with them, when accessing
    the file from an intermediate location, and replicating the contents for high
    availability if replication is enabled on the MongoDB server instances. GridFS
    can be used to address such use cases in MongoDB. We will also see how GridFS
    manages large content that exceeds 16 MB and analyzes the collections it uses
    for storing the content behind the scene. For test purpose, we will not use data
    exceeding 16 MB but something smaller to see GridFS in action.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Look at the recipe *Installing single node MongoDB* in [Chapter 1](ch01.html
    "Chapter 1. Installing and Starting the Server"), *Installing and Starting the
    Server* and start a single instance of Mongo. That is the only prerequisite for
    this recipe. Start a Mongo shell and connect to the started server. Additionally,
    we will use the mongofiles utility to store data in GridFS from command line.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Download the code bundle of the book and save the image file `glimpse_of_universe-wide.jpg`
    to your local drive (you may choose any other large file as the matter of fact
    and provide appropriate names of the file with the commands we execute). For the
    sake of the example, the image is saved in the home directory. We will split our
    steps into three parts.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With the server up and running, execute the following command from the operating
    system's shell with the current directory being the home directory. There are
    two arguments here. The first one is the name of the file on the local filesystem
    and the second one is the name that would be attached to the uploaded content
    in MongoDB.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Let's now query the collections to see how this content is actually stored in
    the collections behind the scenes. With the shell open, execute the following
    two queries. Make sure that in the second query, you ensure to mention not selecting
    the data field.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have put a file to GridFS from the operating system''s local filesystem,
    we will see how we can get the file to the local filesystem. Execute the following
    from the operating system shell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we will delete the file we uploaded as follows. From the operating
    system shell, execute the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Confirm the deletion using the following queries again:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Mongo distribution comes with a tool called mongofiles, which lets us upload
    the large content to Mongo server that gets stored using the GridFS specification.
    GridFS is not a different product but a specification that is standard and followed
    by different drivers for MongoDB for storing data greater than 16 MB, which is
    the maximum document size. It can even be used for files less than 16 MB, as we
    did in our recipe, but there isn't really a good reason to do that. There is nothing
    stopping us from implementing our own way of storing these large files, but it
    is preferred to follow the standard. This is because all drivers support it and
    does the heavy lifting of splitting of big file into small chunks and assembling
    them back when needed.
  prefs: []
  type: TYPE_NORMAL
- en: We kept the image downloaded from the Packt Publishing site and uploaded using
    mongofiles to MongoDB. The command to do that is `put` and the `-l` option gives
    the name of the file on the local drive that we want to upload. Finally, the name
    `universe.jpg` is the name of the file we want it to be stored as on GridFS.
  prefs: []
  type: TYPE_NORMAL
- en: 'On successful execution, we should see something like the following on the
    console:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: This gives us some details of the upload, the unique `_id` for the uploaded
    file, the name of the file, the chunk size, which is the size of the chunk this
    big file is broken into (by default 256 KB), the date of upload, the checksum
    of the uploaded content, and the total length of upload. This checksum can be
    computed beforehand and then compared after the upload to check if the uploaded
    content was not corrupt.
  prefs: []
  type: TYPE_NORMAL
- en: 'Execute the following query from the mongo shell in test database:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: We see that the output we saw for the `put` command of mongofiles same as the
    document queried above in the collection `fs.files`. This is the collection where
    all the uploaded file details are put when some data is added to GridFS. There
    will be one document per upload. Applications can later also modify this document
    to add their own custom meta data along with the standard details added to my
    Mongo when adding the data. Applications can very well use this collection to
    add details like, the photographer, the location where the image was taken, where
    was it taken, and details like tags for individuals in the image in this collection
    if the document is for an image upload.
  prefs: []
  type: TYPE_NORMAL
- en: 'The file content is something that contains this data. Let''s execute the following
    query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'We have deliberately left out the data field from the result selected. Let''s
    look at the structure of the result document:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: For the file we uploaded, we have 11 chunks of a maximum 256 KB each. When a
    file is being requested, the `fs.chunks` collection is searched by the `file_id`
    that comes from the `_id` field of `fs.files` collection and the field `n`, which
    is the chunk's sequence. A unique index is created on these two fields when this
    collection is created for the first time when a file is uploaded using GridFS
    for the fast retrieval of chunks using the file ID sorted by chunk sequence number.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to `put`, the `get` option is used to retrieve the files from the GridFS
    and put them on local filesystem. The difference in the command is to use the
    `get` instead of `put`, the `-l` still is used to provide the name of the file
    that this file would be saved as on the local filesystem and the final command
    line parameter is the name of the file as stored in GridFS. This is the value
    of the `filename` field in `fs.files` collection. Finally, the `delete` command
    of mongofiles simply removes the entry of the file from `fs.files` and `fs.chunks`
    collections. The name of the file given for delete is again the value present
    in the `filename` field of the `fs.files` collection.
  prefs: []
  type: TYPE_NORMAL
- en: Some important use cases of using GridFS are when there is some user generated
    contents like large reports on some static data that doesn't change too often
    and are expensive to generate frequently. Instead of running them all the times,
    it can be run once and stored until a change in the static data is detected; in
    which case, the stored report is deleted and re-executed on next request of the
    data. The filesystem may not always be available to the application to write the
    files to, in which case this is a good alternative. There are cases where one
    might be interested in some intermediate chunk of the data stored, in which case
    the chunk containing the required data be accessed. You get some nice features
    like the MD5 content of the data, which is stored automatically and is available
    for use by the application.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have seen what GridFS is, let's see some scenarios where using GridFS
    might not be a very good idea. The performance of accessing the content from MongoDB
    using GridFS and directly from the filesystem will not be same. Direct filesystem
    access will be faster than GridFS and **Proof of Concept** (**POC**) for the system
    to be developed is recommended to measure the performance hit and see if it is
    within the acceptable limits; if so, the trade off in performance might be worth
    for the benefits we get. Also, if your application server is fronted with CDN,
    you might not actually need a lot of IO for static data stored in GridFS. Since
    GridFS stores the data in multiple documents in collections, atomically updating
    them is not possible. If we know the content is less than 16 MB, which is the
    case in lot of user-generated content, or some small files uploaded, we may skip
    GridFS altogether and store the content in one document as BSON supports storing
    binary content in the document. Refer to the previous recipe *Storing binary data
    in Mongo* for more details.
  prefs: []
  type: TYPE_NORMAL
- en: We would rarely use mongofiles utility to store, retrieve, and delete data from
    GridFS. Though it may occasionally be used, we will mostly perform these operations
    from an application. In the next couple of recipes, we will see how to connect
    to GridFS to store, retrieve, and delete files using Java and Python clients.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Though this is not much to do with Mongo, Openstack is an **Infrastructure
    as a Service** (**IaaS**) platform and offers a variety of services for Compute,
    Storage, Networking, and so on. One of the image storage service called **Glance**
    supports a lot of persistent stores to store the images. One of the supported
    stores by Glace is MongoDB''s GridFS. You can find more information on how to
    configure Glance to use GridFS at the following URL: [http://docs.openstack.org/trunk/config-reference/content/ch_configuring-openstack-image-service.html](http://docs.openstack.org/trunk/config-reference/content/ch_configuring-openstack-image-service.html).'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can refer to the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Storing data to GridFS from Java client*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Storing data to GridFS from Python client*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Storing data to GridFS from Java client
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous recipe, we saw how to store data to GridFS using a command-line
    utility that comes with MongoDB to manage large data files: mongofiles. To get
    an idea of what GridFS is and what collections are used behind the scenes to store
    the data, refer to the previous recipe *Storing large data in Mongo using GridFS*.'
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will look at storing data to GridFS using a Java client.
    The program will be a highly scaled down version of mongofiles utility and focus
    only on how to store, retrieve, and delete data rather than trying to provide
    a lot of options like mongofiles do.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Refer to the recipe *Installing single node MongoDB* from [Chapter 1](ch01.html
    "Chapter 1. Installing and Starting the Server"), *Installing and Starting the
    Server*, for all the necessary setup for this recipe. If you are interested in
    more details on Java drivers, refer to the recipes *Implementing aggregation in
    Mongo using a Java client* and *Executing MapReduce in Mongo using a Java client*
    in [Chapter 3](ch03.html "Chapter 3. Programming Language Drivers"), *Programming
    Language Drivers*. Open a mongo shell and connect to the local mongod instance
    listening to port `27017`. For this recipe, we will be using the project `mongo-cookbook-gridfs`.
    This project is available in the source code bundle downloadable from Packt site.
    The folder needs to be extracted on the local filesystem. Open a terminal of your
    operating system and go to the root of the project extracted. It should be the
    directory where the file `pom.xml` is found. Also, save the file `glimpse_of_universe-wide.jpg`
    on the local filesystem, similar to the previous recipe, found in the downloadable
    bundle for the book from the Packt site.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We are assuming that the collections of GridFS are clean and no prior data is
    uploaded. If there is nothing crucial in the database, you can execute the following
    to clear the collection. Do exercise caution before dropping the collections.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Open an operating system shell and execute the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: The file I need to upload was placed in the home directory. You can choose to
    give the file path of the image file after the `put` command. Bear in mind if
    the path contains spaces, the whole path need to be given within single quotes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If the preceding command runs successfully, we should expect the following
    output to the command line:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the preceding execution is successful, which we can confirm from the console
    output, execute the following from the mongo shell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will get the file from GridFS to local filesystem, execute the following
    to perform this operation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Confirm the file is present on the local filesystem at the mentioned location.
    We should see the following printed to the console output to indicate a successful
    write operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we will delete the file from GridFS:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'On successful deletion, we should see the following output in the console:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The class `com.packtpub.mongo.cookbook.GridFSTests` accepts three types of
    operations: `put` to upload file to GridFS, `get` to get contents from GridFS
    to local filesystem, and `delete` to delete files from GridFS.'
  prefs: []
  type: TYPE_NORMAL
- en: The class accepts up to three parameters, the first one is the operation with
    valid values as `get`, `put`, and `delete`. The second parameter is relevant for
    `get` and `put` operations and is the name of the file on local filesystem to
    write the downloaded content to be written or source the content from for upload
    respectively. The third parameter is the name of the file in GridFS, which is
    not necessarily same as the name on local filesystem. For `delete`, however, only
    the filename on GridFS is needed which would be deleted.
  prefs: []
  type: TYPE_NORMAL
- en: Let's see some important snippets of code from the class which is specific to
    GridFS.
  prefs: []
  type: TYPE_NORMAL
- en: Open the class `com.packtpub.mongo.cookbook.GridFSTests` in your favorite IDE
    and look for the methods `handlePut` , `handleGet`, and `handleDelete`. These
    are the methods where all the logic is. We will start with the `handlePut` method
    first, which is for uploading the contents of the file from local filesystem to
    GridFS.
  prefs: []
  type: TYPE_NORMAL
- en: 'Irrespective of the operation we perform, we will create an instance of the
    class `com.mongodb.gridfs.GridFS`. In our case, we instantiated it as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: The constructor of this class takes the database instance of class `com.mongodb.DB`.
    Once the instance of GridFS is created, we will invoke the method `createFile`
    on it. This method accepts two parameters, the first one is the `InputStream`
    sourcing the bytes of the content to be uploaded and the second parameter is the
    name of the file on GridFS for the file that would be saved on GridFS. However,
    this method doesn't create the file on GridFS but returns and instance of `com.mongodb.gridfs.GridFSInputFile`.
    The upload will happen only when we call `save` method in this returned object.
    There are few overloaded variants of this `createFile` method. Please refer to
    Javadocs of the class `com.mongodb.gridfs.GridFS` for more details.
  prefs: []
  type: TYPE_NORMAL
- en: Our next method is `handleGet`, which gets the contents of the file saved on
    GridFS to the local filesystem. Similar to the `com.mongodb.DBCollection` class,
    the class `com.mongodb.gridfs.GridFS` has the `find` and `findOne` methods for
    searching. However, instead of accepting any DBObject query, `find` and `findOne`
    in GridFS accept filename or the ObjectID value of the document to search in `fs.files`
    collection. Similarly, the return value is not a DBCursor but an instance of `com.mongodb.gridfs.GridFSDBFile`.
    This class has various methods that get the `InputStream` of the bytes of content
    present in the file on GridFS, `writeTo` file or `OutputStream` and a method,
    `getLength` that gives the number of bytes in the file. Refer to the Javadocs
    of the class `com.mongodb.gridfs.GridFSDBFile` for details.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we look at the method `handleDelete`, which is used to delete the
    files on GridFS and is the simplest of the lot. The method on the object of GridFS
    is `remove`, which accepts a string argument: the name of the file to delete on
    the server. The `return` type of this method is `void`. So irrespective of whether
    the content is present on GridFS or not, the method will not return a value nor
    throw an exception if a name is provided to this method for a file that doesn''t
    exist.'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can refer to the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Storing binary data in Mongo*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Storing data to GridFS from Python client*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Storing data to GridFS from Python client
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the recipe *Storing large data in Mongo using GridFS*, we saw what GridFS
    is and how it could be used to store the large files in MongoDB. In the previous
    recipe, we saw to use GridFS API from a Java client. In this recipe, we will see
    how to store image data into MongoDB using GridFS from a Python program.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Refer to the recipe *Connecting to the single node using a Java client* from
    [Chapter 1](ch01.html "Chapter 1. Installing and Starting the Server"), *Installing
    and Starting the Server*, for all the necessary setup for this recipe. If you
    are interested in more detail on Python drivers refer to the following recipes:
    *Executing query and insert operations with PyMongo* and *Executing update and
    delete operations using PyMongo* in [Chapter 3](ch03.html "Chapter 3. Programming
    Language Drivers"), *Programming Language Drivers*. Download and save the image
    `glimpse_of_universe-wide.jpg` from the downloadable bundle available with the
    book from the Packt site to local filesystem as we did in the previous recipe.'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Open a Python interpreter by typing in the following in the operating system
    shell. Note that the current directory is same as the directory where the image
    file `glimpse_of_universe-wide.jpg` is placed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the required packages as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the Python shell is opened, create a `MongoClient` and a database object
    to the test database as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'To clear the GridFS-related collections execute the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the instance of GridFS as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will read the file and upload its contents to GridFS. First, create
    the file object as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: Now put the file into GridFS as follows
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: On successfully executing `put`, we should see the ObjectID for the file uploaded.
    This would be same as the `_id` field of the `fs.files` collection for this file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Execute the following query from the Python shell. It should print out the `dict`
    object with the details of the upload. Verify the contents
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will get the uploaded content and write it to a file on the local filesystem.
    Let''s get the `GridOut` instance representing the object to read the data out
    of GridFS as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'With this instance available, let''s write the data to the file to a file on
    local filesystem as follows. First, open a handle to the file on local filesystem
    to write to as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'We will then write content to it as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: Now verify the file on the current directory on the local filesystem. A new
    file called `universe.jpg` will be created with same number of bytes as the source
    present in it. Verify it by opening it in an image viewer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's look at the steps we executed. In the Python shell, we import two packages,
    `pymongo` and `gridfs`, and instantiate the `pymongo.MongoClient` and `gridfs.GridFS`
    instances. The constructor of the class `gridfs.GridFS` takes on an argument,
    which is the instance of `pymongo.Database`.
  prefs: []
  type: TYPE_NORMAL
- en: We open a file in binary mode using the `open` function and pass the file object
    to the GridFS `put` method. There is an additional argument called `filename`
    passed, which would be the name of the file put into GridFS. The first parameter
    need not be a file object but any object with a `read` method defined.
  prefs: []
  type: TYPE_NORMAL
- en: Once the `put` operation succeeds, the `return` value is an ObjectID for the
    uploaded document in `fs.files` collection. A query on `fs.files` can confirm
    that the file is uploaded. Verify that the size of the data uploaded matches the
    size of the file.
  prefs: []
  type: TYPE_NORMAL
- en: Our next objective is to get the file from GridFS on to the local filesystem.
    Intuitively, one would imagine if the method to put a file in GridFS is `put`,
    then the method to get a file would be `get`. True, the method is indeed `get`,
    however, it will get only based on the `ObjectId` that was returned by the `put`
    method. So, if you are okay to fetch by `ObjectId`, `get` is the method for you.
    However, if you want to get by the filename, the method to use is `get_last_version`.
    It accepts the name of the filename that we uploaded and the return type of this
    method is of type `gridfs.gridfs_file.GridOut`. This class contains the method
    `read`, which will read out all the bytes from the uploaded file to GridFS. We
    open a file called `universe.jpg` for writing in binary mode and write all the
    bytes read from the `GridOut` object.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can refer to the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Storing binary data in Mongo*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Storing data to GridFS from Java client*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing triggers in Mongo using oplog
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In a relational database, a trigger is a code that gets invoked when an `insert`,
    `update`, or a `delete` operation is executed on a table in the database. A trigger
    can be invoked either before or after the operation. Triggers are not implemented
    in MongoDB out of the box and in case you need some sort of notification for your
    application whenever any `insert`/`update`/`delete` operations are executed, you
    are left to manage that by yourself in the application. One approach is to have
    some sort of data access layer in the application, which is the only place to
    query, insert, update, or delete documents from the collections. However, there
    are few challenges to it. First, you need to explicitly code the logic to accommodate
    this requirement in the application, which may or may not be feasible. If the
    database is shared and multiple applications access it, things become even more
    difficult. Secondly, the access needs to be strictly regulated and no other source
    of insert/update/delete be permitted.
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, we need to look at running some sort of logic in a layer close
    to the database. One way to track all write operations is by using an oplog. Note
    that read operations cannot be tracked using oplogs. In this recipe, we will write
    a small Java application that would tail an oplog and get all the `insert`, `update`
    and `delete` operations happening on a Mongo instance. Note that this program
    is implemented in Java and works equally well in any other programming language.
    The crux lies in the logic for the implementation, the platform for implementation
    can be any. Also, this works only if the mongod instance is started as a part
    of replica set and not a standalone instance. Also, this trigger like functionality
    can only be invoked only after the operation is performed and not before the data
    gets inserted/updated or deleted from the collection.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Refer to the recipe *Starting multiple instances as part of a replica set* from
    [Chapter 1](ch01.html "Chapter 1. Installing and Starting the Server"), *Installing
    and Starting the Server*, for all the necessary setup for this recipe. If you
    are interested in more details on Java drivers, refer to the following recipes
    *Executing query and insert operations using a Java client* and *Executing update
    and delete operations using a Java client* in [Chapter 3](ch03.html "Chapter 3. Programming
    Language Drivers"), *Programming Language Drivers*. Prerequisites of these two
    recipes are all we need for this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Refer to the recipe *Creating and tailing a capped collection cursors in MongoDB*
    in this chapter to know more about capped collections and tailable cursors if
    you are not aware or need a refresher. Finally, though not mandatory, [Chapter
    4](ch04.html "Chapter 4. Administration"), *Administration*, explains oplog in
    depth in the recipe *Understanding and analyzing oplogs*. This recipe will not
    explain oplog in depth as we did in [Chapter 4](ch04.html "Chapter 4. Administration"),
    *Administration*. Open a shell and connect it to the primary of the replica set.
  prefs: []
  type: TYPE_NORMAL
- en: For this recipe, we will be using the project `mongo-cookbook-oplogtrigger`.
    This project is available in the source code bundle downloadable from Packt site.
    The folder needs to be extracted on the local filesystem. Open a command line
    shell and go to the root of the project extracted. It should be the directory
    where the file `pom.xml` is found. Also, the `TriggerOperations.js` file would
    be needed to trigger operations in the database that we intend to capture.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Open an operating system shell and execute the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'With the Java program started, we will open the shell as follows with the file
    `TriggerOperations.js` present in the current directory and the mongod instance
    listening to port `27000` as the primary:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the shell is connected, execute the following function we loaded from
    the JavaScript:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: Observe the output printed out on the console where the Java program `com.packtpub.mongo.cookbook.OplogTrigger`
    is being executed using Maven.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The functionality we implemented is pretty handy for a lot of use cases but
    let's see what we did at a higher level first. The Java program `com.packtpub.mongo.cookbook.OplogTrigger`
    is something that acts as a trigger when new data is inserted, updated, or deleted
    from a collection in MongoDB. It uses oplog collection that is the backbone of
    the replication in Mongo to implement this functionality.
  prefs: []
  type: TYPE_NORMAL
- en: The JavaScript we have just acts as a source of producing, updating, and deleting
    data from the collection. You may choose to open the `TriggerOperations.js` file
    and take a look at how it is implemented. The collection on which it performs
    is present in the test database and is called `oplogTriggerTest`.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we execute the JavaScript function, we should see something like the following
    printed to the output console:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: The Maven program will be continuously running and never terminate as the Java
    program doesn't. You may hit *Ctrl* + *C* to stop the execution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s analyze the Java program, which is where the meat of the content is.
    The first assumption is that for this program to work, a replica set must be set
    up as we will use Mongo''s oplog collection. The Java programs created a connection
    to the primary of the replica set members, connects to the local database, and
    gets the `oplog.rs` collection. Then, all it does is find the last or nearly the
    last timestamp in the oplog. This is done to prevent the whole oplog to be replayed
    on startup but to mark a point towards the end in the oplog. Here is the code
    to find this timestamp value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: The oplog is sorted in the reverse natural order to find the time in the last
    document in it. Since oplogs follow the first in first out pattern, sorting the
    oplog in the descending natural order is equivalent to sorting by the timestamp
    in descending order.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once this is done, finding the timestamp as before, we query the oplog collection
    as usual but with two additional options:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: The query finds all documents greater than a particular timestamp and adds two
    options, `Bytes.QUERYOPTION_TAILABLE` and `Bytes.QUERYOPTION_AWAITDATA`. The latter
    option can only be added when the former option is added. This not only queries
    and returns the data, but also waits for some time when the execution reaches
    the end of the cursor for some more data. Eventually, when no data arrives, it
    terminates.
  prefs: []
  type: TYPE_NORMAL
- en: During every iteration, store the last seen timestamp as well. This is used
    when the cursor closes when no more data is available and we query again to get
    a new tailable cursor instance. The query this time will use the timestamp we
    have stored on previous iteration, when the last document was seen. This process
    continues indefinitely and we basically tail the collection in a similar way to
    how we tail a file in Unix using the `tail` command.
  prefs: []
  type: TYPE_NORMAL
- en: The oplog document contains a field called `op` for the operation whose value
    is `i`, `u`, and `d` for insert, update, and delete, respectively. The field `o`
    contains the inserted or deleted object's ID (`_id`) in case of insert and delete.
    In case of update, the file `o2` contains the `_id`. All we do is simply check
    for these conditions and print out the operation and the ID of the document inserted/deleted
    or updated.
  prefs: []
  type: TYPE_NORMAL
- en: Something to be careful about is as follows. Obviously, the deleted documents
    would not be available in the collection so, the `_id` would not really be useful
    if you intend to query. Also, be careful when selecting a document after update
    using the ID we get as some other operation later in the oplog might already have
    performed more updates on the same document and our application's tailable cursor
    has yet to reach that point. This is common in case of high-volume systems. Similarly,
    for inserts we have a similar problem. The document we might query using the provided
    ID might be updated/deleted already. Applications using this logic to track these
    operations must be aware of them.
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, take a look at the oplog that contains more details. Like the
    document inserted, the `update` statement executed, and so on. Updates in the
    oplog collection are idempotent, which means they can be applied any number of
    times without unintended side effects. For instance, if the actual update was
    to increment the value by 1, the update in the oplog collection will have the
    `set` operator with the final value to be expected. This way, the same update
    can be applied multiple times. The logic you would use would then have to be more
    sophisticated to implement such scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Also, failovers are not handled here. This is needed for production based systems.
    The infinite loop on the other hand opens a new cursor as soon as the first one
    terminates. There could be a sleep duration introduced before the oplog is queried
    again to avoid overwhelming the server with queries. Note that the program given
    here is not a production quality code but just a simple demo of the technique
    that is being used by a lot of other systems to get notified for new data insert,
    delete, and updates to collections in MongoDB.
  prefs: []
  type: TYPE_NORMAL
- en: MongoDB didn't have the text search feature until version 2.4 and prior to that
    all full text search was handled using external search engines like Solr or Elasticsearch.
    Even now, though the text search feature in MongoDB is production ready, many
    would still use an external dedicated search indexer. It won't be a surprise if
    the decision is taken to use an external full text index search tool instead of
    leveraging the MongoDB's inbuilt one. In case of Elasticsearch, the abstraction
    to flow the data in to the indexes is known as a river. The MongoDB river in Elasticsearch,
    which adds data to the indexes as and when the data gets added to the collections
    in Mongo is built on the same logic as we saw in the simple program implemented
    in Java.
  prefs: []
  type: TYPE_NORMAL
- en: Flat plane 2D geospatial queries in Mongo using geospatial indexes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will see what geospatial queries are and then see how to
    apply these queries on flat planes. We will put it to use in a test map application.
  prefs: []
  type: TYPE_NORMAL
- en: Geospatial queries can be executed on data in which geospatial indexes are created.
    There are two types of geospatial indexes. The first one is called the 2D indexes
    and is the simpler of the two, it assumes that the data is given as *x,y* coordinates.
    The second one is called 3D or spherical indexes and is relatively more complicated.
    In this recipe, we will explore the 2D indexes and execute some queries on 2D
    data. The data on which we are going to work upon is a 25 x 25 grid with some
    coordinates representing bus stops, restaurants, hospitals, and gardens.
  prefs: []
  type: TYPE_NORMAL
- en: '![Flat plane 2D geospatial queries in Mongo using geospatial indexes](img/B04831_05_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Refer to the recipe *Connecting to the single node using a Java client* from
    [Chapter 1](ch01.html "Chapter 1. Installing and Starting the Server"), *Installing
    and Starting the Server*, for all the necessary setup for this recipe. Download
    the data file `2dMapLegacyData.json` and keep it on the local filesystem ready
    to import. Open a mongo shell connecting to the local MongoDB instance.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Execute the following command from the operating system shell to import the
    data into the collection. The file `2dMapLegacyData.json` is present in the current
    directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'If we see something like the following on the screen, we can confirm that the
    import has gone through successfully:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'After the successful import, from the opened mongo shell, verify the collection
    and its content by executing the following query:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: This should give you the feel of the data in the collection.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is to create 2D geospatial index on this data. Execute the following
    to create a 2D index:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: With the index created, we will now try to find the nearest restaurant from
    the place an individual is standing. Assuming the person is not fussy about the
    type of cuisine, let's execute the following query assuming that the person is
    standing at location (12, 8), as shown in the image. Also, we are interested in
    just three nearest places.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: This should give us three results, starting with the nearest restaurant with
    the subsequent ones given in increasing distance. If we look at the image given
    earlier, we kind of agree with the results given here.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let''s add more options to the query. The individual has to walk and thus wants
    the distance to be restricted to a particular value in the results. Let''s rewrite
    the query with the following modification:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: Observe the number of results retrieved this time around.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s now go through what we did. Before we continue, let''s define what exactly
    we mean by the distance between two points. Suppose on a cartesian plane that
    we have two points (x[1], y[1]) and (x[2], y[2]), the distance between them would
    be computed using the formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '*√(x[1] – x[2])² + (y[1] – y[2])²*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose the two points are (2, 10) and (12, 3), the distance would be: √(2
    – 12)² + (10 – 3)² = √(-10)² + (7)² = √149 =12.207.'
  prefs: []
  type: TYPE_NORMAL
- en: After knowing how calculations for distance calculation are done behind the
    scenes by MongoDB, let's see what we did right from step 1.
  prefs: []
  type: TYPE_NORMAL
- en: We started by importing the data normally into a collection, `areaMap` in the
    `test` database and created an index as `db.areaMap.ensureIndex({co:'2d'})`. The
    index is created on the field `co` in the document and the value is a special
    value, `2d`, which denotes that this is a special type of index called 2D geospatial
    index. Usually, we give this value as `1` or `-1` in other cases denoting the
    order of the index.
  prefs: []
  type: TYPE_NORMAL
- en: There are two types of indexes. The first is a 2D index that is commonly used
    for planes whose span is less and do not involve spherical surfaces. It could
    be something like a map of the building, a locality, or even a small city where
    the curvature of the earth covering the portion of the land is not really significant.
    However, once the span of the map increases and covers the globe, 2D indexes will
    be inaccurate for predicting the values as the curvature of the earth needs to
    be considered in the calculations. In such cases, we go for spherical indexes,
    which we will discuss soon.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the 2D index is created, we can use it to query the collection and find
    some points near the point queried. Execute the following query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'It will query for documents that are of the type R, which are of type `restaurants`
    and closes to the co-ordinates (12,8). The results returned by this query will
    be in the increasing order of the distance from the point in question, (12, 8)
    in this case. The limit just limits the result to top three documents. We may
    also provide the `$maxDistance` in the query, which will restrict the results
    with a distance less than or equal to the provided value. We queried for locations
    not more than four units away, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: Spherical indexes and GeoJSON compliant data in Mongo
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we continue with this recipe, we need to look at the previous recipe
    *Flat plane 2D geospatial queries in Mongo using geospatial indexes* to get an
    understanding of what geospatial indexes are in MongoDB and how to use the 2D
    indexes. So far, we have imported the JSON documents in a non-standard format
    in MongoDB collection, created geospatial indexes, and queried them it. This approach
    works perfectly fine and in fact, it was the only option available until MongoDB
    2.4\. version 2.4 of MongoDB supports an additional way to store, index, and query
    the documents in the collections. There is a standard way to represent geospatial
    data particularly meant for geodata exchange in JSON and the specification of
    GeoJSON mentions it in detail in the following link: [http://geojson.org/geojson-spec.html](http://geojson.org/geojson-spec.html).
    We can now store the data in this format.'
  prefs: []
  type: TYPE_NORMAL
- en: There are various geographic figure types supported by this specification. However,
    for our use case, we will be using the type `Point`. First let's see how the document
    we imported before using a non-standard format looked and how the one using GeoJSON
    format looks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Document in non-standard format:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'Document in GeoJSON format:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: It looks more complicated than the non-standard format and for our particular
    case I do agree. However, when representing polygons and other lines, the non-standard
    format might have to store multiple documents. In this case, it can be stored
    in a single document just by changing the value of the `type` field. Refer to
    the specification for more details.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The prerequisites for this recipe are same as the prerequisites for the previous
    recipe except that the file to be imported would be `2dMapGeoJSONData.json` and
    `countries.geo.json`. Download these files from the Packt site and keep them on
    the local filesystem for importing them later.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Special thanks to Johan Sundström for sharing the world data. The GeoJSON for
    the world is taken from [https://github.com/johan/world.geo.json](https://github.com/johan/world.geo.json).
    The file is massaged to enable importing and index creation in Mongo. Version
    2.4 doesn't support MultiPolygon and thus all MultiPolygon type of shapes are
    omitted. The shortcoming seems to be fixed in Version 2.6 though.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Import the GeoJSON compatible data in a new collection as follows. This contains
    26 documents similar to what we imported last time around, except that they are
    formatted using the GeoJSON format.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a Geospatial index on this collections as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now first query the collection `areaMapGeoJSON` collection as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: Next, we will try to find all the restaurants that fall within the square drawn
    between the points (0, 0), (0, 11), (11, 11), and (11, 0). Refer to the figure
    given in the introduction of the previous recipe for getting a clear visual of
    the points and the results to expect.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Write the following query and observe the results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: Check if it contains the three restaurants at coordinates (2, 6), (10, 5), and
    (10, 1) as expected.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will next try and perform some operations that would find all the matching
    objects that lie completely within another enclosing polygon. Suppose that we
    want to find some bus stops that lie within a given square block. Such use cases
    can be addressed using the `$geoWithin` operator, and the query to achieve it
    is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: Verify the results; we should have three bus stops in the result. Refer to the
    image of the map in the previous recipe's introduction to get the expected results
    of the query.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'When we execute the above commands, they just print the documents in ascending
    order of the distance. However, we don''t see the actual distance in the result.
    Let''s execute the same query as in point number 3 and additionally, get the calculated
    distances as following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: The query returns one document with an array within the field called results
    containing the matching documents and the calculated distances. The result also
    contains some additional stats giving the maximum distance, the average of the
    distances in the result, the total documents scanned, and the time taken in milliseconds.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We will finally query on the world map collection to find which country the
    provided coordinate lies in. Execute the following query as follows from the mongo
    shell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: All the possible operations we can perform with the `worldMap` collection are
    numerous and not all are practically possible to cover in this recipe. I would
    encourage you to play around with this collection and try out different use cases.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Starting from version MongoDB 2.4, the standard way for storing geospatial data
    in JSON is also supported. Note that the legacy approach that we saw is also supported.
    However, if you are starting afresh, it is recommended to go ahead with this approach
    for the following reasons.
  prefs: []
  type: TYPE_NORMAL
- en: It is a standard and anybody aware of the specification would easily be able
    to understand the structure of the document
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It makes storing complex shapes, polygons, and multiple lines easy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It also lets us query easily for the intersection of the shapes using the `$geoIntersect`
    and other new set of operators
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For using GeoJSON-compatible documents, we import JSON documents in the file
    `2dMapGeoJSONData.json` into the collection `areaMapGeoJSON` and create the index
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: The collection has data similar to what we had imported into the `areaMap` collection
    in the previous recipe but with a different structure that is compatible to JSON
    format. The type here used is 2Dsphere and not 2D. The 2Dsphere type of index
    also considers the spherical surfaces in calculations. Note that the field `co`,
    on which we are creating the geospatial index, is not an array of coordinates
    but a document itself that is GeoJSON compatible.
  prefs: []
  type: TYPE_NORMAL
- en: We query where the value of the `$near` operator is not an array of the coordinates,
    as we did in our previous recipe, but a document with the key `$geometry` and
    the value is a GeoJSON-compatible document for a point with the coordinates. The
    results, irrespective of the query we use are identical. Refer to point 3 in this
    recipe and point 5 in the previous recipe to see the difference in the query.
    The approach using GeoJSON looks more complicated but it has some advantages which
    we will soon see.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that we cannot mix two approaches. Try executing the
    query in the GeoJSON format that we just executed on the collection `areaMap`
    and see that although we do not get any errors, the results are not correct.
  prefs: []
  type: TYPE_NORMAL
- en: 'We used the `$geoIntersects` operator in point 5 of this recipe. This is only
    possible when the documents are stored in GeoJSON format in the database. The
    query simply finds all the points in our case that intersect any shape we create.
    We create a polygon using the GeoJSON format as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: The coordinates are for the square, giving the four corners in a clockwise direction
    with the last coordinate the same as the first denoting it to be complete. The
    query executed is the same as `$near`, apart from the fact that the `$near` operator
    is replaced by the `$geoIntersects` and the value of the `$geometry` field is
    the GeoJSON document of the polygon with which we wish to find the intersecting
    points in the `areaMapGeoJSON` collection. If we look at the results obtained
    and look at the figure in the introduction section or previous recipe, they indeed
    are what we expected.
  prefs: []
  type: TYPE_NORMAL
- en: We also saw what the `$geoWithin` operator is in point number 12, which is pretty
    handy to use when we want to find the points or even within another polygon. Note
    that only shapes completely inside the given polygon will be returned. Suppose
    that, similar to our `worldMap` collection, we have a `cities` collection with
    their coordinates specified in a similar manner. We can then use the polygon of
    a country to query all the polygons that lie within it in the `cities` collection,
    thus giving us the cities. Obviously, an easier and faster way would be to store
    the country code in the city document. Alternatively, if we have some data missing
    in the city's collection and the country is not present, one point anywhere within
    the city's polygon (since a city entirely lies in one country) can be used and
    a query can be executed on the `worldMap` collection to get its country, which
    we have demonstrated in point number 12.
  prefs: []
  type: TYPE_NORMAL
- en: A combination of what we saw previously can be put to good use to compute the
    distances between two points or even execute some geometric operation.
  prefs: []
  type: TYPE_NORMAL
- en: Some of the functionalities like getting the centroid of a polygon figure stored
    as GeoJSON in the collection or even the area of a polygon are not supported out
    of the box and there should have been some utility functions to help compute these
    given the coordinates. These features are good and are commonly required, and
    perhaps we might have some support in future release; such operations are to be
    implemented by developers themselves. Also, there is no straightforward way to
    find if there is an overlap between two polygons, what the coordinates are, where
    they overlap, the area of overlap, and so on. The `$geoIntersects` operator we
    saw does tell us what polygons do intersect with the given polygon, point, or
    line.
  prefs: []
  type: TYPE_NORMAL
- en: Though nothing related to Mongo, the GeoJSON format doesn't have support for
    circles, and hence storing circles in Mongo using GeoJSON format is not possible.
    Refer to the following link [http://docs.mongodb.org/manual/reference/operator/query-geospatial/](http://docs.mongodb.org/manual/reference/operator/query-geospatial/)
    for more details on geospatial operators.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing full text search in Mongo
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Many of us (I won''t be wrong to say all of us) use Google every day to search
    content on the web. To explain in short: the text that we provide in the text
    box on Google''s page is used to search the pages on the web it has indexed. The
    search results are then returned to us in some order determined by Google''s page
    rank algorithm. We might want to have a similar functionality in our database
    that lets us search for some text content and give the corresponding search results.
    Note that this text search is not same as finding the text as part of the sentence,
    which can easily be done using regex. It goes way beyond that and can be used
    to get results that contain the same word, a similar sounding word, have a similar
    base word, or even a synonym in the actual sentence.'
  prefs: []
  type: TYPE_NORMAL
- en: Since MongoDB Version 2.4, text indexes have been introduced, which let us create
    text indexes on a particular field in the document and enable text search on those
    words. In this recipe, we will be importing some documents and creating text indexes
    on them, which we will later query to retrieve the results.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A simple, single node is what we would need for the test. Refer to the recipe
    *Installing single node MongoDB* from [Chapter 1](ch01.html "Chapter 1. Installing
    and Starting the Server"), *Installing and Starting the Server*, for how to start
    the server. However, do not start the server yet. There would be an additional
    flag provided during the startup to enable text search. Download the file `BlogEntries.json`
    from the Packt site and keep it on your local drive ready to be imported.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Start the MongoDB server listening to port `27017` as follows. Once the server
    is started, we will be creating the test data in a collection as follows. With
    the file `BlogEntries.json` placed in the current directory, we will be creating
    the collection `userBlog` as follows using `mongoimport`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, connect to the `mongo` process from a mongo shell by typing the following
    command from the operating system shell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: 'Once connected, get a feel of the documents in the `userBlog` collection as
    follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: The field `blog_text` is of our interest and this is the one on which we will
    be creating a text search index.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Create a text index on the field `blog_text` of the document as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, execute the following search on the collection from the mongo shell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: Look at the results obtained.
  prefs: []
  type: TYPE_NORMAL
- en: 'Execute another search as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s now see how it all works. A text search is done by a process called
    reverse indexes. In simple terms, this is a mechanism where the sentences are
    broken up into words and then those individual words point back to the document
    which they belong to. The process is not straightforward though, so let''s see
    what happens in this process step by step at a high level:'
  prefs: []
  type: TYPE_NORMAL
- en: Consider the following input sentence, `I played cricket yesterday`. The first
    step is to break this sentence into tokens and they become [`I`, `played`, `cricket`,
    `yesterday`].
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, the stop words from the broken down sentence are removed and we are left
    with a subset of these. Stop words are a list of very common words that are eliminated
    as it makes no sense to index them as they can potentially affect the accuracy
    of the search when used in the search query. In this case, we will be left with
    the following words [`played`, `cricket`, `yesterday`]. Stop words are language
    specific and will be different for different languages.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, these words are stemmed to their base words, in this case it will be
    [`play`, `cricket`, `yesterday`]. Stemming is process of reduction of a word to
    its root. For instance, all the words `play`, `playing`, `played`, and `plays`
    have the same root word, `play`. There are a lot of algorithms and frameworks
    present for stemming a word to its root form. Refer to the Wikipedia [http://en.wikipedia.org/wiki/Stemming](http://en.wikipedia.org/wiki/Stemming)
    page for more information on stemming and the algorithms used for this purpose.
    Similar to eliminating stop words, the stemming algorithm is language dependent.
    The examples given here were for the English language.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If we look at the index creation process, it is created as follows `db.userBlog.ensureIndex({''blog_text'':''text''})`.
    The key given in the JSON argument is the name of the field on which the text
    index is to be created and the value will always be the text denoting that the
    index to be created is a text index. Once the index is created, at a high level,
    the preceding three steps get executed on the content of the field on which the
    index is created in each document and a reverse index is created. You can also
    choose to create a text index on more than one field. Suppose that we had two
    fields, `blog_text1` and `blog_text2`; we can create the index as `{''blog_text1'':
    ''text'', ''blog_text2'':''text''}`. The value `{''$**'':''text''}` creates an
    index on all fields of the document.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we executed the search operation by invoking the following: `db.userBlog.find({$text:
    {$search : ''plot zoo''}})`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This command runs the text search on the collection `userBlog` and the search
    string used is `plot zoo`. This searches for the value `plot` or `zoo` in the
    text in any order. If we look at the results, we see that we have two documents
    matched and the documents are ordered by the score. This score tells us how relevant
    the document searched is, and the higher the score, the more relevant it is. In
    our case, one of the documents had both the words plot and zoo in it, and thus
    got a higher score than a document, as we see here:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To get the scores in the result, we need to modify the query a bit, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: 'We now have an additional document provided in the `find` method that asks
    for the score calculated for the text match. The results still are not ordered
    in descending order of score. Let''s see how to sort the results by score:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, the query is same as before, it's just the additional `sort`
    function that we have added, which will sort the results by descending order of
    score.
  prefs: []
  type: TYPE_NORMAL
- en: When the search is executed as `{$text:{$search:'Zoo -plot'}`, it searches for
    all the documents that contain the word `zoo` and do not contain the word `plot`,
    thus we get only one result. The `-` sign is for negation and leaves out the document
    from the search result containing that word. However, do not expect to find all
    documents without the word plot by just giving `-plot` in the search.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we look at the contents returned as the result of the search, it contains
    the entire matched document in the result. If we are not interested in the entire
    document, but only a few documents, we can use projection to get the desired fields
    of the document. The following query, for instance, `db.userBlog.find({$text:
    {$search : ''plot zoo''}},{_id:1})` will be same as finding all the documents
    in the `userBlog` collection containing the words zoo or plot, but the results
    will contain the `_id` field from the resulting documents.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If multiple fields are used for creation of index, then we may have different
    weights for different fields in the document. For instance, suppose blog_text1
    and blog_text2 are two fields of a collection. We can create an index where `blog_text1`
    has higher weight than `blog_text2` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: This gives the content in `blog_text1` twice as much weight as that in `blog_text2`.
    Thus, if a word is found in two documents but is present in the `blog_text1` field
    of the first document and `blog_text2` of second document, then the score of first
    document will be more than the second. Note that we also have provided the name
    of the index using the name field as `MyCustomIndexName`.
  prefs: []
  type: TYPE_NORMAL
- en: We also see from the language key that the language in this case is English.
    MongoDB supports various languages for implementing text search. Languages are
    important when indexing the content as they decide the stop words, and stemming
    of words is language specific too.
  prefs: []
  type: TYPE_NORMAL
- en: Visit the link [http://docs.mongodb.org/manual/reference/command/text/#text-search-languages](http://docs.mongodb.org/manual/reference/command/text/#text-search-languages)
    for more details on the languages supported by Mongo for text search.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, how do we choose the language while creating the index? By default, if
    nothing is provided, the index is created assuming the language is English. However,
    if we know the language is French, we create the index as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: 'Suppose that we had originally created the index using the French language,
    the `getIndexes` method would return the following document:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: 'However, if the language was different per document basis, which is pretty
    common in scenarios like blogs, we have a way out. If we look at the document
    above, the value of the `language_override` field is language. This means that
    we can store the language of the content using this field on a per document basis.
    In its absence, the value will be assumed as the default value, `french` in the
    preceding case. Thus, we can have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To use MongoDB text search in production, you would need version 2.6 or higher.
    Integrating MongoDB with other systems like Solr and Elasticsearch is also an
    option. In the next recipe, we will see how to integrate Mongo with Elasticsearch
    using the mongo-connector.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For more information on the `$text` operator, visit [http://docs.mongodb.org/manual/reference/operator/query/text/](http://docs.mongodb.org/manual/reference/operator/query/text/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integrating MongoDB for full text search with Elasticsearch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'MongoDB has integrated text search features, as we saw in the previous recipe.
    However, there are multiple reasons why one would not use the Mongo text search
    feature and fall back to a conventional search engine like Solr or Elasticsearch,
    and the following are few of them:'
  prefs: []
  type: TYPE_NORMAL
- en: The text search feature is production ready in version 2.6\. In version 2.4,
    it was introduced in beta and not suitable for production use cases.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Products like Solr and Elasticsearch are built on top of Lucene, which has proven
    itself in the search engine arena. Solr and Elasticsearch are pretty stable products
    too.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You might already have expertise on products like Solr and Elasticsearch and
    would like to use it as a full text search engine rather than MongoDB.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some particular feature that you might find missing in MongoDB search which
    your application might require, for example, facets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up a dedicated search engine does need additional efforts to integrate
    it with a MongoDB instance. In this recipe, we will see how to integrate a MongoDB
    instance with a search engine, Elasticsearch.
  prefs: []
  type: TYPE_NORMAL
- en: We will be using the mongo-connector for integration purpose. It is an open
    source project that is available at [https://github.com/10gen-labs/mongo-connector](https://github.com/10gen-labs/mongo-connector).
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Refer to the recipe *Connecting to a single node using a Python client*, in
    [Chapter 1](ch01.html "Chapter 1. Installing and Starting the Server"), *Installing
    and Starting the Server* for installing and setting up Python client. The tool
    pip is used for getting the mongo-connector. However, if you are working on a
    Windows platform, the steps to install pip was not mentioned earlier. Visit the
    URL [https://sites.google.com/site/pydatalog/python/pip-for-windows](https://sites.google.com/site/pydatalog/python/pip-for-windows)
    to get pip for windows.
  prefs: []
  type: TYPE_NORMAL
- en: The prerequisites for starting the single instance are all we need for this
    recipe. We would, however, start the server as a one node replica set for demonstration
    purpose in this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Download the file `BlogEntries.json` from the Packt site and keep it on your
    local drive ready to be imported.
  prefs: []
  type: TYPE_NORMAL
- en: 'Download elastic search from the following URL for your target platform: [http://www.elasticsearch.org/overview/elkdownloads/](http://www.elasticsearch.org/overview/elkdownloads/).
    Extract the downloaded archive and from the shell, go to the `bin` directory of
    the extraction.'
  prefs: []
  type: TYPE_NORMAL
- en: We will get the mongo-connector source from GitHub.com and run it. A Git client
    is needed for this purpose. Download and install the Git client on your machine.
    Visit the URL [http://git-scm.com/downloads](http://git-scm.com/downloads) and
    follow the instructions for installing Git on your target operating system. If
    you are not comfortable installing Git on your operating system, then there is
    an alternative available that lets you download the source as an archive.
  prefs: []
  type: TYPE_NORMAL
- en: 'Visit the following URL [https://github.com/10gen-labs/mongo-connector](https://github.com/10gen-labs/mongo-connector).
    Here, we will get an option that lets us download the current source as an archive,
    which we can then extract on our local drive. The following image shows that the
    download option available on the bottom-right corner:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Getting ready](img/B04831_05_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Note that we can also install mongo-connector in a very easy way using pip
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: However, the version in PyPi is a very old with not many features supported
    and thus using the latest from the repository is recommended.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to the previous recipe, where we saw text search in Mongo, we will use
    the same five documents to test our simple search. Download and keep the `BlogEntries.json`
    file.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'At this point, it is assumed that Python and PyMongo are installed and pip
    for your operating system platform is installed. We will now get mongo-connector
    from source. If you have already installed the Git client, we will be executing
    the following on the operating system shell. If you have decided to download the
    repository as an archive, you may skip this step. Go to the directory where you
    would like to clone the connector repository and execute the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: The preceding setup will also install the Elasticsearch client that will be
    used by this application.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We will now start a single mongo instance but as a replica set. From the operating
    system console, execute the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: 'Start a mongo shell and connect to the started instance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: 'From the mongo shell initiate the replica set as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: The replica set will be initiated in a few moments. Meanwhile, we can proceed
    to starting the `elasticsearch` server instance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Execute the following from the command after going to the `bin` directory of
    the extracted `elasticsearch` archive:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE
- en: We won't be getting into the Elasticsearch settings, and we will start it in
    the default mode.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once started, enter the following URL in the browser `http://localhost:9200/_nodes/process?pretty`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If we see a JSON document as the following, giving the process details, we have
    successfully started `elasticsearch`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs: []
  type: TYPE_PRE
- en: Once the `elasticsearch` server and mongo instance are up and running, and the
    necessary Python libraries are installed, we will start the connector that will
    sync the data between the started mongo instance and the `elasticsearch` server.
    For the sake of this test, we will be using the collection `user_blog` in the
    `test` database. The field on which we would like to have text search implemented
    is the field `blog_text` in the document.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Start the mongo-connector from the operating system shell as follows. The following
    command was executed with the mongo-connector's directory as the current directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs: []
  type: TYPE_PRE
- en: Import the `BlogEntries.json` file into the collection using `mongoimport` utility
    as follows. The command is executed with the `.json` file present in the current
    directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs: []
  type: TYPE_PRE
- en: 'Open a browser of your choice and enter the following URL in it: `http://localhost:9200/_search?q=blog_text:facebook`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You should see something like the following in the browser:![How to do it…](img/B04831_05_04.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Mongo-connector basically tails the oplog to find new updates that it publishes
    to another endpoint. We used elasticsearch in our case, but it could be even be
    Solr. You may choose to write a custom DocManager that would plugin with the connector.
    Refer to the wiki [https://github.com/10gen-labs/mongo-connector/wiki](https://github.com/10gen-labs/mongo-connector/wiki)
    for more details, and the readme for [https://github.com/10gen-labs/mongo-connector](https://github.com/10gen-labs/mongo-connector)
    gives some detailed information too.
  prefs: []
  type: TYPE_NORMAL
- en: 'We gave the connector the options `-m`, `-t`, `-n`, `--fields`, and `-d` and
    what they mean is explained in the table as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Option | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `-m` | URL of the MongoDB host to which the connector connects to get the
    data to be synchronized. |'
  prefs: []
  type: TYPE_TB
- en: '| `-t` | The target URL of the system with which the data is to be synchronized
    with. Elasticsearch in this case. The URL format will depend on the target system.
    Should you choose to implement your own DocManager, the format will be one that
    your DocManager understands. |'
  prefs: []
  type: TYPE_TB
- en: '| `-n` | This is the namespace that we would like keep synchronized with the
    external system. The connector will just be looking for changes in these namespaces
    while tailing the oplog for data. The value will be comma separated if more than
    one namespaces are to be synchronized. |'
  prefs: []
  type: TYPE_TB
- en: '| `--fields` | These are the fields from the document that will be sent to
    the external system. In our case, it doesn''t make sense to index the entire document
    and waste resources. It is recommended to add to the index just the fields that
    you would like to add text search support. The identifier `_id` and the namespace
    of the source is also present in the result, as we can see in the preceding screenshot.
    The `_id` field can then be used to query the target collection. |'
  prefs: []
  type: TYPE_TB
- en: '| `-d` | This is the document manager to be used, in our case we have used
    the elasticsearch''s document manager. |'
  prefs: []
  type: TYPE_TB
- en: For more supported options, refer to the readme of the connector's page on GitHub.
  prefs: []
  type: TYPE_NORMAL
- en: Once the insert is executed on the MongoDB server, the connector detects the
    newly added documents to the collection of its interest, `user_blog`, and starts
    sending the data to be indexed from the newly documents to the elasticsearch.
    To confirm the addition, we execute a query in the browser to view the results.
  prefs: []
  type: TYPE_NORMAL
- en: Elasticsearch will complain that the index names have upper case characters
    in them. The mongo-connector doesn't take care of this, and thus the name of the
    collection has to be in lower case. For example, the name `userBlog` will fail.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have not done any additional configuration on elasticsearch as that was not
    the objective of the recipe. We were more interested in integrating MongoDB and
    elasticsearch. You will have to refer to elasticsearch documentation for more
    advanced config options. If integrating with elasticsearch is required, there
    is a concept called rivers in elasticsearch that can be used as well. Rivers are
    elasticsearch's way to get data from another data source. For MongoDB, the code
    for the river can be found at [https://github.com/richardwilly98/elasticsearch-river-mongodb/](https://github.com/richardwilly98/elasticsearch-river-mongodb/).
    The readme in this repository has steps on how to set it up.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we saw a recipe, *Implementing triggers in Mongo using oplog*,
    on how to implement trigger-like functionalities using Mongo. This connector and
    MongoDB river for elasticsearch rely on the same logic for getting the data out
    of Mongo as and when it is needed.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You may find additional elasticsearch documentation at [http://www.elasticsearch.org/guide/en/elasticsearch/reference/](http://www.elasticsearch.org/guide/en/elasticsearch/reference/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
