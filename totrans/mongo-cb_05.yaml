- en: Chapter 5. Advanced Operations
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第5章。高级操作
- en: 'In this chapter, we will cover the following recipes:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下内容：
- en: Atomic find and modify operations
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 原子查找和修改操作
- en: Implementing atomic counters in Mongo
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Mongo中实现原子计数器
- en: Implementing server-side scripts
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现服务器端脚本
- en: Creating and tailing a capped collection cursors in MongoDB
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在MongoDB中创建和追踪封顶集合游标
- en: Converting a normal collection to capped collection
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将普通集合转换为封顶集合
- en: Storing binary data in Mongo
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Mongo中存储二进制数据
- en: Storing large data in Mongo using GridFS
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用GridFS在Mongo中存储大数据
- en: Storing data to GridFS from Java client
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从Java客户端将数据存储到GridFS
- en: Storing data to GridFS from Python client
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从Python客户端将数据存储到GridFS
- en: Implementing triggers in Mongo using oplog
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用oplog在Mongo中实现触发器
- en: Flat plane (2D) geospatial queries in Mongo using geospatial indexes
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Mongo中使用平面（2D）地理空间索引进行查询
- en: Spherical indexes and GeoJSON compliant data in Mongo
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Mongo中使用球形索引和GeoJSON兼容数据
- en: Implementing full text search in Mongo
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Mongo中实现全文搜索
- en: Integrating MongoDB for full text search with Elasticsearch
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将MongoDB集成到Elasticsearch进行全文搜索
- en: Introduction
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: In [Chapter 2](ch02.html "Chapter 2. Command-line Operations and Indexes"),
    *Command-line Operations and Indexes*, we saw how to perform basic operations
    from the shell to query, update, and insert documents, and also saw different
    types of indexes and index creation. In this chapter, we will see some of the
    advanced features of Mongo, such as GridFS, Geospatial Indexes, and Full text
    search. Other recipes we will see include an introduction and use of capped collections
    and implementing server-side scripts in MongoDB.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第2章](ch02.html "第2章。命令行操作和索引")中，*命令行操作和索引*，我们看到了如何从shell执行基本操作来查询、更新和插入文档，还看到了不同类型的索引和索引创建。在本章中，我们将看到Mongo的一些高级功能，如GridFS、地理空间索引和全文搜索。我们还将看到其他配方，包括封顶集合的介绍和使用以及在MongoDB中实现服务器端脚本。
- en: Atomic find and modify operations
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 原子查找和修改操作
- en: In [Chapter 2](ch02.html "Chapter 2. Command-line Operations and Indexes"),
    *Command-line Operations and Indexes*, we had some recipes that explained various
    CRUD operations we perform in MongoDB. There was one concept that we didn't cover
    and it is atomically find and modify documents. Modification consists of both
    update and delete operations. In this recipe, we will go through the basics of
    MongoDB's `findAndModify` operation. In the next recipe, we will use this method
    to implement a counter.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第2章](ch02.html "第2章。命令行操作和索引")中，*命令行操作和索引*，我们有一些配方解释了我们在MongoDB中执行的各种CRUD操作。有一个概念我们没有涵盖到，那就是原子查找和修改文档。修改包括更新和删除操作。在这个配方中，我们将介绍MongoDB的`findAndModify`操作的基础知识。在下一个配方中，我们将使用这种方法来实现一个计数器。
- en: Getting ready
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备就绪
- en: Look at the recipe *Installing single node MongoDB* in [Chapter 1](ch01.html
    "Chapter 1. Installing and Starting the Server"), *Installing and Starting the
    Server* and start a single instance of MongoDB. That is the only prerequisite
    for this recipe. Start a mongo shell and connect to the started server.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 查看[第1章](ch01.html "第1章。安装和启动服务器")中的*安装单节点MongoDB*和*安装和启动服务器*的配方，并启动MongoDB的单个实例。这是这个配方的唯一先决条件。启动mongo
    shell并连接到已启动的服务器。
- en: How to do it…
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作…
- en: 'We will test a document in `atomicOperationsTest` collection. Execute the following
    from the shell:'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将在`atomicOperationsTest`集合中测试一个文档。从shell执行以下操作：
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Execute the following from the mongo shell and observe the output:'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从mongo shell执行以下操作并观察输出：
- en: '[PRE1]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We will execute another one this time but with slightly different parameters;
    observe the output for this operation:'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这次我们将执行另一个操作，但参数略有不同；观察此操作的输出：
- en: '[PRE2]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We will execute another update this time that would upsert the document as
    follows:'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这次我们将执行另一个更新，将会插入文档，如下所示：
- en: '[PRE3]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now, query the collection once as follows and see the documents present:'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，按照以下方式查询集合并查看当前存在的文档：
- en: '[PRE4]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We will finally execute the delete as follows:'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将按以下方式执行删除：
- en: '[PRE5]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: How it works…
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理…
- en: If we perform find and update operations independently by first finding the
    document and then updating it in MongoDB, the results might not be as expected.
    There might be an interleaving update between the find and the update operations,
    which may have changed the document state. In some of the specific use cases,
    like implementing atomic counters, this is not acceptable and thus we need a way
    to atomically find, update, and return a document. The returned value is either
    the one before the update is applied or after the update is applied and is decided
    by the invoking client.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在MongoDB中首先查找文档，然后再更新它，结果可能不如预期。在查找和更新操作之间可能存在交错的更新，这可能已更改文档状态。在某些特定用例中，比如实现原子计数器，这是不可接受的，因此我们需要一种方法来原子地查找、更新和返回文档。返回的值是在更新应用之前或之后的值，由调用客户端决定。
- en: Now that we have executed the steps in the preceding section, let's see what
    we actually did and what all these fields in the JSON document passed as the parameter
    to the `findAndModify` operation mean. Starting with step 3, we gave a document
    as a parameter to the `findAndModify` function that contains the fields `query`,
    `update`, and `new`.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经执行了前一节中的步骤，让我们看看我们实际做了什么，以及作为参数传递给`findAndModify`操作的JSON文档中的所有这些字段的含义。从第3步开始，我们将一个包含字段`query`、`update`和`new`的文档作为参数传递给`findAndModify`函数。
- en: The `query` field specifies the search parameters that would be used to find
    the document and the `update` field contains the modifications that need to be
    applied. The third field, new, if set to `true`, tells MongoDB to return the updated
    document.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '`query`字段指定用于查找文档的搜索参数，`update`字段包含需要应用的修改。第三个字段`new`，如果设置为`true`，告诉MongoDB返回更新后的文档。'
- en: In step 4, we actually added a new field to the document passed as a parameter
    called **fields** that is used to select a limited set of fields from the result
    document returned. Also, the value of the field new is `true`, which tells that
    we want the updated document that is, the one after the update operation is executed
    and not the one before.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在第4步中，我们实际上向作为参数传递的文档添加了一个名为**fields**的新字段，用于从返回的结果文档中选择一组有限的字段。此外，`new`字段的值为`true`，表示我们希望更新的文档，即在执行更新操作之后的文档，而不是之前的文档。
- en: In step 5 contains a new field called `upsert`, which upserts (update + insert)
    the document. That is, if the document with the given query is found, it is updated
    else a new one is created and updated. If the document didn't exist and an upsert
    happened, having the value of the parameter `new` as `false` will return `null`.
    This is because there was nothing present before the update operation was executed.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 第5步包含一个名为`upsert`的新字段，该字段执行upsert（更新+插入）文档。也就是说，如果找到具有给定查询的文档，则更新该文档，否则创建并更新一个新文档。如果文档不存在并且发生了upsert，那么将参数`new`的值设置为`false`将返回`null`。这是因为在执行更新操作之前没有任何内容存在。
- en: Finally, in step 7, instead of the `update` field, we used the `remove` field
    with the value `true` indicating that the document is to be removed. Also, the
    value of the new field is `false`, which means that we expect the document that
    got deleted.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在第7步中，我们使用了`remove`字段，其值为`true`，表示要删除文档。此外，`new`字段的值为`false`，这意味着我们期望被删除的文档。
- en: See also
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参阅
- en: An interesting use case of atomic `FindandModify` operations is developing an
    atomic counter in Mongo. In our next recipe, we will see how to implement this
    use case.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 原子`FindandModify`操作的一个有趣的用例是在Mongo中开发原子计数器。在下一个配方中，我们将看到如何实现这个用例。
- en: Implementing atomic counters in Mongo
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Mongo中实现原子计数器
- en: Atomic counters are a necessity for a large number of use cases. Mongo doesn't
    have a built in feature for atomic counters; nevertheless, it can be easily implemented
    using some of its cool offerings. In fact, with the help of previously described
    `findAndModify()` command, implementing is quite simple. Refer to the previous
    recipe *Atomic find and modify operations* to know what atomic find and modify
    operations are in Mongo.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 原子计数器是许多用例的必需品。Mongo没有原子计数器的内置功能；然而，可以使用一些其很酷的功能很容易地实现。事实上，借助先前描述的`findAndModify()`命令，实现起来非常简单。参考之前的配方*原子查找和修改操作*，了解Mongo中的原子查找和修改操作是什么。
- en: Getting ready
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备就绪
- en: Look at the recipe *Installing single node MongoDB* in [Chapter 1](ch01.html
    "Chapter 1. Installing and Starting the Server"), *Installing and Starting the
    Server* and start a single instance of Mongo. That is the only prerequisite for
    this recipe. Start a mongo shell and connect to the started server.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 查看[第1章](ch01.html "第1章。安装和启动服务器")中的配方*安装单节点MongoDB*，开始Mongo的单个实例。这是此配方的唯一先决条件。启动mongo
    shell并连接到已启动的服务器。
- en: How to do it…
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作…
- en: 'Execute the following piece of code from the mongo shell:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从mongo shell中执行以下代码：
- en: '[PRE6]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now from the shell invoke the following:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在从shell中调用以下命令：
- en: '[PRE7]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: How it works…
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理…
- en: The function is as simple as a `findAndModify` operation on a collection used
    to store all the counters. The counter identifier is the `_id` field of the document
    stored and the value of the counter is stored in the field `count`. The document
    passed to the `findAndModify` operations accepts the query, which uniquely identifies
    the document storing the current count—a query using the `_id` field. The update
    operation is an `$inc` operation that will increment the value of the `count`
    field by 1\. But what if the document doesn't exist? This will happen on the first
    invocation of the counter. To take care of this scenario, we will set the `upsert`
    flag to `true`. The value of `count` will always start with 1 and there is no
    way it would accept any user-defined start number for the sequence or a custom
    increment step. To address such requirements, we will have to specifically add
    a document with the initialized values to the counters collection. Finally, we
    are interested in the state of the counter after the value is incremented; hence,
    we set the value of the field `new` as `true`.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数就是在用于存储所有计数器的集合上执行的`findAndModify`操作。计数器标识符是存储的文档的`_id`字段，计数器的值存储在`count`字段中。传递给`findAndModify`操作的文档接受查询，该查询唯一标识存储当前计数的文档，即使用`_id`字段的查询。更新操作是一个`$inc`操作，将通过1递增`count`字段的值。但是如果文档不存在怎么办？这将发生在对计数器的第一次调用。为了处理这种情况，我们将将`upsert`标志设置为`true`。`count`的值将始终从1开始，没有办法接受任何用户定义的序列起始数字或自定义递增步长。为了满足这样的要求，我们将不得不将具有初始化值的文档添加到计数器集合中。最后，我们对计数器值递增后的状态感兴趣；因此，我们将`new`字段的值设置为`true`。
- en: 'On invoking this method thrice (as we did), we should see the following in
    the collection counters. Simply execute the following query:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在调用此方法三次（就像我们做的那样）后，我们应该在计数器集合中看到以下内容。只需执行以下查询：
- en: '[PRE8]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Using this small function, we now have implemented atomic counters in Mongo.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个小函数，我们现在已经在Mongo中实现了原子计数器。
- en: See also
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参阅
- en: We can store such common code on a Mongo server that would be available for
    execution in other functions. Look at the recipe *Implementing* *server-side scripts*
    to see how we can store JavaScript functions on the Mongo server. This allows
    us even to invoke this function from other programming language clients.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这样的通用代码存储在Mongo服务器上，以便在其他函数中执行。查看配方*实现服务器端脚本*，了解如何在Mongo服务器上存储JavaScript函数。这甚至允许我们从其他编程语言客户端调用此函数。
- en: Implementing server-side scripts
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现服务器端脚本
- en: In this recipe, we will see how to write server stored JavaScript similar to
    stored procedures in relational databases. This is a common use case where other
    pieces of code require access to these common functions and we have them in one
    central place. To demonstrate server-side scripts, the function will simply add
    two numbers.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们将看到如何编写服务器存储的JavaScript，类似于关系数据库中的存储过程。这是一个常见的用例，其他代码片段需要访问这些常见函数，我们将它们放在一个中心位置。为了演示服务器端脚本，该函数将简单地添加两个数字。
- en: There are two parts to this recipe. First, we see how to load the scripts from
    the collections on the client-side JavaScript shell and secondly, we will see
    how to execute these functions on the server.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这个配方有两个部分。首先，我们看看如何从客户端JavaScript shell中的集合加载脚本，其次，我们将看到如何在服务器上执行这些函数。
- en: Note
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: The documentation specifically mentions that it is not recommended to use server-side
    scripts. Security is one concern though if the data is not properly audited and
    hence we need to be careful with what functions are defined. Since Mongo 2.4,
    the server-side JavaScript engine is V8, which can execute multiple threads in
    parallel as opposed to the engine prior to version 2.4 of Mongo, which executes
    only one thread at a time.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 文档明确提到不建议使用服务器端脚本。安全性是一个问题，尽管如果数据没有得到适当的审计，因此我们需要小心定义哪些函数。自Mongo 2.4以来，服务器端JavaScript引擎是V8，可以并行执行多个线程，而不是Mongo
    2.4之前的引擎，每次只能执行一个线程。
- en: Getting ready
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Look at the recipe *Installing single node MongoDB* in [Chapter 1](ch01.html
    "Chapter 1. Installing and Starting the Server"), *Installing and Starting the
    Server* and start a single instance of Mongo. That is the only prerequisite for
    this recipe. Start a mongo shell and connect to the started server.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 查看[第1章](ch01.html "第1章。安装和启动服务器")中的配方*安装单节点MongoDB*，*安装和启动服务器*并启动Mongo的单个实例。这是这个配方的唯一先决条件。启动一个mongo
    shell并连接到已启动的服务器。
- en: How to do it…
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Create a new function called `add` and save it to the collection `db.system.js`
    as follows. The current database should be test:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为`add`的新函数，并将其保存到集合`db.system.js`中，如下所示。当前数据库应该是test：
- en: '[PRE9]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Now that this function is defined, load all the functions as follows:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在这个函数已经定义，加载所有函数如下：
- en: '[PRE10]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now, invoke `add` and see if it works:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，调用`add`并查看是否有效：
- en: '[PRE11]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We will now use this function and execute this on the server-side instead:
    Execute the following from the shell:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们将使用这个函数，并在服务器端执行它：从shell执行以下操作：
- en: '[PRE12]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Execute the following steps (you can execute the preceding command):'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行以下步骤（可以执行前面的命令）：
- en: '[PRE13]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: How it works…
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: The collection `system.js` is a special MongoDB collection used to store JavaScript
    code. We add a new server-side JavaScript using the `save` function in this collection.
    The `save` function is just a convenience function that inserts the document if
    it is not present or updates an existing one. The objective is to add a new document
    to this collection which you may add even using `insert` or `upsert`.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 集合`system.js`是一个特殊的MongoDB集合，用于存储JavaScript代码。我们使用该集合中的`save`函数添加一个新的服务器端JavaScript。`save`函数只是一个方便的函数，如果文档不存在则插入文档，如果文档已存在则更新文档。目标是向该集合添加一个新文档，即使您可以使用`insert`或`upsert`来添加。
- en: 'The secret lies in the method `loadServerScripts`. Let''s look at the code
    of this method: `this.system.js.find().forEach(function(u){eval(u._id + " = "
    + u.value);});`'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 秘密在于`loadServerScripts`方法。让我们看看这个方法的代码：`this.system.js.find().forEach(function(u){eval(u._id
    + " = " + u.value);});`
- en: It evaluates a JavaScript using the `eval` function and assigns the function
    defined in the `value` attribute of the document to a variable named with the
    name given in the `_id` field of the document for each document present in the
    collection `system.js`.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 它使用`eval`函数评估JavaScript，并为`system.js`集合中每个文档的`value`属性中定义的函数分配一个与文档的`_id`字段中给定的名称相同的变量。
- en: 'For example, if the following document is present in the collection `system.js`,
    `{ _id : ''add'', value : function(num1, num2) {return num1 + num2}}`, then the
    function given in the `value` field of the document will be assigned to the variable
    named as `add` in the current shell. The value `add` is given in the `_id` field
    of the document.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '例如，如果集合`system.js`中存在以下文档，`{ _id : ''add'', value : function(num1, num2) {return
    num1 + num2}}`，那么文档的`value`字段中给定的函数将分配给当前shell中名为`add`的变量。文档的`_id`字段中给定了值`add`。'
- en: These scripts do not really execute on the server but their definition is stored
    on the server in a collection. The JavaScript method `loadServerScripts`, just
    instantiates some variables in the current shell and make those functions available
    for invocation. It is the JavaScript interpreter of the shell that executes these
    functions and not the server. The collection `system.js` is defined in the scope
    of the database. Once loaded, these act as JavaScript functions defined in the
    shell and hence the functions are available throughout the scope of the shell
    irrespective of the database currently active.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这些脚本实际上并不在服务器上执行，但它们的定义存储在服务器的一个集合中。JavaScript方法`loadServerScripts`只是在当前shell中实例化一些变量，并使这些函数可用于调用。执行这些函数的是shell的JavaScript解释器，而不是服务器。集合`system.js`在数据库的范围内定义。一旦加载，这些函数就像在shell中定义的JavaScript函数一样，在shell的范围内都是可用的，而不管当前活动的数据库是什么。
- en: As far as security is concerned, if the shell is connected to the server with
    security enabled, then the user invoking `loadServerScripts` must have privileges
    to read the collections in the database. For more details on enabling security
    and various roles a user can have, refer to the recipe *Setting up users in Mongo*
    in [Chapter 4](ch04.html "Chapter 4. Administration"), *Administration*. As we
    saw earlier, the function `loadServerScripts` reads data from the collection `system.js`
    and if the user doesn't have privileges to read from the collection, the function
    invocation will fail. Apart from that, the functions executed from the shell after
    being loaded should have appropriate privileges. For instance, if a function inserts/updates
    in any collection, the user should have read and write privileges on that particular
    collection accessed from the function.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: Executing scripts on the server is perhaps what one would expect to be server-side
    script as opposed to executing in the shell connected. In this case, the functions
    are evaluated on the server's JavaScript engine and the security checks are more
    stringent as long running functions can hold locks, having detrimental effects
    on the performance. The wrapper to invoke the execution of a JavaScript code on
    the server-side is the `db.eval` function accepting the code to evaluate on the
    server-side along with the parameters if any.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: Before evaluating the function, the write operation takes a global lock; this
    can be skipped if the parameter `nolock` is used. For instance, the preceding
    `add` function can be invoked as follows instead of calling `db.eval` and achieving
    the same results. We additionally provided the `nolock` field to instruct the
    server not to acquire the global lock before evaluating the function. If this
    function were to perform write operations on a collection, then the `nolock` field
    is ignored.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'If security is enabled on the server, the invoking user needs to have the following
    four roles: `userAdminAnyDatabase`, `dbAdminAnyDatabase`, `readWriteAnyDatabase`,
    and `clusterAdmin` (on the admin database) to successfully invoke the `db.eval`
    function.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: Programming languages do provide a way for invocation of such server-side scripts
    using the `eval` function. For instance, in Java API, the class `com.mongodb.DB`
    has the method `eval` to invoke server-side JavaScript code. Such server-side
    executions are highly useful when we want to avoid unnecessary network traffic
    for the data and get the result to the clients. However, too much logic on the
    database server can quickly make things difficult to maintain and affect the performance
    of the server badly.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As of MongoDB 3.0.3, the `db.eval()` method is being deprecated and it is advised
    that users do not rely on this method but instead use client-side scripts. See
    [https://jira.mongodb.org/browse/SERVER-17453](https://jira.mongodb.org/browse/SERVER-17453)
    for more details.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: Creating and tailing a capped collection cursors in MongoDB
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Capped collections are fixed size collections where documents are added towards
    the end of the collection, similar to a queue. As capped collection have a fixed
    size, older documents are removed if the limit is reached.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: They are naturally sorted by the order of the insertion and any retrieval needed
    on them required ordered by time can be retrieved using the `$natural` sort order.
    This makes document retrieval very fast.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: The following figure gives a pictorial representation of a capped collection
    of a size which is good enough to hold up to three documents of equal size (which
    is too small for any practical use, but good for understanding). As we can see
    in the image, the collection is similar to a circular queue where the oldest document
    is replaced by the newly added document should the collection become full. The
    tailable cursors are special types of cursors that tail the collection similar
    to a tail command in Unix. These cursors iterate through the collection similar
    to a normal cursors do, but additionally wait for data to be available in the
    collection if it is not available. We will see capped collections and tailable
    cursors in detail in this recipe.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 下图给出了一个有限大小的集合的图形表示，足以容纳最多三个相等大小的文档（对于任何实际用途来说都太小，但用于理解是很好的）。正如我们在图像中所看到的，该集合类似于循环队列，其中最旧的文档将被新添加的文档替换，如果集合变满。可追加的游标是特殊类型的游标，类似于Unix中的tail命令，它们遍历集合，类似于普通游标，但同时等待集合中的数据是否可用。我们将在本节详细介绍有限集合和可追加游标。
- en: '![Creating and tailing a capped collection cursors in MongoDB](img/B04831_05_01.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![在MongoDB中创建和追加有限集合游标](img/B04831_05_01.jpg)'
- en: Getting ready
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Look at the recipe *Installing single node MongoDB* recipe in [Chapter 1](ch01.html
    "Chapter 1. Installing and Starting the Server"), *Installing and Starting the
    Server* and start a single instance of Mongo. That is the only prerequisite for
    this recipe. Start a MongoDB shell and connect to the started server.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 查看[第1章](ch01.html "第1章。安装和启动服务器")中的配方*安装单节点MongoDB*，*安装和启动服务器*并启动Mongo的单个实例。这是本配方的唯一先决条件。启动MongoDB
    shell并连接到已启动的服务器。
- en: How to do it…
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 操作步骤...
- en: 'There are two parts to this recipe: in the first part, we will create a capped
    collection called `testCapped` and try performing some basic operations on it.
    Next, we will be creating a tailable cursor on this capped collection.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这个配方有两个部分：在第一部分中，我们将创建一个名为`testCapped`的有限集合，并尝试对其执行一些基本操作。接下来，我们将在这个有限集合上创建一个可追加游标。
- en: Drop the collection if one already exists with this name.
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果已存在具有此名称的集合，请删除该集合。
- en: '[PRE15]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now create a capped collection as follows. Note the size given here is the
    size in bytes allocated for the collection and not the number of documents it
    contains:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在按以下方式创建一个有限集合。请注意，此处给定的大小是为集合分配的字节数，而不是它包含的文档数量：
- en: '[PRE16]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We will now insert 100 documents in the capped collection as follows:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们将按以下方式在有限集合中插入100个文档：
- en: '[PRE17]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now query the collection as follows:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在按以下方式查询集合：
- en: '[PRE18]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Try to remove the data from the collection as follows:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试按以下方式从集合中删除数据：
- en: '[PRE19]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: We will now create and demonstrate a tailable cursor. It is recommended that
    you type/copy the following pieces of code into a text editor and keep it handy
    for execution.
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们将创建并演示一个可追加游标。建议您将以下代码片段输入/复制到文本编辑器中，并随时准备执行。
- en: 'To insert data in a collection, we will be using the following fragment of
    code. Execute this piece of code in the shell:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要在集合中插入数据，我们将使用以下代码片段。在shell中执行此代码片段：
- en: '[PRE20]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'To tail a capped collection, we use the following piece of code:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要追加有限集合，我们使用以下代码片段：
- en: '[PRE21]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Open a shell and connect to the running mongod process. This will be the second
    shell opened and connected to the server. Copy and paste the code mentioned in
    step 8 in this shell and execute it.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个shell并连接到正在运行的mongod进程。这将是第二个打开并连接到服务器的shell。在此shell中复制并粘贴第8步中提到的代码，然后执行它。
- en: Observe how the records inserted are shown as they are inserted into the capped
    collection.
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 观察插入的记录如何显示为它们插入到有限集合中。
- en: How it works…
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理...
- en: We will create a capped collection explicitly using the `createCollection` function.
    This is the only way a capped collection is created. There are two parameters
    to the `createCollection` function. The first one is the name of the collection
    and the second is a JSON document that contains the two fields, `capped` and `size`,
    which are used to inform the user that the collection is capped or not and the
    size of the collection in bytes respectively. An additional field `max` can be
    provided to specify the maximum number of documents in the collection. The field
    size is required even if the `max` field is specified. We then insert and query
    the documents. When we try to remove the documents from the collection, we would
    see an error that removal is not permitted from the capped collection. It allows
    the documents to be deleted only when new documents are added and there isn't
    space available to accommodate them.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`createCollection`函数显式创建一个有限集合。这是创建有限集合的唯一方法。`createCollection`函数有两个参数。第一个是集合的名称，第二个是一个包含两个字段`capped`和`size`的JSON文档，用于通知用户集合是否被限制以及集合的大小（以字节为单位）。还可以提供一个额外的`max`字段来指定集合中的最大文档数。即使指定了`max`字段，也需要`size`字段。然后我们插入和查询文档。当我们尝试从集合中删除文档时，我们会看到一个错误，即不允许从有限集合中删除文档。它只允许在添加新文档并且没有空间可容纳它们时才能删除文档。
- en: What we see next is a tailable cursor we created. We start two shells and one
    of them is a normal insertion of documents with an interval of 1 second between
    subsequent insertions. In the second shell, we create a cursor and iterate through
    it and print the documents that we get from the cursor onto the shell. The additional
    options we added to the cursor make the difference though. There are two options
    added, `DBQuery.Option.tailable` and `DBQuery.Option.awaitData`. These options
    are for instructing that the cursor is tailable, rather than normal, where the
    last position is marked and we can resume where we left off, and secondly to wait
    for more data for some time rather than returning immediately when no data is
    available and when we reach towards the end of the cursor, respectively. The `awaitData`
    option can be used with tailable cursors only. The combination of these two options
    gives us a feel similar to the tail command in Unix filesystem.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们看到的是我们创建的可追溯游标。我们启动两个shell，其中一个是以1秒的间隔插入文档的普通插入。在第二个shell中，我们创建一个游标并遍历它，并将从游标获取的文档打印到shell上。然而，我们添加到游标的附加选项使得有所不同。添加了两个选项，`DBQuery.Option.tailable`和`DBQuery.Option.awaitData`。这些选项用于指示游标是可追溯的，而不是正常的，其中最后的位置被标记，我们可以恢复到上次离开的位置，其次是在没有数据可用时等待更多数据一段时间，以及当我们接近游标的末尾时立即返回而不是返回。`awaitData`选项只能用于可追溯游标。这两个选项的组合使我们感觉类似于Unix文件系统中的tail命令。
- en: 'For a list of available options, visit the following page: [http://docs.mongodb.org/manual/reference/method/cursor.addOption/](http://docs.mongodb.org/manual/reference/method/cursor.addOption/).'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 有关可用选项的列表，请访问以下页面：[http://docs.mongodb.org/manual/reference/method/cursor.addOption/](http://docs.mongodb.org/manual/reference/method/cursor.addOption/)。
- en: There's more…
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多…
- en: In the next recipe, we will see how to convert a normal collection to a capped
    collection.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个配方中，我们将看到如何将普通集合转换为固定集合。
- en: Converting a normal collection to a capped collection
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将普通集合转换为固定集合
- en: This recipe will demonstrate the process of converting a normal collection to
    a capped collection.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 本配方将演示将普通集合转换为固定集合的过程。
- en: Getting ready
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备就绪
- en: Look at the recipe *Installing single node MongoDB* in [Chapter 1](ch01.html
    "Chapter 1. Installing and Starting the Server"), *Installing and Starting the
    Server* and start a single instance of Mongo. That is the only prerequisite for
    this recipe. Start a mongo shell and connect to the started server.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 查看[第1章](ch01.html "第1章。安装和启动服务器")中的*安装单节点MongoDB*和*安装和启动服务器*的配方，并启动Mongo的单个实例。这是本配方的唯一先决条件。启动mongo
    shell并连接到已启动的服务器。
- en: How to do it…
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做…
- en: 'Execute the following to ensure you are in the `test` database:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行以下操作以确保您在`test`数据库中：
- en: '[PRE22]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Create a normal collection as follows. We will be adding 100 documents to it,
    type/copy the following code snippet on to the mongo shell and execute it. The
    command is as follows:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照以下方式创建一个普通集合。我们将向其添加100个文档，将以下代码片段输入/复制到mongo shell上并执行。命令如下：
- en: '[PRE23]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Query the collection as follows to confirm it contains the data:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照以下方式查询集合以确认其中包含数据：
- en: '[PRE24]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Now, query the collection `system.namespaces` as follows and note the result
    document:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，按照以下方式查询集合`system.namespaces`，并注意结果文档：
- en: '[PRE25]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Execute the following command to convert the collection to capped collection:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行以下命令将集合转换为固定集合：
- en: '[PRE26]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Query the collection to take a look at the data:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查询集合以查看数据：
- en: '[PRE27]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Query the collection `system.namespaces` as follows and note the result document:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照以下方式查询集合`system.namespaces`，并注意结果文档：
- en: '[PRE28]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: How it works…
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: 'We created a normal collection with 100 documents and then tried to convert
    it to a capped collection with 100 bytes size. The command has the following JSON
    document passed to the `runCommand` function, `{convertToCapped : <name of normal
    collection>, size: <size in bytes of the capped collection>}`. This command creates
    a capped collection with the mentioned size and loads the documents in natural
    ordering from the normal collection to the target capped collection. If the size
    of the capped collection reaches the limit mentioned, the old documents are removed
    in the FIFO order making space for new documents. Once this is done, the created
    capped collection is renamed. Executing a find on the capped collection confirms
    that not all 100 documents originally present in the normal collection are present
    in the capped collection. A query on the `system.namespaces` collection before
    and after the execution of the `convertToCapped` command shows the change in the
    `collection` attributes. Note that, this operation acquires a global write lock
    blocking all read and write operations in this database. Also, any indexes present
    on the original collection are not created for the capped collection, upon conversion.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '我们创建了一个包含100个文档的普通集合，然后尝试将其转换为具有100字节大小的固定集合。命令将以下JSON文档传递给`runCommand`函数，`{convertToCapped:
    <普通集合的名称>, size: <固定集合的字节大小>}`。此命令创建一个具有指定大小的固定集合，并将文档以自然顺序从普通集合加载到目标固定集合中。如果固定集合的大小达到所述限制，旧文档将以FIFO顺序删除，为新文档腾出空间。完成后，创建的固定集合将被重命名。在固定集合上执行查找确认，最初在普通集合中存在的100个文档并不都存在于固定集合中。在执行`convertToCapped`命令之前和之后对`system.namespaces`集合进行查询，显示了`collection`属性的变化。请注意，此操作获取全局写锁，阻止此数据库中的所有读取和写入操作。此外，对于转换后的固定集合，不会创建原始集合上存在的任何索引。'
- en: There's more…
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多…
- en: Oplog is an important collection used for replication in MongoDB and is a capped
    collection. For more information on replication and oplogs, refer to the recipe
    *Understanding and analyzing oplogs* in [Chapter 4](ch04.html "Chapter 4. Administration"),
    *Administration*. In a recipe later in this chapter, we will use this oplog to
    implement a feature similar to after insert/update/delete trigger of a relational
    database.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: Storing binary data in Mongo
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we saw how to store text values, dates, and numbers fields in a document.
    Binary content also needs to be stored at times in the database. Consider cases
    where users would need to store files in a database. In relational databases,
    the BLOB data type is most commonly used to address this requirement. MongoDB
    also supports binary contents to be stored in a document in the collection. The
    catch is that the total size of the document shouldn't exceed 16 MB, which is
    the upper limit of the document size as of the writing this book. In this recipe,
    we will store a small image file into Mongo's document and also retrieve it later.
    If the content you wish to store in MongoDB collections is greater than 16 MB,
    then MongoDB offers an out of the box solution called **GridFS**. We will see
    how to use GridFS in another recipe later in this chapter.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Look at the recipe *Installing single node MongoDB* in [Chapter 1](ch01.html
    "Chapter 1. Installing and Starting the Server"), *Installing and Starting the
    Server* and start a single instance of MongoDB. Also, the program to write binary
    content to the document is written in Java. Refer to the recipes *Executing query
    and insert operations using a Java client*, *Implementing aggregation in Mongo
    using a Java client* and *Executing MapReduce in Mongo using a Java client* in
    [Chapter 3](ch03.html "Chapter 3. Programming Language Drivers"), *Programming
    Language Drivers*, for more details on Java drivers. Open a mongo shell and connect
    to the local MongoDB instance listening to port `27017`. For this recipe, we will
    be using the project `mongo-cookbook-bindata`. This project is available in the
    source code bundle downloadable from Packt site. The folder needs to be extracted
    on the local filesystem. Open a command line shell and go to the root of the project
    extracted. It should be the directory where the file `pom.xml` is found.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'On the operating system shell with the `pom.xml` present in the current directory
    of the `mongo-cookbook-bindata` project, execute the following command:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Observe the output; the execution should be successful.
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Switch to mongo shell that is connected to the local instance and execute the
    following query:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Scroll through the document and take a note of the fields in the document.
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works…
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If we scroll through the large document printed out, we see that the fields
    are `fileName`, `size`, and `data`. The first two fields are of type string and
    number respectively, which we populated on document creation and hold the name
    of the file we provide and the size in bytes. The data field is a field of BSON
    type BinData, where we see the data encoded in Base64 format.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: 'The following lines of code show how we populated the DBObject that we added
    to the collection:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: As we see above, two fields `fileName` and `size` are used to store the name
    of the file and the size of the file and are of type string and number respectively.
    The field data is added to the `DBObject` as a byte array, it gets stored automatically
    as the BSON type BinData in the document.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: See also
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: What we saw in this recipe is straightforward as long as the document size is
    less than 16 MB. If the size of the files stored exceeds this value, we have to
    resort to solutions like GridFS, which is explained in next recipe *Storing large
    data in Mongo using GridFS*.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: Storing large data in Mongo using GridFS
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A document size in MongoDB can be up to 16 MB. But does that mean we cannot
    store data more than 16 MB in size? There are cases where you prefer to store
    videos and audio files in database rather than in a filesystem for a number of
    advantages such as a few of them are storing metadata along with them, when accessing
    the file from an intermediate location, and replicating the contents for high
    availability if replication is enabled on the MongoDB server instances. GridFS
    can be used to address such use cases in MongoDB. We will also see how GridFS
    manages large content that exceeds 16 MB and analyzes the collections it uses
    for storing the content behind the scene. For test purpose, we will not use data
    exceeding 16 MB but something smaller to see GridFS in action.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: MongoDB中的文档大小可以达到16 MB。 但这是否意味着我们不能存储超过16 MB大小的数据？ 有些情况下，您更喜欢将视频和音频文件存储在数据库中，而不是在文件系统中，因为有许多优势，比如存储与它们一起的元数据，从中间位置访问文件时，以及在MongoDB服务器实例上启用复制时为了高可用性而复制内容。
    GridFS可以用来解决MongoDB中的这些用例。 我们还将看到GridFS如何管理超过16 MB的大容量，并分析其用于在幕后存储内容的集合。 为了测试目的，我们不会使用超过16
    MB的数据，而是使用一些更小的数据来查看GridFS的运行情况。
- en: Getting ready
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Look at the recipe *Installing single node MongoDB* in [Chapter 1](ch01.html
    "Chapter 1. Installing and Starting the Server"), *Installing and Starting the
    Server* and start a single instance of Mongo. That is the only prerequisite for
    this recipe. Start a Mongo shell and connect to the started server. Additionally,
    we will use the mongofiles utility to store data in GridFS from command line.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 查看[第1章](ch01.html "第1章。安装和启动服务器")中的配方*安装单节点MongoDB*，*安装和启动服务器*并启动Mongo的单个实例。
    这是此配方的唯一先决条件。 启动Mongo shell并连接到已启动的服务器。 另外，我们将使用mongofiles实用程序从命令行将数据存储在GridFS中。
- en: How to do it…
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: Download the code bundle of the book and save the image file `glimpse_of_universe-wide.jpg`
    to your local drive (you may choose any other large file as the matter of fact
    and provide appropriate names of the file with the commands we execute). For the
    sake of the example, the image is saved in the home directory. We will split our
    steps into three parts.
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载该书的代码包，并将图像文件`glimpse_of_universe-wide.jpg`保存到本地驱动器（您可以选择任何其他大文件作为事实，并使用我们执行的命令提供适当的文件名）。
    为了举例，图像保存在主目录中。 我们将把我们的步骤分为三个部分。
- en: With the server up and running, execute the following command from the operating
    system's shell with the current directory being the home directory. There are
    two arguments here. The first one is the name of the file on the local filesystem
    and the second one is the name that would be attached to the uploaded content
    in MongoDB.
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在服务器运行并且当前目录为主目录的情况下，从操作系统的shell中执行以下命令。 这里有两个参数。 第一个是本地文件系统上文件的名称，第二个是将附加到MongoDB中上传内容的名称。
- en: '[PRE32]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Let's now query the collections to see how this content is actually stored in
    the collections behind the scenes. With the shell open, execute the following
    two queries. Make sure that in the second query, you ensure to mention not selecting
    the data field.
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们查询集合，看看这些内容实际上是如何在幕后的集合中存储的。 打开shell，执行以下两个查询。 确保在第二个查询中，您确保不选择数据字段。
- en: '[PRE33]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Now that we have put a file to GridFS from the operating system''s local filesystem,
    we will see how we can get the file to the local filesystem. Execute the following
    from the operating system shell:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经从操作系统的本地文件系统中将文件放入了GridFS，我们将看到如何将文件获取到本地文件系统。 从操作系统shell中执行以下操作：
- en: '[PRE34]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Finally, we will delete the file we uploaded as follows. From the operating
    system shell, execute the following:'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将删除我们上传的文件。 从操作系统shell中，执行以下操作：
- en: '[PRE35]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Confirm the deletion using the following queries again:'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次使用以下查询确认删除：
- en: '[PRE36]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: How it works…
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理...
- en: Mongo distribution comes with a tool called mongofiles, which lets us upload
    the large content to Mongo server that gets stored using the GridFS specification.
    GridFS is not a different product but a specification that is standard and followed
    by different drivers for MongoDB for storing data greater than 16 MB, which is
    the maximum document size. It can even be used for files less than 16 MB, as we
    did in our recipe, but there isn't really a good reason to do that. There is nothing
    stopping us from implementing our own way of storing these large files, but it
    is preferred to follow the standard. This is because all drivers support it and
    does the heavy lifting of splitting of big file into small chunks and assembling
    them back when needed.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: Mongo分发包带有一个名为mongofiles的工具，它允许我们将大容量上传到Mongo服务器，该服务器使用GridFS规范进行存储。 GridFS不是一个不同的产品，而是一个标准规范，由不同的MongoDB驱动程序遵循，用于存储大于16
    MB的数据，这是最大文档大小。 它甚至可以用于小于16 MB的文件，就像我们在我们的示例中所做的那样，但实际上没有一个很好的理由这样做。 没有什么能阻止我们实现自己的存储这些大文件的方式，但最好遵循标准。
    这是因为所有驱动程序都支持它，并且在需要时进行大文件的分割和组装。
- en: We kept the image downloaded from the Packt Publishing site and uploaded using
    mongofiles to MongoDB. The command to do that is `put` and the `-l` option gives
    the name of the file on the local drive that we want to upload. Finally, the name
    `universe.jpg` is the name of the file we want it to be stored as on GridFS.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从Packt Publishing网站下载的图像，并使用mongofiles上传到MongoDB。 执行此操作的命令是`put`，`-l`选项给出了我们要上传的本地驱动器上的文件的名称。
    最后，名称`universe.jpg`是我们希望它在GridFS上存储的文件的名称。
- en: 'On successful execution, we should see something like the following on the
    console:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 成功执行后，我们应该在控制台上看到以下内容：
- en: '[PRE37]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: This gives us some details of the upload, the unique `_id` for the uploaded
    file, the name of the file, the chunk size, which is the size of the chunk this
    big file is broken into (by default 256 KB), the date of upload, the checksum
    of the uploaded content, and the total length of upload. This checksum can be
    computed beforehand and then compared after the upload to check if the uploaded
    content was not corrupt.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: 'Execute the following query from the mongo shell in test database:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: We see that the output we saw for the `put` command of mongofiles same as the
    document queried above in the collection `fs.files`. This is the collection where
    all the uploaded file details are put when some data is added to GridFS. There
    will be one document per upload. Applications can later also modify this document
    to add their own custom meta data along with the standard details added to my
    Mongo when adding the data. Applications can very well use this collection to
    add details like, the photographer, the location where the image was taken, where
    was it taken, and details like tags for individuals in the image in this collection
    if the document is for an image upload.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: 'The file content is something that contains this data. Let''s execute the following
    query:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'We have deliberately left out the data field from the result selected. Let''s
    look at the structure of the result document:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: For the file we uploaded, we have 11 chunks of a maximum 256 KB each. When a
    file is being requested, the `fs.chunks` collection is searched by the `file_id`
    that comes from the `_id` field of `fs.files` collection and the field `n`, which
    is the chunk's sequence. A unique index is created on these two fields when this
    collection is created for the first time when a file is uploaded using GridFS
    for the fast retrieval of chunks using the file ID sorted by chunk sequence number.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: Similar to `put`, the `get` option is used to retrieve the files from the GridFS
    and put them on local filesystem. The difference in the command is to use the
    `get` instead of `put`, the `-l` still is used to provide the name of the file
    that this file would be saved as on the local filesystem and the final command
    line parameter is the name of the file as stored in GridFS. This is the value
    of the `filename` field in `fs.files` collection. Finally, the `delete` command
    of mongofiles simply removes the entry of the file from `fs.files` and `fs.chunks`
    collections. The name of the file given for delete is again the value present
    in the `filename` field of the `fs.files` collection.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: Some important use cases of using GridFS are when there is some user generated
    contents like large reports on some static data that doesn't change too often
    and are expensive to generate frequently. Instead of running them all the times,
    it can be run once and stored until a change in the static data is detected; in
    which case, the stored report is deleted and re-executed on next request of the
    data. The filesystem may not always be available to the application to write the
    files to, in which case this is a good alternative. There are cases where one
    might be interested in some intermediate chunk of the data stored, in which case
    the chunk containing the required data be accessed. You get some nice features
    like the MD5 content of the data, which is stored automatically and is available
    for use by the application.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have seen what GridFS is, let's see some scenarios where using GridFS
    might not be a very good idea. The performance of accessing the content from MongoDB
    using GridFS and directly from the filesystem will not be same. Direct filesystem
    access will be faster than GridFS and **Proof of Concept** (**POC**) for the system
    to be developed is recommended to measure the performance hit and see if it is
    within the acceptable limits; if so, the trade off in performance might be worth
    for the benefits we get. Also, if your application server is fronted with CDN,
    you might not actually need a lot of IO for static data stored in GridFS. Since
    GridFS stores the data in multiple documents in collections, atomically updating
    them is not possible. If we know the content is less than 16 MB, which is the
    case in lot of user-generated content, or some small files uploaded, we may skip
    GridFS altogether and store the content in one document as BSON supports storing
    binary content in the document. Refer to the previous recipe *Storing binary data
    in Mongo* for more details.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: We would rarely use mongofiles utility to store, retrieve, and delete data from
    GridFS. Though it may occasionally be used, we will mostly perform these operations
    from an application. In the next couple of recipes, we will see how to connect
    to GridFS to store, retrieve, and delete files using Java and Python clients.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  id: totrans-199
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Though this is not much to do with Mongo, Openstack is an **Infrastructure
    as a Service** (**IaaS**) platform and offers a variety of services for Compute,
    Storage, Networking, and so on. One of the image storage service called **Glance**
    supports a lot of persistent stores to store the images. One of the supported
    stores by Glace is MongoDB''s GridFS. You can find more information on how to
    configure Glance to use GridFS at the following URL: [http://docs.openstack.org/trunk/config-reference/content/ch_configuring-openstack-image-service.html](http://docs.openstack.org/trunk/config-reference/content/ch_configuring-openstack-image-service.html).'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: See also
  id: totrans-201
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can refer to the following recipes:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: '*Storing data to GridFS from Java client*'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Storing data to GridFS from Python client*'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Storing data to GridFS from Java client
  id: totrans-205
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous recipe, we saw how to store data to GridFS using a command-line
    utility that comes with MongoDB to manage large data files: mongofiles. To get
    an idea of what GridFS is and what collections are used behind the scenes to store
    the data, refer to the previous recipe *Storing large data in Mongo using GridFS*.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will look at storing data to GridFS using a Java client.
    The program will be a highly scaled down version of mongofiles utility and focus
    only on how to store, retrieve, and delete data rather than trying to provide
    a lot of options like mongofiles do.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Refer to the recipe *Installing single node MongoDB* from [Chapter 1](ch01.html
    "Chapter 1. Installing and Starting the Server"), *Installing and Starting the
    Server*, for all the necessary setup for this recipe. If you are interested in
    more details on Java drivers, refer to the recipes *Implementing aggregation in
    Mongo using a Java client* and *Executing MapReduce in Mongo using a Java client*
    in [Chapter 3](ch03.html "Chapter 3. Programming Language Drivers"), *Programming
    Language Drivers*. Open a mongo shell and connect to the local mongod instance
    listening to port `27017`. For this recipe, we will be using the project `mongo-cookbook-gridfs`.
    This project is available in the source code bundle downloadable from Packt site.
    The folder needs to be extracted on the local filesystem. Open a terminal of your
    operating system and go to the root of the project extracted. It should be the
    directory where the file `pom.xml` is found. Also, save the file `glimpse_of_universe-wide.jpg`
    on the local filesystem, similar to the previous recipe, found in the downloadable
    bundle for the book from the Packt site.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  id: totrans-210
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We are assuming that the collections of GridFS are clean and no prior data is
    uploaded. If there is nothing crucial in the database, you can execute the following
    to clear the collection. Do exercise caution before dropping the collections.
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Open an operating system shell and execute the following:'
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: The file I need to upload was placed in the home directory. You can choose to
    give the file path of the image file after the `put` command. Bear in mind if
    the path contains spaces, the whole path need to be given within single quotes.
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If the preceding command runs successfully, we should expect the following
    output to the command line:'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Once the preceding execution is successful, which we can confirm from the console
    output, execute the following from the mongo shell:'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Now, we will get the file from GridFS to local filesystem, execute the following
    to perform this operation:'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Confirm the file is present on the local filesystem at the mentioned location.
    We should see the following printed to the console output to indicate a successful
    write operation:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Finally, we will delete the file from GridFS:'
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'On successful deletion, we should see the following output in the console:'
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: How it works…
  id: totrans-228
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The class `com.packtpub.mongo.cookbook.GridFSTests` accepts three types of
    operations: `put` to upload file to GridFS, `get` to get contents from GridFS
    to local filesystem, and `delete` to delete files from GridFS.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: The class accepts up to three parameters, the first one is the operation with
    valid values as `get`, `put`, and `delete`. The second parameter is relevant for
    `get` and `put` operations and is the name of the file on local filesystem to
    write the downloaded content to be written or source the content from for upload
    respectively. The third parameter is the name of the file in GridFS, which is
    not necessarily same as the name on local filesystem. For `delete`, however, only
    the filename on GridFS is needed which would be deleted.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: Let's see some important snippets of code from the class which is specific to
    GridFS.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: Open the class `com.packtpub.mongo.cookbook.GridFSTests` in your favorite IDE
    and look for the methods `handlePut` , `handleGet`, and `handleDelete`. These
    are the methods where all the logic is. We will start with the `handlePut` method
    first, which is for uploading the contents of the file from local filesystem to
    GridFS.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: 'Irrespective of the operation we perform, we will create an instance of the
    class `com.mongodb.gridfs.GridFS`. In our case, we instantiated it as follows:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: The constructor of this class takes the database instance of class `com.mongodb.DB`.
    Once the instance of GridFS is created, we will invoke the method `createFile`
    on it. This method accepts two parameters, the first one is the `InputStream`
    sourcing the bytes of the content to be uploaded and the second parameter is the
    name of the file on GridFS for the file that would be saved on GridFS. However,
    this method doesn't create the file on GridFS but returns and instance of `com.mongodb.gridfs.GridFSInputFile`.
    The upload will happen only when we call `save` method in this returned object.
    There are few overloaded variants of this `createFile` method. Please refer to
    Javadocs of the class `com.mongodb.gridfs.GridFS` for more details.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: Our next method is `handleGet`, which gets the contents of the file saved on
    GridFS to the local filesystem. Similar to the `com.mongodb.DBCollection` class,
    the class `com.mongodb.gridfs.GridFS` has the `find` and `findOne` methods for
    searching. However, instead of accepting any DBObject query, `find` and `findOne`
    in GridFS accept filename or the ObjectID value of the document to search in `fs.files`
    collection. Similarly, the return value is not a DBCursor but an instance of `com.mongodb.gridfs.GridFSDBFile`.
    This class has various methods that get the `InputStream` of the bytes of content
    present in the file on GridFS, `writeTo` file or `OutputStream` and a method,
    `getLength` that gives the number of bytes in the file. Refer to the Javadocs
    of the class `com.mongodb.gridfs.GridFSDBFile` for details.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we look at the method `handleDelete`, which is used to delete the
    files on GridFS and is the simplest of the lot. The method on the object of GridFS
    is `remove`, which accepts a string argument: the name of the file to delete on
    the server. The `return` type of this method is `void`. So irrespective of whether
    the content is present on GridFS or not, the method will not return a value nor
    throw an exception if a name is provided to this method for a file that doesn''t
    exist.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: See also
  id: totrans-238
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can refer to the following recipes:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: '*Storing binary data in Mongo*'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Storing data to GridFS from Python client*'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Storing data to GridFS from Python client
  id: totrans-242
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the recipe *Storing large data in Mongo using GridFS*, we saw what GridFS
    is and how it could be used to store the large files in MongoDB. In the previous
    recipe, we saw to use GridFS API from a Java client. In this recipe, we will see
    how to store image data into MongoDB using GridFS from a Python program.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-244
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Refer to the recipe *Connecting to the single node using a Java client* from
    [Chapter 1](ch01.html "Chapter 1. Installing and Starting the Server"), *Installing
    and Starting the Server*, for all the necessary setup for this recipe. If you
    are interested in more detail on Python drivers refer to the following recipes:
    *Executing query and insert operations with PyMongo* and *Executing update and
    delete operations using PyMongo* in [Chapter 3](ch03.html "Chapter 3. Programming
    Language Drivers"), *Programming Language Drivers*. Download and save the image
    `glimpse_of_universe-wide.jpg` from the downloadable bundle available with the
    book from the Packt site to local filesystem as we did in the previous recipe.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  id: totrans-246
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Open a Python interpreter by typing in the following in the operating system
    shell. Note that the current directory is same as the directory where the image
    file `glimpse_of_universe-wide.jpg` is placed:'
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Import the required packages as follows:'
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Once the Python shell is opened, create a `MongoClient` and a database object
    to the test database as follows:'
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'To clear the GridFS-related collections execute the following:'
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Create the instance of GridFS as follows:'
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Now, we will read the file and upload its contents to GridFS. First, create
    the file object as follows:'
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: Now put the file into GridFS as follows
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: On successfully executing `put`, we should see the ObjectID for the file uploaded.
    This would be same as the `_id` field of the `fs.files` collection for this file.
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Execute the following query from the Python shell. It should print out the `dict`
    object with the details of the upload. Verify the contents
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Now, we will get the uploaded content and write it to a file on the local filesystem.
    Let''s get the `GridOut` instance representing the object to read the data out
    of GridFS as follows:'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'With this instance available, let''s write the data to the file to a file on
    local filesystem as follows. First, open a handle to the file on local filesystem
    to write to as follows:'
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'We will then write content to it as follows:'
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: Now verify the file on the current directory on the local filesystem. A new
    file called `universe.jpg` will be created with same number of bytes as the source
    present in it. Verify it by opening it in an image viewer.
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works…
  id: totrans-271
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's look at the steps we executed. In the Python shell, we import two packages,
    `pymongo` and `gridfs`, and instantiate the `pymongo.MongoClient` and `gridfs.GridFS`
    instances. The constructor of the class `gridfs.GridFS` takes on an argument,
    which is the instance of `pymongo.Database`.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: We open a file in binary mode using the `open` function and pass the file object
    to the GridFS `put` method. There is an additional argument called `filename`
    passed, which would be the name of the file put into GridFS. The first parameter
    need not be a file object but any object with a `read` method defined.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: Once the `put` operation succeeds, the `return` value is an ObjectID for the
    uploaded document in `fs.files` collection. A query on `fs.files` can confirm
    that the file is uploaded. Verify that the size of the data uploaded matches the
    size of the file.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: Our next objective is to get the file from GridFS on to the local filesystem.
    Intuitively, one would imagine if the method to put a file in GridFS is `put`,
    then the method to get a file would be `get`. True, the method is indeed `get`,
    however, it will get only based on the `ObjectId` that was returned by the `put`
    method. So, if you are okay to fetch by `ObjectId`, `get` is the method for you.
    However, if you want to get by the filename, the method to use is `get_last_version`.
    It accepts the name of the filename that we uploaded and the return type of this
    method is of type `gridfs.gridfs_file.GridOut`. This class contains the method
    `read`, which will read out all the bytes from the uploaded file to GridFS. We
    open a file called `universe.jpg` for writing in binary mode and write all the
    bytes read from the `GridOut` object.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: See also
  id: totrans-276
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can refer to the following recipes:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: '*Storing binary data in Mongo*'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Storing data to GridFS from Java client*'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing triggers in Mongo using oplog
  id: totrans-280
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In a relational database, a trigger is a code that gets invoked when an `insert`,
    `update`, or a `delete` operation is executed on a table in the database. A trigger
    can be invoked either before or after the operation. Triggers are not implemented
    in MongoDB out of the box and in case you need some sort of notification for your
    application whenever any `insert`/`update`/`delete` operations are executed, you
    are left to manage that by yourself in the application. One approach is to have
    some sort of data access layer in the application, which is the only place to
    query, insert, update, or delete documents from the collections. However, there
    are few challenges to it. First, you need to explicitly code the logic to accommodate
    this requirement in the application, which may or may not be feasible. If the
    database is shared and multiple applications access it, things become even more
    difficult. Secondly, the access needs to be strictly regulated and no other source
    of insert/update/delete be permitted.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, we need to look at running some sort of logic in a layer close
    to the database. One way to track all write operations is by using an oplog. Note
    that read operations cannot be tracked using oplogs. In this recipe, we will write
    a small Java application that would tail an oplog and get all the `insert`, `update`
    and `delete` operations happening on a Mongo instance. Note that this program
    is implemented in Java and works equally well in any other programming language.
    The crux lies in the logic for the implementation, the platform for implementation
    can be any. Also, this works only if the mongod instance is started as a part
    of replica set and not a standalone instance. Also, this trigger like functionality
    can only be invoked only after the operation is performed and not before the data
    gets inserted/updated or deleted from the collection.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-283
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Refer to the recipe *Starting multiple instances as part of a replica set* from
    [Chapter 1](ch01.html "Chapter 1. Installing and Starting the Server"), *Installing
    and Starting the Server*, for all the necessary setup for this recipe. If you
    are interested in more details on Java drivers, refer to the following recipes
    *Executing query and insert operations using a Java client* and *Executing update
    and delete operations using a Java client* in [Chapter 3](ch03.html "Chapter 3. Programming
    Language Drivers"), *Programming Language Drivers*. Prerequisites of these two
    recipes are all we need for this recipe.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: Refer to the recipe *Creating and tailing a capped collection cursors in MongoDB*
    in this chapter to know more about capped collections and tailable cursors if
    you are not aware or need a refresher. Finally, though not mandatory, [Chapter
    4](ch04.html "Chapter 4. Administration"), *Administration*, explains oplog in
    depth in the recipe *Understanding and analyzing oplogs*. This recipe will not
    explain oplog in depth as we did in [Chapter 4](ch04.html "Chapter 4. Administration"),
    *Administration*. Open a shell and connect it to the primary of the replica set.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: For this recipe, we will be using the project `mongo-cookbook-oplogtrigger`.
    This project is available in the source code bundle downloadable from Packt site.
    The folder needs to be extracted on the local filesystem. Open a command line
    shell and go to the root of the project extracted. It should be the directory
    where the file `pom.xml` is found. Also, the `TriggerOperations.js` file would
    be needed to trigger operations in the database that we intend to capture.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  id: totrans-287
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Open an operating system shell and execute the following:'
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'With the Java program started, we will open the shell as follows with the file
    `TriggerOperations.js` present in the current directory and the mongod instance
    listening to port `27000` as the primary:'
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Once the shell is connected, execute the following function we loaded from
    the JavaScript:'
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: Observe the output printed out on the console where the Java program `com.packtpub.mongo.cookbook.OplogTrigger`
    is being executed using Maven.
  id: totrans-294
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works…
  id: totrans-295
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The functionality we implemented is pretty handy for a lot of use cases but
    let's see what we did at a higher level first. The Java program `com.packtpub.mongo.cookbook.OplogTrigger`
    is something that acts as a trigger when new data is inserted, updated, or deleted
    from a collection in MongoDB. It uses oplog collection that is the backbone of
    the replication in Mongo to implement this functionality.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: The JavaScript we have just acts as a source of producing, updating, and deleting
    data from the collection. You may choose to open the `TriggerOperations.js` file
    and take a look at how it is implemented. The collection on which it performs
    is present in the test database and is called `oplogTriggerTest`.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: 'When we execute the JavaScript function, we should see something like the following
    printed to the output console:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: The Maven program will be continuously running and never terminate as the Java
    program doesn't. You may hit *Ctrl* + *C* to stop the execution.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s analyze the Java program, which is where the meat of the content is.
    The first assumption is that for this program to work, a replica set must be set
    up as we will use Mongo''s oplog collection. The Java programs created a connection
    to the primary of the replica set members, connects to the local database, and
    gets the `oplog.rs` collection. Then, all it does is find the last or nearly the
    last timestamp in the oplog. This is done to prevent the whole oplog to be replayed
    on startup but to mark a point towards the end in the oplog. Here is the code
    to find this timestamp value:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: The oplog is sorted in the reverse natural order to find the time in the last
    document in it. Since oplogs follow the first in first out pattern, sorting the
    oplog in the descending natural order is equivalent to sorting by the timestamp
    in descending order.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: 'Once this is done, finding the timestamp as before, we query the oplog collection
    as usual but with two additional options:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: The query finds all documents greater than a particular timestamp and adds two
    options, `Bytes.QUERYOPTION_TAILABLE` and `Bytes.QUERYOPTION_AWAITDATA`. The latter
    option can only be added when the former option is added. This not only queries
    and returns the data, but also waits for some time when the execution reaches
    the end of the cursor for some more data. Eventually, when no data arrives, it
    terminates.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: During every iteration, store the last seen timestamp as well. This is used
    when the cursor closes when no more data is available and we query again to get
    a new tailable cursor instance. The query this time will use the timestamp we
    have stored on previous iteration, when the last document was seen. This process
    continues indefinitely and we basically tail the collection in a similar way to
    how we tail a file in Unix using the `tail` command.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: The oplog document contains a field called `op` for the operation whose value
    is `i`, `u`, and `d` for insert, update, and delete, respectively. The field `o`
    contains the inserted or deleted object's ID (`_id`) in case of insert and delete.
    In case of update, the file `o2` contains the `_id`. All we do is simply check
    for these conditions and print out the operation and the ID of the document inserted/deleted
    or updated.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: Something to be careful about is as follows. Obviously, the deleted documents
    would not be available in the collection so, the `_id` would not really be useful
    if you intend to query. Also, be careful when selecting a document after update
    using the ID we get as some other operation later in the oplog might already have
    performed more updates on the same document and our application's tailable cursor
    has yet to reach that point. This is common in case of high-volume systems. Similarly,
    for inserts we have a similar problem. The document we might query using the provided
    ID might be updated/deleted already. Applications using this logic to track these
    operations must be aware of them.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, take a look at the oplog that contains more details. Like the
    document inserted, the `update` statement executed, and so on. Updates in the
    oplog collection are idempotent, which means they can be applied any number of
    times without unintended side effects. For instance, if the actual update was
    to increment the value by 1, the update in the oplog collection will have the
    `set` operator with the final value to be expected. This way, the same update
    can be applied multiple times. The logic you would use would then have to be more
    sophisticated to implement such scenarios.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: Also, failovers are not handled here. This is needed for production based systems.
    The infinite loop on the other hand opens a new cursor as soon as the first one
    terminates. There could be a sleep duration introduced before the oplog is queried
    again to avoid overwhelming the server with queries. Note that the program given
    here is not a production quality code but just a simple demo of the technique
    that is being used by a lot of other systems to get notified for new data insert,
    delete, and updates to collections in MongoDB.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: MongoDB didn't have the text search feature until version 2.4 and prior to that
    all full text search was handled using external search engines like Solr or Elasticsearch.
    Even now, though the text search feature in MongoDB is production ready, many
    would still use an external dedicated search indexer. It won't be a surprise if
    the decision is taken to use an external full text index search tool instead of
    leveraging the MongoDB's inbuilt one. In case of Elasticsearch, the abstraction
    to flow the data in to the indexes is known as a river. The MongoDB river in Elasticsearch,
    which adds data to the indexes as and when the data gets added to the collections
    in Mongo is built on the same logic as we saw in the simple program implemented
    in Java.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: Flat plane 2D geospatial queries in Mongo using geospatial indexes
  id: totrans-313
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will see what geospatial queries are and then see how to
    apply these queries on flat planes. We will put it to use in a test map application.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: Geospatial queries can be executed on data in which geospatial indexes are created.
    There are two types of geospatial indexes. The first one is called the 2D indexes
    and is the simpler of the two, it assumes that the data is given as *x,y* coordinates.
    The second one is called 3D or spherical indexes and is relatively more complicated.
    In this recipe, we will explore the 2D indexes and execute some queries on 2D
    data. The data on which we are going to work upon is a 25 x 25 grid with some
    coordinates representing bus stops, restaurants, hospitals, and gardens.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: '![Flat plane 2D geospatial queries in Mongo using geospatial indexes](img/B04831_05_02.jpg)'
  id: totrans-316
  prefs: []
  type: TYPE_IMG
- en: Getting ready
  id: totrans-317
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Refer to the recipe *Connecting to the single node using a Java client* from
    [Chapter 1](ch01.html "Chapter 1. Installing and Starting the Server"), *Installing
    and Starting the Server*, for all the necessary setup for this recipe. Download
    the data file `2dMapLegacyData.json` and keep it on the local filesystem ready
    to import. Open a mongo shell connecting to the local MongoDB instance.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  id: totrans-319
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Execute the following command from the operating system shell to import the
    data into the collection. The file `2dMapLegacyData.json` is present in the current
    directory.
  id: totrans-320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'If we see something like the following on the screen, we can confirm that the
    import has gone through successfully:'
  id: totrans-322
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'After the successful import, from the opened mongo shell, verify the collection
    and its content by executing the following query:'
  id: totrans-324
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: This should give you the feel of the data in the collection.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is to create 2D geospatial index on this data. Execute the following
    to create a 2D index:'
  id: totrans-327
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: With the index created, we will now try to find the nearest restaurant from
    the place an individual is standing. Assuming the person is not fussy about the
    type of cuisine, let's execute the following query assuming that the person is
    standing at location (12, 8), as shown in the image. Also, we are interested in
    just three nearest places.
  id: totrans-329
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: This should give us three results, starting with the nearest restaurant with
    the subsequent ones given in increasing distance. If we look at the image given
    earlier, we kind of agree with the results given here.
  id: totrans-331
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let''s add more options to the query. The individual has to walk and thus wants
    the distance to be restricted to a particular value in the results. Let''s rewrite
    the query with the following modification:'
  id: totrans-332
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: Observe the number of results retrieved this time around.
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works…
  id: totrans-335
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s now go through what we did. Before we continue, let''s define what exactly
    we mean by the distance between two points. Suppose on a cartesian plane that
    we have two points (x[1], y[1]) and (x[2], y[2]), the distance between them would
    be computed using the formula:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: '*√(x[1] – x[2])² + (y[1] – y[2])²*'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose the two points are (2, 10) and (12, 3), the distance would be: √(2
    – 12)² + (10 – 3)² = √(-10)² + (7)² = √149 =12.207.'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: After knowing how calculations for distance calculation are done behind the
    scenes by MongoDB, let's see what we did right from step 1.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: We started by importing the data normally into a collection, `areaMap` in the
    `test` database and created an index as `db.areaMap.ensureIndex({co:'2d'})`. The
    index is created on the field `co` in the document and the value is a special
    value, `2d`, which denotes that this is a special type of index called 2D geospatial
    index. Usually, we give this value as `1` or `-1` in other cases denoting the
    order of the index.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: There are two types of indexes. The first is a 2D index that is commonly used
    for planes whose span is less and do not involve spherical surfaces. It could
    be something like a map of the building, a locality, or even a small city where
    the curvature of the earth covering the portion of the land is not really significant.
    However, once the span of the map increases and covers the globe, 2D indexes will
    be inaccurate for predicting the values as the curvature of the earth needs to
    be considered in the calculations. In such cases, we go for spherical indexes,
    which we will discuss soon.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the 2D index is created, we can use it to query the collection and find
    some points near the point queried. Execute the following query:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  id: totrans-343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'It will query for documents that are of the type R, which are of type `restaurants`
    and closes to the co-ordinates (12,8). The results returned by this query will
    be in the increasing order of the distance from the point in question, (12, 8)
    in this case. The limit just limits the result to top three documents. We may
    also provide the `$maxDistance` in the query, which will restrict the results
    with a distance less than or equal to the provided value. We queried for locations
    not more than four units away, as follows:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: Spherical indexes and GeoJSON compliant data in Mongo
  id: totrans-346
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we continue with this recipe, we need to look at the previous recipe
    *Flat plane 2D geospatial queries in Mongo using geospatial indexes* to get an
    understanding of what geospatial indexes are in MongoDB and how to use the 2D
    indexes. So far, we have imported the JSON documents in a non-standard format
    in MongoDB collection, created geospatial indexes, and queried them it. This approach
    works perfectly fine and in fact, it was the only option available until MongoDB
    2.4\. version 2.4 of MongoDB supports an additional way to store, index, and query
    the documents in the collections. There is a standard way to represent geospatial
    data particularly meant for geodata exchange in JSON and the specification of
    GeoJSON mentions it in detail in the following link: [http://geojson.org/geojson-spec.html](http://geojson.org/geojson-spec.html).
    We can now store the data in this format.'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: There are various geographic figure types supported by this specification. However,
    for our use case, we will be using the type `Point`. First let's see how the document
    we imported before using a non-standard format looked and how the one using GeoJSON
    format looks.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: 'Document in non-standard format:'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE75]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Document in GeoJSON format:'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE76]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: It looks more complicated than the non-standard format and for our particular
    case I do agree. However, when representing polygons and other lines, the non-standard
    format might have to store multiple documents. In this case, it can be stored
    in a single document just by changing the value of the `type` field. Refer to
    the specification for more details.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-354
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The prerequisites for this recipe are same as the prerequisites for the previous
    recipe except that the file to be imported would be `2dMapGeoJSONData.json` and
    `countries.geo.json`. Download these files from the Packt site and keep them on
    the local filesystem for importing them later.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-356
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Special thanks to Johan Sundström for sharing the world data. The GeoJSON for
    the world is taken from [https://github.com/johan/world.geo.json](https://github.com/johan/world.geo.json).
    The file is massaged to enable importing and index creation in Mongo. Version
    2.4 doesn't support MultiPolygon and thus all MultiPolygon type of shapes are
    omitted. The shortcoming seems to be fixed in Version 2.6 though.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  id: totrans-358
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Import the GeoJSON compatible data in a new collection as follows. This contains
    26 documents similar to what we imported last time around, except that they are
    formatted using the GeoJSON format.
  id: totrans-359
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  id: totrans-360
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Create a Geospatial index on this collections as follows:'
  id: totrans-361
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  id: totrans-362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'We will now first query the collection `areaMapGeoJSON` collection as follows:'
  id: totrans-363
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  id: totrans-364
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: Next, we will try to find all the restaurants that fall within the square drawn
    between the points (0, 0), (0, 11), (11, 11), and (11, 0). Refer to the figure
    given in the introduction of the previous recipe for getting a clear visual of
    the points and the results to expect.
  id: totrans-365
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Write the following query and observe the results:'
  id: totrans-366
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  id: totrans-367
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: Check if it contains the three restaurants at coordinates (2, 6), (10, 5), and
    (10, 1) as expected.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: 'We will next try and perform some operations that would find all the matching
    objects that lie completely within another enclosing polygon. Suppose that we
    want to find some bus stops that lie within a given square block. Such use cases
    can be addressed using the `$geoWithin` operator, and the query to achieve it
    is as follows:'
  id: totrans-369
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  id: totrans-370
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: Verify the results; we should have three bus stops in the result. Refer to the
    image of the map in the previous recipe's introduction to get the expected results
    of the query.
  id: totrans-371
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'When we execute the above commands, they just print the documents in ascending
    order of the distance. However, we don''t see the actual distance in the result.
    Let''s execute the same query as in point number 3 and additionally, get the calculated
    distances as following:'
  id: totrans-372
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  id: totrans-373
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: The query returns one document with an array within the field called results
    containing the matching documents and the calculated distances. The result also
    contains some additional stats giving the maximum distance, the average of the
    distances in the result, the total documents scanned, and the time taken in milliseconds.
  id: totrans-374
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We will finally query on the world map collection to find which country the
    provided coordinate lies in. Execute the following query as follows from the mongo
    shell:'
  id: totrans-375
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  id: totrans-376
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: All the possible operations we can perform with the `worldMap` collection are
    numerous and not all are practically possible to cover in this recipe. I would
    encourage you to play around with this collection and try out different use cases.
  id: totrans-377
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works…
  id: totrans-378
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Starting from version MongoDB 2.4, the standard way for storing geospatial data
    in JSON is also supported. Note that the legacy approach that we saw is also supported.
    However, if you are starting afresh, it is recommended to go ahead with this approach
    for the following reasons.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: It is a standard and anybody aware of the specification would easily be able
    to understand the structure of the document
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It makes storing complex shapes, polygons, and multiple lines easy
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It also lets us query easily for the intersection of the shapes using the `$geoIntersect`
    and other new set of operators
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For using GeoJSON-compatible documents, we import JSON documents in the file
    `2dMapGeoJSONData.json` into the collection `areaMapGeoJSON` and create the index
    as follows:'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  id: totrans-384
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: The collection has data similar to what we had imported into the `areaMap` collection
    in the previous recipe but with a different structure that is compatible to JSON
    format. The type here used is 2Dsphere and not 2D. The 2Dsphere type of index
    also considers the spherical surfaces in calculations. Note that the field `co`,
    on which we are creating the geospatial index, is not an array of coordinates
    but a document itself that is GeoJSON compatible.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
- en: We query where the value of the `$near` operator is not an array of the coordinates,
    as we did in our previous recipe, but a document with the key `$geometry` and
    the value is a GeoJSON-compatible document for a point with the coordinates. The
    results, irrespective of the query we use are identical. Refer to point 3 in this
    recipe and point 5 in the previous recipe to see the difference in the query.
    The approach using GeoJSON looks more complicated but it has some advantages which
    we will soon see.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that we cannot mix two approaches. Try executing the
    query in the GeoJSON format that we just executed on the collection `areaMap`
    and see that although we do not get any errors, the results are not correct.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: 'We used the `$geoIntersects` operator in point 5 of this recipe. This is only
    possible when the documents are stored in GeoJSON format in the database. The
    query simply finds all the points in our case that intersect any shape we create.
    We create a polygon using the GeoJSON format as follows:'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  id: totrans-389
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: The coordinates are for the square, giving the four corners in a clockwise direction
    with the last coordinate the same as the first denoting it to be complete. The
    query executed is the same as `$near`, apart from the fact that the `$near` operator
    is replaced by the `$geoIntersects` and the value of the `$geometry` field is
    the GeoJSON document of the polygon with which we wish to find the intersecting
    points in the `areaMapGeoJSON` collection. If we look at the results obtained
    and look at the figure in the introduction section or previous recipe, they indeed
    are what we expected.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: We also saw what the `$geoWithin` operator is in point number 12, which is pretty
    handy to use when we want to find the points or even within another polygon. Note
    that only shapes completely inside the given polygon will be returned. Suppose
    that, similar to our `worldMap` collection, we have a `cities` collection with
    their coordinates specified in a similar manner. We can then use the polygon of
    a country to query all the polygons that lie within it in the `cities` collection,
    thus giving us the cities. Obviously, an easier and faster way would be to store
    the country code in the city document. Alternatively, if we have some data missing
    in the city's collection and the country is not present, one point anywhere within
    the city's polygon (since a city entirely lies in one country) can be used and
    a query can be executed on the `worldMap` collection to get its country, which
    we have demonstrated in point number 12.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
- en: A combination of what we saw previously can be put to good use to compute the
    distances between two points or even execute some geometric operation.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
- en: Some of the functionalities like getting the centroid of a polygon figure stored
    as GeoJSON in the collection or even the area of a polygon are not supported out
    of the box and there should have been some utility functions to help compute these
    given the coordinates. These features are good and are commonly required, and
    perhaps we might have some support in future release; such operations are to be
    implemented by developers themselves. Also, there is no straightforward way to
    find if there is an overlap between two polygons, what the coordinates are, where
    they overlap, the area of overlap, and so on. The `$geoIntersects` operator we
    saw does tell us what polygons do intersect with the given polygon, point, or
    line.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
- en: Though nothing related to Mongo, the GeoJSON format doesn't have support for
    circles, and hence storing circles in Mongo using GeoJSON format is not possible.
    Refer to the following link [http://docs.mongodb.org/manual/reference/operator/query-geospatial/](http://docs.mongodb.org/manual/reference/operator/query-geospatial/)
    for more details on geospatial operators.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
- en: Implementing full text search in Mongo
  id: totrans-395
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Many of us (I won''t be wrong to say all of us) use Google every day to search
    content on the web. To explain in short: the text that we provide in the text
    box on Google''s page is used to search the pages on the web it has indexed. The
    search results are then returned to us in some order determined by Google''s page
    rank algorithm. We might want to have a similar functionality in our database
    that lets us search for some text content and give the corresponding search results.
    Note that this text search is not same as finding the text as part of the sentence,
    which can easily be done using regex. It goes way beyond that and can be used
    to get results that contain the same word, a similar sounding word, have a similar
    base word, or even a synonym in the actual sentence.'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
- en: Since MongoDB Version 2.4, text indexes have been introduced, which let us create
    text indexes on a particular field in the document and enable text search on those
    words. In this recipe, we will be importing some documents and creating text indexes
    on them, which we will later query to retrieve the results.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-398
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A simple, single node is what we would need for the test. Refer to the recipe
    *Installing single node MongoDB* from [Chapter 1](ch01.html "Chapter 1. Installing
    and Starting the Server"), *Installing and Starting the Server*, for how to start
    the server. However, do not start the server yet. There would be an additional
    flag provided during the startup to enable text search. Download the file `BlogEntries.json`
    from the Packt site and keep it on your local drive ready to be imported.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  id: totrans-400
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Start the MongoDB server listening to port `27017` as follows. Once the server
    is started, we will be creating the test data in a collection as follows. With
    the file `BlogEntries.json` placed in the current directory, we will be creating
    the collection `userBlog` as follows using `mongoimport`:'
  id: totrans-401
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE86]'
  id: totrans-402
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'Now, connect to the `mongo` process from a mongo shell by typing the following
    command from the operating system shell:'
  id: totrans-403
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE87]'
  id: totrans-404
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'Once connected, get a feel of the documents in the `userBlog` collection as
    follows:'
  id: totrans-405
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE88]'
  id: totrans-406
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: The field `blog_text` is of our interest and this is the one on which we will
    be creating a text search index.
  id: totrans-407
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Create a text index on the field `blog_text` of the document as follows:'
  id: totrans-408
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE89]'
  id: totrans-409
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'Now, execute the following search on the collection from the mongo shell:'
  id: totrans-410
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE90]'
  id: totrans-411
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: Look at the results obtained.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
- en: 'Execute another search as follows:'
  id: totrans-413
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE91]'
  id: totrans-414
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: How it works…
  id: totrans-415
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s now see how it all works. A text search is done by a process called
    reverse indexes. In simple terms, this is a mechanism where the sentences are
    broken up into words and then those individual words point back to the document
    which they belong to. The process is not straightforward though, so let''s see
    what happens in this process step by step at a high level:'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
- en: Consider the following input sentence, `I played cricket yesterday`. The first
    step is to break this sentence into tokens and they become [`I`, `played`, `cricket`,
    `yesterday`].
  id: totrans-417
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, the stop words from the broken down sentence are removed and we are left
    with a subset of these. Stop words are a list of very common words that are eliminated
    as it makes no sense to index them as they can potentially affect the accuracy
    of the search when used in the search query. In this case, we will be left with
    the following words [`played`, `cricket`, `yesterday`]. Stop words are language
    specific and will be different for different languages.
  id: totrans-418
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, these words are stemmed to their base words, in this case it will be
    [`play`, `cricket`, `yesterday`]. Stemming is process of reduction of a word to
    its root. For instance, all the words `play`, `playing`, `played`, and `plays`
    have the same root word, `play`. There are a lot of algorithms and frameworks
    present for stemming a word to its root form. Refer to the Wikipedia [http://en.wikipedia.org/wiki/Stemming](http://en.wikipedia.org/wiki/Stemming)
    page for more information on stemming and the algorithms used for this purpose.
    Similar to eliminating stop words, the stemming algorithm is language dependent.
    The examples given here were for the English language.
  id: totrans-419
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If we look at the index creation process, it is created as follows `db.userBlog.ensureIndex({''blog_text'':''text''})`.
    The key given in the JSON argument is the name of the field on which the text
    index is to be created and the value will always be the text denoting that the
    index to be created is a text index. Once the index is created, at a high level,
    the preceding three steps get executed on the content of the field on which the
    index is created in each document and a reverse index is created. You can also
    choose to create a text index on more than one field. Suppose that we had two
    fields, `blog_text1` and `blog_text2`; we can create the index as `{''blog_text1'':
    ''text'', ''blog_text2'':''text''}`. The value `{''$**'':''text''}` creates an
    index on all fields of the document.'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we executed the search operation by invoking the following: `db.userBlog.find({$text:
    {$search : ''plot zoo''}})`.'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
- en: 'This command runs the text search on the collection `userBlog` and the search
    string used is `plot zoo`. This searches for the value `plot` or `zoo` in the
    text in any order. If we look at the results, we see that we have two documents
    matched and the documents are ordered by the score. This score tells us how relevant
    the document searched is, and the higher the score, the more relevant it is. In
    our case, one of the documents had both the words plot and zoo in it, and thus
    got a higher score than a document, as we see here:'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
- en: 'To get the scores in the result, we need to modify the query a bit, as follows:'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE92]'
  id: totrans-424
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: 'We now have an additional document provided in the `find` method that asks
    for the score calculated for the text match. The results still are not ordered
    in descending order of score. Let''s see how to sort the results by score:'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE93]'
  id: totrans-426
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: As we can see, the query is same as before, it's just the additional `sort`
    function that we have added, which will sort the results by descending order of
    score.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
- en: When the search is executed as `{$text:{$search:'Zoo -plot'}`, it searches for
    all the documents that contain the word `zoo` and do not contain the word `plot`,
    thus we get only one result. The `-` sign is for negation and leaves out the document
    from the search result containing that word. However, do not expect to find all
    documents without the word plot by just giving `-plot` in the search.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
- en: 'If we look at the contents returned as the result of the search, it contains
    the entire matched document in the result. If we are not interested in the entire
    document, but only a few documents, we can use projection to get the desired fields
    of the document. The following query, for instance, `db.userBlog.find({$text:
    {$search : ''plot zoo''}},{_id:1})` will be same as finding all the documents
    in the `userBlog` collection containing the words zoo or plot, but the results
    will contain the `_id` field from the resulting documents.'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
- en: 'If multiple fields are used for creation of index, then we may have different
    weights for different fields in the document. For instance, suppose blog_text1
    and blog_text2 are two fields of a collection. We can create an index where `blog_text1`
    has higher weight than `blog_text2` as follows:'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]'
  id: totrans-431
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: This gives the content in `blog_text1` twice as much weight as that in `blog_text2`.
    Thus, if a word is found in two documents but is present in the `blog_text1` field
    of the first document and `blog_text2` of second document, then the score of first
    document will be more than the second. Note that we also have provided the name
    of the index using the name field as `MyCustomIndexName`.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
- en: We also see from the language key that the language in this case is English.
    MongoDB supports various languages for implementing text search. Languages are
    important when indexing the content as they decide the stop words, and stemming
    of words is language specific too.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
- en: Visit the link [http://docs.mongodb.org/manual/reference/command/text/#text-search-languages](http://docs.mongodb.org/manual/reference/command/text/#text-search-languages)
    for more details on the languages supported by Mongo for text search.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
- en: 'So, how do we choose the language while creating the index? By default, if
    nothing is provided, the index is created assuming the language is English. However,
    if we know the language is French, we create the index as follows:'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE95]'
  id: totrans-436
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: 'Suppose that we had originally created the index using the French language,
    the `getIndexes` method would return the following document:'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE96]'
  id: totrans-438
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: 'However, if the language was different per document basis, which is pretty
    common in scenarios like blogs, we have a way out. If we look at the document
    above, the value of the `language_override` field is language. This means that
    we can store the language of the content using this field on a per document basis.
    In its absence, the value will be assumed as the default value, `french` in the
    preceding case. Thus, we can have the following:'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE97]'
  id: totrans-440
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: There's more…
  id: totrans-441
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To use MongoDB text search in production, you would need version 2.6 or higher.
    Integrating MongoDB with other systems like Solr and Elasticsearch is also an
    option. In the next recipe, we will see how to integrate Mongo with Elasticsearch
    using the mongo-connector.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
- en: See also
  id: totrans-443
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For more information on the `$text` operator, visit [http://docs.mongodb.org/manual/reference/operator/query/text/](http://docs.mongodb.org/manual/reference/operator/query/text/)
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integrating MongoDB for full text search with Elasticsearch
  id: totrans-445
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'MongoDB has integrated text search features, as we saw in the previous recipe.
    However, there are multiple reasons why one would not use the Mongo text search
    feature and fall back to a conventional search engine like Solr or Elasticsearch,
    and the following are few of them:'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
- en: The text search feature is production ready in version 2.6\. In version 2.4,
    it was introduced in beta and not suitable for production use cases.
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Products like Solr and Elasticsearch are built on top of Lucene, which has proven
    itself in the search engine arena. Solr and Elasticsearch are pretty stable products
    too.
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You might already have expertise on products like Solr and Elasticsearch and
    would like to use it as a full text search engine rather than MongoDB.
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some particular feature that you might find missing in MongoDB search which
    your application might require, for example, facets.
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up a dedicated search engine does need additional efforts to integrate
    it with a MongoDB instance. In this recipe, we will see how to integrate a MongoDB
    instance with a search engine, Elasticsearch.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
- en: We will be using the mongo-connector for integration purpose. It is an open
    source project that is available at [https://github.com/10gen-labs/mongo-connector](https://github.com/10gen-labs/mongo-connector).
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-453
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Refer to the recipe *Connecting to a single node using a Python client*, in
    [Chapter 1](ch01.html "Chapter 1. Installing and Starting the Server"), *Installing
    and Starting the Server* for installing and setting up Python client. The tool
    pip is used for getting the mongo-connector. However, if you are working on a
    Windows platform, the steps to install pip was not mentioned earlier. Visit the
    URL [https://sites.google.com/site/pydatalog/python/pip-for-windows](https://sites.google.com/site/pydatalog/python/pip-for-windows)
    to get pip for windows.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
- en: The prerequisites for starting the single instance are all we need for this
    recipe. We would, however, start the server as a one node replica set for demonstration
    purpose in this recipe.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
- en: Download the file `BlogEntries.json` from the Packt site and keep it on your
    local drive ready to be imported.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
- en: 'Download elastic search from the following URL for your target platform: [http://www.elasticsearch.org/overview/elkdownloads/](http://www.elasticsearch.org/overview/elkdownloads/).
    Extract the downloaded archive and from the shell, go to the `bin` directory of
    the extraction.'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
- en: We will get the mongo-connector source from GitHub.com and run it. A Git client
    is needed for this purpose. Download and install the Git client on your machine.
    Visit the URL [http://git-scm.com/downloads](http://git-scm.com/downloads) and
    follow the instructions for installing Git on your target operating system. If
    you are not comfortable installing Git on your operating system, then there is
    an alternative available that lets you download the source as an archive.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
- en: 'Visit the following URL [https://github.com/10gen-labs/mongo-connector](https://github.com/10gen-labs/mongo-connector).
    Here, we will get an option that lets us download the current source as an archive,
    which we can then extract on our local drive. The following image shows that the
    download option available on the bottom-right corner:'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
- en: '![Getting ready](img/B04831_05_03.jpg)'
  id: totrans-460
  prefs: []
  type: TYPE_IMG
- en: Note
  id: totrans-461
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Note that we can also install mongo-connector in a very easy way using pip
    as follows:'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE98]'
  id: totrans-463
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: However, the version in PyPi is a very old with not many features supported
    and thus using the latest from the repository is recommended.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
- en: Similar to the previous recipe, where we saw text search in Mongo, we will use
    the same five documents to test our simple search. Download and keep the `BlogEntries.json`
    file.
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  id: totrans-466
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'At this point, it is assumed that Python and PyMongo are installed and pip
    for your operating system platform is installed. We will now get mongo-connector
    from source. If you have already installed the Git client, we will be executing
    the following on the operating system shell. If you have decided to download the
    repository as an archive, you may skip this step. Go to the directory where you
    would like to clone the connector repository and execute the following:'
  id: totrans-467
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE99]'
  id: totrans-468
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: The preceding setup will also install the Elasticsearch client that will be
    used by this application.
  id: totrans-469
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We will now start a single mongo instance but as a replica set. From the operating
    system console, execute the following:'
  id: totrans-470
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE100]'
  id: totrans-471
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: 'Start a mongo shell and connect to the started instance:'
  id: totrans-472
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE101]'
  id: totrans-473
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
- en: 'From the mongo shell initiate the replica set as follows:'
  id: totrans-474
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE102]'
  id: totrans-475
  prefs: []
  type: TYPE_PRE
  zh: '[PRE102]'
- en: The replica set will be initiated in a few moments. Meanwhile, we can proceed
    to starting the `elasticsearch` server instance.
  id: totrans-476
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Execute the following from the command after going to the `bin` directory of
    the extracted `elasticsearch` archive:'
  id: totrans-477
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE103]'
  id: totrans-478
  prefs: []
  type: TYPE_PRE
  zh: '[PRE103]'
- en: We won't be getting into the Elasticsearch settings, and we will start it in
    the default mode.
  id: totrans-479
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once started, enter the following URL in the browser `http://localhost:9200/_nodes/process?pretty`.
  id: totrans-480
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If we see a JSON document as the following, giving the process details, we have
    successfully started `elasticsearch`.
  id: totrans-481
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE104]'
  id: totrans-482
  prefs: []
  type: TYPE_PRE
  zh: '[PRE104]'
- en: Once the `elasticsearch` server and mongo instance are up and running, and the
    necessary Python libraries are installed, we will start the connector that will
    sync the data between the started mongo instance and the `elasticsearch` server.
    For the sake of this test, we will be using the collection `user_blog` in the
    `test` database. The field on which we would like to have text search implemented
    is the field `blog_text` in the document.
  id: totrans-483
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Start the mongo-connector from the operating system shell as follows. The following
    command was executed with the mongo-connector's directory as the current directory.
  id: totrans-484
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE105]'
  id: totrans-485
  prefs: []
  type: TYPE_PRE
  zh: '[PRE105]'
- en: Import the `BlogEntries.json` file into the collection using `mongoimport` utility
    as follows. The command is executed with the `.json` file present in the current
    directory.
  id: totrans-486
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE106]'
  id: totrans-487
  prefs: []
  type: TYPE_PRE
  zh: '[PRE106]'
- en: 'Open a browser of your choice and enter the following URL in it: `http://localhost:9200/_search?q=blog_text:facebook`.'
  id: totrans-488
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You should see something like the following in the browser:![How to do it…](img/B04831_05_04.jpg)
  id: totrans-489
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works…
  id: totrans-490
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Mongo-connector basically tails the oplog to find new updates that it publishes
    to another endpoint. We used elasticsearch in our case, but it could be even be
    Solr. You may choose to write a custom DocManager that would plugin with the connector.
    Refer to the wiki [https://github.com/10gen-labs/mongo-connector/wiki](https://github.com/10gen-labs/mongo-connector/wiki)
    for more details, and the readme for [https://github.com/10gen-labs/mongo-connector](https://github.com/10gen-labs/mongo-connector)
    gives some detailed information too.
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
- en: 'We gave the connector the options `-m`, `-t`, `-n`, `--fields`, and `-d` and
    what they mean is explained in the table as follows:'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
- en: '| Option | Description |'
  id: totrans-493
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-494
  prefs: []
  type: TYPE_TB
- en: '| `-m` | URL of the MongoDB host to which the connector connects to get the
    data to be synchronized. |'
  id: totrans-495
  prefs: []
  type: TYPE_TB
- en: '| `-t` | The target URL of the system with which the data is to be synchronized
    with. Elasticsearch in this case. The URL format will depend on the target system.
    Should you choose to implement your own DocManager, the format will be one that
    your DocManager understands. |'
  id: totrans-496
  prefs: []
  type: TYPE_TB
- en: '| `-n` | This is the namespace that we would like keep synchronized with the
    external system. The connector will just be looking for changes in these namespaces
    while tailing the oplog for data. The value will be comma separated if more than
    one namespaces are to be synchronized. |'
  id: totrans-497
  prefs: []
  type: TYPE_TB
- en: '| `--fields` | These are the fields from the document that will be sent to
    the external system. In our case, it doesn''t make sense to index the entire document
    and waste resources. It is recommended to add to the index just the fields that
    you would like to add text search support. The identifier `_id` and the namespace
    of the source is also present in the result, as we can see in the preceding screenshot.
    The `_id` field can then be used to query the target collection. |'
  id: totrans-498
  prefs: []
  type: TYPE_TB
- en: '| `-d` | This is the document manager to be used, in our case we have used
    the elasticsearch''s document manager. |'
  id: totrans-499
  prefs: []
  type: TYPE_TB
- en: For more supported options, refer to the readme of the connector's page on GitHub.
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
- en: Once the insert is executed on the MongoDB server, the connector detects the
    newly added documents to the collection of its interest, `user_blog`, and starts
    sending the data to be indexed from the newly documents to the elasticsearch.
    To confirm the addition, we execute a query in the browser to view the results.
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
- en: Elasticsearch will complain that the index names have upper case characters
    in them. The mongo-connector doesn't take care of this, and thus the name of the
    collection has to be in lower case. For example, the name `userBlog` will fail.
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  id: totrans-503
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have not done any additional configuration on elasticsearch as that was not
    the objective of the recipe. We were more interested in integrating MongoDB and
    elasticsearch. You will have to refer to elasticsearch documentation for more
    advanced config options. If integrating with elasticsearch is required, there
    is a concept called rivers in elasticsearch that can be used as well. Rivers are
    elasticsearch's way to get data from another data source. For MongoDB, the code
    for the river can be found at [https://github.com/richardwilly98/elasticsearch-river-mongodb/](https://github.com/richardwilly98/elasticsearch-river-mongodb/).
    The readme in this repository has steps on how to set it up.
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we saw a recipe, *Implementing triggers in Mongo using oplog*,
    on how to implement trigger-like functionalities using Mongo. This connector and
    MongoDB river for elasticsearch rely on the same logic for getting the data out
    of Mongo as and when it is needed.
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
- en: See also
  id: totrans-506
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You may find additional elasticsearch documentation at [http://www.elasticsearch.org/guide/en/elasticsearch/reference/](http://www.elasticsearch.org/guide/en/elasticsearch/reference/)
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
