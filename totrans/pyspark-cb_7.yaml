- en: Structured Streaming with PySpark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover how to work with Apache Spark Structured Streaming
    within PySpark. You will learn the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding DStreams
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding global aggregations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Continuous aggregations with structured streaming
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With the prevalence of machine-generated *real-time data*, including but not
    limited to IoT sensors, devices, and beacons, it is increasingly important to
    gain insight into this fire hose of data as quickly as it is being created. Whether
    you are detecting fraudulent transactions, real-time detection of sensor anomalies,
    or sentiment analysis of the next cat video, streaming analytics is an increasingly
    important differentiator and business advantage.
  prefs: []
  type: TYPE_NORMAL
- en: As we progress through these recipes, we will be combining the constructs of
    *batch* and *real-time* processing for the creation of continuous applications.
    With Apache Spark, data scientists and data engineers can analyze their data using
    Spark SQL in batch and in real time, train machine learning models with MLlib,
    and score these models via Spark Streaming.
  prefs: []
  type: TYPE_NORMAL
- en: 'An important reason for the rapid adoption of Apache Spark is that it unifies
    all of these disparate data processing paradigms (machine learning via ML and
    MLlib, Spark SQL, and streaming). As note, in *Spark Streaming: What is It and
    Who’s Using it* ([https://www.datanami.com/2015/11/30/spark-streaming-what-is-it-and-whos-using-it/](https://www.datanami.com/2015/11/30/spark-streaming-what-is-it-and-whos-using-it/)),
    companies such as Uber, Netflix, and Pinterest often showcase their uses case
    through Spark Streaming:'
  prefs: []
  type: TYPE_NORMAL
- en: '*How Uber Uses Spark and Hadoop to Optimize Customer Experience* at [https://www.datanami.com/2015/10/05/how-uber-uses-spark-and-hadoop-to-optimize-customer-experience/](https://www.datanami.com/2015/10/05/how-uber-uses-spark-and-hadoop-to-optimize-customer-experience/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark and Spark Streaming at Netflix at [https://spark-summit.org/2015/events/spark-and-spark-streaming-at-netflix/](https://spark-summit.org/2015/events/spark-and-spark-streaming-at-netflix/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can Spark Streaming survive Chaos Monkey? at [http://techblog.netflix.com/2015/03/can-spark-streaming-survive-chaos-monkey.html](http://techblog.netflix.com/2015/03/can-spark-streaming-survive-chaos-monkey.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Real-time analytics at Pinterest at [https://engineering.pinterest.com/blog/real-time-analytics-pinterest](https://engineering.pinterest.com/blog/real-time-analytics-pinterest)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding Spark Streaming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For real-time processing in Apache Spark, the current focus is on structured
    streaming, which is built on top of the DataFrame/dataset infrastructure. The
    use of DataFrame abstraction allows streaming, machine learning, and Spark SQL
    to be optimized in the Spark SQL Engine Catalyst Optimizer and its regular improvements
    (for example, Project Tungsten). Nevertheless, to more easily understand Spark
    Streaming, it is worthwhile to understand the fundamentals of its Spark Streaming
    predecessor. The following diagram represents a Spark Streaming application data
    flow involving the Spark driver, workers, streaming sources, and streaming targets:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00164.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The description of the preceding diagram is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Starting with the **Spark Streaming Context** (**SSC**), the driver will execute
    long-running tasks on the executors (that is, the Spark workers).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The code defined within the driver (starting `ssc.start()`), the **Receiver**
    on the executors (**Executor 1** in this diagram) receives a data stream from
    the **Streaming Sources**. Spark Streaming can receive **Kafka** or **Twitter**,
    and/or you can build your own custom receiver. With the incoming data stream,
    the receiver divides the stream into blocks and keeps these blocks in memory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These data blocks are replicated to another executor for high availability.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The block ID information is transmitted to the block manager master on the driver,
    thus ensuring that each block of data in memory is tracked and accounted for.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For every batch interval configured within SSC (commonly, this is every 1 second),
    the driver will launch Spark tasks to process the blocks. Those blocks are then
    persisted to any number of target data stores, including cloud storage (for example,
    S3, WASB), relational data stores (for example, MySQL, PostgreSQL, and so on),
    and NoSQL stores.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the following sections, we will review recipes with **Discretized Streams**
    or **DStreams** (the fundamental streaming building block) and then perform global
    aggregations by performing stateful calculations on DStreams. We will then simplify
    our streaming application by using structured streaming while at the same time
    gaining performance optimizations.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding DStreams
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we dive into structured streaming, let''s start by talking about DStreams.
    DStreams are built on top of RDDs and represent a stream of data divided into
    small chunks. The following figure represents these data chunks in micro-batches
    of milliseconds to seconds. In this example, the lines of DStream is micro-batched
    into seconds where each square represents a micro-batch of events that occurred
    within that second window:'
  prefs: []
  type: TYPE_NORMAL
- en: At time interval 1 second, there were five occurrences of the event **blue**
    and three occurrences of the event **green**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At time interval 2 seconds, there is a single occurrence of **gohawks**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At time interval 4 seconds, there are two occurrences of the event **green**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/00165.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Because DStreams are built on top of RDDs, Apache Spark's core data abstraction,
    this allows Spark Streaming to easily integrate with other Spark components such
    as MLlib and Spark SQL.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For these Apache Spark Streaming examples, we will be creating and executing
    a console application via the bash terminal. To make things easier, you will want
    to have two terminal windows open.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As noted in the previous section, we will use two terminal windows:'
  prefs: []
  type: TYPE_NORMAL
- en: One terminal window to transmit an event
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another terminal to receive those events
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Note that the source code for this can be found in the Apache Spark 1.6 Streaming
    Programming Guide at: [https://spark.apache.org/docs/1.6.0/streaming-programming-guide.html](https://spark.apache.org/docs/1.6.0/streaming-programming-guide.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Terminal 1 – Netcat window
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For the first window, we will use Netcat (or nc) to manually send events such
    as blue, green, and gohawks. To start Netcat, use the following command; we will
    direct our events to port `9999`, where our Spark Streaming job will detect:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'To match the previous diagram, we will type in our events so that the console
    screen looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Terminal 2 – Spark Streaming window
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will create a simple PySpark Streaming application using the following code
    called `streaming_word_count.py`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'To run this PySpark Streaming application, execute the following command from
    your `$SPARK_HOME` folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'In terms of how you time this, you should:'
  prefs: []
  type: TYPE_NORMAL
- en: First start with `nc -lk 9999`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Then, start your PySpark Streaming application: `/bin/spark-submit streaming_word_count.py
    localhost 9999`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Then, start typing your events, for example:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For the first second, type `blue blue blue blue blue green green green`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For the second second, type `gohawks`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Wait a second; for the fourth second, type `green green`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The console output from your PySpark streaming application will look something
    similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: To end the streaming application (and the `nc` window, for that matter), execute
    a termination command (for example, *Ctrl* + *C*).
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As noted in the previous subsections, this recipe is comprised of one terminal
    window transmitting event data using `nc`. The second window runs our Spark Streaming
    application, reading from the port that the first window is transmitting to.
  prefs: []
  type: TYPE_NORMAL
- en: 'The important call outs for this code are noted here:'
  prefs: []
  type: TYPE_NORMAL
- en: We're creating a Spark context using two working threads, hence the use of `local[2]`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As noted in the Netcat window, we're using `ssc.socketTextStream` to listen
    to the local socket of the `localhost`, port `9999`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recall that for each 1-second batch, we're not only reading a single line (for
    example, `blue blue blue blue blue green green green`), but also splitting it
    up into individual `words` via `split`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We're using a Python `lambda` function and PySpark `map` and `reduceByKey` functions
    to quickly count the occurrences of words within the 1-second batch. For example,
    in the case of `blue blue blue blue blue green green green`, there are five blue
    and three green events, as reported at *2018-06-21 23:00:30* of our streaming
    application.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ssc.start()` is in reference to the application starting the Spark Streaming
    context.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ssc.awaitTermination()` is waiting for a termination command to stop the streaming
    application (for example, *Ctrl* + *C*); otherwise, the application will continue
    to run.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When using the PySpark console, often there are a lot of messages that are
    sent out to the console that can make it difficult to read the streaming output.
    To make it easier to read, ensure that you have created and modified the `log4j.properties`
    file within the `$SPARK_HOME/conf` folder. To do this, follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Go to the `$SPARK_HOME/conf` folder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'By default, there is a `log4j.properties.template` file. Copy it with the same
    name, removing the `.template`, that is:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Edit the `log4j.properties` in your favorite editor (for example, sublime,
    vi, and so on). In line 19 of the file, change this line:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'To this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: This way, instead of all log information (that is, `INFO`) being directed to
    the console, only errors (that is, `ERROR`) will be directed to the console.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding global aggregations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous section, our recipe provided a snapshot count of events. That
    is, it provided the count of events at the point in time. But what if you want
    to understand a sum of events for some time window? This is the concept of global
    aggregations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00166.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'If we wanted global aggregations, the same example as before (Time 1: 5 blue,
    3 green, Time 2: 1 gohawks, Time 4: 2 greens) would be calculated as:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Time 1: 5 blue, 3 green'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Time 2: 5 blue, 3 green, 1 gohawks'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Time 4: 5 blue, 5 green, 1 gohawks'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Within the traditional batch calculations, this would be similar to a `groupbykey`
    or `GROUP BY` statement. But in the case of streaming applications, this calculation
    needs to be done within milliseconds, which is typically too short of a time window
    to perform a `GROUP BY` calculation. However, with Spark Streaming global aggregations,
    this calculation can be completed quickly by performing a stateful streaming calculation.
    That is, using the Spark Streaming framework, all of the information to perform
    the aggregation is kept in memory (that is, keeping the data in *state*) so that
    it can be calculated in its small time window.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For these Apache Spark Streaming examples, we will be creating and executing
    a console application via the bash terminal. To make things easier, you will want
    to have two terminal windows open.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As noted in the previous section, we will use two terminal windows:'
  prefs: []
  type: TYPE_NORMAL
- en: One terminal window to transmit an event
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another terminal to receive those events
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The source code for this can be found in the Apache Spark 1.6 Streaming Programming
    Guide at: [https://spark.apache.org/docs/1.6.0/streaming-programming-guide.html](https://spark.apache.org/docs/1.6.0/streaming-programming-guide.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Terminal 1 – Netcat window
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For the first window, we will use Netcat (or `nc`) to manually send events
    such as blue, green, and gohawks. To start Netcat, use the following command;
    we will direct our events to port `9999` where our Spark Streaming job will detect:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'To match the previous diagram, we will type in our events so that the console
    screen looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Terminal 2 – Spark Streaming window
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will create a simple PySpark Streaming application using the following code
    called `streaming_word_count.py`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'To run this PySpark Streaming application, execute the following command from
    your `$SPARK_HOME` folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'In terms of how you time this, you should:'
  prefs: []
  type: TYPE_NORMAL
- en: First start with `nc -lk 9999`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, start your PySpark Streaming application: `./bin/spark-submit stateful_streaming_word_count.py
    localhost 9999`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Then, start typing your events, for example:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For the first second, type `blue blue blue blue blue green green green`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For the second second, type `gohawks`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Wait a second; for the fourth second, type `green green`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The console output from your PySpark streaming application will look something
    similar to the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: To end the streaming application (and the `nc` window, for that matter), execute
    a termination command (for example, *Ctrl* + *C*).
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As noted in the previous subsections, this recipe is comprised of one terminal
    window transmitting event data using `nc`. The second window runs our Spark Streaming
    application reading from the port that the first window is transmitting to.
  prefs: []
  type: TYPE_NORMAL
- en: 'The important call outs for this code are noted here:'
  prefs: []
  type: TYPE_NORMAL
- en: We're creating a Spark context using two working threads, hence the use of `local[2]`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As noted in the Netcat window, we're using `ssc.socketTextStream` to listen
    to the local socket of the `localhost`, port `9999`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have created a `updateFunc`, which performs the task of aggregating the previous
    value with the currently aggregated value.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recall that for each 1-second batch, we're not only reading a single line (for
    example, `blue blue blue blue blue green green green`) but also splitting it up
    into individual `words` via `split`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We're using a Python `lambda` function and PySpark `map` and `reduceByKey` functions
    to quickly count the occurrences of words within the 1-second batch. For example,
    in the case of `blue blue blue blue blue green green green`, there are 5 blue
    and 3 green events, as reported at *2018-06-21 23:00:30* of our streaming application.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The difference between the previous streaming application vs. the current stateful
    version is that we're calculating running counts (`running_counts`) with the current
    aggregation (for example, five blue and three green events) with `updateStateByKey`.
    This allows Spark Streaming to keep the state of the current aggregation within
    the context of the previously defined `updateFunc`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ssc.start()` is in reference to the application starting the Spark Streaming
    context.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ssc.awaitTermination()` is waiting for a termination command to stop the streaming
    application (for example, *Ctrl* + *C*); otherwise, the application will continue
    to run.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Continuous aggregation with structured streaming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As noted in earlier chapters, the execution of Spark SQL or DataFrame queries revolves
    around building a logical plan, choosing a physical plan (of the many generated
    physical plans) based on its cost optimizer, and then generating the code (that
    is, code gen) via the Spark SQL Engine Catalyst Optimizer. What structured streaming
    introduces is the concept of an *incremental* execution plan. That is, structured
    streaming repeatedly applies the execution plan for every new block of data it
    receives. This way, the Spark SQL engine can take advantage of the optimizations
    included within Spark DataFrames and apply them to an incoming data stream. Because
    structured streaming is built on top of Spark DataFrames, this means it will also
    be easier to integrate other DataFrame-optimized components, including MLlib,
    GraphFrames, TensorFrames, and so on:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00167.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For these Apache Spark Streaming examples, we will be creating and executing
    a console application via the bash terminal. To make things easier, you will want
    to have two terminal windows open.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As noted in the previous section, we will use two terminal windows:'
  prefs: []
  type: TYPE_NORMAL
- en: One terminal window to transmit an event
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another terminal to receive those events
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The source code for this can be found in the Apache Spark 2.3.1 Structured
    Streaming Programming Guide at: [https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Terminal 1 – Netcat window
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For the first window, we will use Netcat (or `nc`) to manually send events
    such as blue, green, and gohawks. To start Netcat, use this command; we will direct
    our events to port `9999`, where our Spark Streaming job will detect:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'To match the previous diagram, we will type in our events so that the console
    screen looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Terminal 2 – Spark Streaming window
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will create a simple PySpark Streaming application using the following code
    called `structured_streaming_word_count.py`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'To run this PySpark Streaming application, execute the following command from
    your `$SPARK_HOME` folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'In terms of how you time this, you should:'
  prefs: []
  type: TYPE_NORMAL
- en: First start with `nc -lk 9999`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, start your PySpark Streaming application: `./bin/spark-submit stateful_streaming_word_count.py
    localhost 9999`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Then, start typing your events, for example:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For the first second, type `blue blue blue blue blue green green green`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For the second second, type `gohawks`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Wait a second; for the fourth second, type `green green`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The console output from your PySpark streaming application will look something
    similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: To end the streaming application (and the `nc` window, for that matter), execute
    a termination command (for example, *Ctrl* + *C*).
  prefs: []
  type: TYPE_NORMAL
- en: Similar to global aggregations with DStreams, with structured streaming, you
    can easily perform stateful global aggregations within the context of a DataFrame.
    Another optimization you'll notice with structured streaming is that the streaming
    aggregations will only appear whenever there are new events. Specifically notice
    how when we delayed between time = 2s and time = 4s, there is not an extra batch
    being reported to the console.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As noted in the previous subsections, this recipe is comprised of one terminal
    window transmitting event data using `nc`. The second window runs our Spark Streaming
    application, reading from the port that the first window is transmitting to.
  prefs: []
  type: TYPE_NORMAL
- en: 'The important callouts for this code are noted here:'
  prefs: []
  type: TYPE_NORMAL
- en: Instead of creating a Spark context, we're creating a `SparkSession`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With the SparkSession, we can use `readStream` to specify the `socket` *format*
    to specify that we're listening to `localhost` at port `9999`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We use the PySpark SQL functions `split` and `explode` to take our `line` and
    break it down to `words`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To generate our running word count, we need only to create `wordCounts` to run
    a `groupBy` statement and `count()` on `words`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, we will use `writeStream` to write out the `complete` set of `query`
    data to the `console` (as opposed to some other data sink)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Because we're using a Spark session, the application is waiting for a termination
    command to stop the streaming application (for example, <Ctrl><C>) via `query.awaitTermination()`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Because structured streaming is using DataFrames, it is simpler and easier to
    read because we're using the familiar DataFrame abstraction while also gaining
    all the performance optimizations of DataFrames.
  prefs: []
  type: TYPE_NORMAL
