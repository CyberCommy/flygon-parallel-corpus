- en: Deploying Microservices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will deal with two related, yet separate themes: production
    deployments and development deployments. The concerns, processes, and tools that
    are used for these two areas are very different. In both cases, the goal is to
    deploy new software to the cluster, but everything else is different. With production
    deployments, it''s desirable to keep the system stable, be able to have a predictable
    build and deployment experience, and most importantly, to identify and be able
    to roll back bad deployments. With development deployments, it''s desirable to
    have isolated deployments for each developer, a fast turnaround, and the ability
    to avoid cluttering source control or the **continuous integration **/ **continuous
    deployment** (**CI**/**CD**) system (including image registries) with temporary
    development versions. Due to this, divergent emphasis is beneficial to isolate
    production deployments from development deployments.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes deployments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying to multiple environments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding deployment strategies (rolling updates, blue-green deployment,
    canary deployment)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rolling back deployments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Managing versions and upgrades
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Local development deployments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will install many tools, including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: KO
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ksync
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Draft
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Skaffold
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tilt
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is no need to install them ahead of time.
  prefs: []
  type: TYPE_NORMAL
- en: The code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The code is split between two Git repositories:'
  prefs: []
  type: TYPE_NORMAL
- en: You can find the code samples here: [https://github.com/PacktPublishing/Hands-On-Microservices-with-Kubernetes/tree/master/Chapter11](https://github.com/PacktPublishing/Hands-On-Microservices-with-Kubernetes/tree/master/Chapter11)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can find the updated Delinkcious application here: [https://github.com/the-gigi/delinkcious/releases/tag/v0.9](https://github.com/the-gigi/delinkcious/releases/tag/v0.9)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubernetes deployments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We talked about deployments briefly in [Chapter 1](b66b66c3-db8c-474a-84b6-b87091f137f3.xhtml), *Introduction
    to Kubernetes for Developers*, and we've used Kubernetes deployments in almost
    every chapter. However, before diving into more sophisticated patterns and strategies,
    it will be useful to review the basic building blocks and the relationships between
    Kubernetes deployments, Kubernetes services, and scaling or autoscaling.
  prefs: []
  type: TYPE_NORMAL
- en: 'Deployments are Kubernetes resources that manage pods via a ReplicaSet. A Kubernetes
    ReplicaSet is a group of pods that are identified by a common set of labels with
    a certain number of replicas. The connection between the ReplicaSet to its pods
    is the `ownerReferences` field in the pod''s metadata. The ReplicaSet controller
    ensures that the correct number of replicas are always running. If a pod dies
    for whatever reason, the ReplicaSet controller will schedule a new pod in its
    place. The following diagram illustrates this relationship:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/61cb77c8-f8de-4945-8808-8b9b7651c0db.png)'
  prefs: []
  type: TYPE_IMG
- en: Deployment and ReplicaSet
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also observe the ownership chain in the metadata with kubectl. First,
    let''s get the name of the social graph manager pod and find the name of its ReplicaSet
    owner from the `ownerReferences` metadata:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we''ll get the name of the deployment that owns the ReplicaSet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: So, if the ReplicaSet controller takes care of managing the number of pods,
    what does the `Deployment` object add? The `Deployment` object encapsulates the
    concept of a deployment, including a deployment strategy and rollout history.
    It also provides deployment-oriented operations such as updating a deployment
    and rolling back a deployment, which we will look at later.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying to multiple environments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will create a staging environment for Delinkcious in a new
    staging namespace. The `staging` namespace will be a full-fledged copy of the
    default namespace that will serve as our production environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s create the namespace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, in Argo CD, we can create a new project called `staging`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/d9bd625b-c230-4165-9dad-f8fda2201a97.png)Argo CD staging project'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we need to configure all our services so that Argo CD can sync them to
    the staging environment. This can be a little tedious to do in the UI now that
    we have a substantial amount of services. Instead, we will use the Argo CD CLI
    and a Python 3 program called `bootstrap_staging.py` to automate the process.
    The program expects the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The staging namespace has been created.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Argo CD CLI is installed and in the path.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Argo CD service is available through the localhost on port `8080`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Argo CD admin password is configured as the environment variable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To expose Argo CD on the localhost at port `80`, we can run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s break down the program and understand how it works. This is a good foundation
    where you can develop your own custom CI/CD solutions by automating CLI tools.
    The only dependencies are Python''s standard library modules: `subprocess` (allows
    you to run command-line tools) and `os` (for accessing environment variables).
    Here, we only need to run the Argo CD CLI.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `run()` function hides all the implementation details and provides a convenient
    interface where you just need to pass the arguments as a string. The `run()` function
    will prepare a proper command list that can be passed to the `subprocess` module''s
    `check_output()` function, capture the output, and decode it from bytes to a string:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The `login()` function utilizes `run()`, gets the admin password from the environment,
    and constructs the proper command string with all the necessary flags so that
    you can log in as the admin user to Argo CD:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The `get_apps()` function takes a namespace and returns the relevant fields
    of the Argo CD apps in it. This function will be used both on the `default` namespace
    and the `staging` namespace. The function invokes the `app list` command, parses
    the output, and populates a Python dictionary with the relevant information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The `create_project()` function takes all the necessary information to create
    a new Argo CD project. Note that multiple Argo CD projects can coexist in the
    same Kubernetes namespace. It also allows access to all cluster resources, which
    is necessary to create applications. Since we have already created the project
    in the Argo CD UI, there is no need to use it in this program, but it''s good
    to have it around in case we need to create more projects in the future:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The last generic function is called `create_app()`, and takes all the necessary
    information for creating an Argo CD application. It assumes that Argo CD is running
    inside the destination cluster, so `--dest-server` is always `https://kubernetes.default.svc`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The `copy_apps_from_default_to_staging()` function uses some of the functions
    we declared earlier. It gets all the apps from the default namespace, iterates
    over them, and creates the same app in the staging project and namespace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, here''s the `main` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have two environments, let's consider some workflows and promotion
    strategies. Whenever a change is pushed, GitHub CircleCI will detect it. If all
    the tests pass, it will bake a new image for each service and push it to Docker
    Hub. The question is, what should happen on the deployment side? Argo CD has sync
    policies, and we can configure them to automatically sync/deploy whenever a new
    image is available on Docker Hub. For example, a common practice is to automatically
    deploy to staging, deploying to production only after various tests (for example,
    the `smoke` test) have passed on staging. The promotion from staging to production
    may be automated or manual.
  prefs: []
  type: TYPE_NORMAL
- en: There is no one-size-fits-all answer. Even within the same organization, different
    deployment policies and strategies are often employed for projects or services
    with different sets of requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Let's look at some of the more common deployment strategies and what use cases
    they enable.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding deployment strategies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A deployment of a new version of a service in Kubernetes means replacing the
    *N* backing pods of the service, which run version *X* with *N* backing pods running
    version *X+1*. There are multiple ways to get from N pods running version *X*,
    to zero pods running version *X* and *N* pods running version *X+1*. Kubernetes
    deployments support two strategies out of the box: `Recreate` and `RollingUpdate`
    (the default strategy). Blue-green deployments and canary deployments are two
    other popular strategies. Before diving into the various deployment strategies,
    as well as their pros and cons, it''s important to understand the process of updating
    a deployment in Kubernetes.'
  prefs: []
  type: TYPE_NORMAL
- en: A rollout of a new set of pods for a deployment happens if and only if the deployment
    spec's pod template has changed. This typically happens when you change the image
    version of the pod template or the set of labels for a container. Note that scaling
    a deployment (increasing or decreasing its number of replicas) is *not* an update,
    so the deployment strategy is not used. The same version of the image as the current
    running pods will be used in any new pods that are added.
  prefs: []
  type: TYPE_NORMAL
- en: Recreating deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A trivial yet naive way to do this is to terminate all the pods running version
    *X*, and then create a new deployment where the image version in the pod template
    spec is set to *X+1*. There are a couple of problems with this approach:'
  prefs: []
  type: TYPE_NORMAL
- en: The service will be unavailable until the new pods come online.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the new version has issues, the service will be unavailable until the process
    is reversed (ignoring errors and data corruption).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `Recreate` deployment strategy is appropriate for development, or when you
    prefer to have a short outage, but ensure that there is no mix of versions that
    are live at the same time. The short outage may be acceptable, for example, if
    the service pulls its work from a queue and there are no adverse consequences
    if the service is briefly down while upgrading to a new version. Another situation
    is if you want to change the public API of the service or one of its dependencies
    in a non-backward compatible way. In this case, the current pods must be terminated
    in one fell swoop, and the new pods must be deployed. There are solutions for
    multi-phase deployments of incompatible changes, but in some cases, it is easier
    and acceptable to just cut the cord and pay the cost of a short outage.
  prefs: []
  type: TYPE_NORMAL
- en: 'To enable this strategy, edit the deployment''s manifest, change the strategy
    type to be `Recreate`, and remove the `rollingUpdate` section (this is only allowed
    when the type is `RollingUpdate`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: For most services, it is desirable to have service continuity and zero downtime
    when upgrading, as well as an instant rollback in case a problem is detected.
    The `RollingUpdate` strategy addresses these situations.
  prefs: []
  type: TYPE_NORMAL
- en: Rolling updates
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The default deployment strategy is `RollingUpdate`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Rolling updates work as follows: the total number of pods (old and new) is
    going to be the current replica count, plus the max surge. The deployment controller
    will start replacing old pods with new pods, making sure not to exceed the limit.
    The max surge can be an absolute number, such as 4, or a percentage, such as 25%.
    For example, if the number of replicas for the deployment is 4 and the max surge
    is 25%, then an additional new pod can be added, and one of the old pods can be
    terminated. `maxUnavailable` is the number of pods that are below the replica
    count during a deployment.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram illustrates how rolling updates work:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/56de03e9-954a-4146-bb61-287e67118e16.png)Rolling update'
  prefs: []
  type: TYPE_NORMAL
- en: Rolling updates make sense when the new version is compatible with the current
    version. The number of active pods that are ready to handle requests remains within
    a reasonable range of the replica count that you specify using `maxSurge` and
    `maxUnavailable`, and gradually, all the current pods are replaced with new pods.
    The overall service is not disrupted.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, however, you must replace all the pods at once, and for critical
    services that must remain available, the `Recreate` strategy doesn't work. This
    is where blue-green deployments come in.
  prefs: []
  type: TYPE_NORMAL
- en: Blue-green deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Blue-green deployment is a well-known pattern. The idea is that you don't update
    the existing deployment; instead, you create a brand new deployment with the new
    version. Initially, your new version doesn't service traffic. Then, when you verify
    that the new deployment is up and running (you can even run some `smoke` tests
    against it), you switch all the traffic in one fell swoop from the current version
    to the new version. If you encounter any problems after you switch to the new
    version, you can instantly switch all the traffic back to the previous deployment,
    which is still up and running. When you are confident that the new deployment
    is doing well, you can destroy the previous deployment.
  prefs: []
  type: TYPE_NORMAL
- en: One of the greatest advantages of blue-green deployments is that they don't
    have to operate at the level of a single Kubernetes deployment. This can be critical
    in a microservice architecture where you must update multiple interacting services
    at the same time. If you tried to do it just by updating multiple Kubernetes deployments
    at the same time, there could be some services that have already been replaced
    and some that weren't (even if you accept the cost of the `Recreate` strategy).
    If a single service experiences problems during deployment, you now have to roll
    back all the other services. With blue-green deployments, you are safe from these
    issues and are in total control of when you want to switch to the new version
    across all services at once.
  prefs: []
  type: TYPE_NORMAL
- en: How do you switch from blue (current) to green (new)? The traditional approach
    that works with Kubernetes is to do it at the load balancer level. Most systems
    that require such a sophisticated deployment strategy will have a load balancer.
    When you use the load balancer to switch traffic, your green deployment includes
    both a green Kubernetes deployment and a green Kubernetes service, as well as
    any other resources if anything needs to change, such as secrets and config maps.
    If you need to update multiple services, then you'll have a collection of green
    resources that all refer to each other.
  prefs: []
  type: TYPE_NORMAL
- en: If you have an Ingress controller such as contour, then it can often be used
    to switch traffic from blue to green and back, if necessary.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram illustrates how blue-green deployments work:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/2943332f-b426-4dd6-9127-7b0943e69ba9.png)'
  prefs: []
  type: TYPE_IMG
- en: Blue-green deployment
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s do a single-service blue-green deployment for the link manager service.
    We''ll call our starting point *blue*, and we want to deploy the *green* version
    of link manager without disruption. Here''s the plan:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Add the `deployment: blue` label to the current `link-manager` deployment.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Update the `link-manager` service selector to match the `deployment: blue`
    label.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Implement the new version of `LinkManager` that prefixes the description of
    each link with the `[green]` string.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Add the `deployment: green` label to the deployment''s pod template spec.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Bump the version number.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let CircleCI create a new version.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deploy the new version as a separate deployment called `green-link-manager`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Update the `link-manager` service selector to match the `deployment: green`
    label.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Verify the description of the returned links from the service and include the
    `[green]` prefix.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This may sound complicated, but just like many CI/CD processes, once you establish
    a pattern, you can automate and build tooling around it. This lets you execute
    complex workflows without human involvement, or inject human review and approval
    at important junctures (for example, before really deploying to production). Let's
    go over the steps in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Adding deployment – the blue label
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can just edit the deployment and manually add `deployment: blue`, in addition
    to the existing `svc: link` and `app: manager` labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'This will trigger a redeployment of the pods because we changed the labels.
    Let''s verify that the new pods have the `deployment: blue` label. Here is a pretty
    fancy `kubectl` command that uses custom columns to display the name, the deployment
    label, and the IP addresses of all the pods that match `svc=link` and `app=manager`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, all three pods have the `deployment:blue` label, as expected:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We can even verify that the IP addresses match the endpoints of the `link-manager`
    service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Now that the pods are labeled with the `blue` label, we need to update the service.
  prefs: []
  type: TYPE_NORMAL
- en: Updating the link-manager service to match blue pods only
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The service, as you may recall, matches any pods with the `svc: link` and `app:
    manager` labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'By adding the `deployment: blue` label, we didn''t interfere with the matching.
    However, in preparation for our green deployment, we should make sure that the
    service only matches the pods of the current blue deployment.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s add the `deployment: blue` label to the service''s `selector`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'We can verify that it worked by using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Before we switch to the green version, let's make a change in the code to make
    it clear that's it's a different version.
  prefs: []
  type: TYPE_NORMAL
- en: Prefixing the description of each link with [green]
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's do this in the transport layer of the link service.
  prefs: []
  type: TYPE_NORMAL
- en: The target file is [https://github.com/the-gigi/delinkcious/blob/master/svc/link_service/service/transport.go#L26](https://github.com/the-gigi/delinkcious/blob/master/svc/link_service/service/transport.go#L26).
  prefs: []
  type: TYPE_NORMAL
- en: 'The change is very minimal. In the `newLink()` function, we will prefix the
    description with the `[green]` string:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: In order to deploy our new green version, we need to create a new image. This
    requires bumping the Delinkcious version number.
  prefs: []
  type: TYPE_NORMAL
- en: Bumping the version number
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Delinkcious version is maintained in the `[build.sh]` file at ([https://github.com/the-gigi/delinkcious/blob/master/build.sh#L6](https://github.com/the-gigi/delinkcious/blob/master/build.sh#L6))
    that CircleCI is invoked from, that is, the `[.circleci/config.yml]` file at ([https://github.com/the-gigi/delinkcious/blob/master/.circleci/config.yml#L28](https://github.com/the-gigi/delinkcious/blob/master/.circleci/config.yml#L28))
    file.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `STABLE_TAG` variable controls the version numbers. The current version
    is `0.3`. Let''s bump it up to `0.4`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: OK. We bumped the version and we're ready to have CircleCI build a new image.
  prefs: []
  type: TYPE_NORMAL
- en: Letting CircleCI build the new image
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Thanks to GitOps and our CircleCI automation, this step just involves pushing
    our changes to GitHub. CircleCI detects the change, builds the new code, creates
    a new Docker image, and pushes it to the Docker Hub registry. Here it is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/5111b458-c9d5-4b26-97f0-09cfef35f829.png)'
  prefs: []
  type: TYPE_IMG
- en: Docker Hub link service 0.4
  prefs: []
  type: TYPE_NORMAL
- en: Now that the new image has been built and pushed to the Docker Hub registry,
    we can deploy it to the cluster as the green deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying the new (green) version
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'OK – we''ve got our new `delinkcious-link:0.4` image on Docker Hub. Let''s
    deploy it to the cluster. Remember that we want to deploy it side by side with
    our current (blue) deployment, which is called `link-manager`. Let''s create a
    new deployment called `green-link-manager`. The differences it has to our blue
    deployment are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The name is `green-link-manager`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The pod template spec has the `deployment: green` label.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The image version is `0.4`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, it''s time to deploy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Before we update the service to use the green deployment, let''s review the
    cluster. As you can see, we have the blue and green deployments running side by
    side:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Updating the link-manager service to use the green deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, let''s make sure that the service is still using the blue deployment.
    When we get a link description, there shouldn''t be any `[green]` prefix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The description is `nothing to see here..*.*`. This time, instead of interactively
    editing the service using `kubectl edit`, we will use the `kubectl patch` command
    to apply a patch that will switch the deployment label from `blue` to `green`.
    Here is the patch file – `green-patch.yaml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s apply the patch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The last step is to verify that the service now uses the green deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Verifying that the service now uses the green pods to serve requests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s do this methodically, starting with the selector in the service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'OK – the selector is green. Let''s get the links again and see if the `[green]`
    prefix shows up:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Yes! The description is now `[green] nothing to see here...`
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can get rid of the blue deployment and our service will continue running
    against the green deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: We have successfully performed a blue-green deployment on Delinkcious. Let's
    discuss the last pattern, that is, canary deployments.
  prefs: []
  type: TYPE_NORMAL
- en: Canary deployments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Canary deployments are another sophisticated deployment pattern. Consider the
    situation of a massive distributed system with lots of users. You want to introduce
    a new version of the service. You have tested this change to the best of your
    ability, but the production system is too complex to mimic completely in a staging
    environment. As a result, you can't be sure that your new version will not cause
    some problems. What do you do? You use canary deployments. The idea is that some
    changes must be tested in production before you can be reasonably sure they work
    as expected. The canary deployment patterns allow you to limit the damage the
    new version might cause if something goes wrong.
  prefs: []
  type: TYPE_NORMAL
- en: Basic canary deployments on Kubernetes work by running the current version on
    most of your pods, and just a small number of pods with the new version. Most
    of the requests will be processed by the current version, and only a small proportion
    will be processed by the new version.
  prefs: []
  type: TYPE_NORMAL
- en: This assumes a round-robin load balancing algorithm (the default), or any other
    algorithm that distributes requests more or less uniformly across all pods.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram illustrates what canary deployments look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/a183a687-f439-473e-95f6-4b69491d0f41.png)'
  prefs: []
  type: TYPE_IMG
- en: Canary deployment
  prefs: []
  type: TYPE_NORMAL
- en: Note that canary deployments require that your current version and your new
    version can coexist. For example, if your change involved a schema change, then
    your current and new versions are incompatible, and naive canary deployment will
    not work.
  prefs: []
  type: TYPE_NORMAL
- en: 'The nice thing about the basic canary deployment is that it works using existing
    Kubernetes objects and can be configured by an operator from the outside. There''s
    no need for custom code or installing additional components into your cluster.
    However, the basic canary deployment has several limitations:'
  prefs: []
  type: TYPE_NORMAL
- en: The granularity is K/N (the worst case is singletons where N = 1).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can't control different percentages for different requests to the same service
    (for example, canary deployments of read requests only).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can't control all requests for a user who goes to the same version.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In some cases, these limitations are too severe and another solution is needed.
    Sophisticated canary deployments typically utilize application-level knowledge.
    This can be done through Ingress objects, a service mesh, or a dedicated application-level
    traffic shaper. We will look at an example of this in [Chapter 13](b39834c8-859c-42a5-846a-e48b76dfd6cc.xhtml),
    *Service Mesh – Working with Istio*.
  prefs: []
  type: TYPE_NORMAL
- en: It's time for a hands-on canary deployment of the link service.
  prefs: []
  type: TYPE_NORMAL
- en: Employing a basic canary deployment for Delinkcious
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Creating a canary deployment is very similar to blue-green deployment. Our
    `link-manager` service currently runs the green deployment. This means that it
    has a selector with `deployment: green`. Canaries are yellow, so we will create
    a new version of the code that prefixes the link description with `[yellow]`.
    Let''s aim for 10% of requests going to the new version. In order to achieve this,
    we will scale the current versions to nine replicas and add a deployment with
    one pod with the new version. Here is the canary trick – we will drop the deployment
    label from the service selector. This means that it will select both pods; that
    is, `deployment: green` and `deployment: yellow`. We could also drop the labels
    from the deployments (because nobody is selecting based on this label), but it''s
    good to keep them around as metadata, and also in case we want to do another blue-green
    deployment.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the plan:'
  prefs: []
  type: TYPE_NORMAL
- en: Build a new version of the code.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Create a deployment with a replica count of one for the new version, which
    is labeled as `deployment: yellow`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Scale the current green deployment to nine replicas.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Update the service to select for `svc: link` and `app: manager` (ignore `deployment:
    <color>`).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run multiple queries against the service and verify that the ratio of requests
    that are being served by the canary deployment is 10%.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The code change is `trivial: [green]  -> [yellow]`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we need to bump the version in `build.sh` from `0.4` to `0.5`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we push these changes to GitHub, CircleCI will build and push a new image
    to `DockerHub: g1g1/delinkcious-link:0.5`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, we can create a deployment with the new `0.5` version, a single
    replica, and updated labels. Let''s call it `yellow_link_manager.yaml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is deploying our canary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Before changing the service to include the canary deployment, let''s scale
    the green deployment to 9 replicas so that it can receive 90% of the traffic once
    we activate our canary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Alright, we have nine green pods and one yellow (canary) pod running. Let''s
    update the service to select just based on the `svc: link` and `app: manager` labels,
    which will include all ten pods. We need to remove the `deployment: green` label.'
  prefs: []
  type: TYPE_NORMAL
- en: The YAML patch file method we've used before doesn't work here, because it can
    only add or update a label. We'll use a JSON patch this time with the *remove*
    operation and specify the path to the *deployment* key in the selector.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that before the patch, the selector had `deployment: green`, and that
    after the patch, only `svc: link` and `app: manager` remain:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'It''s showtime. We''ll send 30 GET requests to Delinkcious and check the description:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Interesting – we've got 24 green responses and 6 yellow responses. This is much
    higher than expected (three yellow responses on average). I ran it a couple more
    times and got six yellow responses again for the second run, and just one yellow
    response for the third run. This is all running on Minikube, so load balancing
    may be a little special. Let's declare victory.
  prefs: []
  type: TYPE_NORMAL
- en: Using canary deployments for A/B testing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Canary deployments can also be used to support A/B testing. We can deploy as
    many versions as we want, as long as we have enough pods to juggle the load. Each
    version could include special code to log the relevant data, and then you can
    gain insights and correlate user behavior with specific versions. This is possible,
    but you'll probably have to build a lot of tooling and conventions to make it
    usable. If A/B testing is an important part of your design workflow, I recommend
    going with one of the established A/B testing solutions. The A/B testing wheel
    is not worth reinventing, in my opinion.
  prefs: []
  type: TYPE_NORMAL
- en: Let's consider what to do when things go wrong and we need to get back to a
    working state as soon as possible.
  prefs: []
  type: TYPE_NORMAL
- en: Rolling back deployments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When things go wrong in production after a deployment, the best-practice response
    is to roll back the changes and get back to the last previous version known to
    work. The way you go about this depends on the deployment pattern you've employed.
    Let's consider them one by one.
  prefs: []
  type: TYPE_NORMAL
- en: Rolling back standard Kubernetes deployments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Kubernetes deployments keep a history. For example, if we edit the user manager
    deployment and set the image version to `0.5`, then we can see that there are
    two revisions now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The `CHANGE-CAUSE` column is not recorded by default. Let''s make another change
    to version 0.4, but using the `--record=true` flag:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'OK. Let''s roll back to the original 0.3 version. That would be revision 1\.
    We can look at this by using the `rollout history` command at specific revisions,
    too:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, revision 1 has version 0.3\. The command to roll back is as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Rolling back will use the same mechanics of a rolling update, gradually replacing
    pods until all the running pods have the correct version.
  prefs: []
  type: TYPE_NORMAL
- en: Rolling back blue-green deployments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Blue-green deployments are not supported directly by Kubernetes. Switching
    back from green to blue (assuming that the blue deployment''s pods are still running)
    is very simple. You just change the `Service` selector and select `deployment:
    blue` instead of `deployment: green`. The instant switch from blue to green and
    vice versa is the main motivation for the blue-green deployment pattern, so it''s
    no wonder that it''s that easy. Once you''ve switched back to blue, you can delete
    the green deployment and figure out what went wrong.'
  prefs: []
  type: TYPE_NORMAL
- en: Rolling back canary deployments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Canary deployments are arguably even simpler to roll back. The majority of your
    pods run the tried and true version. The canary deployment's pods serve just a
    small amount of requests. If you detect that something is wrong with the canary
    deployment, simply delete the deployment. Your main deployment will keep serving
    incoming requests. If necessary (for example, your canary deployment served a
    small but significant amount of traffic), you can scale up your main deployment
    to make up for the canary pods that are no longer there.
  prefs: []
  type: TYPE_NORMAL
- en: Dealing with a rollback after a schema, API, or payload change
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The deployment strategy you choose often depends on the nature of the change
    the new version introduces. For example, if your change involved a breaking database
    schema change, such as splitting table A into two tables, B and C, then you can't
    simply deploy the new version that reads to/writes from B and C. The database
    needs to be migrated first. However, if you run into problems and want to roll
    back to the previous version, then you'll have the same problem in the reverse
    direction. Your old version will try and read from/write to table A, which doesn't
    exist anymore. The same issue can happen if you change the format of a configuration
    file or payload on some network protocol. API changes can break clients if you
    don't coordinate them.
  prefs: []
  type: TYPE_NORMAL
- en: The way to address those compatibility issues is to perform those changes across
    multiple deployments, where each deployment is fully compatible with the previous
    deployment. This takes some planning and work. Let's consider the case of splitting
    table A into tables B and C. Suppose we're on version 1.0 and eventually want
    to end up with version 2.0.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our first change will be marked as version 1.1\. It will perform the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Create tables B and C (but leave table A in place).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Change the code to write to B and C.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Change the code to read from both A, B, and C and merge the results (old data
    comes from A, while new data comes from B and C).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If data needs to be deleted, it is just marked as deleted instead.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We deploy version 1.1 and if we see that something is wrong, we will roll back
    to version 1.0\. All our old data is still in table A, which version 1.0 is fully
    compatible with. We might have lost or corrupted a small amount of data in tables
    B and C, but that's the price we have to pay for not testing properly earlier.
    Version 1.1 could have been a canary deployment, so only a small amount of requests
    have been lost.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we discover the issues, fix them, and deploy version 1.2, which is just
    like how version 1.1 writes to B and C, but reads from A, B, and C and doesn't
    delete data from A.
  prefs: []
  type: TYPE_NORMAL
- en: We observe for a while until we're confident that version 1.2 works as expected.
  prefs: []
  type: TYPE_NORMAL
- en: The next step is to migrate the data. We write the data in table A into tables
    B and C. The active deployment, version 1.2, keeps reading from B and C and only
    merges missing data from A. We still keep all the old data in A until we finish
    all code changes.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, all the data is in tables B and C. We deploy version 1.3, which
    ignores table A and works fully against tables B and C.
  prefs: []
  type: TYPE_NORMAL
- en: We observe again, and can go back to version 1.2 if we encounter any problems
    with 1.3 and release version 1.4, 1.5, and so on. However, at some point, our
    code will work as expected and then we can rename/retag the final version as 2.0,
    or just cut a new version that is identical except for the version number.
  prefs: []
  type: TYPE_NORMAL
- en: The last step is to delete table A.
  prefs: []
  type: TYPE_NORMAL
- en: This can be a slow process that requires running a lot of tests whenever deploying
    a new version, but it is necessary when you're making dangerous changes that have
    the potential to corrupt your data.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, you'll back up your data before starting, but for high-throughput
    systems, even short outages during bad upgrades can be very costly.
  prefs: []
  type: TYPE_NORMAL
- en: The bottom line is that updates that include schema changes are complicated.
    The way to manage it is to perform a multi-phase upgrade, where each phase is
    compatible with the previous phase. You move forward only when you have proved
    that the current phase works correctly. The benefit of the principle of a single
    microservice owning each data store is that at least DB schema changes are constrained
    to a single service, and don't require coordination across multiple services.
  prefs: []
  type: TYPE_NORMAL
- en: Managing versions and dependencies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Managing versions is a tricky topic. In microservice-based architecture, your
    microservices may have many dependencies, as well as many clients, both internal
    and external. There are several categories of versioned resources, and they all
    require different management strategies and versioning schemes.
  prefs: []
  type: TYPE_NORMAL
- en: Managing public APIs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Public APIs are network APIs that are used outside the cluster, often by a large
    number of users and/or developers who may or may not have a formal relationship
    with your organization. Public APIs may require authentication, but sometimes
    may be anonymous. The versioning scheme for public APIs typically involves just
    a major version, such as V1, V2, and so on. The Kubernetes API is a good example
    of such a versioning scheme, although it also has the concept of API groups and
    uses alpha and beta qualifiers because it caters to developers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Delinkcious has a single public API that used the `<major>.<minor>` versioning
    scheme up to this point:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'This is overkill, and a major version only is enough. Let''s change it (and
    all the impacted tests, of course):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Note that we keep the same version, even when we make breaking changes during
    this book. This is fine because there are no external users so far, so we have
    the liberty to change our public API. However, once we officially release our
    application, we are obligated to consider the burden on our users if we make breaking
    changes without changing the API version. This is is a pretty bad anti-pattern.
  prefs: []
  type: TYPE_NORMAL
- en: Managing cross-service dependencies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Cross-service dependencies are often well defined and documented as internal
    APIs. However, subtle changes to the implementation and/or to the contract can
    significantly impact other services. For example, if we change the structs in
    `object_model/types.go`, a lot of code might have to be modified. In a well-tested
    mono-repo, this is less of a problem because the developer who makes the changes
    can ensure that all the relevant consumers and tests were updated. Many systems
    are built out of multiple repositories, and it might be challenging to identify
    all the consumers. In these cases, breaking changes can remain and be discovered
    after deployment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Delinkcious is a mono-repo, and it is actually not using any versioning scheme
    at all in the URLs of its endpoints. Here is the social graph manager''s API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: This approach is acceptable if you never intend to run multiple versions of
    the same service. In large systems, this is not a scalable approach. There will
    always be some consumers that don't want to upgrade to the latest and greatest
    immediately.
  prefs: []
  type: TYPE_NORMAL
- en: Managing third-party dependencies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are three flavors of third-party dependencies:'
  prefs: []
  type: TYPE_NORMAL
- en: Libraries and packages you build your software against (as discussed in [Chapter
    2](d4214218-a4e9-4df8-813c-e00df71da935.xhtml), *Getting Started with Microservices*)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Third-party services that are accessed through an API by your code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Services you use to operate and run your system
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, if you run your system in the cloud, then your cloud provider is
    a huge dependency (Kubernetes can help mitigate risk). Another great example is
    using a third-party service as your CI/CD solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'When choosing a third-party dependency, you relinquish some (or a lot) control.
    You should always consider what happens if the third-party dependency suddenly
    becomes unavailable or unacceptable. There are many reasons why this can happen:'
  prefs: []
  type: TYPE_NORMAL
- en: Open source project abandoned or loses steam
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Third-party provider shuts down
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Library has too many security vulnerabilities
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Service has too many outages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Assuming that you''ve picked your dependencies wisely, let''s consider two
    cases:'
  prefs: []
  type: TYPE_NORMAL
- en: Upgrading to a new version of a library
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Upgrading to a new API version of a third-party service
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Every such upgrade requires the corresponding upgrade of any component (a library
    or service) in your system that uses these dependencies. Typically, these upgrades
    shouldn't modify the API of any of your services, nor the public interfaces of
    your libraries. They may change the operational profile of your services (hopefully
    for the better, as in less memory, more performance).
  prefs: []
  type: TYPE_NORMAL
- en: Upgrading your services is a simple matter. You just deploy the new version
    of your service that depends on the new third-party dependency and you're good
    to go. Changes to third-party libraries can be a little more involved. You need
    to identify all of your libraries that depend on this third-party library. Upgrade
    your libraries and then identify every service that uses any of your (now-upgraded)
    libraries and upgrade those services too.
  prefs: []
  type: TYPE_NORMAL
- en: It is highly recommended to use semantic versioning for your libraries and packages.
  prefs: []
  type: TYPE_NORMAL
- en: Managing your infrastructure and toolchain
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Your infrastructure and toolchain must be managed carefully too, and even versioned.
    In a large system, your CI/CD pipeline will typically invoke various scripts that
    automate important tasks, such as migrating databases, preprocessing data, and
    provisioning cloud resources. These internal tools can change dramatically. Another
    important category in container-based systems are the versions of your base images.
    The infrastructure of code approach, combined with GitOps, advocates versioning
    and storing those aspects of your system in your source control system (Git) as
    well.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we've covered a lot of dark corners and difficult use cases regarding
    real-world deployments and how to evolve and upgrade large systems safely and
    reliably. Let's get back to the individual developer. There is a very different
    set of requirements and concerns for developers that need a quick edit-test-debug
    cycle in the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Local development deployments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Developers want fast iterations. When I make a code change to some code, I want
    to run the tests as soon as possible, and if something is wrong, to fix it as
    soon as possible. We've seen how well this works with unit tests. However, when
    the system uses a microservice architecture packaged as containers and deployed
    to a Kubernetes cluster, this is not enough. To truly evaluate the impact of a
    change, we often have to build an image (which may include updating Kubernetes
    manifests like Deployments, Secrets, and ConfigMaps) and deploy it to the cluster.
    Developing locally against Minikube is awesome, but even deploying to a local
    Minikube cluster takes time and effort. In [Chapter 10](08f2d84f-2a24-473c-9b0d-20e7fc87fd70.xhtml),
    *Testing Microservices*, we used Telepresence to great effect for interactive
    debugging. However, Telepresence has its own quirks and downsides, and it's not
    always the best tool for the job. In the following subsections, we'll cover several
    other alternatives that may be a better choice in certain circumstances.
  prefs: []
  type: TYPE_NORMAL
- en: Ko
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ko ([https://github.com/google/ko](https://github.com/google/ko)) is a very
    interesting Go-specific tool. Its goal is to streamline and hide the process of
    building images. The idea is that, in your Kubernetes deployment, you replace
    the image path from the registry with a Go import path. Ko will read this import
    path, build a Docker image for you, publish it to a registry (or locally if using
    Minikube), and deploy it to your cluster. Ko provides ways to specify a base image
    and include static data in the generated image.
  prefs: []
  type: TYPE_NORMAL
- en: Let's give it a try and discuss the experience later.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can install Ko through the standard `go get` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Ko requires that you work in `GOPATH`. I don''t typically work inside `GOPATH`
    for various reasons (Delinkcious use Go modules that don''t require `GOPATH`).
    To accommodate Ko, I used the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Here, I have replicated the directory structure Go expects under `GOPATH`, including
    replicating the path on GitHub to Delinkcious. Then, I got all the dependencies
    of Delinkcious recursively using `go get -d ./...`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The last preparatory step is to set Ko for local development. When Ko builds
    an image, we shouldn''t push it to Docker Hub or any remote registry. We want
    a fast local loop. Ko allows you to do this in various ways. One of the simplest
    ways is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Other ways include a configuration file or passing the `-L` flag when running
    Ko.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we can go ahead and use Ko. Here is the `ko-link-manager.yaml` file where
    the image is replaced with the Go import path to the link manager service (`github.com/the-gigi/delinkcious/svc/link_service`).
    Note that I changed `imagePullPolicy` from `Always` to `IfNotPresent`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `Always` policy is the secure and production-ready policy, but when working
    locally, it will ignore the local Ko images and will instead pull from Docker
    Hub:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is running Ko on this modified deployment manifest:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'To test the deployment, let''s run our `smoke` test:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Everything looks good. The link description contains the `[yellow]` prefix
    from our canary deployment work. Let''s change it to `[ko]` and see how fast Ko
    can redeploy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Running Ko again on the modified code takes just 19 seconds, all the way to
    deployment in the cluster. That''s impressive:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'The `smoke` test works and the description now contains the `[ko]` prefix instead
    of `[yellow]`, which proves that Ko works as advertised and indeed built a Docker
    container very quickly and deployed it to the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s take a look at the image that Ko built. In order to do that, we will
    `ssh` into the Minikube node and check the Docker images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: The image appears to have a creation date of the beginning of the Unix epoch
    (1970) for some reason. Other than that, everything looks good. Note that the
    image is larger than our normal link manager because Ko uses [gcr.io/distroless/base:latest](http://gcr.io/distroless/base:latest) as
    a base image by default, while Delinkcious uses the SCRATCH image. You can override
    the base image if you want, using a `.ko.yaml` configuration file.
  prefs: []
  type: TYPE_NORMAL
- en: 'In short, Ko is easy to install, configure, and it works very well. Still,
    I find it too limited:'
  prefs: []
  type: TYPE_NORMAL
- en: It is a Go-only tool.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You must have your code in `GOPATH` and use the standard Go directory structure
    (obsolete with Go 1.11+ modules).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You have to modify your manifests (or create a copy with the Go import path).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It may be a good option to test new Go services before integrating them into
    your CI/CD system.
  prefs: []
  type: TYPE_NORMAL
- en: Ksync
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Ksync is a very interesting tool. It doesn''t build images at all. It syncs
    files directly between a local directory, and a remote directory inside a running
    container in your cluster. It doesn''t get more streamlined than that, especially
    if you sync to a local Minikube cluster. This awesomeness comes with a price,
    though. Ksync works especially well for services that are implemented using dynamic
    languages, such as Python and Node, that can hot-reload the application when changes
    are synced. If your application doesn''t do hot reloading, Ksync can restart the
    container after each change. Let''s get to work:'
  prefs: []
  type: TYPE_NORMAL
- en: Installing Ksync is very simple, but remember to check what you are installing
    before just piping it to `bash`!
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'If you prefer, you can install it with a `go` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'We also need to start the cluster-side component of Ksync, which will create
    a DaemonSet on every node to listen for changes and reflect them into running
    containers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can tell Ksync to watch for changes. This is a blocking operation and
    Ksync will watch forever. We can run it in a separate Terminal or tab:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'The last part of the setup is to establish a mapping between a local directory
    and a remote directory on a target pod or pods. As usual, we identify the pods
    via a label selector. The only Delinkcious service that uses a dynamic language
    is the API gateway, so we''ll use this here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'We can test that Ksync works by modifying our API gateway. Let''s add a Ksync
    message to our `get()` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'A few seconds later, we will see the `Yeah, it works!` message from Ksync.
    This is a great success:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: To recap, Ksync is extremely streamlined and fast. I really like the fact that
    it doesn't bake images, push them to a registry, and then deploy to the cluster.
    If all your workloads use a dynamic language, then using Ksync is a no-brainer.
  prefs: []
  type: TYPE_NORMAL
- en: Draft
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Draft is another tool from Microsoft (originally from Deis) that lets you quickly
    build images without a Dockerfile. It uses standard buildpacks for various languages.
    It doesn''t seem like you can provide your own base image. This is a problem for
    two reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: Your service may be more than just code, and may depend on things that you set
    up in the Dockerfile.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The base images that Draft uses are pretty big.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Draft depends on Helm, so you must have Helm installed on your cluster. The
    installation is very versatile and supports many methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can be sure that Draft works well on Windows, unlike many other tools in
    the cloud-native area where Windows is a second-class citizen. This mindset is
    starting to change since Microsoft, Azure, and AKS are prominent contributors
    to the Kubernetes ecosystem. OK, let''s take Draft for a test drive:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Installing `draft` on macOS (assuming you''ve installed Helm already) is as
    simple as doing the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s configure Draft to push its images directly to Minikube (the same as
    Ko):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'As usual, let''s add a prefix, `[draft]`, to the description:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we let draft prepare by calling the `draft create` command and also choosing
    the Helm release name using `--app`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can deploy to the cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: Unfortunately, draft hung in the `Pushing Docker Image` stage. It worked for
    me in the past, so perhaps it's a new issue with the latest versions.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, draft is pretty simple, but too limited. The big images it creates
    and the inability to provide your own base images are deal breakers. The documentation
    is very sparse, too. I recommend using it only if you're on Windows and the other
    tools don't work well enough.
  prefs: []
  type: TYPE_NORMAL
- en: Skaffold
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Skaffold ([https://skaffold.dev/](https://skaffold.dev/)) is a very complete
    solution. It is very flexible, supports both local development and integration
    with CI/CD, and has excellent documentation. Here are some of the features of
    Skaffold:'
  prefs: []
  type: TYPE_NORMAL
- en: Detect code changes, build image, push, and deploy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can sync source files to pods directly (just like Ksync).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It has a sophisticated conceptual model with builders, testers, deployers, tag
    polices, and push strategies.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can customize every aspect.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integrate with your CI/CD pipeline by running Skaffold from end to end, or use
    specific stages as building blocks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Per-environment configuration via profiles, user-level config, environment variables,
    or command-line flags.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is a client-side tool – there is no need to install anything in your cluster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Automatically forward container ports to the local machine.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Aggregate logs from the deployed pods.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is a diagram that illustrates the workflow of Skaffold:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/bf28c932-48d4-457f-bf40-9049f773ad0e.png)'
  prefs: []
  type: TYPE_IMG
- en: Skaffold
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s install Skaffold and take it for a ride:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let''s create a configuration file in the `link_service` directory. Skaffold
    will ask us some questions about which Dockerfile to use for different elements,
    such as the database and the service itself:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s try to build an image with Skaffold:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'Oh, no – it fails. I did some searching and there is an open issue:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: Skaffold is a big solution. It does much more than just local development. It
    has a non-trivial learning curve, too (for example, syncing files requires manually
    setting up each directory and file type). If you like its model and you use it
    in your CI/CD solution, then it makes sense to use it for local development as
    well. Definitely check it out and make up your own mind. The fact that it can
    build images as well as directly sync files is a big plus if you have a hybrid
    system similar to Delinkcious.
  prefs: []
  type: TYPE_NORMAL
- en: Tilt
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Last, but absolutely not least, is Tilt. Tilt is my favorite development tool
    by far. Tilt is also very comprehensive and flexible. It is centered around a
    Tiltfile written in a language called Starlark ([https://github.com/bazelbuild/starlark/](https://github.com/bazelbuild/starlark/)),
    which is a subset of Python. I was hooked right way. What's special about Tilt
    is that it goes beyond just building an image automatically and deploying it to
    the cluster or syncing files. It actually gives you a complete live development
    environment that presents a lot of information, highlights events and errors,
    and lets you drill down and understand what's happening in your cluster. Let's
    get started.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s install Tilt and then get to business:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: I wrote a Tiltfile for the link service that's very generic.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s break this down and analyze it. First, we need all the YAML files under
    the k8s subdirectory. We could just hard code them, but where''s the fun in that?
    Also, there will be a different list of YAML files for different services. Skylark
    is Python-like, but you can''t use Python libraries. For example, the glob library
    is great for enumerating files with wildcards. Here is the Python code to list
    all files with the `.yaml` suffix in the `k8s` subdirectory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'We can''t do that directly in Starlark, but we can use the `local()` function,
    which allows us to run any command and capture the output. Therefore, we can execute
    the previous Python code by running the Python interpreter with a little script
    through Tilt''s `local()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: There are a few extra details here. First, we convert the list of files returned
    from glob into a comma-separated string. However, the `local()` function returns
    a Tilt object called Blob. We just want a plain string, so we convert the blob
    into a string by wrapping the `local()` call with the `str()` function. Finally,
    we remove the last character (the final `[:-1]`), which is a newline (because
    we used Python's `print()` function).
  prefs: []
  type: TYPE_NORMAL
- en: The end result is that, in the `yaml_files` variable, we have a string that
    is a comma-separated list of all the YAML manifests.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we split this comma-separated string back into a Python/Starlark list
    of file names:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'For each of these files, we call Tilt''s `k8s_yaml()` function. This function
    tells Tilt to monitor these files for changes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: Next, we repeat the same trick as before and execute a Python one-liner that
    extracts the service name from the current directory name. All the Delinkcious
    service directories follow the same naming convention, that is, `<service name>_service`.
    This one-liner splits the current directory, disposes of the last component (which
    is always `service`), and joins the components back via `-` as a separator.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we need to get the service name:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have the service name, the final step is to build the image by
    calling Tilt''s `docker_build()` function. Remember that the naming convention
    for Docker images that Delinkcious uses is `g1g1/delinkcious-<service name>`.
    I am also using a special `Dockerfile.dev` here, which is different than the production
    Dockerfile, and is more convenient for debugging and troubleshooting. If you don''t
    specify a Docker file, then the default is `Dockerfile`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: This may seem very complicated and convoluted, but the benefit is that I can
    drop this file in any service directory and it will work as is.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the link service, the equivalent hardcoded file would be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: That's not too bad, but every time you add a new manifest, you have to remember
    to update your Tiltfile, and you'll need to keep a separate Tiltfile for each
    service.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see Tilt in action. When we type `tilt up`, we will see the following
    text UI:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/61e22ed1-9d4e-4a31-b388-80021dc862a6.png)'
  prefs: []
  type: TYPE_IMG
- en: Tilt
  prefs: []
  type: TYPE_NORMAL
- en: There are many things you can do in the Tilt console, including checking logs
    and exploring errors. Tilt constantly displays updates and the status of the system,
    and always attempts to surface the most useful information.
  prefs: []
  type: TYPE_NORMAL
- en: 'It''s interesting to see that Tilt build images with its own tag:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s make our standard change and see how Tilt reacts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'Tilt detected the change and built a new image, then promptly deployed it to
    the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s try our hand at some file syncing. We must run Flask in debug mode for
    hot reloading to work. This is as simple as adding `FLASK_DEBUG=1` to `ENTRYPOINT`
    in the Dockerfile:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: 'It''s up to you to decide if you want a separate `Dockerfile.dev` file to use
    with Tilt, as we used for the link service. Here is a Tiltfile for the API gateway
    service that uses the live update facilities of Tilt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, we can run `tilt up` and hit the `/links` endpoint:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: 'Tilt will show us the request and the successful `200` response:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/835fcab4-2df6-498d-ac9f-812d89085516.png)'
  prefs: []
  type: TYPE_IMG
- en: Tilt API gateway
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s make a little change and see if tilt picks it up and syncs the code
    in the container. In the `resources.py` file, let''s add to the result of the GET
    links the key-value pair - `tilt: Yeah, sync works!!`.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see in the following screenshot, Tilt detected the code change in
    `resources.py` and copied the new file into the container:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/72b50246-0e8c-417b-8910-4c470a15f49d.png)'
  prefs: []
  type: TYPE_IMG
- en: Tilt API gateway 2
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s invoke the endpoint again and observe the results. It works as intended.
    We got the expected key-value after the links in the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: Overall, Tilt is extremely well done. It's based on a solid conceptual model,
    is very well executed, and it addresses the problems of local development better
    than any of the other tools. Tiltfile and Starlark are powerful and concise. It
    supports both full-fledged Docker builds and file syncing for dynamic languages.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered a broad swatch of topics related to deployments
    to Kubernetes. We started with a deep dive into the Kubernetes deployment object
    and considered and implemented deployments to multiple environments (for example,
    staging and production). We delved into advanced deployment strategies like rolling
    updates, blue-green deployments, and canary deployments, and experimented with
    all of them on Delinkcious. Then, we looked at rolling back failed deployments
    and the crucial topic of managing dependencies and versions. Later on, we switched
    gears into local development, and surveyed multiple tools for fast iterations
    where you make changes to your code, and they are automatically deployed to your
    cluster. We covered Ko, Ksync, Draft, Skaffold, and my personal favorite, Tilt.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, you should have a deep understanding of the various deployment
    strategies, when to employ them on your system, and good hands-on experience with
    local development tools for Kubernetes that you can integrate into your workflow.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will take it to the next level and get serious about
    monitoring our system. We will look into failure modes, how to design self-healing
    systems, autoscaling, provisioning, and performance. Then, we will consider logging,
    collecting metrics, and distributed tracing.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Refer to the following links if you want to find out more about what was covered
    in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '**KO**: [https://github.com/google/ko](https://github.com/google/ko)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ksync**: [https://vapor-ware.github.io/ksync/](https://vapor-ware.github.io/ksync/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Draft**: [https://draft.sh/](https://draft.sh/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Skaffold**: [https://skaffold.dev/](https://skaffold.dev/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tilt**: [https://docs.tilt.dev](https://docs.tilt.dev)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
