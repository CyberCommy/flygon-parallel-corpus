- en: Distributed Memory Management
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the last two decades, the industry has seen a paradigm shift to big data
    and machine learning architectures that involve processing terabytes/petabytes
    of data as quickly as possible. As computing power became cheaper, there was a
    need to use multiple processors to speed up processing to a larger scale. This
    has led to distributed computing. Distributed computing refers to an arrangement
    of computer systems that are connected via some networking/distribution middleware.
    All the connected systems share resources and coordinate their activities via
    middleware so that they work in a way that is perceived as a single system by
    the end user. Distributed computing is needed due to the huge volume and throughput
    requirements of modern applications. Some typical examples of scenarios where
    computing demands cannot be fulfilled by a single system and that need to be distributed
    across a grid of computers are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Google performs at least 1.5 trillion searches per year.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: IOT devices send multiple terabytes of data to event hubs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data warehouses receive and compute terabytes of records in minimal time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, we will discuss distributed memory management and the need
    for distributed computing. We will also learn about how messages are passed across
    communication networks for distributed systems, as well as various types of communicated
    networks.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Advantages of distributed systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shared memory model versus distributed memory model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Types of communication network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Properties of communication networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring topologies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Programming distributed memory machines using message passing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Collectives
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To complete this chapter, you'll need knowledge of programming in C and C# Windows
    platform API invocation programming.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to distributed systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have already discussed how distributed computing works in this book. In this
    section, we will try to understand distributed computing with a small example
    that works on arrays.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s say we have an array of 1,040 elements and we would like to find the
    sum of all the numbers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'If the total time that''s taken to add numbers is x (let''s say all of the
    numbers are large) and we want to compute them all as fast as possible, we can
    take advantage of distributed computing. We would divide the array into multiple
    arrays (let''s say, four arrays), each with 25% of the original number of elements,
    and send each array to a different processor to calculate the sum, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b7f7894c-f365-49da-af19-6ef8f87398a9.png)'
  prefs: []
  type: TYPE_IMG
- en: In this arrangement, the total time that's taken to add all the numbers is reduced
    to (x/4 + d) or (x/number of processors +d), where d is the time that's taken
    to collate the sums from all the processors and add them to get the final results.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the advantages of distributed systems are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Systems can be scaled to any level without any hardware restrictions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No single point of failure, which makes them more fault-tolerant
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Highly available
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Very efficient when handling big data problems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distributed systems are often confused with parallel systems, but there are
    subtle differences between them. **Parallel systems** are an arrangement of multi-processors
    that are placed mostly in single, but sometimes in multiple, containers in close
    vicinity. **Distributed systems**, on the other hand, consist of multiple processors
    (each having its own memory and I/O devices) that are connected together via a
    network that enables data exchange.
  prefs: []
  type: TYPE_NORMAL
- en: Shared versus distributed memory model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To achieve high performance, the **multi-processor** and **multi-computer** architectures
    have evolved. With the multi-processor architecture, multiple processors share
    a common memory and communicate with each other by reading/writing to the shared
    memory. With multi-computers, multiple computers that don't share a single physical
    memory communicate with each other by passing messages. **Distributed Shared Memory**
    (**DSM**) deals with sharing memory in a physical, non-shared (distributed) architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Let's look at each one and talk about their differences.
  prefs: []
  type: TYPE_NORMAL
- en: Shared memory model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the case of shared memory models, multiple processors share a single common
    memory space. Since multiple processors share memory space, there needs to be
    some synchronization measures in place to avoid data corruption and race conditions.
    As we have seen so far in this book, synchronization comes with performance overheads.
    The following is an example representation of the shared memory model. As you
    can see, there are **n** processors in the arrangement, all of which have access
    to a commonly shared memory block:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/eb451b6f-2779-413e-bc9a-455b1f0052fc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The features of the shared memory model are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'All the processors have access to the entire memory block. The memory block
    can be a single piece of memory composed of memory modules, as shown in the following
    diagram:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/7287eb33-3361-43f9-93b4-a602c63f4f90.png)'
  prefs: []
  type: TYPE_IMG
- en: Processors communicate with each other by creating shared variables in the main
    memory.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The efficiency of parallelization largely depends on the speed of the service
    bus.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Due to the speed of the service bus, the system can only be scaled up to n number
    of processors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shared memory models are also known as **symmetric multiprocessing** (**SMP**)
    models since all the processors have access to all the available memory blocks.
  prefs: []
  type: TYPE_NORMAL
- en: Distributed memory model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the case of the distributed memory model, the memory space is no longer
    shared across processors. In fact, the processors don''t share common physical
    locations; instead, they can be remotely placed. Each processor has its own private
    memory space and I/O devices. Data is stored across processors rather than in
    single memory. Each processor can work on its own local data, but to access data
    that''s been stored in other processor memories, they need to connect via a communication
    network. Data is passed via **message passing** across processors using the *send
    message* and *receive message* instructions. The following is a diagrammatic representation
    of a distributed memory model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b9184ffd-8815-494e-984a-75466e0d829b.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding diagram depicts each processor, along with its own memory space
    and interaction with **communication networks** via I/O interfaces. Let's try
    to understand the various types of communication networks that can be used in
    distributed systems.
  prefs: []
  type: TYPE_NORMAL
- en: Types of communication network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Communication networks are the links that connect two or more nodes in a typical
    computer network. Communication networks are classified into two categories:'
  prefs: []
  type: TYPE_NORMAL
- en: Static communication networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dynamic communication networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's take a look at both.
  prefs: []
  type: TYPE_NORMAL
- en: Static communication networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Static communication networks contain links, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dd8b6d89-e3e9-42dd-9add-60e7da9f3c9a.png)'
  prefs: []
  type: TYPE_IMG
- en: Links are used to connect nodes together, thereby creating a complete communication
    network where any node can talk to any other node.
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic communication networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Dynamic communication networks have links and switches, as shown in the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/747dae92-c2d8-4f26-b3f1-bf16ee8c33ae.png)'
  prefs: []
  type: TYPE_IMG
- en: Switches are devices that have input/output ports, and they redirect input data
    to output ports. This means that pathways are dynamic. If one processor wants
    to send data to another, it needs to be done via a switch, as demonstrated in
    the preceding diagram.
  prefs: []
  type: TYPE_NORMAL
- en: Properties of communication networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While designing a communication network, we need to consider the following
    characteristics:'
  prefs: []
  type: TYPE_NORMAL
- en: Topology
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Routing algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Switching strategy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Flow control
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's look at these characteristics in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Topology
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Topology refers to how nodes (bridges, switches, and infrastructure devices)
    are connected. Some common topologies include crossbar, ring, 2D mesh, 3D mesh,
    higherD mesh, 2D torus, 3D torus, higherD torus, hypercube, tree, butterfly, perfect
    shuffle, and dragonfly.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the case of the crossbar topology, every node in the network is connected
    to every other node (though they may not be connected directly). Thus, messages
    can be passed via a number of routes to avoid any conflicts. Here is a typical
    crossbar topology:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/72c0ae88-b930-4d0e-ac5c-3b30d31f49e2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the case of the mesh topology, or meshnet, as it''s popularly called, nodes
    connect to each other directly without having a dependency on other nodes in the
    network. This way, all the nodes can relay information independently. The mesh
    can be partially or fully connected. Here is a typical fully connected mesh:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8ba15fd5-ddef-42d3-ba5d-1a63f576c968.png)'
  prefs: []
  type: TYPE_IMG
- en: We will look at topology in more detail later in this chapter, in the *Exploring
    topologies *section.
  prefs: []
  type: TYPE_NORMAL
- en: Routing algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Routing is a process via which a packet of information is sent across the network
    so that it reaches the intended node. Routing can be adaptive, that is, it responds
    to changes in the network topology by continuously taking information from adjacent
    nodes, or non-adaptive, that is, they are static and is where routing information
    is downloaded to nodes when the network is booted. Routing algorithms need to
    be chosen to make sure there are no deadlocks. For example, in 2D torus, all the
    pathways go from east to west and north to south to avoid any deadlock scenarios.
    We will look at 2D torus in more detail later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Switching strategy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Choosing an appropriate switching strategy can increase the performance of
    a network. The two most prominent switching strategies are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Circuit switching**: In circuit switching, the full path is reserved for
    an entire message, such as the telephone. To begin a call on a telephone network,
    a dedicated circuit needs to be established between the caller and callee and
    the circuit persists during the entire call duration.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Packet switching**: In packet switching, the message is broken into separately
    routed packets, such as the internet. In terms of cost benefits, it''s far better
    than circuit switching as the cost of the link is shared across users. Packet
    switching is primarily used for asynchronous scenarios such as sending emails
    or file transfer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Flow control
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Flow control is a process by which a network makes sure that packets are transferred
    across the sender and received efficiently and without error. In the case of the
    network topology, the speeds of the sender and receiver can vary, which can lead
    to bottlenecks or loss of packets in some cases. With flow control, we can make
    decisions in case there''s congestion on the network. Some strategies include
    storing data temporarily into buffers, rerouting data to other nodes, instructing
    source nodes to temporarily halt, discarding data, and so on. The following are
    some common flow control algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Stop and wait**: The entire message is broken into parts. The sender sends
    a part to the receiver and waits for an acknowledgement to come within a specific
    time period (timeout). Once the sender receives an acknowledgment, it sends the
    next part of the message.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sliding window**: The receiver assigns a transmitting window for a sender
    to send messages. The sender has to stop transmitting when the window is full
    so that the receiver can process messages and advertise the next transmitting
    window. This approach works best when the receiver is storing data in a buffer
    and thus can only receive the buffer capacity.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring topologies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far, we''ve looked at some complete communication networks where each processor
    can communicate with the others directly, without the need for any switch. This
    arrangement serves well when there is a small number of processors but can become
    a real pain if the number of processors needs to be increased. There are various
    other performance topologies available that can be used. There are two important
    aspects while measuring the performance of a graph in a topology:'
  prefs: []
  type: TYPE_NORMAL
- en: '**The diameter of the graph**: The longest path between the nodes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bisection bandwidth**: The bandwidth across the smallest cut that divides
    the network into two equal halves. This is important for networks where each processor
    needs to communicate with the others.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The following are examples of some network topologies.
  prefs: []
  type: TYPE_NORMAL
- en: Linear and ring topologies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: These topologies work well with 1D arrays. In the case of the linear topology,
    all the processors are in a linear arrangement with one input and output flow,
    whereas in the case of the ring topology, processors form a loop back to the start
    processor.
  prefs: []
  type: TYPE_NORMAL
- en: Let's look at them in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Linear arrays
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'All the processors are in a linear arrangement, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9152fa9a-b82e-455c-842d-3151c377c2bd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This arrangement will have the following values for the diameter and bisection
    bandwidth:'
  prefs: []
  type: TYPE_NORMAL
- en: Diameter = n-1, where n is the number of processors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bisection bandwidth = 1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ring or torus
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'All the processors are in ring arrangements and information flows from one
    processor to another, making a loopback to the originating processor. Then, this
    makes a ring, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ae99c2b1-8be7-4553-9c32-36af39d9a9ab.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This arrangement will have the following values for the diameter and bisection
    bandwidth:'
  prefs: []
  type: TYPE_NORMAL
- en: Diameter = n/2, where n is the number of processors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bisection bandwidth = 2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Meshes and tori
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: These topologies work well with 2D and 3D arrays. Let's look at them in more
    detail.
  prefs: []
  type: TYPE_NORMAL
- en: 2D mesh
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the case of the mesh, nodes connect to each other directly without having
    a dependency on other nodes in a network. All the nodes are in a 2D mesh arrangement,
    as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/34161c13-68fb-401a-9714-63d04dfba3bf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This arrangement will have the following values for the diameter and bisection
    bandwidth:'
  prefs: []
  type: TYPE_NORMAL
- en: Diameter = 2 * ( sqrt ( n ) – 1 ), where n is the number of processors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bisection bandwidth = sqrt( n )
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 2D torus
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'All the processors are in a 2D torus arrangement, as shown in the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/52becade-ebc1-4083-b14f-fb61aef07e7c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This arrangement will have the following values for the diameter and bisection
    bandwidth:'
  prefs: []
  type: TYPE_NORMAL
- en: Diameter = sqrt( n ), where n is the number of processors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bisection bandwidth = 2 * sqrt(n)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Programming distributed memory machines using message passing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will discuss how to program distributed memory machines
    using Microsoft's **message passing interface** (**MPI**).
  prefs: []
  type: TYPE_NORMAL
- en: MPI is a standard, portable system that has been developed for distributed and
    parallel systems. It defines the basic set of functions that are utilized by parallel
    hardware vendors to support distributed memory communication. In the following
    sections, we will discuss the advantages of using MPI over old messaging libraries
    and explain how to install and run a simple MPI program.
  prefs: []
  type: TYPE_NORMAL
- en: Why MPI?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An advantage of MPI is that MPI routines can be called from a variety of languages,
    such as C, C++, C#, Java, Python, and more. MPI is highly portable compared to
    old messaging libraries, and MPI routines are speed-optimized for each piece of
    hardware that they are supposed to run.
  prefs: []
  type: TYPE_NORMAL
- en: Installing MPI on Windows
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: MPI can be downloaded and installed as a ZIP file from [https://www.open-mpi.org/software/ompi/v1.10/](https://www.open-mpi.org/software/ompi/v1.10/).
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, you can download the Microsoft version of MPI from [https://github.com/Microsoft/Microsoft-MPI/releases](https://github.com/Microsoft/Microsoft-MPI/releases).
  prefs: []
  type: TYPE_NORMAL
- en: Sample program using MPI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The following is a simple `HelloWorld` program that we can run using MPI. The
    program prints the processor number that the code is being executed on after a
    delay of two seconds. The same code can be run on multiple processors (we are
    able to specify the processor count).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s create a new console application project in Visual Studio and write
    the following code in the `Program.cs` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '`GetCurrentProcessorNumber()` is a utility function that gives the processor
    number of where our code is being executed. As you can see from the preceding
    code, there is no magic – it runs as a single thread and prints `Hello` and the
    current processor number.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will install `msmpisetup.exe` from the Microsoft MPI link we provided in
    the *Installing MPI on Windows* section. Once installed, we need to execute the
    following command from Command Prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Here, `n` signifies the number of processors that we want the program to run
    on.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the output of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/48ac4d96-b03a-4576-9e1f-1cb7510007ca.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, we can run the same program on multiple processors using MPI.
  prefs: []
  type: TYPE_NORMAL
- en: Basic send/receive use
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: MPI is a C++ implementation, and most of the documentation on the Microsoft
    website will only be available in C++. However, it's easy to create a .NET compiled
    wrapper and use it in any of our projects. There are some third-party .NET implementations
    available as well for MPI but, unfortunately, there is no support for .NET Core
    implementations as of now.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the syntax of an `MPI_Send` function that sends a buffer of data to
    another processor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The method returns when the buffer can be safely reused.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the syntax of an `MPU_Recv` function, which will receive a buffer of
    data from another processor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This method doesn't return until the buffer is received.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a typical example of using the send and receive functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: When running via MPI, the communicator will send data that will be received
    by the receive function in another processor.
  prefs: []
  type: TYPE_NORMAL
- en: Collectives
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Collectives, as the name suggests, is a communication method wherein all the
    processors in a communicator are involved. Collectives help us achieve these tasks.
    Two collective methods that are primarily used for this are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`MPI_BCAST`: This **distributes**data from one (root) process to another processor
    in a communicator'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MPI_REDUCE`: This **combines** data from all the processors within a communicator
    and returns it to the root process'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we understand collectives, we have come to the end of this chapter
    and ultimately the end of this book. Now, it's time to see what we have learned!
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed distributed memory management implementations.
    We learned about distributed memory management models, such as shared memory and
    distributed memory processors, as well as their implementation. In the end, we
    discussed what an MPI is and how it can be utilized. We also discussed communication
    networks and various design considerations for implementing efficient networks.
    Now, you should have a good understanding of network topologies, routing algorithms,
    switching strategies, and flow controls.
  prefs: []
  type: TYPE_NORMAL
- en: In this book, we have covered various programming constructs that are available
    in .NET Core 3.1 to achieve parallel programming. Parallel programming, if used
    correctly, can greatly enhance the performance and responsiveness of applications.
    The new features and syntaxes that are available in .NET Core 3.1 really make
    writing/debugging and maintaining parallel code easier. We also covered how we
    used to write multithreaded code before TPL came into being, for comparison's
    sake.
  prefs: []
  type: TYPE_NORMAL
- en: With new constructs for asynchronous programming (async and await), we learned
    how to take full advantage of non-blocking I/Os while the program flow is synchronous.
    Then, we discussed new features such as async streams and async main methods,
    which help us write async code more easily. We also discussed parallel tooling
    support in Visual Studio that can help us debug code better. Finally, we discussed
    how to write unit test cases for parallel code to make our code more robust.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we wrapped up this book by introducing distributed programming techniques
    and how to use them in .NET Core.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ____________ is an arrangement of multi-processors placed mostly in single containers
    but sometimes in multiple containers in close vicinity to each other.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the case of the dynamic communication network, any node can send data to
    any other node.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'True'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'False'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Which of the following are characteristics of a communication network?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Topology
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Switching strategy
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Flow control
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Shared memory
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the case of the distributed memory model, the memory space is shared across
    processors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'True'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'False'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Circuit switching can be used for asynchronous scenarios.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'True'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'False'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
