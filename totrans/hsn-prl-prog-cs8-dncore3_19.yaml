- en: Distributed Memory Management
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分布式内存管理
- en: 'In the last two decades, the industry has seen a paradigm shift to big data
    and machine learning architectures that involve processing terabytes/petabytes
    of data as quickly as possible. As computing power became cheaper, there was a
    need to use multiple processors to speed up processing to a larger scale. This
    has led to distributed computing. Distributed computing refers to an arrangement
    of computer systems that are connected via some networking/distribution middleware.
    All the connected systems share resources and coordinate their activities via
    middleware so that they work in a way that is perceived as a single system by
    the end user. Distributed computing is needed due to the huge volume and throughput
    requirements of modern applications. Some typical examples of scenarios where
    computing demands cannot be fulfilled by a single system and that need to be distributed
    across a grid of computers are as follows:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的二十年中，行业已经看到了一个向大数据和机器学习架构的转变，这些架构涉及尽可能快地处理TB / PB级别的数据。随着计算能力变得更加便宜，需要使用多个处理器来加速处理规模更大的数据。这导致了分布式计算。分布式计算是指通过某种网络/分发中间件连接的计算机系统的安排。所有连接的系统共享资源，并通过中间件协调它们的活动，以便它们以最终用户感知为单个系统的方式工作。由于现代应用程序的巨大容量和吞吐量要求，需要分布式计算。一些典型的示例场景，其中单个系统无法满足计算需求，需要在计算机网格上分布的情况如下：
- en: Google performs at least 1.5 trillion searches per year.
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 谷歌每年至少进行1500亿次搜索。
- en: IOT devices send multiple terabytes of data to event hubs.
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 物联网设备向事件中心发送多个TB的数据。
- en: Data warehouses receive and compute terabytes of records in minimal time.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据仓库在最短的时间内接收和计算TB级别的记录。
- en: In this chapter, we will discuss distributed memory management and the need
    for distributed computing. We will also learn about how messages are passed across
    communication networks for distributed systems, as well as various types of communicated
    networks.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论分布式内存管理和分布式计算的需求。我们还将了解分布式系统中如何通过通信网络传递消息，以及各种类型的通信网络。
- en: 'This chapter will cover the following topics:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Advantages of distributed systems
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分布式系统的优势
- en: Shared memory model versus distributed memory model
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 共享内存模型与分布式内存模型
- en: Types of communication network
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通信网络的类型
- en: Properties of communication networks
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通信网络的属性
- en: Exploring topologies
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索拓扑结构
- en: Programming distributed memory machines using message passing
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用消息传递编程来编程分布式内存机器
- en: Collectives
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集合
- en: Technical requirements
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: To complete this chapter, you'll need knowledge of programming in C and C# Windows
    platform API invocation programming.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 要完成本章，您需要了解在C和C# Windows平台API调用编程中的编程知识。
- en: Introduction to distributed systems
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分布式系统简介
- en: We have already discussed how distributed computing works in this book. In this
    section, we will try to understand distributed computing with a small example
    that works on arrays.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在本书中讨论了分布式计算的工作原理。在本节中，我们将尝试通过一个在数组上工作的小例子来理解分布式计算。
- en: 'Let''s say we have an array of 1,040 elements and we would like to find the
    sum of all the numbers:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个包含1040个元素的数组，我们想找出所有数字的总和：
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'If the total time that''s taken to add numbers is x (let''s say all of the
    numbers are large) and we want to compute them all as fast as possible, we can
    take advantage of distributed computing. We would divide the array into multiple
    arrays (let''s say, four arrays), each with 25% of the original number of elements,
    and send each array to a different processor to calculate the sum, as follows:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 如果将数字相加所需的总时间为x（假设所有数字都很大），并且我们希望尽快计算它们，我们可以利用分布式计算。我们将数组分成多个数组（假设有四个数组），每个数组包含原始元素数量的25％，并将每个数组发送到不同的处理器以计算总和，如下所示：
- en: '![](img/b7f7894c-f365-49da-af19-6ef8f87398a9.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b7f7894c-f365-49da-af19-6ef8f87398a9.png)'
- en: In this arrangement, the total time that's taken to add all the numbers is reduced
    to (x/4 + d) or (x/number of processors +d), where d is the time that's taken
    to collate the sums from all the processors and add them to get the final results.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种安排中，将所有数字相加所需的总时间减少到（x/4 + d）或（x/处理器数量 + d），其中d是从所有处理器收集总和并将它们相加以获得最终结果所需的时间。
- en: 'Some of the advantages of distributed systems are as follows:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式系统的一些优势如下：
- en: Systems can be scaled to any level without any hardware restrictions
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 系统可以在没有任何硬件限制的情况下扩展到任何级别
- en: No single point of failure, which makes them more fault-tolerant
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有单点故障，使它们更具容错性
- en: Highly available
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高度可用
- en: Very efficient when handling big data problems
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理大数据问题时非常高效
- en: Distributed systems are often confused with parallel systems, but there are
    subtle differences between them. **Parallel systems** are an arrangement of multi-processors
    that are placed mostly in single, but sometimes in multiple, containers in close
    vicinity. **Distributed systems**, on the other hand, consist of multiple processors
    (each having its own memory and I/O devices) that are connected together via a
    network that enables data exchange.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式系统经常与并行系统混淆，但它们之间有微妙的区别。**并行系统**是一种多处理器的排列，它们大多放置在单个容器中，但有时也放置在多个容器中。**分布式系统**则由多个处理器组成（每个处理器都有自己的内存和I/O设备），它们通过网络连接在一起，实现数据交换。
- en: Shared versus distributed memory model
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 共享与分布式内存模型
- en: To achieve high performance, the **multi-processor** and **multi-computer** architectures
    have evolved. With the multi-processor architecture, multiple processors share
    a common memory and communicate with each other by reading/writing to the shared
    memory. With multi-computers, multiple computers that don't share a single physical
    memory communicate with each other by passing messages. **Distributed Shared Memory**
    (**DSM**) deals with sharing memory in a physical, non-shared (distributed) architecture.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: Let's look at each one and talk about their differences.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: Shared memory model
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the case of shared memory models, multiple processors share a single common
    memory space. Since multiple processors share memory space, there needs to be
    some synchronization measures in place to avoid data corruption and race conditions.
    As we have seen so far in this book, synchronization comes with performance overheads.
    The following is an example representation of the shared memory model. As you
    can see, there are **n** processors in the arrangement, all of which have access
    to a commonly shared memory block:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/eb451b6f-2779-413e-bc9a-455b1f0052fc.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
- en: 'The features of the shared memory model are as follows:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: 'All the processors have access to the entire memory block. The memory block
    can be a single piece of memory composed of memory modules, as shown in the following
    diagram:'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/7287eb33-3361-43f9-93b4-a602c63f4f90.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
- en: Processors communicate with each other by creating shared variables in the main
    memory.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The efficiency of parallelization largely depends on the speed of the service
    bus.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Due to the speed of the service bus, the system can only be scaled up to n number
    of processors.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shared memory models are also known as **symmetric multiprocessing** (**SMP**)
    models since all the processors have access to all the available memory blocks.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: Distributed memory model
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the case of the distributed memory model, the memory space is no longer
    shared across processors. In fact, the processors don''t share common physical
    locations; instead, they can be remotely placed. Each processor has its own private
    memory space and I/O devices. Data is stored across processors rather than in
    single memory. Each processor can work on its own local data, but to access data
    that''s been stored in other processor memories, they need to connect via a communication
    network. Data is passed via **message passing** across processors using the *send
    message* and *receive message* instructions. The following is a diagrammatic representation
    of a distributed memory model:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b9184ffd-8815-494e-984a-75466e0d829b.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
- en: The preceding diagram depicts each processor, along with its own memory space
    and interaction with **communication networks** via I/O interfaces. Let's try
    to understand the various types of communication networks that can be used in
    distributed systems.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: Types of communication network
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Communication networks are the links that connect two or more nodes in a typical
    computer network. Communication networks are classified into two categories:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: Static communication networks
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dynamic communication networks
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's take a look at both.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: Static communication networks
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Static communication networks contain links, as shown in the following diagram:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dd8b6d89-e3e9-42dd-9add-60e7da9f3c9a.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
- en: Links are used to connect nodes together, thereby creating a complete communication
    network where any node can talk to any other node.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic communication networks
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Dynamic communication networks have links and switches, as shown in the following
    diagram:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/747dae92-c2d8-4f26-b3f1-bf16ee8c33ae.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
- en: Switches are devices that have input/output ports, and they redirect input data
    to output ports. This means that pathways are dynamic. If one processor wants
    to send data to another, it needs to be done via a switch, as demonstrated in
    the preceding diagram.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: Properties of communication networks
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While designing a communication network, we need to consider the following
    characteristics:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: Topology
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Routing algorithm
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Switching strategy
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Flow control
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's look at these characteristics in more detail.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: Topology
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Topology refers to how nodes (bridges, switches, and infrastructure devices)
    are connected. Some common topologies include crossbar, ring, 2D mesh, 3D mesh,
    higherD mesh, 2D torus, 3D torus, higherD torus, hypercube, tree, butterfly, perfect
    shuffle, and dragonfly.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: 'In the case of the crossbar topology, every node in the network is connected
    to every other node (though they may not be connected directly). Thus, messages
    can be passed via a number of routes to avoid any conflicts. Here is a typical
    crossbar topology:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/72c0ae88-b930-4d0e-ac5c-3b30d31f49e2.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
- en: 'In the case of the mesh topology, or meshnet, as it''s popularly called, nodes
    connect to each other directly without having a dependency on other nodes in the
    network. This way, all the nodes can relay information independently. The mesh
    can be partially or fully connected. Here is a typical fully connected mesh:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8ba15fd5-ddef-42d3-ba5d-1a63f576c968.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
- en: We will look at topology in more detail later in this chapter, in the *Exploring
    topologies *section.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: Routing algorithms
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Routing is a process via which a packet of information is sent across the network
    so that it reaches the intended node. Routing can be adaptive, that is, it responds
    to changes in the network topology by continuously taking information from adjacent
    nodes, or non-adaptive, that is, they are static and is where routing information
    is downloaded to nodes when the network is booted. Routing algorithms need to
    be chosen to make sure there are no deadlocks. For example, in 2D torus, all the
    pathways go from east to west and north to south to avoid any deadlock scenarios.
    We will look at 2D torus in more detail later in this chapter.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: Switching strategy
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Choosing an appropriate switching strategy can increase the performance of
    a network. The two most prominent switching strategies are as follows:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: '**Circuit switching**: In circuit switching, the full path is reserved for
    an entire message, such as the telephone. To begin a call on a telephone network,
    a dedicated circuit needs to be established between the caller and callee and
    the circuit persists during the entire call duration.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Packet switching**: In packet switching, the message is broken into separately
    routed packets, such as the internet. In terms of cost benefits, it''s far better
    than circuit switching as the cost of the link is shared across users. Packet
    switching is primarily used for asynchronous scenarios such as sending emails
    or file transfer.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Flow control
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Flow control is a process by which a network makes sure that packets are transferred
    across the sender and received efficiently and without error. In the case of the
    network topology, the speeds of the sender and receiver can vary, which can lead
    to bottlenecks or loss of packets in some cases. With flow control, we can make
    decisions in case there''s congestion on the network. Some strategies include
    storing data temporarily into buffers, rerouting data to other nodes, instructing
    source nodes to temporarily halt, discarding data, and so on. The following are
    some common flow control algorithms:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: '**Stop and wait**: The entire message is broken into parts. The sender sends
    a part to the receiver and waits for an acknowledgement to come within a specific
    time period (timeout). Once the sender receives an acknowledgment, it sends the
    next part of the message.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sliding window**: The receiver assigns a transmitting window for a sender
    to send messages. The sender has to stop transmitting when the window is full
    so that the receiver can process messages and advertise the next transmitting
    window. This approach works best when the receiver is storing data in a buffer
    and thus can only receive the buffer capacity.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring topologies
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far, we''ve looked at some complete communication networks where each processor
    can communicate with the others directly, without the need for any switch. This
    arrangement serves well when there is a small number of processors but can become
    a real pain if the number of processors needs to be increased. There are various
    other performance topologies available that can be used. There are two important
    aspects while measuring the performance of a graph in a topology:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: '**The diameter of the graph**: The longest path between the nodes.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bisection bandwidth**: The bandwidth across the smallest cut that divides
    the network into two equal halves. This is important for networks where each processor
    needs to communicate with the others.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The following are examples of some network topologies.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: Linear and ring topologies
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: These topologies work well with 1D arrays. In the case of the linear topology,
    all the processors are in a linear arrangement with one input and output flow,
    whereas in the case of the ring topology, processors form a loop back to the start
    processor.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: Let's look at them in more detail.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: Linear arrays
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'All the processors are in a linear arrangement, as shown in the following diagram:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9152fa9a-b82e-455c-842d-3151c377c2bd.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
- en: 'This arrangement will have the following values for the diameter and bisection
    bandwidth:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: Diameter = n-1, where n is the number of processors
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bisection bandwidth = 1
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ring or torus
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'All the processors are in ring arrangements and information flows from one
    processor to another, making a loopback to the originating processor. Then, this
    makes a ring, as shown in the following diagram:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ae99c2b1-8be7-4553-9c32-36af39d9a9ab.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
- en: 'This arrangement will have the following values for the diameter and bisection
    bandwidth:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: Diameter = n/2, where n is the number of processors
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bisection bandwidth = 2
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Meshes and tori
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: These topologies work well with 2D and 3D arrays. Let's look at them in more
    detail.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: 2D mesh
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the case of the mesh, nodes connect to each other directly without having
    a dependency on other nodes in a network. All the nodes are in a 2D mesh arrangement,
    as shown in the following diagram:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/34161c13-68fb-401a-9714-63d04dfba3bf.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
- en: 'This arrangement will have the following values for the diameter and bisection
    bandwidth:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: Diameter = 2 * ( sqrt ( n ) – 1 ), where n is the number of processors
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bisection bandwidth = sqrt( n )
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 2D torus
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'All the processors are in a 2D torus arrangement, as shown in the following
    diagram:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/52becade-ebc1-4083-b14f-fb61aef07e7c.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
- en: 'This arrangement will have the following values for the diameter and bisection
    bandwidth:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: Diameter = sqrt( n ), where n is the number of processors
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bisection bandwidth = 2 * sqrt(n)
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Programming distributed memory machines using message passing
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will discuss how to program distributed memory machines
    using Microsoft's **message passing interface** (**MPI**).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: MPI is a standard, portable system that has been developed for distributed and
    parallel systems. It defines the basic set of functions that are utilized by parallel
    hardware vendors to support distributed memory communication. In the following
    sections, we will discuss the advantages of using MPI over old messaging libraries
    and explain how to install and run a simple MPI program.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: Why MPI?
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An advantage of MPI is that MPI routines can be called from a variety of languages,
    such as C, C++, C#, Java, Python, and more. MPI is highly portable compared to
    old messaging libraries, and MPI routines are speed-optimized for each piece of
    hardware that they are supposed to run.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: Installing MPI on Windows
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: MPI can be downloaded and installed as a ZIP file from [https://www.open-mpi.org/software/ompi/v1.10/](https://www.open-mpi.org/software/ompi/v1.10/).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, you can download the Microsoft version of MPI from [https://github.com/Microsoft/Microsoft-MPI/releases](https://github.com/Microsoft/Microsoft-MPI/releases).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: Sample program using MPI
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The following is a simple `HelloWorld` program that we can run using MPI. The
    program prints the processor number that the code is being executed on after a
    delay of two seconds. The same code can be run on multiple processors (we are
    able to specify the processor count).
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s create a new console application project in Visual Studio and write
    the following code in the `Program.cs` file:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '`GetCurrentProcessorNumber()` is a utility function that gives the processor
    number of where our code is being executed. As you can see from the preceding
    code, there is no magic – it runs as a single thread and prints `Hello` and the
    current processor number.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: 'We will install `msmpisetup.exe` from the Microsoft MPI link we provided in
    the *Installing MPI on Windows* section. Once installed, we need to execute the
    following command from Command Prompt:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Here, `n` signifies the number of processors that we want the program to run
    on.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the output of the preceding code:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/48ac4d96-b03a-4576-9e1f-1cb7510007ca.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
- en: As you can see, we can run the same program on multiple processors using MPI.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: Basic send/receive use
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: MPI is a C++ implementation, and most of the documentation on the Microsoft
    website will only be available in C++. However, it's easy to create a .NET compiled
    wrapper and use it in any of our projects. There are some third-party .NET implementations
    available as well for MPI but, unfortunately, there is no support for .NET Core
    implementations as of now.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the syntax of an `MPI_Send` function that sends a buffer of data to
    another processor:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The method returns when the buffer can be safely reused.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the syntax of an `MPU_Recv` function, which will receive a buffer of
    data from another processor:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This method doesn't return until the buffer is received.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a typical example of using the send and receive functions:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: When running via MPI, the communicator will send data that will be received
    by the receive function in another processor.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: Collectives
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Collectives, as the name suggests, is a communication method wherein all the
    processors in a communicator are involved. Collectives help us achieve these tasks.
    Two collective methods that are primarily used for this are as follows:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '`MPI_BCAST`: This **distributes**data from one (root) process to another processor
    in a communicator'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MPI_REDUCE`: This **combines** data from all the processors within a communicator
    and returns it to the root process'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we understand collectives, we have come to the end of this chapter
    and ultimately the end of this book. Now, it's time to see what we have learned!
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed distributed memory management implementations.
    We learned about distributed memory management models, such as shared memory and
    distributed memory processors, as well as their implementation. In the end, we
    discussed what an MPI is and how it can be utilized. We also discussed communication
    networks and various design considerations for implementing efficient networks.
    Now, you should have a good understanding of network topologies, routing algorithms,
    switching strategies, and flow controls.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: In this book, we have covered various programming constructs that are available
    in .NET Core 3.1 to achieve parallel programming. Parallel programming, if used
    correctly, can greatly enhance the performance and responsiveness of applications.
    The new features and syntaxes that are available in .NET Core 3.1 really make
    writing/debugging and maintaining parallel code easier. We also covered how we
    used to write multithreaded code before TPL came into being, for comparison's
    sake.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: With new constructs for asynchronous programming (async and await), we learned
    how to take full advantage of non-blocking I/Os while the program flow is synchronous.
    Then, we discussed new features such as async streams and async main methods,
    which help us write async code more easily. We also discussed parallel tooling
    support in Visual Studio that can help us debug code better. Finally, we discussed
    how to write unit test cases for parallel code to make our code more robust.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 通过新的异步编程构造（async和await），我们学习了如何充分利用非阻塞I/O，同时程序流程是同步的。然后，我们讨论了诸如异步流和异步主方法之类的新功能，这些功能可以帮助我们更轻松地编写异步代码。我们还讨论了Visual
    Studio中的并行工具支持，可以帮助我们更好地调试代码。最后，我们讨论了如何为并行代码编写单元测试用例，以使我们的代码更加健壮。
- en: Then, we wrapped up this book by introducing distributed programming techniques
    and how to use them in .NET Core.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们通过介绍分布式编程技术以及如何在.NET Core中使用它们来结束了这本书。
- en: Questions
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: ____________ is an arrangement of multi-processors placed mostly in single containers
    but sometimes in multiple containers in close vicinity to each other.
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ____________是将多处理器放置在单个容器中，但有时也放置在彼此紧邻的多个容器中的一种安排。
- en: In the case of the dynamic communication network, any node can send data to
    any other node.
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在动态通信网络的情况下，任何节点都可以向任何其他节点发送数据。
- en: 'True'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 真
- en: 'False'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假
- en: Which of the following are characteristics of a communication network?
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下哪些是通信网络的特征？
- en: Topology
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 拓扑
- en: Switching strategy
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 切换策略
- en: Flow control
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 流量控制
- en: Shared memory
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 共享内存
- en: In the case of the distributed memory model, the memory space is shared across
    processors.
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在分布式内存模型的情况下，内存空间在处理器之间共享。
- en: 'True'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 真
- en: 'False'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假
- en: Circuit switching can be used for asynchronous scenarios.
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 电路切换可以用于异步场景。
- en: 'True'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 真
- en: 'False'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假
