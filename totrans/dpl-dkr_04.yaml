- en: Scaling the Containers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will be taking our service and trying to scale it horizontally
    with multiple instances of the same container. We will cover the following topics
    in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Orchestration options and their pros/cons
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Service discovery
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: State reconciliation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The deployment of your own Docker Swarm cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying our word service from the previous chapter onto that cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Service discovery
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we get any further, we really need to get deeply familiar with the conceptual
    Docker container connectivity, which is, unsurprisingly, in some ways very similar
    to building high-availability services with servers in a non-containerized world.
    Because of this, covering this topic in some depth will not only expand your understanding
    of Docker networking, but also help in generally building out resilient services.
  prefs: []
  type: TYPE_NORMAL
- en: A recap of Docker networking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous chapter, we covered a bit of the Docker networking layout,
    so we will cover the main points here:'
  prefs: []
  type: TYPE_NORMAL
- en: By default, Docker containers run on an isolated virtual network on the host
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each container has its own network address in that network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By default, `localhost` for a container is *not* the host machine's `localhost`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is high overhead of manual work in order to connect containers manually
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Manual networking connections between containers are inherently fragile
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the parallel world of setting up a local server network, the base experience
    of Docker connectivity is very much akin to hooking up your whole network with
    static IPs. While this approach is not very difficult to get working, maintaining
    it is extremely hard and laborious, which is why we need something better than
    that.
  prefs: []
  type: TYPE_NORMAL
- en: Service Discovery in depth
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Since we don''t want to deal with this fragile system of keeping and maintaining
    hardcoded IP addresses, we need to figure out a way so our connections are flexible
    and require no adjustments from the client if the target service dies or a new
    one is created. It would also be nice if each connection to the same service is
    equally balanced between all instances of the same service. Ideally, our services
    would look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/533c4c3e-976c-4621-a9ad-e138dc097b9e.png)'
  prefs: []
  type: TYPE_IMG
- en: For this exact use case for the Internet, DNS was created so that clients would
    have a way to find servers even if the IP address or network changes from anywhere
    in the world. As an added benefit, we have target addresses that are easier to
    remember (DNS names such as [https://google.com](https://google.com) instead of
    something such as `https://123.45.67.89` ) and the ability to distribute the processing
    to as many handling services as we want.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you have not worked with DNS in depth, the main principles are reduced to
    these basic steps:'
  prefs: []
  type: TYPE_NORMAL
- en: The user (or app) wants to connect to a server (that is, [google.com](http://www.google.com)).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The local machine either uses its own cached DNS answer or goes out to the DNS
    system and searches for this name.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The local machine gets back the IP address ( `123.45.67.89` ) that it should
    use as the target.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The local machine connects to the IP address.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The DNS system is much more complicated than the single sentences mentioned
    here. While DNS is a really good thing to know about in any server-oriented tech
    position, here, it was sufficient just to know that the input to the DNS system
    is a hostname and the output is the real target (IP). If you would like to know
    more about how the DNS system actually works, I recommend that you visit [https://en.wikipedia.org/wiki/Domain_Name_System](https://en.wikipedia.org/wiki/Domain_Name_System)
    at your leisure.
  prefs: []
  type: TYPE_NORMAL
- en: If we coerce the DNS handling that is implemented in almost all clients already
    as a way to automatically discover services, we could make ourselves the service
    discovery mechanism that we have been looking for! If we make it smart enough,
    it can tell us where the running container is, load balance between all instances
    of the same container, and provide us with a static name to use as our target.
    As one may expect, almost all container service discovery systems have this exact
    pattern of functionality; it just generally differs if it is done as either a
    client-side discovery pattern, server-side discovery pattern, or some sort of
    a hybrid system.
  prefs: []
  type: TYPE_NORMAL
- en: Client-side discovery pattern
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This type of pattern isn't used often, but it pretty much involves using a service-aware
    client to discover other services and to load balance between them. The advantage
    here is that the client can make intelligent decisions about where to connect
    to and in which manner, but the downside is that this decision making is distributed
    onto each service and hard to maintain but it is not dependent on a single source
    of truth (single service registry) that could take down a whole cluster if it
    fails.
  prefs: []
  type: TYPE_NORMAL
- en: 'The architecture generally looks something similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '**![](assets/9e41d219-feb2-4831-bc70-82b533f54c11.png)**'
  prefs: []
  type: TYPE_NORMAL
- en: Server-side discovery pattern
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The more common service discovery pattern is a centralized server-side discovery
    pattern where the DNS system is used to direct the clients to the container. In
    this particular way of finding services, a container registers and de-registers
    itself from the service registry, which holds the state of the system. This state,
    in turn, is used to populate the DNS entries that the client then contacts to
    find the target(s) that it is trying to connect to. While this system is generally
    pretty stable and flexible, it sometimes suffers from really tricky issues that
    generally hamper DNS systems elsewhere, such as DNS caching, which uses stale
    IP addresses until the **time-to-live** (**TTL**) expires or when the app itself
    caches the DNS entry regardless of updates (NGINX and Java apps are notorious
    for this).
  prefs: []
  type: TYPE_NORMAL
- en: '**![](assets/087d23be-79c8-4b67-bec8-0feaeb036efb.png)**'
  prefs: []
  type: TYPE_NORMAL
- en: Hybrid systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This grouping includes all other combinations that we haven't covered yet, but
    it covers the class of largest deployments that use a tool, HAProxy, which we
    will cover in some detail later. What it basically does is tie a specific port
    on the host (that is, `<host>:10101` ) to a load-balanced target somewhere else
    in the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: From the client perspective, they are connecting to a single and stable location
    and the HAProxy then tunnels it seamlessly to the right target.
  prefs: []
  type: TYPE_NORMAL
- en: '**![](assets/a7a98255-eb1a-47bc-a559-f6712e281787.png)**'
  prefs: []
  type: TYPE_NORMAL
- en: This setup supports both pull and push refreshing of methods and is very resilient,
    but we will take a deep dive into this type of setup in later chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Picking the (un)available options
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'With all of these types of service discoveries available, we should be able
    to handle any container scaling that we want, but we need to keep something very
    important in mind: almost all service discovery tooling is intimately bound to
    the system used for deploying and managing the containers (also known as container
    orchestration) due to the fact that updates to container endpoints are generally
    just an orchestration system implementation detail. Because of this, service discovery
    systems usually aren''t as portable as one might like, so the choice of this infrastructure
    piece usually gets decided by your orchestration tooling (with a few exceptions
    here and there).'
  prefs: []
  type: TYPE_NORMAL
- en: Container orchestration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we somewhat hinted earlier, service discovery is a critical part of deploying
    a container-based system in any capacity. Without something like that, you might
    as well just use bare-metal servers as the majority of advantages gained using
    containers have been lost. To have an effective service discovery system, you
    are pretty much mandated to use some sort of container orchestration platform,
    and luckily (or maybe un-luckily?), options for container orchestration have been
    sprouting at an almost alarming rate! In general terms, though, at the time of
    writing this book (and in my humble opinion), the popular and stable choices come
    down to mainly these:'
  prefs: []
  type: TYPE_NORMAL
- en: Docker Swarm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apache Mesos/Marathon
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cloud-based offerings (Amazon ECS, Google Container Engine, Azure Container
    Service, and so on)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Each one has its own vocabulary and the way in which the infrastructure pieces
    connect, so before we go any further, we need to cover the pertinent vocabulary
    in regard to orchestration services that will mostly be reusable between all of
    them:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Node**: An instance of Docker Engine. Generally used only when talking about
    cluster-connected instances.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Service**: A functionality grouping that is composed of one or more running
    instances of the same Docker image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Task**: A specific and unique instance of a running service. This is usually
    a single running Docker container.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scaling**: The count of tasks specified for a service to run. This usually
    determines how much throughput a service can support.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Manager node**: A node in charge of management and orchestration duties of
    the cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Worker node**: A node designated as a task runner.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: State reconciliation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Besides our just-learned dictionary, we also need to understand the underlying
    algorithm of almost all orchestration frameworks, state reconciliation, which
    deserves its own little section here. The basic principle that this works on is
    a very simple three-step process, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The user setting the desired count(s) of each service or a service disappearing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The orchestration framework seeing what is needed in order to change the current
    state to the desired state (delta evaluation).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Executing whatever is needed to take the cluster to that state (known as state
    reconciliation).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](assets/23380424-de88-49b9-8fe9-76b0e8f78015.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For example, if we currently have five running tasks for a service in the cluster
    and change the desired state to only three tasks, our management/orchestration
    system will see that the difference is `-2` and thus pick two random tasks and
    kill them seamlessly. Conversely, if we have three tasks running and we want five
    instead, the management/orchestration system will see that the desired delta is
    `+2` so it will pick two places with available resources for it and start two
    new tasks. A short explanation of two state transitions should also help clarify
    this process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Using this very simple but powerful logic, we can dynamically scale up and down
    our services without worrying about the intermediate stages (to a degree). Internally,
    keeping and maintaining states is such a difficult task that most orchestration
    frameworks use a special, high-speed key-value store component to do this for
    them (that is, `etcd`, `ZooKeeper`, and `Consul`).
  prefs: []
  type: TYPE_NORMAL
- en: Since our system only cares about where our current state is and where it needs
    to be, this algorithm also doubles as the system for building resilience as a
    dead node, or the container will reduce the current task count for applications
    and will trigger a state transition back to the desired counts automatically.
    As long as services are mostly stateless and you have the resources to run the
    new services, these clusters are resilient to almost any type of failure and now
    you can hopefully see how a few simple concepts tie together to create such a
    robust infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: With our new understanding of management and orchestration framework basics,
    we will now take a brief look at each one of our available options (Docker Swarm,
    Kubernetes, Marathon) and see how they compare with each other.
  prefs: []
  type: TYPE_NORMAL
- en: Docker Swarm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Out of the box, Docker contains an orchestration framework and a management
    platform very architecturally similar to the one covered just a second ago, called
    Docker Swarm. Swarm allows a pretty quick and simple way to get scaling integrated
    with your platform with minimal ramp-up time and given that it is already a part
    of Docker itself, you really don't need much else to deploy a simple set of services
    in a clustered environment. As an added benefit, it contains a pretty solid service
    discovery framework, has multi-host networking capability, and uses TLS for communication
    between nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-host networking capability is the ability of a system to create a virtual
    network across multiple physical machines that are transparent from the point
    of view of the container. Using one of these, your containers can communicate
    with each other as if they were on the same physical network, simplifying the
    connectivity logic and reducing operational costs. We will look into this aspect
    of clustering in depth a bit later.
  prefs: []
  type: TYPE_NORMAL
- en: The cluster configuration for Docker Swarm can be a simple YAML file, but the
    downside is that GUI tools are, at the time of writing this, somewhat lacking,
    though Portainer ([https://portainer.io](https://portainer.io)) and Shipyard ([https://shipyard-project.com](https://shipyard-project.com))
    are getting to be pretty decent, so this might not be a problem for too long.
    Additionally, some large-scale ops tooling is missing and it seems that generally,
    features of Swarm are heavily evolving and thus in a state of flux, so my personal
    recommendation would be to use this type of orchestration if you need to get something
    up and running quickly on small-to-largish scales. As this product gets more and
    more mature (and since Docker Inc. is placing a lot of development resources behind
    this), it will probably improve significantly, and I expect it to match Kubernetes
    features in many respect so keep an eye out for its feature news.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Kubernetes is Google''s cloud platform and orchestration engine that currently
    provides a bit more in terms of features than does Swarm. The setup of Kubernetes
    is much more difficult as you need: a master, a node (the worker according to
    our earlier dictionary), and pods (grouping of one or more containers). Pods are
    always co-located and co-scheduled, so handling their dependencies is a bit easier
    to deal with but you do not get the same isolation. The interesting thing to keep
    in mind here is that all containers within the pod share the same IP address/ports,
    share volumes, and are generally within the same isolation group. It is almost
    better to think of a pod as a small virtual machine running many services than
    many containers running in parallel.'
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes has been gaining a massive amount of community traction lately and
    is probably the most deployed cluster orchestration and management system in use,
    though to be fair, finding exact figures is tricky, with a majority of them being
    deployed in private clouds. Given that Google has been using this system for a
    while and on such a large scale, it has a pretty proven track record and I would
    probably recommended it for medium-to-large scales. If you don't mind the overhead
    of setting everything up, I think even smaller scales would be acceptable, but
    in that space, Docker Swarm is so easy to use that using Kubernetes for it is
    generally impractical.
  prefs: []
  type: TYPE_NORMAL
- en: At the time of writing this book, both Mesos and Docker EE have included capabilities
    to support Kubernetes so if you would want to bet on an orchestration engine,
    this would probably be it.![](assets/3aae9d0c-3b9c-4f3e-9f49-b200f63fb339.png)
  prefs: []
  type: TYPE_NORMAL
- en: Apache Mesos/Marathon
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When you really need to dial up the scaling to levels of Twitter and Airbnb,
    you probably need something even more powerful than Swarm or Kubernetes, which
    is where Mesos and Marathon come into play. Apache Mesos was not actually built
    with Docker in mind but as a general cluster-management tooling that provides
    resource management in a consistent way for applications that run on top of it
    with APIs. You can run anything from scripts, actual applications, and multiple
    platforms (such as HDFS and Hadoop) with relative ease. For container-based orchestration
    and scheduling on this platform these days, Marathon is the general go-to here.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned a little bit earlier, Kubernetes support has been now available
    again for Mesos after being in a broken state for a while so the suggestion of
    Marathon may change by the time you read this text.
  prefs: []
  type: TYPE_NORMAL
- en: Marathon runs as an application (in a very loose sense of the word) on top of
    Mesos as the container orchestration platform and provides all kind of niceties,
    such as a great UI (though Kubernetes has one too), metrics, constraints, persistent
    volumes (experimental at the time of writing this), and many others. As a platform,
    Mesos and Marathon are probably the most powerful combo for handling clusters
    in the tens-of-thousands-of-nodes range, but to get everything pieced together,
    unless you use the pre-packaged DC/OS solution ([https://dcos.io/](https://dcos.io/)),
    it is in my experience really, really tricky to get up and running compared to
    the other two. If you need to cover the range of medium-to-largest of scales with
    added flexibility in order to run other platforms (such as Chronos) on it too,
    currently, I would strongly recommend this combo.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/8ceb3bee-952b-4aba-93d8-8ce66365f06b.png)'
  prefs: []
  type: TYPE_IMG
- en: Cloud-based offerings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If all of this seems too much trouble and you don''t mind paying a hefty premium
    every month for it, all the big cloud players have some sort of container-based
    service offering. Since these vary wildly both in functionality and feature set,
    anything that would get put onto this page in that regard will probably be outdated
    by the time it gets published, and we are more interested in deploying services
    on our own, so I will leave you links to the appropriate offerings that will have
    up-to-date information if you choose this route:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Amazon ECS**: [https://aws.amazon.com/ecs/](https://aws.amazon.com/ecs/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Google Container Engine**: [https://cloud.google.com/container-engine/](https://cloud.google.com/container-engine/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Microsoft Azure** (**Azure Container Service**): [https://azure.microsoft.com/en-us/services/container-service/](https://azure.microsoft.com/en-us/services/container-service/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Oracle Container Cloud Service**: [https://cloud.oracle.com/container](https://cloud.oracle.com/container)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Docker Cloud**: [https://cloud.docker.com/](https://cloud.docker.com/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Probably many others that I have missed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Personally, I would recommend this approach for small-to-medium deployments
    due to ease of use and tested environments. If your needs expand past these scales,
    implementing your service on scalable groups of virtual machines on **Virtual
    Private Clouds** (**VPCs**) with the same cloud service provider is generally
    one of the ways to go as you can tailor your infrastructure in the exact way that
    your needs expand, though the upfront DevOps costs are not small, so decide accordingly.
    A good rule of thumb to remember with pretty much any cloud offering is that with
    easy tooling already provided you get much quicker deployments counterbalanced
    by increased costs (usually hidden) and lack of flexibility/customizability.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing orchestration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With our newly-gained understanding of the orchestration and management offerings
    out there, it is time to try this out ourselves. In our next exercise, we will
    first try to use Docker Swarm to create and play a bit with a local cluster and
    then we will try to deploy our service from the previous chapter onto it.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up a Docker Swarm cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Since all the functionality to set up a Docker Swarm cluster is already included
    in the Docker installation, this is actually a really easy thing to do. Let''s
    see what commands we have available to us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'A few things to note here--some more apparent than others:'
  prefs: []
  type: TYPE_NORMAL
- en: You create a swarm with `docker swarm init`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You join a cluster with `docker swarm join` and the machine can be a worker
    node, a manager node, or both
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Authentication is managed using tokens (unique strings that need to match)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If something happens to a manager node, such as a restart or power cycle, and
    you have set up auto-locking of the swarm, you will need an unlock key to unlock
    the TLS keys
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So far, so good, so let's see whether we can set up a swarm with our machine
    serving both as a manager and a worker to see how this works.
  prefs: []
  type: TYPE_NORMAL
- en: Initializing a Docker Swarm cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To create our swarm, we first need to instantiate it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We have created a swarm with that command and we are automatically enrolled
    as a manager node. If you take a look at the output, the command for adding worker
    nodes is just `docker swarm join --token <token> <ip>`, but we are interested
    in a single-node deployment for now, so we won't need to worry about it. Given
    that our manager node is also a worker node, we can just use it as-is to throw
    a few services on it.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Most of the commands we will initially need are accessible through the `docker
    services` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: As you might be suspecting, given how similar these commands are to some of
    the ones for managing containers, once you move to an orchestration platform as
    opposed to fiddling with containers directly, the ideal management of your services
    would be done through the orchestration itself. I would probably expand this and
    go as far as to say that if you are working with containers too much while having
    an orchestration platform, you did not set something up or you did not set it
    up correctly.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now try to get some sort of service running on our Swarm, but since
    we are just exploring how all this works, we can use a very slimmed down (and
    a very insecure) version of our Python web server from [Chapter 2](9b436a64-9b02-4e8c-bbe9-f3bb729152ea.xhtml),
    *Rolling Up the Sleeves*. Create a new folder and add this to a new `Dockerfile`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s build it so that our local registry has an image to pull from when we
    define our service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'With the image in place, let''s deploy it on our swarm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The warning shown is actually very important: our service is only available
    on our local machine''s Docker registry when we built it, so using a Swarm service
    that is spread between multiple nodes will have issues since other machines will
    not be able to load the same image. For this reason, having the image registry
    available from a single source to all of the nodes is mandatory for cluster deployments.
    We will cover this issue in more detail as we progress through this and following
    chapters.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we check out `http://127.0.0.1:8000`, we can see that our service is running!
    Let''s see this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/de4bc4a5-6c91-4c2a-a740-66b70e72e1b6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If we scale this service to three instances, we can see how our orchestration
    tool is handling the state transitions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'You can see how this is adjusting the container instances to fit our specified
    parameters. What if we now add something in the mix that will happen in real life-a
    container death:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, the swarm will bounce back up like nothing happened, and this
    is exactly why containerization is so powerful: not only can we spread processing
    tasks among many machines and flexibly scale the throughput, but with identical
    services we don''t really care very much if some (hopefully small) percentage
    of services dies, as the framework will make it completely seamless for the client.
    With the built-in service discovery of Docker Swarm, the load balancer will shift
    the connection to whatever container is running/available so anyone trying to
    connect to our server should not see much of a difference.'
  prefs: []
  type: TYPE_NORMAL
- en: Cleaning up
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As with any service that we are finished with, we need to make sure that we
    clean up any resources we have used up so far. In the case of Swarm, we should
    probably remove our service and destroy our cluster until we need it again. You
    can do both of those things using `docker service rm` and `docker swarm leave`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The reason why we had to use the `--force` flag here is due to the fact that
    we are a manager node and we are the last one in the cluster, so by default, Docker
    will prevent this action without it. In a multi-node setup, you will not generally
    need this flag.
  prefs: []
  type: TYPE_NORMAL
- en: With this action, we are now back at where we started and are ready to do this
    with a real service.
  prefs: []
  type: TYPE_NORMAL
- en: Using Swarm to orchestrate our words service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we built a simple service that can be used to add and
    list words entered on a form. But if you remember, we heavily used somewhat of
    an implementation detail to connect the services together, making it extremely
    fragile if not downright hacked-up together. With our new-found knowledge of service
    discovery and our understanding of Docker Swarm orchestration, we can try to get
    our old code ready for real cluster deployment and move away from the fragile
    setup we had earlier.
  prefs: []
  type: TYPE_NORMAL
- en: The application server
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Copy the old application server folder from [Chapter 3](a4ccf20b-743c-4b30-b416-2303314d91ba.xhtml), *Service
    Decomposition,* to a new folder and we will change our main handler code ( `index.js`
    )  since we have to accommodate the fact that we will not be the only instance
    reading from and writing to the database anymore.
  prefs: []
  type: TYPE_NORMAL
- en: As always, all code can also be found at [https://github.com/sgnn7/deploying_with_docker](https://github.com/sgnn7/deploying_with_docker).
    This particular implementation can be found in `chapter_4/clustered_application`.Warning!
    As you start thinking about similar containers running in parallel, you have to
    start being extra careful about data changes that can and will occur outside of
    the container's realm of control. For this reason, keeping or caching the state
    in any form in a running container is usually a recipe for disaster and data inconsistencies.
    To avoid this issue, in general, you should try to make sure that you re-read
    the information from your upstream sources (that is, the database) before doing
    any transformation or passing of the data like we do here.
  prefs: []
  type: TYPE_NORMAL
- en: index.js
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This file is pretty much the same one from the last chapter but we will be
    making a few changes to eliminate caching:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'If may have noticed, many things are similar, but there are fundamental changes
    too:'
  prefs: []
  type: TYPE_NORMAL
- en: We don't pre-load the words on start as the list might change from the time
    the service initializes and the user requests data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We load the saved words on each `GET` request in order to make sure we always
    get fresh data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When we save the word, we just insert it into the database and don't preserve
    it in the application as we will get new data on `GET` re-display.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using this approach, any changes done to the data in the database by any app
    instances will immediately be reflected in all of them. Additionally, if a database
    administrator changed any of the data, we will also see those changes within the
    application. Since our service also uses an environment variable for the database
    host, we should not need to change it to the support service discovery.
  prefs: []
  type: TYPE_NORMAL
- en: Caution! Be aware that because we read the database on each `GET` request, our
    changes to support clustering are not free and come with an increase in database
    queries, which can become a real bottleneck when the networking, cache invalidation,
    or disk transfers become overly saturated by these requests. Additionally, since
    we read the database before we display the data, slowdowns in the backend processing
    of our database `find()` will be user-visible, possibly causing undesired user
    experience, so keep these things in mind as you develop container-friendly services.
  prefs: []
  type: TYPE_NORMAL
- en: The web server
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Our web server changes will be a bit trickier due to a quirk/feature of the
    NGINX configuration processing that may also impact you if you do Java-based DNS
    resolution. Essentially, NGINX caches DNS entries so hard that effectively, once
    it reads the configuration files, any new DNS resolution within that configuration
    will not actually take place at all unless some extra flags ( `resolver` ) are
    specified. With the Docker service being constantly mutable and relocatable, this
    is a serious issue that must be worked around to function properly on the Swarm.
    Here, you have a couple of options:'
  prefs: []
  type: TYPE_NORMAL
- en: Run a DNS forwarder (such as `dnsmasq`) in parallel with NGINX and use that
    as the resolver. This requires running both `dnsmasq` and NGINX in the same container.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Populate the NGINX configuration container start with the same resolvers from
    the system using something such as `envsubst`: this requires all containers to
    be in the same user-defined network.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hardcode the DNS resolver IP (`127.0.0.11`): this also requires all containers
    to be in the same user-defined network.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For robustness, we will use the second option, so copy the web server from
    the previous chapter into a new folder and rename it to `nginx_main_site.conf.template`.
    We will then add a resolver configuration to it and a variable `$APP_NAME` for
    our proxy host endpoint:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Since NGINX does not handle environment variable substitution in the configuration
    files, we will write a wrapper script around it. Add a new file called `start_nginx.sh`
    and include the following content in it that takes the host''s resolvers and generates
    the new main_site config:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: To get this to run, we finally need to make sure we start NGINX with this script
    instead of the one built in, so we need to modify our `Dockerfile` as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Open up our Dockerfile and make sure that it has the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Here, the main change is the start up script `CMD` override and turning the
    configuration into a template with the rest pretty much left alone.
  prefs: []
  type: TYPE_NORMAL
- en: Database
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Unlike the other two containers, we will leave the database in one container
    due to a combination of things:'
  prefs: []
  type: TYPE_NORMAL
- en: MongoDB can scale to high GB/low TB dataset sizes easily with vertical scaling.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Databases are extremely difficult to scale up without in-depth knowledge of
    volumes (covered in the next chapter).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sharding and replica sets of databases are generally complicated enough for
    whole books to be written on this topic alone.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We may cover this topic in a later chapter, but here, it would derail us from
    our general goal of learning how to deploy services so we will just have our single
    database instance that we used in the previous chapter for now.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying it all
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we did for our simple web server, we will begin by creating another Swarm
    cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we need to create our overlay network for the service-discovery hostname
    resolution to work. You don''t need to know much about this other than it creates
    an isolated network that we will add all the services to:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we will build and launch our containers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: If you are having trouble with getting these services up and running, you can
    check the logs with `docker service logs <service_name>` in order to figure out
    what went wrong. You can also use `docker logs <container_id>` if a specific container
    is having trouble.
  prefs: []
  type: TYPE_NORMAL
- en: 'With these in place, we can now check whether our code works at `http://127.0.0.1:8080`
    (username: `user`, password: `test`):'
  prefs: []
  type: TYPE_NORMAL
- en: '**![](assets/aa5b1c35-7269-44d6-85cf-1858566d42d6.png)**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Looks like it is working! Once we put in our credentials, we should be redirected
    to the main application page:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/996362dd-b32a-47dd-a7aa-06d28749de5e.png)'
  prefs: []
  type: TYPE_IMG
- en: Does the database work if we put in some words?
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/03ad27dd-0617-426d-9359-8ba8653640c5.png)'
  prefs: []
  type: TYPE_IMG
- en: Indeed! We have really created a 1-node swarm-backed service, and it is scalable
    plus load balanced!
  prefs: []
  type: TYPE_NORMAL
- en: The Docker stack
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As it was pretty obvious from just a few paragraphs before, a manual setup
    of these services seems somewhat of a pain, so here we introduce a new tool that
    can help us do this much easier: Docker Stack. This tool uses a YAML file to get
    things to deploy all the services easily and repeatedly.'
  prefs: []
  type: TYPE_NORMAL
- en: 'First we will clean up our old exercise before trying to use Docker stack configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can write our YAML configuration file--you can easily notice the parallels
    that the CLI has to this configuration file:'
  prefs: []
  type: TYPE_NORMAL
- en: You can find more information about all the available options usable in Docker
    stack YAML files by visiting [https://docs.docker.com/docker-cloud/apps/stack-yaml-reference/](https://docs.docker.com/docker-cloud/apps/stack-yaml-reference/).
    Generally, anything you can set with the CLI commands, you can do the same with
    the YAML configuration.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'What about starting our stack? That''s easy too! Stack has almost the same
    commands as `docker services`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: If you go to `http://127.0.0.1:8080` in your browser again, you will see that
    our app works just like before! We have managed to deploy our whole cluster worth
    of images with a single file on a Docker Swarm cluster!
  prefs: []
  type: TYPE_NORMAL
- en: Clean up
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We are not the kind to leave useless services around, so we will remove our
    stack and stop our Swarm cluster as we prepare for the next chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: We won't need to clean up the network or running containers as they will automatically
    get removed by Docker once our stack is gone. With this part done, we can now
    move on to the next chapter about volumes with a clean slate.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we covered a multitude of things like: what service discovery
    is and why we need it, container orchestration basics and state reconciliation
    principles, as well as some major players in the orchestration world. With that
    knowledge in hand, we went on to implement a single-node full cluster using Docker
    Swarm to show how something like this can be done and near the end we used Docker
    stack to manage groups of services together, hopefully showing you how this can
    all be turned from theory to practice.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will start exploring the intricate world of Docker volumes
    and data persistence, so stick with us.
  prefs: []
  type: TYPE_NORMAL
