- en: Chapter 6. Autoscaling Microservices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spring Cloud provides the support essential for the deployment of microservices
    at scale. In order to get the full power of a cloud-like environment, the microservices
    instances should also be capable of scaling out and shrinking automatically based
    on traffic patterns.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter will detail out how to make microservices elastically grow and
    shrink by effectively using the actuator data collected from Spring Boot microservices
    to control the deployment topology by implementing a simple life cycle manager.
  prefs: []
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you will learn about the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: The basic concept of autoscaling and different approaches for autoscaling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The importance and capabilities of a life cycle manager in the context of microservices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Examining the custom life cycle manager to achieve autoscaling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Programmatically collecting statistics from the Spring Boot actuator and using
    it to control and shape incoming traffic
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reviewing the microservice capability model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter will cover the **Application Lifecycle Management** capability
    in the microservices capability model discussed in [Chapter 3](ch03.html "Chapter 3. Applying
    Microservices Concepts"), *Applying Microservices Concepts*, highlighted in the
    following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Reviewing the microservice capability model](img/B05447_6_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We will see a basic version of the life cycle manager in this chapter, which
    will be enhanced in later chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling microservices with Spring Cloud
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 5](ch05.html "Chapter 5. Scaling Microservices with Spring Cloud"),
    *Scaling Microservices with Spring Cloud*, you learned how to scale Spring Boot
    microservices using Spring Cloud components. The two key concepts of Spring Cloud
    that we implemented are self-registration and self-discovery. These two capabilities
    enable automated microservices deployments. With self-registration, microservices
    can automatically advertise the service availability by registering service metadata
    to a central service registry as soon as the instances are ready to accept traffic.
    Once the microservices are registered, consumers can consume the newly registered
    services from the very next moment by discovering service instances using the
    registry service. Registry is at the heart of this automation.
  prefs: []
  type: TYPE_NORMAL
- en: This is quite different from the traditional clustering approach employed by
    the traditional JEE application servers. In the case of JEE application servers,
    the server instances' IP addresses are more or less statically configured in a
    load balancer. Therefore, the cluster approach is not the best solution for automatic
    scaling in Internet-scale deployments. Also, clusters impose other challenges,
    such as they have to have exactly the same version of binaries on all cluster
    nodes. It is also possible that the failure of one cluster node can poison other
    nodes due to the tight dependency between nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The registry approach decouples the service instances. It also eliminates the
    need to manually maintain service addresses in the load balancer or configure
    virtual IPs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Scaling microservices with Spring Cloud](img/B05447_6_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As shown in the diagram, there are three key components in our automated microservices
    deployment topology:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Eureka** is the central registry component for microservice registration
    and discovery. REST APIs are used by both consumers as well as providers to access
    the registry. The registry also holds the service metadata such as the service
    identity, host, port, health status, and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **Eureka** client, together with the **Ribbon** client, provide client-side
    dynamic load balancing. Consumers use the Eureka client to look up the Eureka
    server to identify the available instances of a target service. The Ribbon client
    uses this server list to load-balance between the available microservice instances.
    In a similar way, if the service instance goes out of service, these instances
    will be taken out of the Eureka registry. The load balancer automatically reacts
    to these dynamic topology changes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The third component is the **microservices** instances developed using Spring
    Boot with the actuator endpoints enabled.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, there is one gap in this approach. When there is need for an additional
    microservice instance, a manual task is required to kick off a new instance. In
    an ideal scenario, the starting and stopping of microservice instances also require
    automation.
  prefs: []
  type: TYPE_NORMAL
- en: For example, when there is a requirement to add another Search microservice
    instance to handle the increase in traffic volumes or a load burst scenario, the
    administrator has to manually bring up a new instance. Also, when the Search instance
    is idle for some time, it needs to be manually taken out of service to have optimal
    infrastructure usage. This is especially relevant when services run on a pay-as-per-usage
    cloud environment.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the concept of autoscaling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Autoscaling is an approach to automatically scaling out instances based on the
    resource usage to meet the SLAs by replicating the services to be scaled.
  prefs: []
  type: TYPE_NORMAL
- en: 'The system automatically detects an increase in traffic, spins up additional
    instances, and makes them available for traffic handling. Similarly, when the
    traffic volumes go down, the system automatically detects and reduces the number
    of instances by taking active instances back from the service:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding the concept of autoscaling](img/B05447_6_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As shown in the preceding diagram, autoscaling is done, generally, using a set
    of reserve machines.
  prefs: []
  type: TYPE_NORMAL
- en: As many of the cloud subscriptions are based on a pay-as-you-go model, this
    is an essential capability when targeting cloud deployments. This approach is
    often called **elasticity**. It is also called **dynamic resource provisioning
    and deprovisioning**. Autoscaling is an effective approach specifically for microservices
    with varying traffic patterns. For example, an Accounting service would have high
    traffic during month ends and year ends. There is no point in permanently provisioning
    instances to handle these seasonal loads.
  prefs: []
  type: TYPE_NORMAL
- en: In the autoscaling approach, there is often a resource pool with a number of
    spare instances. Based on the demand, instances will be moved from the resource
    pool to the active state to meet the surplus demand. These instances are not pretagged
    for any particular microservices or prepackaged with any of the microservice binaries.
    In advanced deployments, the Spring Boot binaries are downloaded on demand from
    an artifact repository such as Nexus or Artifactory.
  prefs: []
  type: TYPE_NORMAL
- en: The benefits of autoscaling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are many benefits in implementing the autoscaling mechanism. In traditional
    deployments, administrators reserve a set of servers against each application.
    With autoscaling, this preallocation is no longer required. This prefixed server
    allocation may result in underutilized servers. In this case, idle servers cannot
    be utilized even when neighboring services struggle for additional resources.
  prefs: []
  type: TYPE_NORMAL
- en: 'With hundreds of microservice instances, preallocating a fixed number of servers
    to each of the microservices is not cost effective. A better approach is to reserve
    a number of server instances for a group of microservices without preallocating
    or tagging them against a microservice. Instead, based on the demand, a group
    of services can share a set of available resources. By doing so, microservices
    can be dynamically moved across the available server instances by optimally using
    the resources:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The benefits of autoscaling](img/B05447_6_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As shown in the preceding diagram, there are three instances of the **M1**
    microservice, one instance of **M2**, and one instance of **M3** up and running.
    There is another server kept unallocated. Based on the demand, the unallocated
    server can be used for any of the microservices: **M1**, **M2**, or **M3**. If
    **M1** has more service requests, then the unallocated instance will be used for
    **M1**. When the service usage goes down, the server instance will be freed up
    and moved back to the pool. Later, if the **M2** demand increases, the same server
    instance can be activated using **M2**.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the key benefits of autoscaling are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**It has high availability and is fault tolerant**: As there are multiple service
    instances, even if one fails, another instance can take over and continue serving
    clients. This failover will be transparent to the consumers. If no other instance
    of this service is available, the autoscaling service will recognize this situation
    and bring up another server with the service instance. As the whole process of
    bringing up or bringing down instances is automatic, the overall availability
    of the services will be higher than the systems implemented without autoscaling.
    The systems without autoscaling require manual intervention to add or remove service
    instances, which will be hard to manage in large deployments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, assume that two of instances of the Booking service are running.
    If there is an increase in the traffic flow, in a normal scenario, the existing
    instance might become overloaded. In most of the scenarios, the entire set of
    services will be jammed, resulting in service unavailability. In the case of autoscaling,
    a new Booking service instance can be brought up quickly. This will balance the
    load and ensure service availability.
  prefs: []
  type: TYPE_NORMAL
- en: '**It increases scalability**: One of the key benefits of autoscaling is horizontal
    scalability. Autoscaling allows us to selectively scale up or scale down services
    automatically based on traffic patterns.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**It has optimal usage and is cost saving**: In a pay-as-you-go subscription
    model, billing is based on actual resource utilization. With the autoscaling approach,
    instances will be started and shut down based on the demand. Hence, resources
    are optimally utilized, thereby saving cost.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**It gives priority to certain services or group of services**: With autoscaling,
    it is possible to give priority to certain critical transactions over low-value
    transactions. This will be done by removing an instance from a low-value service
    and reallocating it to a high-value service. This will also eliminate situations
    where a low-priority transaction heavily utilizes resources when high-value transactions
    are cramped up for resources.![The benefits of autoscaling](img/B05447_6_5.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For instance, the **Booking** and **Reports** services run with two instances,
    as shown in the preceding diagram. Let's assume that the **Booking** service is
    a revenue generation service and therefore has a higher value than the **Reports**
    service. If there are more demands for the **Booking** service, then one can set
    policies to take one **Reports** service out of the service and release this server
    for the **Booking** service.
  prefs: []
  type: TYPE_NORMAL
- en: Different autoscaling models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Autoscaling can be applied at the application level or at the infrastructure
    level. In a nutshell, application scaling is scaling by replicating application
    binaries only, whereas infrastructure scaling is replicating the entire virtual
    machine, including application binaries.
  prefs: []
  type: TYPE_NORMAL
- en: Autoscaling an application
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this scenario, scaling is done by replicating the microservices, not the
    underlying infrastructure, such as virtual machines. The assumption is that there
    is a pool of VMs or physical infrastructures available to scale up microservices.
    These VMs have the basic image fused with any dependencies, such as JRE. It is
    also assumed that microservices are homogeneous in nature. This gives flexibility
    in reusing the same virtual or physical machines for different services:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Autoscaling an application](img/B05447_6_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As shown in the preceding diagram, in scenario **A**, **VM3** is used for **Service
    1**, whereas in scenario **B**, the same **VM3** is used for **Service 2**. In
    this case, we only swapped the application library and not the underlying infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: This approach gives faster instantiation as we are only handling the application
    binaries and not the underlying VMs. The switching is easier and faster as the
    binaries are smaller in size and there is no OS boot required either. However,
    the downside of this approach is that if certain microservices require OS-level
    tuning or use polyglot technologies, then dynamically swapping microservices will
    not be effective.
  prefs: []
  type: TYPE_NORMAL
- en: Autoscaling the infrastructure
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In contrast to the previous approach, in this case, the infrastructure is also
    provisioned automatically. In most cases, this will create a new VM on the fly
    or destroy the VMs based on the demand:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Autoscaling the infrastructure](img/B05447_6_7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As shown in the preceding diagram, the reserve instances are created as VM images
    with predefined service instances. When there is demand for **Service 1**, **VM3**
    is moved to an active state. When there is a demand for **Service 2**, **VM4**
    is moved to the active state.
  prefs: []
  type: TYPE_NORMAL
- en: This approach is efficient if the applications depend upon the parameters and
    libraries at the infrastructure level, such as the operating system. Also, this
    approach is better for polyglot microservices. The downside is the heavy nature
    of VM images and the time required to spin up a new VM. Lightweight containers
    such as Dockers are preferred in such cases instead of traditional heavyweight
    virtual machines.
  prefs: []
  type: TYPE_NORMAL
- en: Autoscaling in the cloud
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Elasticity or autoscaling is one of the fundamental features of most cloud providers.
    Cloud providers use infrastructure scaling patterns, as discussed in the previous
    section. These are typically based on a set of pooled machines.
  prefs: []
  type: TYPE_NORMAL
- en: For example, in AWS, these are based on introducing new EC2 instances with a
    predefined AMI. AWS supports autoscaling with the help of autoscaling groups.
    Each group is set with a minimum and maximum number of instances. AWS ensures
    that the instances are scaled on demand within these bounds. In case of predictable
    traffic patterns, provisioning can be configured based on timelines. AWS also
    provides ability for applications to customize autoscaling policies.
  prefs: []
  type: TYPE_NORMAL
- en: Microsoft Azure also supports autoscaling based on the utilization of resources
    such as the CPU, message queue length, and so on. IBM Bluemix supports autoscaling
    based on resources such as CPU usage.
  prefs: []
  type: TYPE_NORMAL
- en: Other PaaS platforms, such as CloudBees and OpenShift, also support autoscaling
    for Java applications. Pivotal Cloud Foundry supports autoscaling with the help
    of Pivotal Autoscale. Scaling policies are generally based on resource utilization,
    such as the CPU and memory thresholds.
  prefs: []
  type: TYPE_NORMAL
- en: There are components that run on top of the cloud and provide fine-grained controls
    to handle autoscaling. Netflix Fenzo, Eucalyptus, Boxfuse, and Mesosphere are
    some of the components in this category.
  prefs: []
  type: TYPE_NORMAL
- en: Autoscaling approaches
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Autoscaling is handled by considering different parameters and thresholds. In
    this section, we will discuss the different approaches and policies that are typically
    applied to take decisions on when to scale up or down.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling with resource constraints
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This approach is based on real-time service metrics collected through monitoring
    mechanisms. Generally, the resource-scaling approach takes decisions based on
    the CPU, memory, or the disk of machines. This can also be done by looking at
    the statistics collected on the service instances themselves, such as heap memory
    usage.
  prefs: []
  type: TYPE_NORMAL
- en: 'A typical policy may be spinning up another instance when the CPU utilization
    of the machine goes beyond 60%. Similarly, if the heap size goes beyond a certain
    threshold, we can add a new instance. The same applies to downsizing the compute
    capacity when the resource utilization goes below a set threshold. This is done
    by gradually shutting down servers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Scaling with resource constraints](img/B05447_6_8.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In typical production scenarios, the creation of additional services is not
    done on the first occurrence of a threshold breach. The most appropriate approach
    is to define a sliding window or a waiting period.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are some of the examples:'
  prefs: []
  type: TYPE_NORMAL
- en: An example of a **response sliding window** is if 60% of the response time of
    a particular transaction is consistently more than the set threshold value in
    a 60-second sampling window, increase service instances
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In a **CPU sliding window**, if the CPU utilization is consistently beyond 70%
    in a 5 minutes sliding window, then a new instance is created
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An example of the **exception sliding window** is if 80% of the transactions
    in a sliding window of 60 seconds or 10 consecutive executions result in a particular
    system exception, such as a connection timeout due to exhausting the thread pool,
    then a new service instance is created
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In many cases, we will set a lower threshold than the actual expected thresholds.
    For example, instead of setting the CPU utilization threshold at 80%, set it at
    60% so that the system gets enough time to spin up an instance before it stops
    responding. Similarly, when scaling down, we use a lower threshold than the actual.
    For example, we will use 40% CPU utilization to scale down instead of 60%. This
    allows us to have a cool-down period so that there will not be any resource struggle
    when shutting down instances.
  prefs: []
  type: TYPE_NORMAL
- en: Resource-based scaling is also applicable to service-level parameters such as
    the throughput of the service, latency, applications thread pool, connection pool,
    and so on. These can also be at the application level, such as the number of **sales
    orders** processing in a service instance, based on internal benchmarking.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling during specific time periods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Time-based scaling is an approach to scaling services based on certain periods
    of the day, month, or year to handle seasonal or business peaks. For example,
    some services may experience a higher number of transactions during office hours
    and a considerably low number of transactions outside office hours. In this case,
    during the day, services autoscale to meet the demand and automatically downsize
    during the non-office hours:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Scaling during specific time periods](img/B05447_6_9.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Many airports worldwide impose restrictions on night-time landing. As a result,
    the number of passengers checking in at the airports during the night time is
    less compared to the day time. Hence, it is cost effective to reduce the number
    of instances during the night time.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling based on the message queue length
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This is particularly useful when the microservices are based on asynchronous
    messaging. In this approach, new consumers are automatically added when the messages
    in the queue go beyond certain limits:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Scaling based on the message queue length](img/B05447_6_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This approach is based on the competing consumer pattern. In this case, a pool
    of instances is used to consume messages. Based on the message threshold, new
    instances are added to consume additional messages.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling based on business parameters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this case, adding instances is based on certain business parameters—for
    example, spinning up a new instance just before handling **sales closing** transactions.
    As soon as the monitoring service receives a preconfigured business event (such
    as **sales closing minus 1 hour**), a new instance will be brought up in anticipation
    of large volumes of transactions. This will provide fine-grained control on scaling
    based on business rules:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Scaling based on business parameters](img/B05447_6_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Predictive autoscaling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Predictive scaling is a new paradigm of autoscaling that is different from the
    traditional real-time metrics-based autoscaling. A prediction engine will take
    multiple inputs, such as historical information, current trends, and so on, to
    predict possible traffic patterns. Autoscaling is done based on these predictions.
    Predictive autoscaling helps avoid hardcoded rules and time windows. Instead,
    the system can automatically predict such time windows. In more sophisticated
    deployments, predictive analysis may use cognitive computing mechanisms to predict
    autoscaling.
  prefs: []
  type: TYPE_NORMAL
- en: In the cases of sudden traffic spikes, traditional autoscaling may not help.
    Before the autoscaling component can react to the situation, the spike would have
    hit and damaged the system. The predictive system can understand these scenarios
    and predict them before their actual occurrence. An example will be handling a
    flood of requests immediately after a planned outage.
  prefs: []
  type: TYPE_NORMAL
- en: Netflix Scryer is an example of such a system that can predict resource requirements
    in advance.
  prefs: []
  type: TYPE_NORMAL
- en: Autoscaling BrownField PSS microservices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will examine how to enhance microservices developed in [Chapter
    5](ch05.html "Chapter 5. Scaling Microservices with Spring Cloud"), *Scaling Microservices
    with Spring Cloud*, for autoscaling. We need a component to monitor certain performance
    metrics and trigger autoscaling. We will call this component the **life cycle
    manager**.
  prefs: []
  type: TYPE_NORMAL
- en: The service life cycle manager, or the application life cycle manager, is responsible
    for detecting scaling requirements and adjusting the number of instances accordingly.
    It is responsible for starting and shutting down instances dynamically.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will take a look at a primitive autoscaling system to understand
    the basic concepts, which will be enhanced in later chapters.
  prefs: []
  type: TYPE_NORMAL
- en: The capabilities required for an autoscaling system
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A typical autoscaling system has capabilities as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The capabilities required for an autoscaling system](img/B05447_6_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The components involved in the autoscaling ecosystem in the context of microservices
    are explained as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Microservices**: These are sets of the up-and-running microservice instances
    that keep sending health and metrics information. Alternately, these services
    expose actuator endpoints for metrics collection. In the preceding diagram, these
    are represented as **Microservice 1** through **Microservice 4**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Service Registry**: A service registry keeps track of all the services, their
    health states, their metadata, and their endpoint URI.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Load Balancer**: This is a client-side load balancer that looks up the service
    registry to get up-to-date information about the available service instances.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lifecycle Manager**: The life cycle manger is responsible for autoscaling,
    which has the following subcomponents:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Metrics Collector**: A metrics collection unit is responsible for collecting
    metrics from all service instances. The life cycle manager will aggregate the
    metrics. It may also keep a sliding time window. The metrics could be infrastructure-level
    metrics, such as CPU usage, or application-level metrics, such as transactions
    per minute.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scaling policies**: Scaling policies are nothing but sets of rules indicating
    when to scale up and scale down microservices—for example, 90% of CPU usage above
    60% in a sliding window of 5 minutes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Decision Engine**: A decision engine is responsible for making decisions
    to scale up and scale down based on the aggregated metrics and scaling policies.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deployment Rules**: The deployment engine uses deployment rules to decide
    which parameters to consider when deploying services. For example, a service deployment
    constraint may say that the instance must be distributed across multiple availability
    regions or a 4 GB minimum of memory required for the service.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deployment Engine**: The deployment engine, based on the decisions of the
    decision engine, can start or stop microservice instances or update the registry
    by altering the health states of services. For example, it sets the health status
    as "out of service" to take out a service temporarily.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing a custom life cycle manager using Spring Boot
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The life cycle manager introduced in this section is a minimal implementation
    to understand autoscaling capabilities. In later chapters, we will enhance this
    implementation with containers and cluster management solutions. Ansible, Marathon,
    and Kubernetes are some of the tools useful in building this capability.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will implement an application-level autoscaling component
    using Spring Boot for the services developed in [Chapter 5](ch05.html "Chapter 5. Scaling
    Microservices with Spring Cloud"), *Scaling Microservices with Spring Cloud*.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the deployment topology
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following diagram shows a sample deployment topology of BrownField PSS
    microservices:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding the deployment topology](img/B05447_6_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As shown in the diagram, there are four physical machines. Eight VMs are created
    from four physical machines. Each physical machine is capable of hosting two VMs,
    and each VM is capable of running two Spring Boot instances, assuming that all
    services have the same resource requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Four VMs, **VM1** through **VM4**, are active and are used to handle traffic.
    **VM5** to **VM8** are kept as reserve VMs to handle scalability. **VM5** and
    **VM6** can be used for any of the microservices and can also be switched between
    microservices based on scaling demands. Redundant services use VMs created from
    different physical machines to improve fault tolerance.
  prefs: []
  type: TYPE_NORMAL
- en: Our objective is to scale out any services when there is increase in traffic
    flow using four VMs, **VM5** through **VM8**, and scale down when there is not
    enough load. The architecture of our solution is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the execution flow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Have a look at the following flowchart:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding the execution flow](img/B05447_6_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As shown in the preceding diagram, the following activities are important for
    us:'
  prefs: []
  type: TYPE_NORMAL
- en: The Spring Boot service represents microservices such as Search, Book, Fares,
    and Check-in. Services at startup automatically register endpoint details to the
    Eureka registry. These services are actuator-enabled, so the life cycle manager
    can collect metrics from the actuator endpoints.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The life cycle manager service is nothing but another Spring Boot application.
    The life cycle manager has a metrics collector that runs a background job, periodically
    polls the Eureka server, and gets details of all the service instances. The metrics
    collector then invokes the actuator endpoints of each microservice registered
    in the Eureka registry to get the health and metrics information. In a real production
    scenario, a subscription approach for data collection is better.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With the collected metrics information, the life cycle manager executes a list
    of policies and derives decisions on whether to scale up or scale down instances.
    These decisions are either to start a new service instance of a particular type
    on a particular VM or to shut down a particular instance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the case of shutdown, it connects to the server using an actuator endpoint
    and calls the shutdown service to gracefully shut down an instance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the case of starting a new instance, the deployment engine of the life cycle
    manager uses the scaling rules and decides where to start the new instance and
    what parameters are to be used when starting the instance. Then, it connects to
    the respective VMs using SSH. Once connected, it executes a preinstalled script
    (or passes this script as a part of the execution) by passing the required constraints
    as a parameter. This script fetches the application library from a central Nexus
    repository in which the production binaries are kept and initiates it as a Spring
    Boot application. The port number is parameterized by the life cycle manager.
    SSH needs to be enabled on the target machines.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this example, we will use **TPM** (**Transactions Per Minute**) or **RPM**
    (**Requests Per Minute**) as sampler metrics for decision making. If the Search
    service has more than 10 TPM, then it will spin up a new Search service instance.
    Similarly, if the TPM is below 2, one of the instances will be shut down and released
    back to the pool.
  prefs: []
  type: TYPE_NORMAL
- en: 'When starting a new instance, the following policies will be applied:'
  prefs: []
  type: TYPE_NORMAL
- en: The number of service instances at any point should be a minimum of 1 and a
    maximum of 4\. This also means that at least one service instance will always
    be up and running.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A scaling group is defined in such a way that a new instance is created on a
    VM that is on a different physical machine. This will ensure that the services
    run across different physical machines.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These policies could be further enhanced. The life cycle manager ideally provides
    options to customize these rules through REST APIs or Groovy scripts.
  prefs: []
  type: TYPE_NORMAL
- en: A walkthrough of the life cycle manager code
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will take a look at how a simple life cycle manager is implemented. This
    section will be a walkthrough of the code to understand the different components
    of the life cycle manager.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The full source code is available under the `Chapter 6` project in the code
    files. The `chapter5.configserver`, `chapter5.eurekaserver`, `chapter5.search`,
    and `chapter5.search-apigateway` are copied and renamed as `chapter6.*`, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps to implement the custom life cycle manager:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a new Spring Boot application and name it `chapter6.lifecyclemanager`.
    The project structure is shown in the following diagram:![A walkthrough of the
    life cycle manager code](img/B05447_6_15.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The flowchart for this example is as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A walkthrough of the life cycle manager code](img/B05447_6_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The components of this diagram are explained in details here.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a `MetricsCollector` class with the following method. At the startup
    of the Spring Boot application, this method will be invoked using `CommandLineRunner`,
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The preceding method looks for the services registered in the Eureka server
    and gets all the instances. In the real world, rather than polling, the instances
    should publish metrics to a common place, where metrics aggregation will happen.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following `DecisionEngine` code accepts the metric and applies certain
    scaling policies to determine whether the service requires scaling up or not:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Based on the service ID, the policies that are related to the services will
    be picked up and applied. In this case, a minimal TPM scaling policy is implemented
    in `TpmScalingPolicy`, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: If the policy returns `true`, `DecisionEngine` then invokes `DeploymentEngine`
    to spin up another instance. `DeploymentEngine` makes use of `DeploymentRules`
    to decide how to execute scaling. The rules can enforce the number of min and
    max instances, in which region or machine the new instance has to be started,
    the resources required for the new instance, and so on. `DummyDeploymentRule`
    simply makes sure the max instance is not more than 2.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`DeploymentEngine`, in this case, uses the **JSch** (**Java Secure Channel**)
    library from JCraft to SSH to the destination server and start the service. This
    requires the following additional Maven dependency:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The current SSH implementation is kept simple enough as we will change this
    in future chapters. In this example, `DeploymentEngine` sends the following command
    over the SSH library on the target machine:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Integration with Nexus happens from the target machine using Linux scripts with
    Nexus CLI or using `curl`. In this example, we will not explore Nexus.
  prefs: []
  type: TYPE_NORMAL
- en: The next step is to change the Search microservice to expose a new gauge for
    TPM. We have to change all the microservices developed earlier to submit this
    additional metric.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will only examine Search in this chapter, but in order to complete it, all
    the services have to be updated. In order to get the `gauge.servo.tpm` metrics,
    we have to add `TPMCounter` to all the microservices.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code counts the transactions over a sliding window of 1 minute:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code needs to be added to `SearchController` to set the `tpm`
    value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code is from the get REST endpoint (the search method) of `SearchRestController`,
    which submits the `tpm` value as a gauge to the actuator endpoint:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Running the life cycle manager
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Perform the following steps to run the life cycle manager developed in the
    previous section:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Edit `DeploymentEngine.java` and update the password to reflect the machine''s
    password, as follows. This is required for the SSH connection:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Build all the projects by running Maven from the root folder (`Chapter 6`)
    via the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, run RabbitMQ, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Ensure that the Config server is pointing to the right configuration repository.
    We need to add a property file for the life cycle manager.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run the following commands from the respective project folders:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Once all the services are started, open a browser window and load `http://localhost:8001`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Execute the flight search 11 times, one after the other, within a minute. This
    will trigger the decision engine to instantiate another instance of the Search
    microservice.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Open the Eureka console (`http://localhost:8761`) and watch for a second **SEARCH-SERVICE**.
    Once the server is started, the instances will appear as shown here:![Running
    the life cycle manager](img/B05447_6_17.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned the importance of autoscaling when deploying large-scale
    microservices.
  prefs: []
  type: TYPE_NORMAL
- en: We also explored the concept of autoscaling and the different models of and
    approaches to autoscaling, such as the time-based, resource-based, queue length-based,
    and predictive ones. We then reviewed the role of a life cycle manager in the
    context of microservices and reviewed its capabilities. Finally, we ended this
    chapter by reviewing a sample implementation of a simple custom life cycle manager
    in the context of BrownField PSS microservices.
  prefs: []
  type: TYPE_NORMAL
- en: Autoscaling is an important supporting capability required when dealing with
    large-scale microservices. We will discuss a more mature implementation of the
    life cycle manager in [Chapter 9](ch09.html "Chapter 9. Managing Dockerized Microservices
    with Mesos and Marathon"), *Managing Dockerized Microservices with Mesos and Marathon*.
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter will explore the logging and monitoring capabilities that are
    indispensable for successful microservice deployments.
  prefs: []
  type: TYPE_NORMAL
