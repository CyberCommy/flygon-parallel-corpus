- en: Reach for the Cloud
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we are going to learn how to build a Kubernetes cluster that
    runs on Amazon Web Services from first principles. In order to learn how Kubernetes
    works, we are going to manually launch the EC2 instances that will form this first
    cluster and manually install and configure the Kubernetes components.
  prefs: []
  type: TYPE_NORMAL
- en: The cluster that we will build is suitable for you to use when learning about
    managing Kubernetes and for developing applications that can run on Kubernetes.
    With these instructions, we are aiming to build the simplest possible cluster
    that we can deploy to AWS. Of course, this does mean that there are some things
    that you will want to do differently when building a cluster of mission-critical
    applications. But don't worry—there are three chapters in Part 3 - Ready for Production
    where we cover everything you need to know to get your cluster ready for even
    the most demanding applications.
  prefs: []
  type: TYPE_NORMAL
- en: Running a Kubernetes cluster on AWS costs money. The configuration we will cover
    in our instructions (a basic cluster with one master and one worker node) at the
    time of writing will cost around US 75 dollars a month. So if you are just using
    your cluster for experimentation and learning, remember to shut the instances
    down when you are finished for the day.
  prefs: []
  type: TYPE_NORMAL
- en: If you have finished with the cluster, terminate the instances and make sure
    that the EBS volumes have been deleted, because you will pay for these storage
    volumes even if the instances they are attached to have been stopped.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter is designed to be a learning experience, so read through and type
    out the commands as you read them. If you have the e-book version of this book,
    then resist the urge to copy and paste, as you will learn more if you type out
    the commands and take some time to understand what you are doing. There are tools
    that will do everything this chapter covers and more just by running one command,
    but hopefully building your first cluster manually, step by step, will give you
    some valuable insight into what is required to make a Kubernetes cluster tick.
  prefs: []
  type: TYPE_NORMAL
- en: Cluster architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The cluster we are going to set up in this chapter will be formed of two EC2
    instances—one that will run all the components for the Kubernetes control plane
    and another worker node that you can use to run your applications.
  prefs: []
  type: TYPE_NORMAL
- en: Because we start from scratch, this chapter will also lay out one method for
    isolating your Kubernetes cluster in a private network while allowing you easy
    access to the machines from your own workstation.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will achieve this by using an additional instance as a bastion host that
    will allow incoming SSH connections from the outside world, as shown in the following
    diagram. If your AWS account already has some infrastructure in place that can
    achieve this, then feel free to skip this section:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/1043a804-7d8a-4850-a577-1c74900d0ccf.png)The architecture of the
    cluster you will set up in this chapter'
  prefs: []
  type: TYPE_NORMAL
- en: Creating an AWS account
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you don't already have an AWS account, then head over to [https://aws.amazon.com/](https://aws.amazon.com/)
    and sign up for one. Before you can create resources in your account, you will
    need to add a credit card to your account to cover any charges.
  prefs: []
  type: TYPE_NORMAL
- en: When you first sign up for an AWS account, you will be eligible for a free usage
    tier on some services for the first 12 months. Unfortunately, this free tier doesn't
    provide quite enough resources to run Kubernetes, but in this chapter, we have
    optimized our choice of instances for their low cost, so you should be able to
    follow the examples without spending too much.
  prefs: []
  type: TYPE_NORMAL
- en: Creating an IAM user
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When you sign up for an AWS account, the email address and password you choose
    will be used to log in to the root account. Before you start to interact with
    AWS, it is a good idea to create an IAM user that you will use to interact with
    AWS. The advantage of this is that if you wish, you can give each IAM user as
    much or as little access as they need to AWS services. If you use the root account,
    you automatically have full access and have no way to manage or revoke privileges.
    Go through the following steps to set up the account:'
  prefs: []
  type: TYPE_NORMAL
- en: Once you have logged into the AWS console, go to the Identity and Access Management
    dashboard by clicking on Services and typing `IAM` into the search box.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: From the sidebar, choose Users to view the IAM users in your AWS account. If
    you have only just set up a new account, there won't be any users here yet—the
    root account doesn't count as a user.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Start the process of setting up a new user account by clicking on the Add user
    button at the top of the screen.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Start by choosing a username for your user. Check both boxes to enable **Programmatic
    access** (so you can use the command-line client) and **AWS Management Console
    access** so you can log into the web console, as shown in the preceding screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/799d9334-a224-448c-a56f-903fc5fc0ed4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'On the next screen, you can configure the permissions for your user. Choose Attach
    existing policies directly, then choose the AdministratorAccess policy, as shown
    in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/ba3b9e52-806b-49df-8619-1639c1cffb85.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Review your settings, then click Create user:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/495a44df-8027-4b9b-8a3e-53ae35bb5b90.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Once your user has been created, take a note of the credentials. You will need
    the **Access key ID** and **Secret access key** shortly to configure the AWS command-line
    client. Also take a note of the console sign-in link, as this is unique to your
    AWS account, shown as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/c16d5213-97fe-40d3-af81-f55a87316886.png)'
  prefs: []
  type: TYPE_IMG
- en: Once you have set up an IAM user for yourself, log out of the root account in
    your browser and check that you can sign back in using your username and password.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You might want to set up two-factor authentication for your AWS account for
    greater security. Remember that anyone with Administrator access to the account
    can access or delete any of the resources in your account.
  prefs: []
  type: TYPE_NORMAL
- en: Getting the CLI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can control AWS using the web console, but your control of AWS will be more
    precise if you do everything from the command line with the AWS command-line client.
  prefs: []
  type: TYPE_NORMAL
- en: You should follow the instructions provided by AWS to install the command-line
    client on your system (or by using your systems package manager) using the instructions
    found at [https://docs.aws.amazon.com/cli/latest/userguide/installing.html](https://docs.aws.amazon.com/cli/latest/userguide/installing.html)).
  prefs: []
  type: TYPE_NORMAL
- en: Once you have installed the command-line client, run the `aws configure` command
    to configure the CLI with your credentials. This command will update the `aws
    config` files in your home directory.
  prefs: []
  type: TYPE_NORMAL
- en: At this stage, you should choose an AWS region for your cluster. For testing
    and experimentation, it makes sense for you to choose one that is located relatively
    close to your location. Doing so will improve latency when you access your instances
    with `ssh` or `connect` to the Kubernetes API.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up a key pair
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we launch an EC2 instance, we want to be able to access it via SSH. We
    can register a key pair in the EC2 console to allow us to log in once our instance
    has been launched.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is possible for us to instruct AWS to generate a key pair for you (that
    you can then download). But the best practice is to generate a key pair on your
    workstation and upload the public part to AWS. This ensures that you (and only
    you) have control of your instance, since the private half of your key will never
    leave your own machine. To set up the key pair, go through the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'You may already have a key pair on your machine that you wish to use. You can
    check for existing keys by looking in your `.ssh` directory, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In this example, you can see that I have one key pair in my `.ssh` directory—the
    private key has the default name of `id_rsa` and the public key is called `id_rsa.pub`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If you don''t already have a key pair set up, or if you want to create a fresh
    one, then you can use the `ssh-keygen` command to create a new one, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This command creates a new key pair using your email address as a label.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, choose where to save the new key pair. If you don''t already have a key
    pair, just press *Enter* to write it to the default location as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Next, you will be asked for a passphrase (password). If you just press *Enter*,
    then the key will be created without any password protection, as shown in the
    following command. If you choose a password, make sure that you remember it or
    store it securely, as you won't be able to use your SSH key (or access your instances)
    without it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Once you have an SSH key pair on your machine, you can go about importing it
    into your AWS account. Remember that you only need to import the public part of
    your key pair. This will be in a file that ends with the `.pub` extension.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'From the AWS EC2 console (click on Services and then search for EC2), choose
    **Key Pairs** from the menu on the left of the screen, as shown in the following
    screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/e53ef48a-a859-437d-a809-c6aac3c95f1a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'From this screen, choose **Import Key Pair** to bring up a dialog where you
    can upload your key pair, as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/701fb56c-3d73-4bdc-88ad-da0ea9fc0573.png)'
  prefs: []
  type: TYPE_IMG
- en: Choose a name that will identify your key pair within AWS (I chose `eds_laptop`).
    Then, either navigate to your key's location or just paste its text into the large
    text box, and then click **Import**. Once you have imported your key, you should
    see it listed on the **Key Pairs** page.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you are using AWS in more than one region, you will need to import a key
    pair in each region that you want to launch instances in.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will set up a new VPC in your AWS account. A VPC, or virtual private cloud,
    allows us to have a private network that is isolated from all the other users
    of EC2 and the internet that we can launch instances onto.
  prefs: []
  type: TYPE_NORMAL
- en: 'It provides a secure foundation that we can use to build a secure network for
    our cluster, as shown in the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The `VpcId` will be unique to your account, so I am going to set a shell variable
    that I can use to refer to it whenever we need. You can do the same with the `VpcId`
    from your account, or you might prefer to just type it out each time you need
    it.
  prefs: []
  type: TYPE_NORMAL
- en: 'The rest of the steps in this chapter follow this pattern, but if you don''t
    understand what is happening, don''t be afraid to look at the shell variables
    and correlate the IDs with the resources in the AWS console, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Kubernetes names your instances based on the internal DNS hostnames that AWS
    assigns to them. If we enable DNS support in the VPC, then we will be able to
    resolve these hostnames when using the DNS server provided inside the VPC, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Kubernetes makes extensive use of AWS resource tags, so it knows which resources
    it can use and which resources are managed by Kubernetes. The key for these tags
    is `kubernetes.io/cluster/<cluster_name>`. For resources that might be shared
    between several distinct clusters, we use the `shared` value. This means that
    Kubernetes can make use of them, but won't ever remove them from your account.
  prefs: []
  type: TYPE_NORMAL
- en: We would use this for resources such as VPCs. Resources where the life cycle
    is fully managed by Kubernetes have a tag value of `owned` and may be deleted
    by Kubernetes if they are no longer required. Kubernetes typically creates these
    tags automatically when it creates resources such as instances in an autoscaling
    group, EBS volumes, or load balancers.
  prefs: []
  type: TYPE_NORMAL
- en: I like to name the clusters I create after famous people from the history of
    computer science. The cluster I created for this chapter is named after Grace
    Hopper, who designed the COBOL programming language.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s add a tag to our new VPC so that Kubernetes will be able to use it,
    as shown in the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'When we created our VPC, a main route table was automatically created. We will
    use this for routing in our private subnet. Let''s grab the ID to use later, as
    shown in the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we will add a second route table to manage routing for the public subnets
    in our VPC, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we will give the route tables names so we can keep track of them later:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Next, we are going to create two subnets for our cluster to use. Because I am
    creating my cluster in the `eu-west-1` region (Ireland), I am going to create
    these subnets in the `eu-west-1a` subnet. You should choose an availability zone
    for your cluster from the region you are using by running `aws ec2 describe-availability-zones`.
    In Part 3, we will learn how to create high-availability clusters that span multiple
    availability zones.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start by creating a subnet for instances that will only be accessible
    from within our private network. We are going to use a `/20 netmask` on the CIDR
    block, as shown in the following command; with this, AWS will give us 4089 IP
    addresses that will be available to be assigned to our EC2 instances and to pods
    launched by Kubernetes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let''s add another subnet to the same availability zone, as shown in
    the following command. We will use this subnet for instances that need to be accessible
    from the internet, such as public load balancers and bastion hosts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we should associate this subnet with the public route table, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'In order for the instances in our public subnet to communicate with the internet,
    we will create an internet gateway, attach it to our VPC, and then add a route
    to the route table, routing traffic bound for the internet to the gateway, as
    shown in the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to configure the instances in the private subnet, we will need them
    to be able to make outbound connections to the internet in order to install software
    packages and so on. To make this possible, we will add a NAT gateway to the public
    subnet and then add a route to the private route table for internet-bound traffic,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'At this stage, you may have to wait a few moments for the NAT gateway to be
    created before creating the route, as shown in the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Setting up a bastion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will use the first host we are going to launch as a bastion host that will
    allow us to connect to other servers that are only accessible from within the
    private side of our VPC network.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will be creating a security group to allow SSH traffic to this instance.
    We will use the `aws ec2 create-security-group` command to create a security group
    for our bastion host, as shown in the following command. A security group is an
    abstraction that AWS provides in order to group related firewall rules together
    and apply them to groups of hosts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have created a security group, we can attach a rule to it to allow
    SSH ingress on port `22`, as shown in the following command. This will allow you
    to access your host with an SSH client. Here, I am allowing ingress from the CIDR
    range `0.0.0.0/0`, but if your internet connection has a stable IP address, you
    might want to limit access to just your own IP:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have set up the security group for the bastion host, we can go about
    launching our first EC2 instance. In this chapter, I will be using Ubuntu Linux
    (a popular Linux distribution). Before we can launch the instance, we will need
    to discover the ID of the AMI (Amazon machine image) for the operating system
    we want to use.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Ubuntu project regularly publishes updated images to their AWS account
    that can be used to launch EC2 instances. We can run the following command to
    discover the ID of the image that we require:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'We are going to use a `t2.micro` instance for the bastion host (as shown in
    the following command), as the usage for this instance type is included in the
    AWS free tier, so you won''t have to pay for it for the first 12 months after
    you set up your AWS account:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Note that we are passing the ID of the subnet we chose to use, the ID of the
    security group we just created, and the name of the key pair we uploaded.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let''s update the instance with a `Name` tag so we can recognize it when
    looking at the EC2 console, as shown in the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the instance has launched, you should be able to run the `aws ec2 describe-instances`
    command to discover the public IP address of your new instance, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'You should now be able to access the instance with SSH, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'As you log in, you should see a message like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'If you saved your key pair as something other than the default `~/.ssh/id_rsa`,
    you can pass the path to your key using the `-i` flag, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`**ssh -i ~/.ssh/id_aws_rsa ubuntu@$BASTION_IP**`'
  prefs: []
  type: TYPE_NORMAL
- en: 'As an alternative, you can add the key to your SSH agent first by running the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`**ssh-add ~/.ssh/id_aws_rsa**`'
  prefs: []
  type: TYPE_NORMAL
- en: sshuttle
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is possible to forward traffic from your workstation to the private network
    by just using SSH. However, we can make accessing servers via the bastion instance
    much more convenient by using the `sshuttle` tool.
  prefs: []
  type: TYPE_NORMAL
- en: It is simple to install `sshuttle` on your workstation.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can install it on macOS using Homebrew, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also install it on Linux (if you have Python installed), as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'To transparently proxy traffic to the instances inside the private network,
    we can run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Firstly, we pass the SSH login details of our `ubuntu@$BASTION_IP` bastion instance,
    followed by the CIDR of our VPC (so that only traffic destined for the private
    network passes over the tunnel); this can be found by running `aws ec2 describe-vpcs`.
    Finally, we pass the `--dns` flag so that DNS queries on your workstation will
    be resolved by the DNS servers of the remote instance.
  prefs: []
  type: TYPE_NORMAL
- en: Using `sshuttle` requires you to enter your local sudo password in order to
    set up its proxy server.
  prefs: []
  type: TYPE_NORMAL
- en: You might want to run `sshuttle` in a separate terminal or in the background
    so that you still have access to the shell variables we have been using.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can validate that this setup is working correctly by trying to log in to
    our instance through its private DNS name, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: This tests whether you can resolve a DNS entry from the private DNS provided
    by AWS to instances running within your VPC, and whether the private IP address
    now returned by that query is reachable.
  prefs: []
  type: TYPE_NORMAL
- en: If you have any difficulties, check `sshuttle` for any connection errors and
    ensure that you have remembered to enable DNS support in your VPC.
  prefs: []
  type: TYPE_NORMAL
- en: Instance profiles
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order for Kubernetes to make use of its integrations with the AWS cloud APIs,
    we need to set up IAM instance profiles. An instance profile is a way for the
    Kubernetes software to authenticate with the AWS API, and for us to assign fine-grained
    permissions on the actions that Kubernetes can take.
  prefs: []
  type: TYPE_NORMAL
- en: It can be confusing to learn all of the permissions that Kubernetes requires
    to function correctly. You could just set up instance profiles that allow full
    access to AWS, but this would be at the expense of security best practice.
  prefs: []
  type: TYPE_NORMAL
- en: Whenever we assign security permissions, we should be aiming to grant just enough
    permissions for our software to function correctly. To this end, I have collated
    a set of minimal IAM policies that will allow our cluster to function correctly,
    without giving excess permissions away.
  prefs: []
  type: TYPE_NORMAL
- en: You can view these policies at [https://github.com/errm/k8s-iam-policies](https://github.com/errm/k8s-iam-policies),
    where I have documented each policy with a brief description of its purpose.
  prefs: []
  type: TYPE_NORMAL
- en: 'The repository includes a simple shell script that we can use to create an
    IAM instance profile for the master and worker nodes in our cluster, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Kubernetes software
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We are going to launch an instance in which we will install all of the software
    that the different nodes that make up our cluster will need. We will then create
    an AMI, or Amazon machine image, that we can use to launch the nodes on our cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we create a security group for this instance, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'We will need to be able to access this instance from our bastion host in order
    to log in and install software, so let''s add a rule to allow SSH traffic on port
    `22` from instances in the `ssh-bastion` security group, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'We are just using a `t2.mico` instance here since we don''t need a very powerful
    instance just to install packages, as shown in the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'We add a `Name` tag so we can identify the instance later if we need, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Grab the IP address of the instance, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Then log in with `ssh`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we are ready to start configuring the instance with the software and configuration
    that all of the nodes in our cluster will need. Start by synchronizing the apt
    repositories, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Docker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes can work with a number of container runtimes, but Docker is still
    the most widely used.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we install Docker, we will add a `systemd` drop-in config file to the
    Docker service, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'In order for our Kubernetes pods to be accessible to other instances in the
    cluster, we need to set the default policy for the `iptables FORWARD` chain as
    shown in the following command; otherwise, Docker will set this to `DROP` and
    traffic for Kubernetes services will be dropped:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Kubernetes will work well with the version of Docker that is included in the
    Ubuntu repositories, so we can install it simply by installing the `docker.io`
    package, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Check that Docker is installed by running the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Installing Kubeadm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Next, we will install the packages that we need to set up a Kubernetes control
    plane on this host. These packages are described in the following list:'
  prefs: []
  type: TYPE_NORMAL
- en: '`kubelet`: The node agent that Kubernetes uses to control the container runtime.
    This is used to run all the other components of the control plane within Docker
    containers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kubeadm`: This utility is responsible for bootstrapping a Kubernetes cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kubectl`: The Kubernetes command-line client, which will allow us to interact
    with the Kubernetes API server.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'First, add the signing key for the apt repository that hosts the Kubernetes
    packages, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Next add the Kubernetes apt repository, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, resynchronize the package indexes, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, install the required packages, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: This will install the latest version of the packages. If you want to pin to
    a specific version of Kubernetes, try running `apt-cache madison kubeadm` to see
    the different versions available.
  prefs: []
  type: TYPE_NORMAL
- en: 'I have prepared this chapter using Kubernetes 1.10\. If, you wanted to install
    the most recent release of Kubernetes 1.10, you could run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '`**sudo apt-get install kubeadm=1.10.* kubectl=1.10.* kubelet=1.10.***`'
  prefs: []
  type: TYPE_NORMAL
- en: Building an AMI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we are done with installing packages on this instance, we can shut
    it down, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'We can use the `create-image` command to instruct AWS to snapshot the root
    volume of our instance and use it to produce an AMI, as shown in the following
    command (you might need to wait a few moments for the instance to fully stop before
    running the command):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'It can take a few minutes for the image to become available for you to use,
    but you can check on its status with the `describe-images` command, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: While the image is being built, you will see `pending`, but once it is ready
    to use the state will have changed to `available`.
  prefs: []
  type: TYPE_NORMAL
- en: Bootstrapping the cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now we can launch an instance for Kubernetes control plane components. First,
    we will create a security group for this new instance, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'We will need to be able to access this instance from our bastion host in order
    to log in and configure the cluster. We will add a rule to allow SSH traffic on
    port `22` from instances in the `ssh-bastion` security group, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can launch the instance, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'We should give the instance a name, and to ensure that Kubernetes can associate
    all of the resources with our cluster, we will also add the `KubernetesCluster`
    tag with a name for this cluster, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'To ensure that all the Kubernetes components use the same name, we should set
    the hostname to match the name given by the AWS metadata service, as shown in
    the following command. This is because the name from the metadata service is used
    by components that have the AWS cloud provider enabled:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'To correctly configure the kubelet to use the AWS cloud provider, we create
    a `systemd` drop-in file to pass some extra arguments to the kubelet, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Once you have added this file, reload the `systemd` configuration, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to provide `kubeadm` with a configuration file in order to enable the
    AWS cloud provider on each of the components that it will launch. Here, we also
    set `tokenTTL` to `0`, as shown in the following command; this means that the
    token that is issued to allow worker nodes to join the cluster won''t expire.
    This is important, as we plan to manage our workers with an autoscaling group,
    and new nodes could join the group after a while:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we just need to run the following command to bootstrap the master:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: You should see the preceding message followed by some instructions to set up
    the rest of the cluster. Make a note of the `kubeadm join` command as we will
    need it to set up the worker node(s).
  prefs: []
  type: TYPE_NORMAL
- en: 'We can check that the API server is functioning correctly by following the
    instructions given by `kubeadm` to set up `kubectl` on the host, as shown in the
    following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Try running the `kubectl` version. If `kubectl` can correctly connect to the
    host, then you should be able to see the version of the Kubernetes software for
    the client (`kubectl`) and on the server, as shown in the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: What just happened?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So that was easy right? We got the Kubernetes control plane up and running by
    running one command.
  prefs: []
  type: TYPE_NORMAL
- en: The `kubeadm` command is a fantastic tool because it takes a lot of the guesswork
    out of correctly configuring Kubernetes. But let's take a brief intermission from
    setting up our cluster and dig a little bit deeper to discover what actually just
    happened.
  prefs: []
  type: TYPE_NORMAL
- en: Looking though the output from the `kubeadm` command should give us some clues.
  prefs: []
  type: TYPE_NORMAL
- en: The first thing that `kubeadm` did was to establish a private key infrastructure.
    If you take a look at the `/etc/kubernetes/pki` directory, you can see a number
    of `ssl` certificates and private keys, as well as a certificate authority that
    was used to sign each key pair. Now, when we add worker nodes to the cluster,
    they will be able to establish secure communication between the kubelet and the
    `apiserver`.
  prefs: []
  type: TYPE_NORMAL
- en: Next, `kubedam` wrote static pod manifests to the `/etc/kubernetes/manifests/`
    directory. These manifests are just like the pod definitions that you would submit
    to the Kubernetes API sever to run your own applications, but since the API server
    has not yet started, the definition is read directly from the disk by the `kubelet`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `kubelet` is configured to read these static pod manifests in a `systemd
    dropin` that `kubeadm` creates at `etc/systemd/system/kubelet.service.d/10-kubeadm.conf`.
    You can see the following flag among the other configurations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'If you look in `/etc/kubernetes/manifests/`, you will see Kubernetes pod specifications
    for each of the components that form the control plane, as described in the following
    list:'
  prefs: []
  type: TYPE_NORMAL
- en: '`etcd.yaml`: The key value store that stores the state of the API server'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kube-apiserver.yaml`: The API server'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kube-controller-manager.yaml`: The controller manager'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kube-scheduler.yaml`: The scheduler'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Finally, once the API server has started, `kubeadm` submits two add-ons to
    the API, as described in the following list:'
  prefs: []
  type: TYPE_NORMAL
- en: '`kube-proxy`: This is the process that configures iptables on each node to
    make the service IPs route correctly. It is run on each node with a DaemonSet.
    You can look at this configuration by running `kubectl -n kube-system describe
    ds kube-proxy`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kube-dns`: This process provides the DNS server that can be used by applications
    running on the cluster for service discovery. Note that it will not start running
    correctly until you have configured a pod network for your cluster. You can view
    the configuration for `kube-dns` by running `kubectl -n kube-system describe deployment
    kube-dns`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You could try using `kubectl` to explore the different components that make
    up the Kubernetes control plane. Try running the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '`**$ kubectl -n kube-system get pods**`'
  prefs: []
  type: TYPE_NORMAL
- en: '`**$ kubectl -n kube-system describe pods**`'
  prefs: []
  type: TYPE_NORMAL
- en: '`**$ kubectl -n kube-system get daemonsets**`'
  prefs: []
  type: TYPE_NORMAL
- en: '`**$ kubectl -n kube-system get deployments**`'
  prefs: []
  type: TYPE_NORMAL
- en: '`**Before you continue with the next section, log out of the master instance,
    as follows:**`'
  prefs: []
  type: TYPE_NORMAL
- en: '`**$ exit**`'
  prefs: []
  type: TYPE_NORMAL
- en: '`**logout**`'
  prefs: []
  type: TYPE_NORMAL
- en: '`**Connection to 10.0.0.10 closed.**`'
  prefs: []
  type: TYPE_NORMAL
- en: Access the API from your workstation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is convenient to be able to access the Kubernetes API server via `kubectl`
    on your workstation. It means that you can submit any manifests that you may have
    been developing to your cluster running on AWS.
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to allow traffic from the bastion server to access the API server.
    Let''s add a rule to the `K8S-MASTER` security group to allow this traffic, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: If you haven't already installed kubectl on your workstation, turn back to [Chapter
    2](87aab5e7-ff37-4a46-831b-8ff62708b7d8.xhtml), *Start Your Engines*, to learn
    how.
  prefs: []
  type: TYPE_NORMAL
- en: Now we can copy the `kubeconfig` file from the master instance.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you do not already have any clusters configured in your local `~/.kube/config`
    file, you can copy the file from the master, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: If you already have a cluster configured (for example, minikube), then you may
    wish to merge the config for your new cluster, or use another file and pass its
    location to `kubectl` with the `--kubeconfig` flag, or in the `KUBECONFIG` environment
    variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Check that you can connect to the API server using your local `kubectl`, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: If you have any problem connecting, check that `sshuttle` is still running,
    and that you have correctly allowed access from the bastion host to the k8s-master
    security group.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up pod networking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You may have noticed that, when running `kubectl get nodes`, the `NodeStatus`
    is `NotReady`. This is because the cluster we have bootstrapped is missing one
    essential component—the network infrastructure that will allow the pods running
    on our cluster to communicate with one another.
  prefs: []
  type: TYPE_NORMAL
- en: 'The network model of a Kubernetes cluster is somewhat different from that of
    a standard Docker installation. There are many implementations of networking infrastructure
    that can provide cluster networking for Kubernetes, but they all have some key
    attributes in common, as shown in the following list:'
  prefs: []
  type: TYPE_NORMAL
- en: Each pod is assigned its own IP address
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each pod can communicate with any other pod in the cluster without NAT (not
    withstanding additional security policies)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The internal network that the software running inside a pod sees is identical
    to the pod network seen by other pods in the cluster—that is, it sees that the
    IP address is the same and that no port mapping takes place
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This networking arrangement is much simpler (for users of the cluster) than
    Docker's standard networking scheme of mapping internal ports in the container
    to other ports on the host.
  prefs: []
  type: TYPE_NORMAL
- en: It does, however, require some integration between the network infrastructure
    and Kubernetes. Kubernetes manages this integration though an interface called
    the **container network interface** (**CNI**). It is simple to deploy a **CNI**
    plugin to each node of your cluster with a Kubernetes DaemonSet.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to learn more about Kubernetes cluster networking, I recommend reading
    the comprehensive documentation of the underlying concepts at [https://kubernetes.io/docs/concepts/cluster-administration/networking/](https://kubernetes.io/docs/concepts/cluster-administration/networking/).
  prefs: []
  type: TYPE_NORMAL
- en: We will be deploying a CNI plugin called `amazon-vpc-cni-k8s` that integrates
    Kubernetes with the native networking capabilities of the AWS VPC network. This
    plugin works by attaching secondary private IP addresses to the elastic network
    interfaces of the EC2 instances that form the nodes of our cluster, and then assigning
    them to pods as they are scheduled by Kubernetes to go into each node. Traffic
    is then routed directly to the correct node by the AWS VPC network fabric.
  prefs: []
  type: TYPE_NORMAL
- en: 'Deploying this plugin is a similar process to submitting any other manifest
    to the Kubernetes API with `kubectl`, as shown in the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'You can monitor the networking plugin that is being installed and started by
    running the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'We can check that the network has been set up correctly by looking at the node
    status again, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: Launching worker nodes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We are now going to create a new security group for the worker nodes, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'We will allow access to the worker nodes via the bastion host in order for
    us to log in for debugging purposes, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'We want to allow the kubelet and other processes running on the worker nodes
    to be able to connect to the API server on the master node. We do this using the
    following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'Since the kube-dns add-on may run on the master node, let''s allow this traffic
    from the nodes security group, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'We also need the master node to be able to connect to the APIs that are exposed
    by the kubelet in order to stream logs and other metrics. We enable this by entering
    the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we need to allow any pod on any node to be able to connect to any
    other pod. We do this using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: In order to have the worker node(s) register themselves with the master when
    they start up, we will create a user-data script.
  prefs: []
  type: TYPE_NORMAL
- en: This script is run on the first occasion that the node is started. It makes
    some configuration changes, then runs `kubeadm join`, as shown in the following
    command. You should have made a note of the `kubeadm join` command when we initialized
    the master.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'First, we create a launch configuration using the following command. This is
    like a template of the configuration that the autoscaling group will use to launch
    our worker nodes. Many of the arguments are similar to those that we would have
    passed to the EC2 run-instances command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have created the launch configuration, we can create an autoscaling
    group, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: You will need to wait a few moments for the autoscaling group to launch the
    node, and for `kubeadm` to register it with the master, as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: If your node starts but doesn't join the cluster after a few minutes, try logging
    into the node and looking at the `cloud-init` log file. The end of this log will
    include the output from your script.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: Demo time
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Congratulations, if you have made it this far through the chapter! You should
    by now have a fully functional Kubernetes cluster that you can use to experiment
    with and explore Kubernetes more fully.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s demonstrate that the cluster we have built is working by deploying an
    application to our cluster, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'This manifest deploys a simple web application and a service to expose the
    application to the internet using a load balancer. We can view the public DNS
    name of the load balancer by using the `kubectl get service` command, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'Once you have the public address of the load balancer, you might need to wait
    for a few moments before the address starts to resolve. Visit the address in your
    browser; you should see a screen like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/67570712-97ff-4e23-92ac-9b70d0857877.png)'
  prefs: []
  type: TYPE_IMG
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By now, you should have a fully functional Kubernetes cluster that you can use
    to experiment with and explore Kubernetes more fully. Your cluster is correctly
    configured to take advantage of the many integrations that Kubernetes has with
    AWS.
  prefs: []
  type: TYPE_NORMAL
- en: While there are many tools that can automate and assist you in the task of building
    and managing a Kubernetes cluster on AWS, hopefully by learning how to approach
    the task from scratch, you will have a better understanding of the networking
    and computing resources needed to support a Kubernetes cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Part 3, we will build on the knowledge from this chapter and discuss the
    additional components that you will need to add to a cluster to make it suitable
    for hosting production services. The cluster we have just built is a fully functional
    installation of Kubernetes. Read on as we look at the tools and techniques required
    to successfully operate production services on Kubernetes:'
  prefs: []
  type: TYPE_NORMAL
- en: We will look at the tools and procedures you can adopt to manage deploying and
    updating your services effectively using Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will look at the strategies and tools you can adopt to secure your cluster
    and the applications running on it
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will look at the tools typically used with Kubernetes for monitoring and
    log management
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will look at the best ways to architect your applications and your clusters
    in order to meet availability targets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
