- en: Chapter 3. Meeting Docker Swarm Mode
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At Dockercon 16, the Docker team presented a new way of operating Swarm clusters,
    called Swarm Mode. The announcement was slightly anticipated by the introduction
    of a new set of tools, said to *operate distributed systems at any scale* called
    **Swarmkit**.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduce Swarmkit
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduce Swarm Mode
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compare Swarm v1, Swarmkit, and Swarm Mode
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a test Swarmkit cluster, and launch services on it
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do not skip reading the Swarmkit section, because Swarmkit acts as a foundation
    for Swarm mode. Seeing Swarmkit is the way we chose to introduce Swarm Mode concepts,
    such as nodes, services, tasks.
  prefs: []
  type: TYPE_NORMAL
- en: We'll show how to create production-level big Swarm Mode clusters in [Chapter
    4](ch04.html "Chapter 4. Creating a Production-Grade Swarm"), *Creating a Production-Grade
    Swarm*.
  prefs: []
  type: TYPE_NORMAL
- en: Swarmkit
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Alongside with Swarm Mode, the Docker team at DockerCon16 released Swarmkit,
    defined as a:'
  prefs: []
  type: TYPE_NORMAL
- en: '*"Toolkit for orchestrating distributed systems at any scale. It includes primitives
    for node discovery, raft-based consensus, task scheduling, and more."*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Swarms** clusters are made of active nodes, that can either act as managers
    or workers.'
  prefs: []
  type: TYPE_NORMAL
- en: Managers, that coordinate via Raft (that is, they elect leaders when quorum
    is available, as described in [Chapter 2](ch02.html "Chapter 2. Discover the Discovery
    Services"), *Discover the Discovery Services*), are responsible for allocating
    resources, orchestrating services, and dispatching tasks along the cluster. Workers
    run tasks.
  prefs: []
  type: TYPE_NORMAL
- en: The cluster goal is to execute *services*, so what's required to be run is defined
    at high level. For example, a service could be "web". Work units assigned to nodes
    are instead called **tasks**. A task allocated for a "web" service could be, for
    example, a container running the nginx container, and may be named as web.5.
  prefs: []
  type: TYPE_NORMAL
- en: It's very important to notice that we are speaking of services and that a service
    may be containers. May be, it's not necessary. In this book, our focus will be
    of course on containers, but the intention of Swarmkit is to theoretically abstract
    orchestration of any object.
  prefs: []
  type: TYPE_NORMAL
- en: Versions and support
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A note on versions. Docker Swarm mode, which we'll introduce in the upcoming
    sections, is compatible only with Docker 1.12+. With Swarmkit, instead, you can
    orchestrate even previous versions of Docker Engines, for example, 1.11 or 1.10.
  prefs: []
  type: TYPE_NORMAL
- en: Swarmkit architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Swarmkit** is the orchestration mechanism released to handle clusters of
    services of any size.'
  prefs: []
  type: TYPE_NORMAL
- en: In a Swarmkit cluster, nodes can be either **managers** (of the cluster) or
    **workers** (the workhorses of cluster, nodes that execute compute operations).
  prefs: []
  type: TYPE_NORMAL
- en: There should be an odd number of managers, preferably 3 or 5, so that if there
    will be no split brains (as explained in [Chapter 2](ch02.html "Chapter 2. Discover
    the Discovery Services"), *Discover the Discovery Services*), and a majority of
    managers will drive the cluster. A quorum is always required by the Raft consensus
    algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'A Swarmkit cluster can host any arbitrary number of workers: 1, 10, 100, or
    2,000.'
  prefs: []
  type: TYPE_NORMAL
- en: On managers, **services** may be defined and load balanced. For example, a service
    may be "web". A "web" service will be physically made of several **tasks**, running
    on the cluster nodes, including the managers, for example, one task can be a single
    nginx Docker container.
  prefs: []
  type: TYPE_NORMAL
- en: '![Swarmkit architecture](images/image_03_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In Swarmkit, an operator uses a **Swarmctl** binary to interact with the system
    remotely, invoking operations on the leader master. Masters, that run a binary
    called **Swarmd**, agree on a leader via Raft, keep the status of services and
    tasks, and schedule jobs on workers.
  prefs: []
  type: TYPE_NORMAL
- en: Workers run the Docker Engine, and take jobs running them as separate containers.
  prefs: []
  type: TYPE_NORMAL
- en: The Swarmkit architecture can be subject to a redraw, but the core components
    (masters and workers) are going to stay. Rather, new objects can possibly be added
    with plugins, for allocating resources such as networks and volumes.
  prefs: []
  type: TYPE_NORMAL
- en: How a manager chooses the best node for a task
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The way that Swarmkit spawns tasks over the cluster is called **scheduling**.
    The scheduler is an algorithm that uses criteria such as filters to decide where
    to physically start a task.
  prefs: []
  type: TYPE_NORMAL
- en: '![How a manager chooses the best node for a task](images/image_03_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The heart of SwarmKit: swarmd'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The core binary to start a SwarmKit service is called `swarmd`, and that's the
    daemon to create both the master and join slaves.
  prefs: []
  type: TYPE_NORMAL
- en: It can bind itself either to a local UNIX socket and to a TCP socket, but in
    both cases, is manageable by the `swarmctl` utility by connecting to (another)
    dedicated UNIX local socket.
  prefs: []
  type: TYPE_NORMAL
- en: In the example that follows in the next section, we'll use `swarmd` to create
    a first manager listening on port `4242/tcp`, and then again we'll use `swarmd`
    on the other worker nodes, to make them join the manager, and finally we'll use
    `swarmctl` to check some facts about our cluster.
  prefs: []
  type: TYPE_NORMAL
- en: These binaries are encapsulated into the `fsoppelsa/swarmkit` image that's available
    on the Docker Hub, and that we're going to use here to simplify the explanation
    and avoid Go code compilation.
  prefs: []
  type: TYPE_NORMAL
- en: This is the online help for swarmd. It's rather self-explanatory in its tunables,
    so we're not going to cover all options in detail. For our practical purposes,
    the most important options are `--listen-remote-api`, defining the `address:port`
    for `swarmd` to bind on, and `--join-addr`, used from other nodes to join the
    cluster.
  prefs: []
  type: TYPE_NORMAL
- en: '![The heart of SwarmKit: swarmd](images/image_03_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The controller of SwarmKit: swarmctl'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`swarmctl` is the client part of SwarmKit. It''s the tool to use for operating
    SwarmKit clusters, as it is capable of showing the list of joined nodes, the list
    of services and tasks, and other information. Here, again from `fsoppelsa/swarmkit`,
    the `swarmctl` online help:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The controller of SwarmKit: swarmctl](images/image_03_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Provisioning a SwarmKit cluster with Ansible
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we'll provision a SwarmKit cluster initially made of a single
    manager and an arbitrary number of slaves.
  prefs: []
  type: TYPE_NORMAL
- en: To create such a setup, we'll use Ansible to make operations repeatable and
    more robust and, besides illustrating the commands, we'll proceed by examining
    the playbooks structure. You can easily adapt those playbooks to run on your provider
    or locally, but here we'll go on Amazon EC2.
  prefs: []
  type: TYPE_NORMAL
- en: To make this example run, there are some basic requirements.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to follow the example on AWS, of course you must have an AWS account
    and have the access keys configured. Keys are retrievable from the AWS Console
    under your **Account Name** | **Security Credentials**. You will need to copy
    the following key''s values:'
  prefs: []
  type: TYPE_NORMAL
- en: Access Key ID
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Secret Access Key
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'I use `awsctl` to set those keys. Just install it from *brew* (Mac) or from
    your packaging system if you''re using Linux or Windows, and configure it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Answer the prompt questions by pasting the keys when required. Configuration,
    where you can specify, for example, a favorite AWS region (such as `us-west-1`)
    is stored in `~/.aws/config`, while credentials are in `~/.aws/credentials`. In
    this way, keys are configured and read automatically by Docker Machine.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to run the Ansible example instead of the commands, these are the
    software requirements:'
  prefs: []
  type: TYPE_NORMAL
- en: Ansible 2.2+
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Docker client compatible with the image that docker-machine will install on
    EC2 (in our case, the default one is Ubuntu 15.04 LTS), at the time of writing,
    the Docker Client 1.11.2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Docker-machine
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Docker-py client (that's used by Ansible), can be installed with `pip install
    docker-py`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Moreover, the example is using the standard port `4242/tcp`, to make the cluster
    nodes interact with each other. So it's required to open that port in a security
    group.
  prefs: []
  type: TYPE_NORMAL
- en: 'Clone the repository at [https://github.com/fsoppelsa/ansible-swarmkit](https://github.com/fsoppelsa/ansible-swarmkit)
    and begin by setting up the SwarmKit Manager node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![Provisioning a SwarmKit cluster with Ansible](images/image_03_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'After some docker-machine setup, the playbook will start a container on the
    Manager host, acting as a SwarmKit Manager. Here is the play snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: On host, a container named `swarmkit-master` from the image `fsoppelsa/swarmkit`
    runs `swarmd` in manager mode (it listens at `0.0.0.0:4242`). The `swarmd` binary
    uses the Docker Engine on host directly, so Engine's socket is mounted inside
    container. The container maps port `4242` to the host port `4242`, so that `swarmd`
    is reachable by slaves directly by connecting to the host `4242` port.
  prefs: []
  type: TYPE_NORMAL
- en: 'In practice, it''s the equivalent of this Docker command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This command runs in detached mode (`-d`), passes via volumes (`-v`) the Docker
    machine Docker socket inside the container, exposes port `4242` from container
    to host (`-p`), and runs `swarmd` by putting the container itself in listening
    mode on any address, on port `4242`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the playbook has finished, you can source the `swarmkit-master` machine
    credentials and check whether our container is running correctly:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Provisioning a SwarmKit cluster with Ansible](images/image_03_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now it''s time to join some slaves. To start a slave, you can, guess what,
    just run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'But since we want to join at least several nodes to the SwarmKit cluster, we
    go with a little bit of shell scripting:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'This command runs five times the playbook, thus creating five worker nodes.
    The playbook, after creating a machine named `swarmkit-RANDOM` will start a `fsoppelsa/swarmkit`
    container doing the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, swarmd runs in join mode, and joins the cluster initiated on the Master,
    by connecting to port `4242/tcp`. This is the equivalent of the following docker
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The ansible `loop` command will take some minutes to finish, depending on how
    many workers are starting. When the playbook has finished, we can control that
    the cluster was created correctly using `swarmctl`. If you haven''t sourced the
    `swarmkit-master` Machine credential yet, it''s time to:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we invoke the container running the swarmd master, using exec:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![Provisioning a SwarmKit cluster with Ansible](images/image_03_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: So, here we have listed the workers that have joined the master.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a service on SwarmKit
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Using the usual `swarmctl` binary, we can now create a service (web), made of
    nginx containers.
  prefs: []
  type: TYPE_NORMAL
- en: 'We begin by checking to make sure that there are no active services on this
    brand new cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Creating a service on SwarmKit](images/image_03_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'So we''re ready to start one, with this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![Creating a service on SwarmKit](images/image_03_009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This command specifies to create a service named `web`, made of `nginx` container
    images, and replicate it with a factor of `5`, so as to create 5 nginx containers
    across the cluster. It will take some seconds to take effect, because on each
    node of the cluster, Swarm will pull and start the nginx image, but finally:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Creating a service on SwarmKit](images/image_03_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The **5/5** indicates that of 5 desired replicas, 5 are up. We can see in detail
    where those containers got spawned using `swarmctl task ls`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Creating a service on SwarmKit](images/image_03_011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: But, wait, is a nginx service (web.5) running on the manager node? Yes. SwarmKit
    and Swarm Mode managers are allowed to run tasks, by default, and the scheduler
    can dispatch jobs onto them.
  prefs: []
  type: TYPE_NORMAL
- en: In a real production configuration, if you want to reserve managers to not run
    jobs, you need to apply a configuration with labels and constraints. This is a
    topic of [Chapter 5](ch05.html "Chapter 5. Administer a Swarm Cluster"), *Administer
    a Swarm Cluster*.
  prefs: []
  type: TYPE_NORMAL
- en: Swarm mode
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Docker Swarm mode (for Docker Engines of version 1.12 or newer) imports the
    SwarmKit libraries in order to make distributed container orchestration over multiple
    hosts possible and easy to operate.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main difference between SwarmKit and Swarm Mode is that Swarm Mode is integrated
    into Docker itself, starting from version 1.12\. This means that Swarm Mode commands
    such as `swarm`, `nodes`, `service`, and `task` are available *inside* the Docker
    client, and that through the docker command it''s possible to initiate and manage
    Swarms, as well as deploy services and tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '`docker swarm init`: This is to initialize a Swarm cluster'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`docker node ls`: This is used to list the available nodes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`docker service tasks`: This is used to list the tasks associated to a specific
    service'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Old versus new Swarm versus SwarmKit
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'At the time of writing, (August 2016), we have three Docker orchestration systems:
    the old one (that is) Swarm v1, SwarmKit, and the new one (that is) integrated
    Swarm Mode.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Old versus new Swarm versus SwarmKit](images/image_03_012.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The original Swarm v1, the one we showed in [Chapter 1](ch01.html "Chapter 1. Welcome
    to Docker Swarm"), *Welcome to Docker Swarm* and that's still used around, is
    not yet deprecated. It's a way of using (recycling?) older infrastructures. But
    starting from Docker 1.12, the new Swarm Mode is the recommended way to begin
    a new orchestration project, especially if it will need to scale to a big size.
  prefs: []
  type: TYPE_NORMAL
- en: To make things simpler, let's summarize the differences between these projects
    with some tables.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, the old Swarm v1 versus the new Swarm Mode:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Swarm standalone** | **Swarm Mode** |'
  prefs: []
  type: TYPE_TB
- en: '| This is available since Docker 1.8 | This is available since Docker 1.12
    |'
  prefs: []
  type: TYPE_TB
- en: '| This is available as a container | This is integrated into the Docker Engine
    |'
  prefs: []
  type: TYPE_TB
- en: '| This needs an external discovery service (such as Consul, Etcd, or Zookeeper)
    | This doesn''t need an external discovery service, Etcd integrated |'
  prefs: []
  type: TYPE_TB
- en: '| This is not secure by default | This is secure by default |'
  prefs: []
  type: TYPE_TB
- en: '| The replica and scaling features are not available | The replica and scaling
    features are available |'
  prefs: []
  type: TYPE_TB
- en: '| There are no service and task concepts for modeling microservices | There
    are out of the box services, tasks, load balancing, and service discovery |'
  prefs: []
  type: TYPE_TB
- en: '| There is no additional networking available | This has integrated VxLAN (mesh
    networking) |'
  prefs: []
  type: TYPE_TB
- en: 'And now, to clarify ideas, let''s compare SwarmKit and Swarm mode:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **SwarmKit** | **Swarm mode** |'
  prefs: []
  type: TYPE_TB
- en: '| These are released as binaries (`swarmd` and `swarmctl`)--use swarmctl |
    These are integrated into the Docker Engine--use docker |'
  prefs: []
  type: TYPE_TB
- en: '| These are generic tasks | These are container tasks |'
  prefs: []
  type: TYPE_TB
- en: '| These include services and tasks | These include services and tasks |'
  prefs: []
  type: TYPE_TB
- en: '| These include no service advanced features, such as load balancing and VxLAN
    networking | These include out of the box service advanced features, such as load
    balancing and VxLAN networking |'
  prefs: []
  type: TYPE_TB
- en: Swarm Mode zoom in
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we already summarized in the preceding table in Swarm standalone versus Swarm
    mode comparison, the main new features available in Swarm Mode are the integration
    into the engine, no need for an external discovery service, and replica, scale,
    load balancing, and networking included.
  prefs: []
  type: TYPE_NORMAL
- en: Integration into the engine
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With docker 1.12+, some new commands are added to the docker client. We now
    take a survey on the ones that are relevant to the matter of this book.
  prefs: []
  type: TYPE_NORMAL
- en: docker swarm command
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'This is the current command to manage Swarms:'
  prefs: []
  type: TYPE_NORMAL
- en: '![docker swarm command](images/image_03_013.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'It accepts the following options:'
  prefs: []
  type: TYPE_NORMAL
- en: '`init`: This initializes a Swarm. Behind the curtain, this command creates
    a manager for the current Docker host and generates a *secret* (its password that
    workers will pass to the API so as to be authorized to join the cluster).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`join`: This is used by a worker to join a cluster, must specify the *secret*
    and a list of managers IP port values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`join-token`: This is used to manage the `join-tokens`. `join-tokens` are special
    token secrets used to make join managers or workers (managers and workers have
    different token values). This command is a convenient way to make Swarm print
    the necessary command to join a manager or a worker:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'To add a worker to this swarm, run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'To add a manager to this swarm, run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '`update`: This updates the cluster by changing some of its values, for example,
    you can use it to specify a new URL of the certificate endpoint'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`leave`: This commands the current node to leave the cluster. If something
    is blocking the operation, there is a useful `--force` option.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: docker node
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This is the command to handle swarm nodes. You must launch it from a manager,
    so you need to be connected to a manager in order to use it.
  prefs: []
  type: TYPE_NORMAL
- en: '![docker node](images/image_03_014.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '`demote` and `promote`: These are commands used to manage the status of nodes.
    With that mechanism, you can promote a node to a manager, or demote it to a worker.
    In practice, Swarm will try to `demote`/`promote`. We will cover this concept
    just a bit later in this chapter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`inspect`: This is the equivalent of docker info, but for a Swarm node. It
    prints information regarding the node/s.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ls`: This lists the nodes connected to the cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rm`: This attempts to remove a worker. If you want to remove a manager, you
    have before to demote it to worker.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ps`: This shows the list of tasks running on a specified node.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`update`: This allows you to change some configuration values for a node, namely
    tags.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: docker service
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'This is the command to manage the services running on a Swarm cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '![docker service](images/image_03_015.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Apart from the expected commands such as `create`, `inspect`, `ps`, `ls`, `rm`,
    and `update`, there is a new interesting one: `scale`.'
  prefs: []
  type: TYPE_NORMAL
- en: Docker Stack
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Not directly necessary to Swarm operations, but introduced as experimental in
    Docker 1.12, there is the `stack` command. Stacks are now bundles of containers.
    For example, a nginx + php + mysql container setup can be stacked in a self-contained
    Docker Stack, called **Distributed Application Bundle** (**DAB**) and described
    by a JSON file.
  prefs: []
  type: TYPE_NORMAL
- en: The core command of docker stack will be deploy, thanks to which it will be
    possible to create and update DABs. We'll meet stacks later in [Chapter 6](ch06.html
    "Chapter 6. Deploy Real Applications on Swarm"), *Deploy Real Applications on
    Swarm*.
  prefs: []
  type: TYPE_NORMAL
- en: Etcd's Raft is integrated already
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Docker Swarm Mode already integrates RAFT through CoreOS Etcd Raft library.
    There is no further need to integrate external discovery services such as Zookeeper
    or Consul anymore. Swarm directly takes care of essential services such as DNS
    and load balancing.
  prefs: []
  type: TYPE_NORMAL
- en: Installing a Swarm Mode cluster is just a matter of starting Docker hosts and
    running Docker commands, making it super easy to set up.
  prefs: []
  type: TYPE_NORMAL
- en: Load balancing and DNS
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: By design, the cluster managers assign to every service in the swarm a unique
    DNS name and load balances running containers, using an internal DNS to Docker.
    Queries and resolutions work automatically out of the box.
  prefs: []
  type: TYPE_NORMAL
- en: For each service created with a `--name myservice`, every container in the swarm
    will be able to resolve the service IP address just as they were resolving (`dig
    myservice`) internal network names, using the Docker embedded DNS server. So,
    if you have a `nginx-service` (made of nginx containers, for example), you can
    just `ping nginx-service` to reach the frontend head.
  prefs: []
  type: TYPE_NORMAL
- en: Also, in Swarm mode, operators have the possibility to `publish` services ports
    to an external load balancer. Ports are then exposed outside to a port in a range
    from `30000` to `32767`. Internally, Swarm uses iptables and IPVS to execute packet
    filtering and forwarding, and load balancing respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Iptables is the default packet filter firewall used by Linux while IPVS is the
    seasoned IP Virtual Server defined in the Linux kernel, that can be used for load
    balancing traffic, and that's just what Docker Swarm uses.
  prefs: []
  type: TYPE_NORMAL
- en: Ports are published either when a new service is created or updated, using the
    `--publish-add` option. With this option, an internal service is published, and
    gets load balanced.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, if we have a cluster with three workers each running nginx (on
    a service named `nginx-service`), we can expose their target-port to the load
    balancer with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: This will create a mapping between the published port `30000` on any of the
    nodes cluster, and the `nginx` containers (port 80). If you connect any node to
    port `30000`, you will be greeted by the Nginx welcome page.
  prefs: []
  type: TYPE_NORMAL
- en: '![Load balancing and DNS](images/image_03_016.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'But how does this work? As you see in the preceding screenshot, there is an
    associated VirtualIP (`10.255.0.7/16`), or VIP, and it is collocated on the overlay
    network **2xbr2upsr3yl**, created by Swarm for ingress to the load balancer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Load balancing and DNS](images/image_03_017.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'From any host, you are able to reach `nginx-service`, because the DNS name
    resolves to the VIP, here 10.255.0.7, acting as a frontend to the load balancer:'
  prefs: []
  type: TYPE_NORMAL
- en: 'On each node of the swarm, Swarm implements load balancing in kernel, specifically
    inside the namespaces, by adding a MARK rule in the OUTPUT chain inside the network
    namespace dedicated to the network, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Load balancing and DNS](images/image_03_018.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We'll cover networking concepts in greater detail later, in [Chapter 5](ch05.html
    "Chapter 5. Administer a Swarm Cluster"), *Administer a Swarm Cluster* and [Chapter
    8](ch08.html "Chapter 8. Exploring Additional Features of Swarm"), *Exploring
    Additional features of Swarm*.
  prefs: []
  type: TYPE_NORMAL
- en: Promotion and demotion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With the `docker node` command, the cluster operator can promote nodes from
    workers to managers and, vice versa, demote them from managers to workers.
  prefs: []
  type: TYPE_NORMAL
- en: Demoting a node from manager to worker is the only way to remove a manager (now
    worker) from the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: We'll cover promotion and demotion operations in detail in [Chapter 5](ch05.html
    "Chapter 5. Administer a Swarm Cluster"), *Administer a Swarm Cluster*.
  prefs: []
  type: TYPE_NORMAL
- en: Replicas and scale
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Deploying an app on a Swarm cluster means to define and configure services,
    start them and wait for Docker engines scattered across the cluster to launch
    containers. We'll deploy complete apps on Swarm in [Chapter 6](ch06.html "Chapter 6. Deploy
    Real Applications on Swarm"), *Deploy Real Applications on Swarm*.
  prefs: []
  type: TYPE_NORMAL
- en: Services and tasks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The core of the Swarm workload is divided into services. A service is just an
    abstraction to group an arbitrary number of tasks (this number is called the *replica
    factor*, or just *replicas*). Tasks are running containers.
  prefs: []
  type: TYPE_NORMAL
- en: docker service scale
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'With the `docker service scale` command, you order Swarm to ensure that a certain
    number of replicas are running at the same time on the cluster. For example, you
    can start with 10 containers running some *task* distributed over the cluster,
    and then when you need to scale their size to 30 you just execute:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Swarm is ordered to schedule 20 new containers, so it takes the appropriate
    decisions for load balancing, DNS, and networking coherence. If a container for
    *task* goes down, making the replica factor equal to 29, Swarm will reschedule
    another one on another cluster node (that will have a new ID) to maintain the
    factor equal to 30.
  prefs: []
  type: TYPE_NORMAL
- en: A note on replicas and new nodes addition. People frequently ask about Swarm
    automatic capabilities. If you have five workers running 30 tasks, and add five
    new nodes, you should not expect Swarm to balance the 30 tasks across the new
    nodes automatically, moving them from the original to the new nodes. The behavior
    of the Swarm scheduler is conservative, until some event (for example, an operator
    intervention) triggers a new `scale` command. Only in that case, the scheduler
    will take into account the five new nodes and possibly start new replica tasks
    on the 5 new workers.
  prefs: []
  type: TYPE_NORMAL
- en: We'll see how `scale` command works in practice in [Chapter 7](ch07.html "Chapter 7. Scaling
    Up Your Platform"), *Scaling Up Your Platform*.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we met the new actors in the Docker ecosystem: SwarmKit and
    Swarm Mode. We proceeded through a simple implementation of a SwarmKit cluster
    with Ansible on Amazon AWS. Then, we covered the essential concepts of Swarm Mode,
    introducing its interface and its internals, including DNS, load balancing, services,
    replicas, and the promotion/demotion mechanism. Now, it''s time to dive into a
    true Swarm Mode deployment, as we''ll see in [Chapter 4](ch04.html "Chapter 4. Creating
    a Production-Grade Swarm"), *Creating a Production-Grade Swarm*.'
  prefs: []
  type: TYPE_NORMAL
