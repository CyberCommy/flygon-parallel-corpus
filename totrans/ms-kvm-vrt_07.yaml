- en: '*Chapter 5*: Libvirt Storage'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第5章*：Libvirt存储'
- en: This chapter provides you with an insight into the way that KVM uses storage.
    Specifically, we will cover both storage that's internal to the host where we're
    running virtual machines and *shared storage*. Don't let the terminology confuse
    you here – in virtualization and cloud technologies, the term *shared storage*
    means storage space that multiple hypervisors can have access to. As we will explain
    a bit later, the three most common ways of achieving this are by using block-level,
    share-level, or object-level storage. We will use NFS as an example of share-level
    storage, and **Internet Small Computer System Interface** (**iSCSI**) and **Fiber
    Channel** (**FC**) as examples of block-level storage. In terms of object-based
    storage, we will use Ceph. GlusterFS is also commonly used nowadays, so we'll
    make sure that we cover that, too. To wrap everything up in an easy-to-use and
    easy-to-manage box, we will discuss some open source projects that might help
    you while practicing with and creating testing environments.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 这一章节为您提供了KVM使用存储的见解。具体来说，我们将涵盖运行虚拟机的主机内部存储和*共享存储*。不要让术语在这里让您困惑——在虚拟化和云技术中，术语*共享存储*表示多个虚拟化程序可以访问的存储空间。正如我们稍后将解释的那样，实现这一点的三种最常见方式是使用块级、共享级或对象级存储。我们将以NFS作为共享级存储的示例，以**Internet
    Small Computer System Interface**（**iSCSI**）和**Fiber Channel**（**FC**）作为块级存储的示例。在对象级存储方面，我们将使用Ceph。GlusterFS如今也被广泛使用，因此我们也会确保涵盖到它。为了将所有内容整合到一个易于使用和管理的框架中，我们将讨论一些可能在练习和创建测试环境时对您有所帮助的开源项目。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Introduction to storage
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存储介绍
- en: Storage pools
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存储池
- en: NFS storage
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NFS存储
- en: iSCSI and SAN storage
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: iSCSI和SAN存储
- en: Storage redundancy and multipathing
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存储冗余和多路径处理
- en: Gluster and Ceph as a storage backend for KVM
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gluster和Ceph作为KVM的存储后端
- en: Virtual disk images and formats and basic KVM storage operations
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 虚拟磁盘映像和格式以及基本的KVM存储操作
- en: The latest developments in storage – NVMe and NVMeOF
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存储的最新发展—NVMe和NVMeOF
- en: Introduction to storage
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 存储介绍
- en: 'Unlike networking, which is something that most IT people have at least a basic
    understanding of, storage tends to be quite different. In short, yes, it tends
    to be a bit more complex. There are loads of parameters involved, different technologies,
    and…let''s be honest, loads of different types of configuration options and people
    enforcing them. And a *lot* of questions. Here are some of them:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 与网络不同，大多数IT人员至少对网络有基本的了解，而存储往往是完全不同的。简而言之，是的，它往往更加复杂。涉及许多参数、不同的技术，而且……坦率地说，有许多不同类型的配置选项和强制执行这些选项的人。还有*很多*问题。以下是其中一些：
- en: Should we configure one NFS share per storage device or two?
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们应该为每个存储设备配置一个NFS共享还是两个？
- en: Should we create one iSCSI target per storage device or two?
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们应该为每个存储设备创建一个iSCSI目标还是两个？
- en: Should we create one FC target or two?
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们应该创建一个FC目标还是两个？
- en: How many **Logical Unit Numbers** (**LUNs**) per target?
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个**逻辑单元号**（**LUN**）每个目标应该有多少个？
- en: What kind of cluster size should we use?
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们应该使用何种集群大小？
- en: How should we carry out multipathing?
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们应该如何进行多路径处理？
- en: Should we use block-level or share-level storage?
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们应该使用块级还是共享级存储？
- en: Should we use block-level or object-level storage?
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们应该使用块级还是对象级存储？
- en: Which technology or solution should we choose?
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们应该选择哪种技术或解决方案？
- en: How should we configure caching?
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们应该如何配置缓存？
- en: How should we configure zoning or masking?
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们应该如何配置分区或掩码？
- en: How many switches should we use?
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们应该使用多少个交换机？
- en: Should we use some kind of clustering technology on a storage level?
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们应该在存储级别使用某种集群技术吗？
- en: As you can see, the questions just keep piling up, and we've barely touched
    the surface, because there are also questions about which filesystem to use, which
    physical controller we will use to access storage, and what type of cabling—it
    just becomes a big mashup of variables that has many potential answers. What makes
    it worse is the fact that many of those answers can be correct—not just one of
    them.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，问题不断堆积，而我们几乎只是触及了表面，因为还有关于使用哪种文件系统、使用哪种物理控制器来访问存储以及使用何种类型的布线等问题——这些问题变成了一个包含许多潜在答案的大杂烩。更糟糕的是，许多答案都可能是正确的，而不仅仅是其中一个。
- en: Let's get the basic-level mathematics out of the way. In an enterprise-level
    environment, shared storage is usually *the most expensive* part of the environment
    and can also have *the most significant negative impact* on virtual machine performance,
    while at the same time being *the most oversubscribed resource* in that environment.
    Let's think about this for a second—every powered-on virtual machine is constantly
    going to hammer our storage device with I/O operations. If we have 500 virtual
    machines running on a single storage device, aren't we asking a bit too much from
    that storage device?
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先把基本的数学问题解决掉。在企业级环境中，共享存储通常是环境中*最昂贵*的部分，同时也可能对虚拟机性能产生*最显著的负面影响*，同时又是该环境中*最过度订阅的资源*。让我们想一想这个问题——每个开机的虚拟机都会不断地向我们的存储设备发送I/O操作。如果我们在单个存储设备上运行了500台虚拟机，那么我们是不是对存储设备要求过高了？
- en: At the same time, some kind of shared storage concept is a key pillar of virtualized
    environments. The basic principle is very simple – there are loads of advanced
    functionalities that will work so much better with shared storage. Also, many
    operations are much faster if shared storage is available. Even more so, there
    are so many simple options for high availability when we don't have our virtual
    machines stored in the same place where they are being executed.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，某种共享存储概念是虚拟化环境的关键支柱。基本原则非常简单——有许多高级功能可以通过共享存储更好地发挥作用。此外，如果有共享存储可用，许多操作会更快。更重要的是，如果我们的虚拟机存储和执行位置不在同一地方，那么高可用性的简单选项就有很多。
- en: 'As a bonus, we can easily avoid **Single Point Of Failure** (**SPOF**) scenarios
    if we design our shared storage environment correctly. In an enterprise-level
    environment, avoiding SPOF is one of the key design principles. But when we start
    adding switches and adapters and controllers to the *to buy* list, our managers''
    or clients'' heads usually starts to hurt. We talk about performance and risk
    management, while they talk about price. We talk about the fact that their databases
    and applications need to be properly fed in terms of I/O and bandwidth, and they
    feel that you can produce that out of thin air. Just wave your magic wand and
    there we are: unlimited storage performance.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个额外的好处，如果我们正确设计共享存储环境，我们可以轻松避免**单点故障**（**SPOF**）的情况。在企业级环境中，避免SPOF是关键设计原则之一。但是当我们开始将交换机、适配器和控制器添加到*购买*清单上时，我们的经理或客户通常会开始头痛。我们谈论性能和风险管理，而他们谈论价格。我们谈论他们的数据库和应用程序需要适当的I/O和带宽供应，而他们觉得你可以凭空产生这些。只需挥动魔术棒，我们就有了：无限的存储性能。
- en: But the best, and our all-time favorite, apples-to-oranges comparison that your
    clients are surely going to try to enforce on you goes something like this…*"the
    shiny new 1 TB NVMe SSD in my laptop has more than 1,000 times more IOPS and more
    than 5 times more performance than your $50,000 storage device, while costing
    100 times less! You have no idea what you're doing!"*
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，你的客户肯定会试图强加给你的最好的、我们永远喜欢的苹果和橙子比较是这样的……“我的闪亮新款1TB NVMe SSD笔记本电脑的IOPS比你的5万美元的存储设备多1000倍，性能比你的存储设备多5倍，而成本却少100倍！你根本不知道你在做什么！”
- en: If you've been there, we feel for you. Rarely will you see so many discussions
    and fights about a piece of hardware in a box. But it's such an essential piece
    of hardware in a box that it's a good fight to have. So, let's explain some key
    concepts that libvirt uses in terms of storage access and how to work with it.
    Then, let's use our knowledge to extract as much performance as possible out of
    our storage system and libvirt using it.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你曾经有过这种经历，我们为你感到难过。很少会有这么多关于盒子里的一块硬件的讨论和争论。但它是一个如此重要的盒子里的硬件，这是一场很好的争论。因此，让我们解释一些libvirt在存储访问方面使用的关键概念，以及如何利用我们的知识从我们的存储系统和使用它的libvirt中尽可能多地提取性能。
- en: In this chapter, we're basically going to cover almost all of these storage
    types via installation and configuration examples. Each and every one of these
    has its own use case, but generally, it's going to be up to you to choose what
    you're going to use.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们基本上将通过安装和配置示例涵盖几乎所有这些存储类型。每一种都有自己的用例，但一般来说，你将要选择你要使用的是什么。
- en: So, let's start our journey through these supported protocols and learn how
    to configure them. After we cover storage pools, we are going to discuss NFS,
    a typical share-level protocol for virtual machine storage. Then, we're going
    to move to block-level protocols such as iSCSI and FC. Then, we will move to redundancy
    and multipathing to increase the availability and bandwidth of our storage devices.
    We're also going to cover various use cases for not-so-common filesystems (such
    as Ceph, Gluster, and GFS) for KVM virtualization. We're also going to discuss
    the new developments that are de facto trends right now.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们开始我们通过这些支持的协议的旅程，并学习如何配置它们。在我们讨论存储池之后，我们将讨论NFS，这是一种典型的虚拟机存储的共享级协议。然后，我们将转向块级协议，如iSCSI和FC。然后，我们将转向冗余和多路径，以增加我们存储设备的可用性和带宽。我们还将讨论不太常见的文件系统（如Ceph、Gluster和GFS）在KVM虚拟化中的各种用例。我们还将讨论当前的事实趋势的新发展。
- en: Storage pools
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 存储池
- en: When you first start using storage devices—even if they're cheaper boxes—you're
    faced with some choices. They will ask you to do a bit of configuration—select
    the RAID level, configure hot-spares, SSD caching...it's a process. The same process
    applies to a situation in which you're building a data center from scratch or
    extending an existing one. You have to configure the storage to be able to use
    it.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 当你第一次开始使用存储设备时，即使它们是更便宜的盒子，你也会面临一些选择。他们会要求你进行一些配置——选择RAID级别、配置热备份、SSD缓存……这是一个过程。同样的过程也适用于从头开始构建数据中心或扩展现有数据中心的情况。你必须配置存储才能使用它。
- en: Hypervisors are a bit *picky* when it comes to storage, as there are storage
    types that they support and storage types that they don't support. For example,
    Microsoft's Hyper-V supports SMB shares for virtual machine storage, but it doesn't
    really support NFS storage for virtual machine storage. VMware's vSphere Hypervisor
    supports NFS, but it doesn't support SMB. The reason is simple—a company developing
    a hypervisor chooses and qualifies technologies that its hypervisor is going to
    support. Then, it's up to various HBA/controller vendors (Intel, Mellanox, QLogic,
    and so on) to develop drivers for that hypervisor, and it's up to storage vendor
    to decide which types of storage protocols they're going to support on their storage
    device.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到存储时，虚拟化管理程序有点“挑剔”，因为它们支持一些存储类型，而不支持一些存储类型。例如，微软的Hyper-V支持SMB共享用于虚拟机存储，但实际上不支持NFS存储用于虚拟机存储。VMware的vSphere
    Hypervisor支持NFS，但不支持SMB。原因很简单——一家开发虚拟化管理程序的公司选择并验证其虚拟化管理程序将支持的技术。然后，各种HBA/控制器供应商（英特尔、Mellanox、QLogic等）开发该虚拟化管理程序的驱动程序，存储供应商决定他们的存储设备将支持哪些存储协议。
- en: 'From a CentOS perspective, there are many different storage pool types that
    are supported. Here are some of them:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 从CentOS的角度来看，有许多不同类型的存储池得到支持。以下是其中一些：
- en: '**Logical Volume Manager** (**LVM**)-based storage pools'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于**逻辑卷管理器**（**LVM**）的存储池
- en: Directory-based storage pools
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于目录的存储池
- en: Partition-based storage pools
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于分区的存储池
- en: GlusterFS-based storage pools
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于GlusterFS的存储池
- en: iSCSI-based storage pools
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于iSCSI的存储池
- en: Disk-based storage pools
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于磁盘的存储池
- en: HBA-based storage pools, which use SCSI devices
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于HBA的存储池，使用SCSI设备
- en: From the perspective of libvirt, a storage pool can be a directory, a storage
    device, or a file that libvirt manages. That leads us to 10+ different storage
    pool types, as you're going to see in the next section. From a virtual machine
    perspective, libvirt manages virtual machine storage, which virtual machines use
    so that they have the capacity to store data.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 从libvirt的角度来看，存储池可以是libvirt管理的目录、存储设备或文件。这导致了10多种不同的存储池类型，你将在下一节中看到。从虚拟机的角度来看，libvirt管理虚拟机存储，虚拟机使用它来存储数据。
- en: oVirt, on the other hand, sees things a bit differently, as it has its own service
    that works with libvirt to provide centralized storage management from a data
    center perspective. *Data center perspective* might seem like a term that's a
    bit odd. But think about it—a datacenter is some kind of *higher-level* object
    in which you can see all of your resources. A data center uses *storage* and *hypervisors*
    to provide us with all of the services that we need in virtualization—virtual
    machines, virtual networks, storage domains, and so on. Basically, from a data
    center perspective, you can see what's happening on all of your hosts that are
    members of that datacenter. However, from a host level, you can't see what's happening
    on another host. It's a hierarchy that's completely logical from both a management
    and a security perspective.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，oVirt看待事情有所不同，因为它有自己的服务与libvirt合作，从数据中心的角度提供集中的存储管理。*数据中心的角度*可能听起来有点奇怪。但想想看——数据中心是一种*更高级*的对象，你可以在其中看到所有的资源。数据中心使用*存储*和*虚拟化平台*为我们提供虚拟化所需的所有服务——虚拟机、虚拟网络、存储域等。基本上，从数据中心的角度来看，你可以看到所有属于该数据中心成员的主机上发生了什么。然而，从主机级别来看，你无法看到另一个主机上发生了什么。从管理和安全的角度来看，这是一个完全合乎逻辑的层次结构。
- en: 'oVirt can centrally manage these different types of storage pools (and the
    list can get bigger or smaller as the years go by):'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: oVirt可以集中管理这些不同类型的存储池（随着时间的推移，列表可能会变得更长或更短）：
- en: '**Network File System** (**NFS**)'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**网络文件系统**（**NFS**）'
- en: '**Parallel NFS** (**pNFS**)'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**并行NFS**（**pNFS**）'
- en: iSCSI
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: iSCSI
- en: FC
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FC
- en: Local storage (attached directly to KVM hosts)
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本地存储（直接连接到KVM主机）
- en: GlusterFS exports
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GlusterFS导出
- en: POSIX-compliant file systems
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 符合POSIX的文件系统
- en: 'Let''s take care of some terminology first:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先搞清一些术语：
- en: '**Brtfs** is a type of filesystem that supports snapshots, RAID and LVM-like
    functionality, compression, defragmentation, online resizing, and many other advanced
    features. It was deprecated after it was discovered that its RAID5/6 can easily
    lead to a loss of data.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Brtfs**是一种文件系统，支持快照、RAID和类似LVM的功能、压缩、碎片整理、在线调整大小以及许多其他高级功能。在发现其RAID5/6很容易导致数据丢失后，它被弃用了。'
- en: '**ZFS** is a type of filesystem that supports everything that Brtfs does, plus
    read and write caching.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ZFS**是一种文件系统，支持Brtfs的所有功能，还支持读写缓存。'
- en: CentOS has a new way of dealing with storage pools. Although still in technology
    preview state, it's worth going through the complete configuration via this new
    tool, called **Stratis**. Basically, a couple of years ago, Red Hat finally deprecated
    the idea of pushing Brtfs for future releases and started working on Stratis.
    If you've ever used ZFS, that's where this is probably going—an easy-to-manage,
    ZFS-like, volume-managing set of utilities that Red Hat can stand behind in their
    future releases. Also, just like ZFS, a Stratis-based pool can use cache; so,
    if you have an SSD that you'd like to dedicate to pool cache, you can actually
    do that, as well. If you have been expecting Red Hat to support ZFS, there's a
    fundamental Red Hat policy that stands in the way. Specifically, ZFS is not a
    part of the Linux kernel, mostly because of licensing reasons. Red Hat has a policy
    for these situations—if it's not a part of the kernel (upstream), then they don't
    provide nor support it. As it stands, that's not going to happen anytime soon.
    These policies are also reflected in CentOS.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: CentOS有一种新的处理存储池的方式。虽然仍处于技术预览阶段，但通过这个名为**Stratis**的新工具进行完整配置是值得的。基本上，几年前，Red
    Hat最终放弃了推动Brtfs用于未来版本的想法，开始致力于Stratis。如果你曾经使用过ZFS，那么这可能是类似的——一套易于管理的、类似ZFS的卷管理工具，Red
    Hat可以在未来的发布中支持。此外，就像ZFS一样，基于Stratis的池可以使用缓存；因此，如果你有一块SSD想要专门用于池缓存，你也可以做到。如果你一直期待Red
    Hat支持ZFS，那么有一个基本的Red Hat政策阻碍了这一点。具体来说，ZFS不是Linux内核的一部分，主要是因为许可证的原因。Red Hat对这些情况有一个政策——如果它不是内核的一部分（上游），那么他们就不提供也不支持。就目前而言，这不会很快发生。这些政策也反映在了CentOS中。
- en: Local storage pools
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 本地存储池
- en: On the other hand, Stratis is available right now. We're going to use it to
    manage our local storage by creating storage pools. Creating a pool requires us
    to set up partitions or disks beforehand. After we create a pool, we can create
    a volume on top of it. We only have to be very careful about one thing—although
    Stratis can manage XFS filesystems, we shouldn't make changes to Stratis-managed
    XFS filesystems directly from the filesystem level. For example, do not reconfigure
    or reformat a Stratis-based XFS filesystem directly from XFS-based commands because
    you'll create havoc on your system.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，Stratis现在就可以使用。我们将使用它来管理我们的本地存储，创建存储池。创建池需要我们事先设置分区或磁盘。创建池后，我们可以在其上创建卷。我们只需要非常小心一件事——虽然Stratis可以管理XFS文件系统，但我们不应该直接从文件系统级别对Stratis管理的XFS文件系统进行更改。例如，不要使用基于XFS的命令直接重新配置或重新格式化基于Stratis的XFS文件系统，因为这会在系统上造成混乱。
- en: 'Stratis supports various different types of block storage devices:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: Stratis支持各种不同类型的块存储设备：
- en: Hard disks and SSDs
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 硬盘和固态硬盘
- en: iSCSI LUNs
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: iSCSI LUNs
- en: LVM
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LVM
- en: LUKS
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LUKS
- en: MD RAID
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MD RAID
- en: A device mapper multipath
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设备映射器多路径
- en: NVMe devices
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NVMe设备
- en: 'Let''s start from scratch and install Stratis so that we can use it. Let''s
    use the following command:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从头开始安装Stratis，以便我们可以使用它。我们使用以下命令：
- en: '[PRE0]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The first command installs the Stratis service and the corresponding command-line
    utilities. The second one will start and enable the Stratis service.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we are going to go through a complete example of how to use Stratis to
    configure your storage devices. We''re going to cover an example of this layered
    approach. So, what we are going to do is as follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: Create a software RAID10 + spare by using MD RAID.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a Stratis pool out of that MD RAID device.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add a cache device to the pool to use Stratis' cache capability.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a Stratis filesystem and mount it on our local server.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The premise here is simple—the software RAID10+ spare via MD RAID is going
    to approximate the regular production approach, in which you''d have some kind
    of a hardware RAID controller presenting a single block device to the system.
    We''re going to add a cache device to the pool to verify the caching functionality,
    as this is something that we would most probably do if we were using ZFS, as well.
    Then, we are going to create a filesystem on top of that pool and mount it to
    a local directory with the help of the following commands:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This mounted filesystem is XFS-formatted. We could then easily use this filesystem
    via NFS export, which is exactly what we're going to do in the NFS storage lesson.
    But for now, this was just an example of how to create a pool by using Stratis.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: We've covered some basics of local storage pools, which brings us closer to
    our next subject, which is how to use pools from a libvirt perspective. So, that
    will be our next topic.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: Libvirt storage pools
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Libvirt manages its own storage pools, which is done with one thing in mind—to
    provide different pools for virtual machine disks and related data. Keeping in
    mind that libvirt uses what the underlying operating system supports, it''s no
    wonder that it supports loads of different storage pool types. A picture is worth
    a thousand words, so here''s a screenshot of creating a libvirt storage pool from
    virt-manager:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.1 – Different storage pool types supported by libvirt'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_05_01.jpg)'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.1 – Different storage pool types supported by libvirt
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: Out of the box, libvirt already has a predefined default storage pool, which
    is a directory storage pool on the local server. This default pool is located
    in the `/var/lib/libvirt/images` directory. This represents our default location
    where we'll save all the data from locally installed virtual machines.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: 'We''re going to create various different types of storage pools in the following
    sections—an NFS-based pool, an iSCSI and FC pool, and Gluster and Ceph pools:
    the whole nine yards. We''re also going to explain when to use each and every
    one of them as there will be different usage models involved.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: NFS storage pool
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As a protocol, NFS has been around since the mid-80s. It was originally developed
    by Sun Microsystems as a protocol for sharing files, which is what it''s been
    used for up to this day. Actually, it''s still being developed, which is quite
    surprising for a technology that''s so *old*. For example, NFS version 4.2 came
    out in 2016\. In this version, NFS received a very big update, such as the following:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '**Server-side copy**: A feature that significantly enhances the speed of cloning
    operations between NFS servers by carrying out cloning directly between NFS servers'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sparse files** and **space reservation**: Features that enhance the way NFS
    works with files that have unallocated blocks, while keeping an eye on capacity
    so that we can guarantee space availability when we need to write data'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Application data block support**: A feature that helps applications that
    work with files as block devices (disks)'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Better pNFS implementation
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are other bits and pieces that were enhanced in v4.2, but for now, this
    is more than enough. You can find even more information about this in IETF's RFC
    7862 document ([https://tools.ietf.org/html/rfc7862](https://tools.ietf.org/html/rfc7862)).
    We're going to focus our attention on the implementation of NFS v4.2 specifically,
    as it's the best that NFS currently has to offer. It also happens to be the default
    NFS version that CentOS 8 supports.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing that we have to do is install the necessary packages. We''re
    going to achieve that by using the following commands:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The first command installs the necessary utilities to run the NFS server. The
    second one is going to start it and permanently enable it so that the NFS service
    is available after reboot.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: 'Our next task is to configure what we''re going to share via the NFS server.
    For that, we need to *export* a directory and make it available to our clients
    over the network. NFS uses a configuration file, `/etc/exports`, for that purpose.
    Let''s say that we want to create a directory called `/exports`, and then share
    it to our clients in the `192.168.159.0/255.255.255.0` network, and we want to
    allow them to write data on that share. Our `/etc/exports` file should look like
    this:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: These configuration options tell our NFS server which directory to export (`/exports`),
    to which clients (`192.168.159.0/24`), and what options to use (`rw` means read-write).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: 'Some other available options include the following:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: '`ro`: Read-only mode.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sync`: Synchronous I/O operations.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`root_squash`: All I/O operations from `UID 0` and `GID 0` are mapped to configurable
    anonymous UIDs and GIDs (the `anonuid` and `anongid` options).'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`all_squash`: All I/O operations from any UIDs and GIDs are mapped to anonymous
    UIDs and GIDs (`anonuid` and `anongid` options).'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`no_root_squash`: All I/O operations from `UID 0` and `GID 0` are mapped to
    `UID 0` and `GID 0`.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If you need to apply multiple options to the exported directory, you add them
    with a comma between them, as follows:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'You can use fully qualified domain names or short hostnames (if they''re resolvable
    by DNS or any other mechanism). Also, if you don''t like using prefixes (`24`),
    you can use regular netmasks, as follows:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now that we have configured the NFS server, let''s see how we''re going to
    configure libvirt to use that server as a storage pool. As always, there are a
    couple of ways to do this. We could just create an XML file with the pool definition
    and import it to our KVM host by using the `virsh pool-define --file` command.
    Here''s an example of that configuration file:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.2 – Example XML configuration file for NFS pool'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_05_02.jpg)'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.2 – Example XML configuration file for NFS pool
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s explain these configuration options:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '`pool type`: `netfs` means that we are going to use an NFS file share.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`name`: The pool name, as libvirt uses pools as named objects, just like virtual
    networks.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`host` : The address of the NFS server that we are connecting to.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dir path`: The NFS export path that we configured on the NFS server via `/etc/exports`.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`path`: The local directory on our KVM host where that NFS share is going to
    be mounted to.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`permissions`: The permissions used for mounting this filesystem.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`owner` and `group`: The UID and GID used for mounting purposes (that''s why
    we exported the folder earlier with the `no_root_squash` option).'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`label`: The SELinux label for this folder—we''re going to discuss this in
    [*Chapter 16*](B14834_16_Final_ASB_ePub.xhtml#_idTextAnchor302), *Troubleshooting
    Guideline for the KVM Platform*.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If we wanted, we could''ve easily done the same thing via the Virtual Machine
    Manager GUI. First, we would have to select the correct type (the NFS pool) and
    give it a name:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.3 – Selecting the NFS pool type and giving it a name'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_05_03.jpg)'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.3 – Selecting the NFS pool type and giving it a name
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: 'After we click **Forward**, we can move to the final configuration step, where
    we need to tell the wizard which server we''re mounting our NFS share from:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 点击**前进**后，我们可以进入最后的配置步骤，需要告诉向导我们从哪个服务器挂载我们的NFS共享：
- en: '![Figure 5.4 – Configuring NFS server options'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.4–配置NFS服务器选项'
- en: '](img/B14834_05_04.jpg)'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_05_04.jpg)'
- en: Figure 5.4 – Configuring NFS server options
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.4–配置NFS服务器选项
- en: 'When we finish typing in these configuration options (**Host Name** and **Source
    Path**), we can press **Finish**, which will mean exiting the wizard. Also, our
    previous configuration screen, which only contained the **default** storage pool,
    now has our newly configured pool listed as well:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们完成输入这些配置选项（**主机名**和**源路径**）后，我们可以点击**完成**，这意味着退出向导。此外，我们之前的配置屏幕，只包含**默认**存储池，现在也列出了我们新配置的存储池：
- en: '![Figure 5.5 – Newly configured NFS pool visible on the list'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.5–新配置的NFS存储池在列表中可见'
- en: '](img/B14834_05_05.jpg)'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_05_05.jpg)'
- en: Figure 5.5 – Newly configured NFS pool visible on the list
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.5–新配置的NFS存储池在列表中可见
- en: When would we use NFS-based storage pools in libvirt, and for what? Basically,
    we can use them nicely for anything related to the storage of installation images—ISO
    files, virtual floppy disk files, virtual machine files, and so on.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们何时在libvirt中使用基于NFS的存储池，以及为什么？基本上，我们可以很好地用它们来存储安装映像的任何相关内容——ISO文件、虚拟软盘文件、虚拟机文件等等。
- en: Please remember that even though it seemed that NFS is almost gone from enterprise
    environments just a while ago, NFS is still around. Actually, with the introduction
    of NFS 4.1, 4.2, and pNFS, its future on the market actually looks even better
    than a couple of years ago. It's such a familiar protocol with a very long history,
    and it's still quite competitive in many scenarios. If you're familiar with VMware
    virtualization technology, VMware introduced a technology called Virtual Volumes
    in ESXi 6.0\. This is an object-based storage technology that can use both block-
    and NFS-based protocols for its basis, which is a really compelling use case for
    some scenarios. But for now, let's move on to block-level technologies, such as
    iSCSI and FC.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，尽管似乎NFS在企业环境中几乎已经消失了一段时间，但NFS仍然存在。实际上，随着NFS 4.1、4.2和pNFS的引入，它在市场上的未来实际上看起来比几年前更好。这是一个非常熟悉的协议，有着非常悠久的历史，在许多场景中仍然具有竞争力。如果您熟悉VMware虚拟化技术，VMware在ESXi
    6.0中引入了一种称为虚拟卷的技术。这是一种基于对象的存储技术，可以同时使用基于块和NFS的协议作为其基础，这对于某些场景来说是一个非常引人注目的用例。但现在，让我们转向块级技术，比如iSCSI和FC。
- en: iSCSI and SAN storage
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: iSCSI和SAN存储
- en: 'Using iSCSI for virtual machine storage has long been the regular thing to
    do. Even if you take into account the fact that iSCSI isn''t the most efficient
    way to approach storage, it''s still so widely accepted that you''ll find it everywhere.
    Efficiency is compromised for two reasons:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 长期以来，使用iSCSI进行虚拟机存储一直是常规做法。即使考虑到iSCSI并不是处理存储的最有效方式这一事实，它仍然被广泛接受，你会发现它无处不在。效率受到两个原因的影响：
- en: iSCSI encapsulates SCSI commands into regular IP packages, which means segmentation
    and overhead as IP packages have a pretty large header, which means less efficiency.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: iSCSI将SCSI命令封装成常规IP数据包，这意味着IP数据包有一个相当大的头部，这意味着分段和开销，这意味着效率较低。
- en: Even worse, it's TCP-based, which means that there are sequence numbers and
    retransmissions, which can lead to queueing and latency, and the bigger the environment
    is, the more you usually feel these effects affect your virtual machine performance.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更糟糕的是，它是基于TCP的，这意味着有序号和重传，这可能导致排队和延迟，而且环境越大，你通常会感觉到这些影响对虚拟机性能的影响越大。
- en: That being said, the fact that it's based on an Ethernet stack makes it easier
    to deploy iSCSI-based solutions, while at the same time offering some unique challenges.
    For example, sometimes it's difficult to explain to a customer that using the
    same network switch(es) for virtual machine traffic and iSCSI traffic is not the
    best idea. What makes it even worse is the fact that clients are sometimes so
    blinded by their desire to save money that they don't understand that they're
    working against their own best interest. Especially when it comes to network bandwidth.
    Most of us have been there, trying to work with clients' questions such as *"but
    we already have a Gigabit Ethernet switch, why would you need anything faster
    than that?"*
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，它基于以太网堆栈，使得部署基于iSCSI的解决方案更容易，同时也提供了一些独特的挑战。例如，有时很难向客户解释，在虚拟机流量和iSCSI流量使用相同的网络交换机并不是最好的主意。更糟糕的是，客户有时会因为渴望节省金钱而无法理解他们正在违背自己的最佳利益。特别是在涉及网络带宽时。我们大多数人都曾经历过这种情况，试图回答客户的问题，比如“但我们已经有了千兆以太网交换机，为什么你需要比这更快的东西呢？”
- en: The fact of the matter is, with iSCSI's intricacies, more is just – more. The
    more speed you have on the disk/cache/controller side and the more bandwidth you
    have on the networking side, the more chance you have of creating a storage system
    that's faster. All of that can have a big impact on our virtual machine performance.
    As you'll see in the *Storage redundancy and multipathing* section, you can actually
    build a very good storage system yourself—both for iSCSI and FC. This might come
    in real handy when you try to create some kind of a testing lab/environment to
    play with as you develop your KVM virtualization skills. You can apply that knowledge
    to other virtualized environments, as well.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 事实是，对于iSCSI的复杂性来说，更多就意味着更多。在磁盘/缓存/控制器方面拥有更快的速度，以及在网络方面拥有更多的带宽，就有更多的机会创建一个更快的存储系统。所有这些都可能对我们的虚拟机性能产生重大影响。正如您将在*存储冗余和多路径*部分中看到的那样，您实际上可以自己构建一个非常好的存储系统——无论是对于iSCSI还是FC。当您尝试创建某种测试实验室/环境来发展您的KVM虚拟化技能时，这可能会非常有用。您可以将这些知识应用到其他虚拟化环境中。
- en: The iSCSI and FC architectures are very similar—they both need a target (an
    iSCSI target and an FC target) and an initiator (an iSCS initiator and an FC initiator).
    In this terminology, the target is a *server* component, and the initiator is
    a *client* component. To put it simply, the initiator connects to a target to
    get access to block storage that's presented via that target. Then, we can use
    the initiator's identity to *limit* what the initiator is able to see on the target.
    This is where the terminology starts to get a bit different when comparing iSCSI
    and FC.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: 'In iSCSI, the initiator''s identity can be defined by four different properties.
    They are as follows:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '**iSCSI Qualified Name** (**IQN**): This is a unique name that all initiators
    and targets have in iSCSI communication. We can compare this to a MAC or IP address
    in regular Ethernet-based networks. You can think of it this way—an IQN is for
    iSCSI what a MAC or IP address is for Ethernet-based networks.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**IP address**: Every initiator will have a different IP address that it uses
    to connect to the target.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MAC address**: Every initiator has a different MAC address on Layer 2.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fully Qualified Domain Name** (**FQDN**): This represents the name of the
    server as it''s resolved by a DNS service.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: From the iSCSI target perspective—depending on its implementation—you can use
    any one of these properties to create a configuration that's going to tell the
    iSCSI target which IQNs, IP addresses, MAC addresses, or FQDNs can be used to
    connect to it. This is what's called *masking*, as we can *mask* what an initiator
    can *see* on the iSCSI target by using these identities and pairing them with
    LUNs. LUNs are just raw, block capacities that we export via an iSCSI target toward
    initiators. LUNs are *indexed*, or *numbered*, usually from 0 onward. Every LUN
    number represents a different storage capacity that an initiator can connect to.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: For example, we can have an iSCSI target with three different LUNs—`LUN0` with
    20 GB, `LUN1` with 40 GB, and `LUN2` with 60 GB. These will all be hosted on the
    same storage system's iSCSI target. We can then configure the iSCSI target to
    accept an IQN to see all the LUNs, another IQN to only see `LUN1`, and another
    IQN to only see `LUN1` and `LUN2`. This is actually what we are going to configure
    right now.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start by configuring the iSCSI target service. For that, we need to
    install the `targetcli` package, and configure the service (called `target`) to
    run:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Be careful about the firewall configuration; you might need to configure it
    to allow connectivity on port `3260/tcp`, which is the port that the iSCSI target
    portal uses. So, if your firewall has started, type in the following command:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'There are three possibilities for iSCSI on Linux in terms of what storage backend
    to use. We could use a regular filesystem (such as XFS), a block device (a hard
    drive), or LVM. So, that''s exactly what we''re going to do. Our scenario is going
    to be as follows:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: '`LUN0` (20 GB): XFS-based filesystem, on the `/dev/sdb` device'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`LUN1` (40 GB): Hard drive, on the `/dev/sdc` device'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`LUN2` (60 GB): LVM, on the `/dev/sdd` device'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So, after we install the necessary packages and configure the target service
    and firewall, we should start with configuring our iSCSI target. We''ll just start
    the `targetcli` command and check the state, which should be a blank slate as
    we''re just beginning the process:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.6 – The targetcli starting point – empty configuration'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_05_06.jpg)'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.6 – The targetcli starting point – empty configuration
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start with the step-by-step procedure:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: So, let's configure the XFS-based filesystem and configure the `LUN0` file image
    to be saved there. First, we need to partition the disk (in our case, `/dev/sdb`):![Figure
    5.7 – Partitioning /dev/sdb for the XFS filesystem
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B14834_05_07.jpg)'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.7 – Partitioning /dev/sdb for the XFS filesystem
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: The next step is to format this partition, create and use a directory called
    `/LUN0` to mount this filesystem, and serve our `LUN0` image, which we're going
    to configure in the next steps:![Figure 5.8 – Formatting the XFS filesystem, creating
    a directory, and mounting it to that directory
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来是格式化这个分区，创建并使用一个名为`/LUN0`的目录来挂载这个文件系统，并提供我们的`LUN0`镜像，我们将在接下来的步骤中进行配置：![图5.8
    - 格式化XFS文件系统，创建目录，并将其挂载到该目录
- en: '](img/B14834_05_08.jpg)'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_05_08.jpg)'
- en: Figure 5.8 – Formatting the XFS filesystem, creating a directory, and mounting
    it to that directory
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.8 - 格式化XFS文件系统，创建目录，并将其挂载到该目录
- en: The next step is configuring `targetcli` so that it creates `LUN0` and assigns
    an image file for `LUN0`, which will be saved in the `/LUN0` directory. First,
    we need to start the `targetcli` command:![Figure 5.9 – Creating an iSCSI target,
    LUN0, and hosting it as a file
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是配置`targetcli`，使其创建`LUN0`并为`LUN0`分配一个镜像文件，该文件将保存在`/LUN0`目录中。首先，我们需要启动`targetcli`命令：![图5.9
    - 创建iSCSI目标，LUN0，并将其作为文件托管
- en: '](img/B14834_05_09.jpg)'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_05_09.jpg)'
- en: Figure 5.9 – Creating an iSCSI target, LUN0, and hosting it as a file
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.9 - 创建iSCSI目标，LUN0，并将其作为文件托管
- en: 'Next, let''s configure a block device-based LUN backend— `LUN2`—which is going
    to use `/dev/sdc1` (create the partition using the previous example) and check
    the current state:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们配置一个基于块设备的LUN后端— `LUN2`—它将使用`/dev/sdc1`（使用前面的示例创建分区）并检查当前状态：
- en: '![Figure 5.10 – Creating LUN1, hosting it directly from a block device'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.10 - 创建LUN1，直接从块设备托管'
- en: '](img/B14834_05_10.jpg)'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_05_10.jpg)'
- en: Figure 5.10 – Creating LUN1, hosting it directly from a block device
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.10 - 创建LUN1，直接从块设备托管
- en: 'So, `LUN0` and `LUN1` and their respective backends are now configured. Let''s
    finish things off by configuring LVM:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，`LUN0`和`LUN1`及其各自的后端现在已配置完成。让我们通过配置LVM来完成这些事情：
- en: First, we are going to prepare the physical volume for LVM, create a volume
    group out of that volume, and display all the information about that volume group
    so that we can see how much space we have for `LUN2`:![Figure 5.11 – Configuring
    the physical volume for LVM, building a volume group,
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将准备LVM的物理卷，从该卷创建一个卷组，并显示有关该卷组的所有信息，以便我们可以看到我们有多少空间可用于`LUN2`：![图5.11 - 为LVM配置物理卷，构建卷组，并显示有关该卷组的信息
- en: and displaying information about that volume group
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 和显示有关该卷组的信息
- en: '](img/B14834_05_11.jpg)'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_05_11.jpg)'
- en: Figure 5.11 – Configuring the physical volume for LVM, building a volume group,
    and displaying information about that volume group
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.11 - 为LVM配置物理卷，构建卷组，并显示有关该卷组的信息
- en: The next step is to actually create the logical volume, which is going to be
    our block storage device backend for `LUN2` in the iSCSI target. We can see from
    the `vgdisplay` output that we have 15,359 4 MB blocks available, so let's use
    that to create our logical volume, called `LUN2`. Go to `targetcli` and configure
    the necessary settings for `LUN2`:![Figure 5.12 – Configuring LUN2 with the LVM
    backend
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是实际创建逻辑卷，这将是我们iSCSI目标中`LUN2`的块存储设备后端。我们可以从`vgdisplay`输出中看到我们有15,359个4MB块可用，所以让我们用它来创建我们的逻辑卷，称为`LUN2`。转到`targetcli`并配置`LUN2`的必要设置：![图5.12
    - 使用LVM后端配置LUN2
- en: '](img/B14834_05_12.jpg)'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_05_12.jpg)'
- en: Figure 5.12 – Configuring LUN2 with the LVM backend
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.12 - 使用LVM后端配置LUN2
- en: 'Let''s stop here for a second and switch to the KVM host (the iSCSI initiator)
    configuration. First, we need to install the iSCSI initiator, which is part of
    a package called `iscsi-initiator-utils`. So, let''s use the `yum` command to
    install that:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们停在这里，转而转到KVM主机（iSCSI发起者）的配置。首先，我们需要安装iSCSI发起者，这是一个名为`iscsi-initiator-utils`的软件包的一部分。因此，让我们使用`yum`命令来安装它：
- en: '[PRE8]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Next, we need to configure the IQN of our initiator. We usually want this name
    to be reminiscent of the hostname, so, seeing that our host''s FQDN is `PacktStratis01`,
    we''ll use that to configure the IQN. To do that, we need to edit the `/etc/iscsi/initiatorname.iscsi`
    file and configure the `InitiatorName` option. For example, let''s set it to `iqn.2019-12.com.packt:PacktStratis01`.
    The content of the `/etc/iscsi/initiatorname.iscsi` file should be as follows:'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要配置我们发起者的IQN。通常我们希望这个名称能让人联想到主机名，所以，看到我们主机的FQDN是`PacktStratis01`，我们将使用它来配置IQN。为了做到这一点，我们需要编辑`/etc/iscsi/initiatorname.iscsi`文件并配置`InitiatorName`选项。例如，让我们将其设置为`iqn.2019-12.com.packt:PacktStratis01`。`/etc/iscsi/initiatorname.iscsi`文件的内容应该如下所示：
- en: '[PRE9]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Now that this is configured, let's go back to the iSCSI target and create an
    **Access Control List** (**ACL**). The ACL is going to allow our KVM host's initiator
    to connect to the iSCSI target portal:![Figure 5.13 – Creating an ACL so that
    the KVM host's initiator can connect to the iSCSI target
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在这已经配置好了，让我们回到iSCSI目标并创建一个**访问控制列表**（**ACL**）。ACL将允许我们的KVM主机发起者连接到iSCSI目标门户：![图5.13
    - 创建ACL，以便KVM主机的发起者可以连接到iSCSI目标
- en: '](img/B14834_05_13.jpg)'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_05_13.jpg)'
- en: Figure 5.13 – Creating an ACL so that the KVM host's initiator can connect to
    the iSCSI target
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.13 - 创建ACL，以便KVM主机的发起者可以连接到iSCSI目标
- en: 'Next, we need to publish our pre-created file-based and block-based devices
    to the iSCSI target LUNs. So, we need to do this:'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要将我们预先创建的基于文件和基于块的设备发布到iSCSI目标LUNs。因此，我们需要这样做：
- en: '![Figure 5.14 – Adding our file-based and block-based devices to the iSCSI
    target LUNs 0, 1, and 2'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.14 - 将我们的基于文件和基于块的设备添加到iSCSI目标LUNs 0、1和2'
- en: '](img/B14834_05_14.jpg)'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_05_14.jpg)'
- en: Figure 5.14 – Adding our file-based and block-based devices to the iSCSI target
    LUNs 0, 1, and 2
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.14 - 将我们的基于文件和基于块的设备添加到iSCSI目标LUNs 0、1和2
- en: 'The end result should look like this:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 最终结果应该如下所示：
- en: '![Figure 5.15 – The end result'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.15 - 最终结果'
- en: '](img/B14834_05_15.jpg)'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_05_15.jpg)'
- en: Figure 5.15 – The end result
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.15 - 最终结果
- en: 'At this point, everything is configured. We need to go back to our KVM host
    and define a storage pool that will use these LUNs. The easiest way to do that
    would be to use an XML configuration file for the pool. So, here''s our sample
    configuration XML file; we''ll call it `iSCSIPool.xml`:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Let''s explain the file step by step:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '`pool type= ''iscsi''`: We''re telling libvirt that this is an iSCSI pool.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`name` : The pool name.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`host name`: The IP address of the iSCSI target.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`device path`: The IQN of the iSCSI target.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The IQN name in the initiator section: The IQN of the initiator.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`target path`: The location where iSCSI target''s LUNs will be mounted.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, all that''s left for us to do is to define, start, and autostart our new
    iSCSI-backed KVM storage pool:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The target path part of the configuration can be easily checked via `virsh`.
    If we type the following command into the KVM host, we will get the list of available
    LUNs from the `MyiSCSIPool` pool that we just configured:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We get the following result for this command:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.16 – Runtime names for our iSCSI pool LUNs'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_05_16.jpg)'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.16 – Runtime names for our iSCSI pool LUNs
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: If this output reminds you a bit of the VMware vSphere Hypervisor storage runtime
    names, you are definitely on the right track. We will be able to use these storage
    pools in [*Chapter 7*](B14834_07_Final_ASB_ePub.xhtml#_idTextAnchor125), *Virtual
    Machine – Installation, Configuration, and Life-Cycle Management*, when we start
    deploying our virtual machines.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: Storage redundancy and multipathing
  id: totrans-219
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Redundancy is one of the keywords of IT, where any single component failure
    could mean big problems for a company or its customers. The general design principle
    of avoiding SPOF is something that we should always stick to. At the end of the
    day, no network adapter, cable, switch, router, or storage controller is going
    to work forever. So, calculating redundancy into our designs helps our IT environment
    during its normal life cycle.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: At the same time, redundancy can be combined with multipathing to also ensure
    higher throughput. For example, when we connect our physical host to FC storage
    with two controllers with four FC ports each, we can use four paths (if the storage
    is active-passive) or eight paths (if it's active-active) to the same LUN(s) exported
    from this storage device to a host. This gives us multiple additional options
    for LUN access, on top of the fact that it gives us more availability, even in
    the case of failure.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: Getting a regular KVM host to do, for example, iSCSI multipathing is quite a
    bit complex. There are multiple configuration issues and blank spots in terms
    of documentation, and supportability of such a configuration is questionable.
    However, there are products that use KVM that support it out of the box, such
    as oVirt (which we covered before) and **Red Hat Enterprise Virtualization Hypervisor**
    (**RHEV-H**). So, let's use oVirt for this example on iSCSI.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: 'Before you do this, make sure that you have done the following:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: Your Hypervisor host is added to the oVirt inventory.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Your Hypervisor host has two additional network cards, independent of the management
    network.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The iSCSI storage has two additional network cards in the same L2 networks as
    the two additional hypervisor network cards.
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The iSCSI storage is configured so that it has at least a target and a LUN already
    configured in a way that will enable the hypervisor host to connect to it.
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So, as we''re doing this in oVirt, there are a couple of things that we need
    to do. First, from a networking perspective, it would be a good idea to create
    some storage networks. In our case, we''re going to assign two networks for iSCSI,
    and we will call them `iSCSI01` and `iSCSI02`. We need to open the oVirt administration
    panel, hover over `iSCSI01` (for the first one), uncheck the `iSCSI02` network:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.17 – Configuring networks for iSCSI bond'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_05_17.jpg)'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.17 – Configuring networks for iSCSI bond
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is assigning these networks to host network adapters. Go to `compute/hosts`,
    double-click on the host that you added to oVirt''s inventory, select the `iSCSI01`
    on the second network interface and `iSCSI02` on the third network interface.
    The first network interface is already taken by the oVirt management network.
    It should look something like this:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.18 – Assigning virtual networks to the hypervisor''s physical adapters'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_05_18.jpg)'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.18 – Assigning virtual networks to the hypervisor's physical adapters
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: 'Before you close the window down, make sure that you click on the *pencil*
    sign on both `iSCSI01` and `iSCSI02` to set up IP addresses for these two virtual
    networks. Assign network configuration that can connect you to your iSCSI storage
    on the same or different subnets:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.19 – Creating an iSCSI bond on the data center level'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_05_19.jpg)'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.19 – Creating an iSCSI bond on the data center level
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: 'There you go, you have just configured an iSCSI bond. The last part of our
    configuration is enabling it. Again, in the oVirt GUI, go to **Compute** | **Data
    Centers**, select your datacenter with a double-click, and go to the **iSCSI Multipathing**
    tab:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.20 – Configuring iSCSI multipathing on the data center level'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_05_20.jpg)'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.20 – Configuring iSCSI multipathing on the data center level
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: Click on the `iSCSI01` and `iSCSI02` networks in the top part of the pop-up
    window, and the iSCSI target on the lower side.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have covered the basics of storage pools, NFS, and iSCSI, we can
    move on to a standard open source way of deploying storage infrastructure, which
    would be to use Gluster and/or Ceph.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: Gluster and Ceph as a storage backend for KVM
  id: totrans-246
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are other advanced types of filesystems that can be used as the libvirt
    storage backend. So, let's now discuss two of them—Gluster and Ceph. Later, we'll
    also check how libvirt works with GFS2.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: Gluster
  id: totrans-248
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Gluster is a distributed filesystem that's often used for high-availability
    scenarios. Its main advantages over other filesystems are the fact that it's scalable,
    it can use replication and snapshots, it can work on any server, and it's usable
    as a basis for shared storage—for example, via NFS and SMB. It was developed by
    a company called Gluster Inc., which was acquired by RedHat in 2011\. However,
    unlike Ceph, it's a *file* storage service, while Ceph offers *block* and *object*-based
    storage. Object-based storage for block-based devices means direct, binary storage,
    directly to a LUN. There are no filesystems involved, which theoretically means
    less overhead as there's no filesystem, filesystem tables, and other constructs
    that might slow the I/O process down.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: Let's first configure Gluster to show its use case with libvirt. In production,
    that means installing at least three Gluster servers so that we can make high
    availability possible. Gluster configuration is really straightforward, and in
    our example, we are going to create three CentOS 7 machines that we will use to
    host the Gluster filesystem. Then, we will mount that filesystem on our hypervisor
    host and use it as a local directory. We can use GlusterFS directly from libvirt,
    but the implementation is just not as refined as using it via the gluster client
    service, mounting it as a local directory, and using it directly as a directory
    pool in libvirt.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: 'Our configuration will look like this:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.21 – Basic settings for our Gluster cluster'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_05_21.jpg)'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.21 – Basic settings for our Gluster cluster
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: 'So, let''s put that into production. We have to issue a large sequence of commands
    on all of the servers before we configure Gluster and expose it to our KVM host.
    Let''s start with `gluster1`. First, we are going to do a system-wide update and
    reboot to prepare the core operating system for Gluster installation. Type the
    following commands into all three CentOS 7 servers:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Then, we can start deploying the necessary repositories and packages, format
    disks, configure the firewall, and so on. Type the following commands into all
    the servers:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We need to do a bit of networking configuration as well. It would be good if
    these three servers can *resolve* each other, which means either configuring a
    DNS server or adding a couple of lines to our `/etc/hosts` file. Let''s do the
    latter. Add the following lines to your `/etc/hosts` file:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'For the next part of the configuration, we can just log in to the first server
    and use it as the de facto management server for our Gluster infrastructure. Type
    in the following commands:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The first three commands should get you the `peer probe: success` status. The
    third one should return an output similar to this:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.22 – Confirmation that the Gluster servers peered successfully'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_05_22.jpg)'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.22 – Confirmation that the Gluster servers peered successfully
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that this part of the configuration is done, we can create a Gluster-distributed
    filesystem. We can do this by typing the following sequence of commands:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Then, we could mount Gluster as an NFS directory for testing purposes. For
    example, we can create a distributed namespace called `kvmgluster` to all of the
    member hosts (`gluster1`, `gluster2`, and `gluster3`). We can do this by using
    the following commands:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The Gluster part is now ready, so we need to go back to our KVM host and mount
    the Gluster filesystem to it by typing in the following commands:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: We have to pay close attention to Gluster releases on the server and client,
    which is why we downloaded the Gluster repository information for CentOS 8 (we're
    using it on the KVM server) and installed the necessary Gluster client packages.
    That enabled us to mount the filesystem with the last command.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we''ve finished our configuration, we just need to add this directory
    as a libvirt storage pool. Let''s do that by using an XML file with the storage
    pool definition, which contains the following entries:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Let''s say that we saved this file in the current directory, and that the file
    is called `gluster.xml`. We can import and start it in libvirt by using the following
    `virsh` commands:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We should mount this pool automatically on boot so that libvirt can use it.
    Therefore, we need to add the following line to `/etc/fstab`:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Using a directory-based approach enables us to avoid two problems that libvirt
    (and its GUI interface, `virt-manager`) has with Gluster storage pools:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: We can use Gluster's failover capability, which will be managed automatically
    by the Gluster utilities that we installed directly, as libvirt doesn't support
    them yet.
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will avoid creating virtual machine disks *manually*, which is another limitation
    of libvirt's implementation of Gluster support, while directory-based storage
    pools support it without any issues.
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It seems weird that we''re mentioning *failover*, as it seems as though we
    didn''t configure it as a part of any of the previous steps. Actually, we have.
    When we issued the last mount command, we used Gluster''s built-in modules to
    establish connectivity to the *first* Gluster server. That, in turn, means that
    after this connection, we got all of the details about the whole Gluster pool,
    which we configured so that it''s hosted on three servers. If any kind of failure
    happens—which we can easily simulate—this connection will continue working. We
    can simulate this scenario by turning off any of the Gluster servers, for example—`gluster1`.
    You''ll see that the local directory where we mounted Gluster directory still
    works, even though `gluster1` is down. Let''s see that in action (the default
    timeout period is 42 seconds):'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.23 – Gluster failover working; the first node is down, but we''re
    still able to get our files'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_05_23.jpg)'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.23 – Gluster failover working; the first node is down, but we're still
    able to get our files
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: 'If we want to be more aggressive, we can shorten this timeout period to—for
    example—2 seconds by issuing the following command on any of our Gluster servers:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The `number` part is in seconds, and by assigning it a lower number, we can
    directly influence how aggressive the failover process is.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: So, now that everything is configured, we can start using the Gluster pool to
    deploy virtual machines, which we will discuss further in [*Chapter 7*](B14834_07_Final_ASB_ePub.xhtml#_idTextAnchor125),
    *Virtual Machine – Installation, Configuration, and Life-Cycle Management*.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: Seeing as Gluster is a file-based backend that can be used for libvirt, it's
    only natural to describe how to use an advanced block-level and object-level storage
    backend. That's where Ceph comes in, so let's work on that now.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: Ceph
  id: totrans-292
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Ceph can act as file-, block-, and object-based storage. But for the most part,
    we're usually using it as either block- or object-based storage. Again, this is
    a piece of open source software that's designed to work on any server (or a virtual
    machine). In its core, Ceph runs an algorithm called **Controlled Replication
    Under Scalable Hashing** (**CRUSH**). This algorithm tries to distribute data
    across object devices in a pseudo-random manner, and in Ceph, it's managed by
    a cluster map (a CRUSH map). We can easily scale Ceph out by adding more nodes,
    which will redistribute data in a minimum fashion to ensure as small amount of
    replication as possible.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: An internal Ceph component called **Reliable Autonomic Distributed Object Store**
    (**RADOS**) is used for snapshots, replication, and thin provisioning. It's an
    open source project that was developed by the University of California.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: 'Architecture-wise, Ceph has three main services:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: '**ceph-mon** : Used for cluster monitoring, CRUSH maps, and **Object Storage
    Daemon** (**OSD**) maps.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ceph-osd**: This handles actual data storage, replication, and recovery.
    It requires at least two nodes; we''ll use three for clustering reasons.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ceph-mds**: Metadata server, used when Ceph needs filesystem access.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In accordance with best practices, make sure that you always design your Ceph
    environments with the key principles in mind—all of the data nodes need to have
    the same configuration. That means the same amount of memory, the same storage
    controllers (don't use RAID controllers, just plain HBAs without RAID firmware
    if possible), the same disks, and so on. That's the only way to ensure a constant
    level of Ceph performance in your environments.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: 'One very important aspect of Ceph is data placement and how placement groups
    work. Placement groups offer us a chance to split the objects that we create and
    place them in OSDs in an optimal fashion. Translation: the bigger the number of
    placement groups we configure, the better balance we''re going to get.'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: So, let's configure Ceph from scratch. We're going to follow the best practices
    again and deploy Ceph by using five servers—one for administration, one for monitoring,
    and three OSDs.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: 'Our configuration will look like this:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.24 – Basic Ceph configuration for our infrastructure'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_05_24.jpg)'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.24 – Basic Ceph configuration for our infrastructure
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: 'Make sure that these hosts can resolve each other via DNS or `/etc/hosts`,
    and that you configure all of them to use the same NTP source. Make sure that
    you update all of the hosts by using the following:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Also, make sure that you type the following commands into all of the hosts
    as the *root* user. Let''s start by deploying packages, creating an admin user,
    and giving them rights to `sudo`:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Disabling SELinux will make our life easier for this demonstration, as will
    getting rid of the firewall:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Let''s add hostnames to `/etc/hosts` so that administration is easier for us:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Change the last `echo` part to suit your environment—hostnames and IP addresses.
    We''re just using this as an example from our environment. The next step is making
    sure that we can use our admin host to connect to all of the hosts. The easiest
    way to do that is by using SSH keys. So, on `ceph-admin`, log in as root and type
    in the `ssh-keygen` command, and then press the *Enter* key all the way through.
    It should look something like this:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.25 – Generating an SSH key for root for Ceph setup purposes'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_05_25.jpg)'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.25 – Generating an SSH key for root for Ceph setup purposes
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: 'We also need to copy this key to all of the hosts. So, again, on `ceph-admin`,
    use `ssh-copy-id` to copy the keys to all of the hosts:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Accept all of the keys when SSH asks you, and use `ceph123` as the password,
    which we selected in one of the earlier steps. After all of this is done, there''s
    one last step that we need to do on `ceph-admin` before we start deploying Ceph—we
    have to configure SSH to use the `cephadmin` user as a default user to log in
    to all of the hosts. We will do this by going to the `.ssh` directory as root
    on `ceph-admin`, and creating a file called `config` with the following content:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'That was a long pre-configuration, wasn''t it? Now it''s time to actually start
    deploying Ceph. The first step is to configure `ceph-monitor`. So, on `ceph-admin`,
    type in the following commands:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Because of the fact that we selected a configuration in which we have three
    OSDs, we need to configure Ceph so that it uses these additional two hosts. So,
    in the `cluster` directory, edit the file called `ceph.conf` and add the following
    two lines at the end:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: This will make sure that we can only use our example network (`192.168.159.0/24`)
    for Ceph, and that we have two additional OSDs on top of the original one.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that everything''s ready, we have to issue a sequence of commands to configure
    Ceph. So, again, on `ceph-admin`, type in the following commands:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Let''s describe these commands one by one:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: The first command starts the actual deployment process—for the admin, monitor,
    and OSD nodes, with the installation of all the necessary packages.
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second and third commands configure the monitor host so that it's ready
    to accept external connections.
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The two disk commands are all about disk preparation—Ceph will clear the disks
    that we assigned to it (`/dev/sdb` per OSD host) and create two partitions on
    them, one for Ceph data and one for the Ceph journal.
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The last two commands prepare these filesystems for use and activate Ceph. If
    at any time your `ceph-deploy` script stops, check your DNS and `/etc/hosts` and
    `firewalld` configuration, as that's where the problems usually are.
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We need to expose Ceph to our KVM host, which means that we have to do a bit
    of extra configuration. We''re going to expose Ceph as an object pool to our KVM
    host, so we need to create a pool. Let''s call it `KVMpool`. Connect to `ceph-admin`,
    and issue the following commands:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: This command will create a pool called `KVMpool`, with 128 placement groups.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step involves approaching Ceph from a security perspective. We don''t
    want anyone connecting to this pool, so we''re going to create a key for authentication
    to Ceph, which we''re going to use on the KVM host for authentication purposes.
    We do that by typing the following command:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'It''s going to throw us a status message, something like this:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'We can then switch to the KVM host, where we need to do two things:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: Define a secret—an object that's going to link libvirt to a Ceph user—and by
    doing that, we're going to create a secret object with its **Universally Unique
    Identifier** (**UUID**).
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use that secret's UUID to link it to the Ceph key when we define the Ceph storage
    pool.
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The easiest way to do these two steps would be by using two XML configuration
    files for libvirt. So, let''s create those two files. Let''s call the first one,
    `secret.xml`, and here are its contents:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Make sure that you save and import this XML file by typing in the following
    command:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'After you press the *Enter* key, this command is going to throw out a UUID.
    Please copy and paste that UUID someplace safe, as we''re going to need it for
    the pool XML file. In our environment, this first `virsh` command threw out the
    following output:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'We need to assign a value to this secret so that when libvirt tries to use
    this secret, it knows which *password* to use. That''s actually the password that
    we created on the Ceph level, when we used `ceph auth get-create`, which threw
    us the key. So, now that we have both the secret UUID and the Ceph key, we can
    combine them to create a complete authentication object. On the KVM host, we need
    to type in the following command:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Now, we can create the Ceph pool file. Let''s call the config file `ceph.xml`,
    and here are its contents:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-353
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'So, the UUID from the previous step was used in this file to reference which
    secret (identity) is going to be used for Ceph pool access. Now we need to do
    the standard procedure—import the pool, start it, and autostart it—if we want
    to use it permanently (after the KVM host reboot). So, let''s do that with the
    following sequence of commands on the KVM host:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The last command should produce an output similar to this:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.26 – Checking the state of our pools; the Ceph pool is configured
    and ready to be used'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_05_26.jpg)'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.26 – Checking the state of our pools; the Ceph pool is configured and
    ready to be used
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: Now that the Ceph object pool is available for our KVM host, we could install
    a virtual machine on it. We're going to work on that – again – in [*Chapter 7*](B14834_07_Final_ASB_ePub.xhtml#_idTextAnchor125),
    *Virtual Machine – Installation, Configuration, and Life-Cycle Management*.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: Virtual disk images and formats and basic KVM storage operations
  id: totrans-361
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Disk images are standard files stored on the host''s filesystem. They are large
    and act as virtualized hard drives for guests. You can create such files using
    the `dd` command, as shown:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-363
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Here is the translation of this command for you:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: Duplicate data (`dd`) from the input file (`if`) of `/dev/zero` (virtually limitless
    supply of zeros) into the output file (`of`) of `/vms/dbvm_disk2.img` (disk image)
    using blocks of 1 G size (`bs` = block size) and repeat this (`count`) just once
    (`10`).
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: 'Important note:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: '`dd` is known to be a resource-hungry command. It may cause I/O problems on
    the host system, so it''s good to first check the available free memory and I/O
    state of the host system, and only then run it. If the system is already loaded,
    lower the block size to MB and increase the count to match the size of the file
    you wanted (use `bs=1M`, `count=10000` instead of `bs=1G`, `count=10`).'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: '`/vms/dbvm_disk2.img` is the result of the preceding command. The image now
    has 10 GB preallocated and ready to use with guests either as the boot disk or
    second disk. Similarly, you can also create thin-provisioned disk images. Preallocated
    and thin-provisioned (sparse) are disk allocation methods, or you may also call
    it the format:'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: '**Preallocated**: A preallocated virtual disk allocates the space right away
    at the time of creation. This usually means faster write speeds than a thin-provisioned
    virtual disk.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`seek` option with the `dd` command, as shown in the following command:'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-371
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Each comes with its own advantages and disadvantages. If you are looking for
    I/O performance, go for a preallocated format, but if you have a non-IO-intensive
    load, choose thin-provisioned.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, you might be wondering how you can identify what disk allocation method
    a certain virtual disk uses. There is a good utility for finding this out: `qemu-img`.
    This command allows you to read the metadata of a virtual image. It also supports
    creating a new disk and performing low-level format conversion.'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: Getting image information
  id: totrans-374
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `info` parameter of the `qemu-img` command displays information about a
    disk image, including the absolute path of the image, the file format, and the
    virtual and disk size. By looking at the virtual disk size from a QEMU perspective
    and comparing that to the image file size on the disk, you can easily identify
    what disk allocation policy is in use. As an example, let''s look at two of the
    disk images we created:'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-376
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: See the `disk size` line of both the disks. It's showing `10G` for `/vms/dbvm_disk2.img`,
    whereas for `/vms/dbvm_disk2_seek.img`, it's showing `10M` MiB. This difference
    is because the second disk uses a thin-provisioning format. The virtual size is
    what guests see and the disk size is what space the disk reserved on the host.
    If both the sizes are the same, it means the disk is preallocated. A difference
    means that the disk uses the thin-provisioning format. Now, let's attach the disk
    image to a virtual machine; you can attach it using `virt-manager` or the CLI
    alternative, `virsh`.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: Attaching a disk using virt-manager
  id: totrans-378
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Start virt-manager from the host system''s graphical desktop environment. It
    can also be started remotely using SSH, as demonstrated in the following command:'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-380
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'So, let''s use the Virtual Machine Manager to attach the disk to the virtual
    machine:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: In the Virtual Machine Manager main window, select the virtual machine to which
    you want to add the secondary disk.
  id: totrans-382
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Go to the virtual hardware details window and click on the **Add Hardware**
    button located at the bottom-left side of the dialog box.
  id: totrans-383
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In **Add New Virtual Hardware**, select **Storage** and select the **Create
    a disk image for the virtual machine** button and virtual disk size, as in the
    following screenshot:![Figure 5.27 – Adding a virtual disk in virt-manager
  id: totrans-384
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B14834_05_27.jpg)'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.27 – Adding a virtual disk in virt-manager
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: If you want to attach the previously created `dbvm_disk2.img` image, choose
    `dbvm_disk2.img` file from the `/vms` directory or find it in the local storage
    pool, then select it and click `/dev/sdb`) or disk partition (`/dev/sdb1`), or
    LVM logical volume. We could have used any of the previously configured storage
    pools for storing this image either as a file or object or directly to a block
    device.
  id: totrans-387
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Clicking on the `virsh` command.
  id: totrans-388
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using virt-manager to create a virtual disk was easy enough—just a couple of
    clicks of a mouse and a bit of typing. Now, let's see how we can do that via the
    command line—namely, by using `virsh`.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
- en: Attaching a disk using virsh
  id: totrans-390
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`virsh` is a very powerful command-line alternative to virt-manager. You can
    perform an action in a second that would take minutes to perform through a graphical
    interface such as virt-manager. It provides an `attach-disk` option to attach
    a new disk device to a virtual machine. There are lots of switches provided with
    `attach-disk`:'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-392
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'However, in a normal scenario, the following are sufficient to perform hot-add
    disk attachment to a virtual machine:'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-394
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Here, `CentOS8` is the virtual machine to which a disk attachment is executed.
    Then, there is the path of the disk image. `vdb` is the target disk name that
    would be visible inside the guest operating system. `--live` means performing
    the action while the virtual machine is running, and `--config` means attaching
    it persistently across reboot. Not adding a `--config` switch will keep the disk
    attached only until reboot.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
- en: 'Important note:'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
- en: 'Hot plugging support: The `acpiphp` kernel module should be loaded in a Linux
    guest operating system in order to recognize a hot-added disk; `acpiphp` provides
    legacy hot plugging support, whereas `pciehp` provides native hot plugging support
    . `pciehp` is dependent on `acpiphp`. Loading `acpiphp` will automatically load
    `pciehp` as a dependency.'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
- en: 'You can use the `virsh domblklist <vm_name>` command to quickly identify how
    many vDisks are attached to a virtual machine. Here is an example:'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-399
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: This clearly indicates that the two vDisks connected to the virtual machine
    are both file images. They are visible to the guest operating system as `vda`
    and `vdb`, respectively, and in the last column of the disk images path on the
    host system.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
- en: Next, we are going to see how to create an ISO library.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
- en: Creating an ISO image library
  id: totrans-402
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although a guest operating system on the virtual machine can be installed from
    physical media by carrying out a passthrough the host's CD/DVD drive to the virtual
    machine, it's not the most efficient way. Reading from a DVD drive is slow compared
    to reading ISO from a hard disk, so a better way is to store ISO files (or logical
    CDs) used to install operating systems and applications for the virtual machines
    in a file-based storage pool and create an ISO image library.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
- en: 'To create an ISO image library, you can either use virt-manager or a `virsh`
    command. Let''s see how to create an ISO image library using the `virsh` command:'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
- en: 'First, create a directory on the host system to store the `.iso` images:'
  id: totrans-405
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-406
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Set the correct permissions. It should be owned by a root user with permission
    set to `700`. If SELinux is in enforcing mode, the following context needs to
    be set:'
  id: totrans-407
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-408
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Define the ISO image library using the `virsh` command, as shown in the following
    code block:'
  id: totrans-409
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-410
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Verify that the pool (ISO image library) was created:'
  id: totrans-411
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-412
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Now you can copy or move the `.iso` images to the `/iso_lib` directory.
  id: totrans-413
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Upon copying the `.iso` files into the `/iso_lib` directory, refresh the pool
    and then check its contents:'
  id: totrans-414
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-415
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: This will list all the ISO images stored in the directory, along with their
    path. These ISO images can now be used directly with a virtual machine for guest
    operating system installation, software installation, or upgrades.
  id: totrans-416
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Creating an ISO image library is the de facto norm in today's enterprises. It's
    better to have a centralized place where all your ISO images are, and it makes
    it easier to implement some kind of synchronization method (for example, `rsync`)
    if you need to synchronize across different locations.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
- en: Deleting a storage pool
  id: totrans-418
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deleting a storage pool is fairly easy. Please note that deleting a storage
    domain will not remove any file/block devices. It just disconnects the storage
    from virt-manager. The file/block device has to be removed manually.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
- en: 'We can delete a storage pool via virt-manager or by using the `virsh` command.
    Let''s first check how to do it via virt-manager:'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.28 – Deleting a pool'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_05_28.jpg)'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.28 – Deleting a pool
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
- en: First, select the red stop button to stop the pool, and then click on the red
    circle with an **X** to delete the pool.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to use `virsh`, it''s even simpler. Let''s say that we want to
    delete the storage pool called `MyNFSpool` in the previous screenshot. Just type
    in the following commands:'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-426
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: The next logical step after creating a storage pool is to create a storage volume.
    From a logical standpoint, the storage volume slices a storage pool into smaller
    parts. Let's learn how to do that now.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
- en: Creating storage volumes
  id: totrans-428
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Storage volumes are created on top of storage pools and attached as virtual
    disks to virtual machines. In order to create a storage volume, start the Storage
    Management console, navigate to virt-manager, then click **Edit** | **Connection
    Details** | **Storage** and select the storage pool where you want to create a
    new volume. Click on the create new volume button (**+**):'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.29 – Creating a storage volume for the virtual machine'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_05_29.jpg)'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.29 – Creating a storage volume for the virtual machine
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
- en: Next, provide the name of the new volume, choose the disk allocation format
    for it, and click on the `virsh` command. There are several disk formats that
    are supported by libvirt (`raw`, `cow`, `qcow`, `qcow2`, `qed`, and `vmdk`). Use
    the disk format that suits your environment and set the proper size in the `Max
    Capacity` and `Allocation` fields to decide whether you wish to go with preallocated
    disk allocation or thin-provisioned. If you keep the disk size the same in `qcow2`
    format does not support the thick disk allocation method.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
- en: In [*Chapter 8*](B14834_08_Final_ASB_ePub.xhtml#_idTextAnchor143), *Creating
    and Modifying VM Disks, Templates, and Snapshots*, all the disk formats are explained
    in detail. For now, just understand that `qcow2` is a specially designed disk
    format for KVM virtualization. It supports the advanced features needed for creating
    internal snapshots.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
- en: Creating volumes using the virsh command
  id: totrans-435
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The syntax to create a volume using the `virsh` command is as follows:'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-437
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Here, `dedicated_storage` is the storage pool, `vm_vol1` is the volume name,
    and 10 GB is the size:'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-439
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: The `virsh` command and arguments to create a storage volume are almost the
    same regardless of the type of storage pool it is created on. Just enter the appropriate
    input for a `--pool` switch. Now, let's see how to delete a volume using the `virsh`
    command.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
- en: Deleting a volume using the virsh command
  id: totrans-441
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The syntax to delete a volume using the `virsh` command is as follows:'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-443
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Executing this command will remove the `vm_vol2` volume from the `dedicated_storage`
    storage pool.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
- en: The next step in our storage journey is about looking a bit into the future
    as all of the concepts that we mentioned in this chapter have been well known
    for years, some even for decades. The world of storage is changing and moving
    into new and interesting directions, so let's discuss that for a bit next.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
- en: The latest developments in storage – NVMe and NVMeOF
  id: totrans-446
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the past 20 or so years, by far the biggest disruption in the storage world
    in terms of technology has been the introduction of **Solid State Drives** (**SSDs**).
    Now, we know that a lot of people have gotten quite used to having them in their
    computers—laptops, workstations, whichever type of device we use. But again, we're
    discussing storage for virtualization, and enterprise storage concepts overall,
    and that means that our regular SATA SSDs aren't going to make the cut. Although
    a lot of people use them in mid-range storage devices and/or handmade storage
    devices that host ZFS pools (for cache), some of these concepts have a life of
    their own in the latest generations of storage devices. These devices are fundamentally
    changing the way technology is working and redoing parts of modern IT history
    in terms of which protocols are used, how fast they are, how much lower latencies
    they have, and how they approach storage tiering—tiering being a concept that
    differentiates different storage devices or their storage pools based on a capability,
    usually speed.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
- en: Let's briefly explain what we're discussing here by using an example of where
    the storage world is heading. Along with that, the storage world is taking the
    virtualization, cloud, and HPC world along for the ride, so these concepts are
    not outlandish. They already exist, in readily available storage devices that
    you can buy today.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
- en: The introduction of SSDs brought a significant change in the way we access our
    storage devices. It's all about performance and latency, and older concepts such
    as **Advanced Host Controller Interface** (**AHCI**), which we're still actively
    using with many SSDs on the market today, are just not good enough to handle the
    performance that SSDs have. AHCI is a standard way in which a regular hard disk
    (mechanical disk or regular spindle) talks via software to SATA devices. However,
    the key part of that is *hard disk*, which means cylinders, heads sectors—things
    that SSDs just don't have, as they don't spin around and don't need that kind
    of paradigm. That meant that another standard had to be created so that we can
    use SSDs in a more native fashion. That's what **Non-Volatile Memory Express**
    (**NVMe**) is all about—bridging the gap between what SSDs are capable of doing
    and what they can actually do, without using translations from SATA to AHCI to
    PCI Express (and so on).
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
- en: The fast development pace of SSDs and the integration of NVMe made huge advancements
    in enterprise storage possible. That means that new controllers, new software,
    and completely new architectures had to be invented to support this paradigm shift.
    As more and more storage devices integrate NVMe for various purposes—primarily
    for caching, then for storage capacity as well—it's becoming clear that there
    are other problems that need to be solved as well. The first of which is the way
    in which we're going to connect storage devices offering such a tremendous amount
    of capability to our virtualized, cloud, or HPC environments.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
- en: 'In the past 10 or so years, many people argued that FC is going to disappear
    from the market, and a lot of companies hedged their bets on different standards—iSCSI,
    iSCSI over RDMA, NFS over RDMA, and so on. The reasoning behind that seemed solid
    enough:'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
- en: FC is expensive—it requires separate physical switches, separate cabling, and
    separate controllers, all of which cost a lot of money.
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There's licensing involved—when you buy, for example, a Brocade switch that
    has 40 FC ports, that doesn't mean that you can use all of them out of the box,
    as there are licenses to get more ports (8-port, 16-port, and so on).
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: FC storage devices are expensive and often require more expensive disks (with
    FC connectors).
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuring FC requires extensive knowledge and/or training, as you can't simply
    go and configure a stack of FC switches for an enterprise-level company without
    knowing the concepts, and the CLI from the switch vendor, on top of knowing what
    that enterprise's needs are.
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ability of FC as a protocol to speed up its development to reach new speeds
    has been really bad. In simple terms, during the time it took FC to go from 8
    Gbit/s to 32 Gbit/s, Ethernet went from 1 Gbit/s to 25, 40, 50, and 100 Gbit/s
    bandwidth. There's already talk about 400 Gbit/s Ethernet, and there are the first
    devices that support that standard as well. That usually makes customers concerned
    as higher numbers mean better throughput, at least in most people's minds.
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: But what's happening on the market *now* tells us a completely different story—not
    just that FC is back, but that it's back with a mission. The enterprise storage
    companies have embraced that and started introducing storage devices with *insane*
    levels of performance (with the aid of NVMe SSDs, as a first phase). That performance
    needs to be transferred to our virtualized, cloud, and HPC environments, and that
    requires the best possible protocol, in terms of lowest latency, its design, and
    the quality and reliability, and FC has all of that.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
- en: That leads to the second phase, where NVMe SSDs aren't just being used as cache
    devices, but as capacity devices as well.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
- en: Take note of the fact that, right now, there's a big fight brewing on the storage
    memory/storage interconnects market. There are multiple different standards trying
    to compete with Intel's **Quick Path Interconnect** (**QPI**), a technology that's
    been used in Intel CPUs for more than a decade. If this is a subject that's interesting
    to you, there is a link at the end of this chapter, in the *Further reading* section,
    where you can find more information. Essentially, QPI is a point-to-point interconnection
    technology with low latency and high bandwidth that's at the core of today's servers.
    Specifically, it handles communication between CPUs, CPUs and memory, CPUs and
    chipsets, and so on. It's a technology that Intel developed after it got rid of
    the **Front Side Bus** (**FSB**) and chipset-integrated memory controllers. FSB
    was a shared bus that was shared between memory and I/O requests. That approach
    had much higher latency, didn't scale well, and had lower bandwidth and problems
    with situations in which there's a large amount of I/O happening on the memory
    and I/O side. After switching to an architecture where the memory controller was
    a part of the CPU (therefore, memory directly connects to it), it was essential
    for Intel to finally move to this kind of concept.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
- en: If you're more familiar with AMD CPUs, QPI is to Intel what HyperTransport bus
    on a CPU with built-in memory controller is to AMD CPUs.
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
- en: 'As NVMe SSDs became faster, the PCI Express standard also needed to be updated,
    which is the reason why the latest version (PCIe 4.0 – the first products started
    shipping recently) was so eagerly anticipated. But now, the focus has switched
    to two other problems that need resolving for storage systems to work. Let''s
    describe them briefly:'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
- en: Problem number one is simple. For a regular computer user, one or two NVMe SSDs
    will be enough in 99% of scenarios or more. Realistically, the only real reason
    why regular computer users need a faster PCIe bus is for a faster graphics cards.
    But for storage manufacturers, it's completely different. They want to produce
    enterprise storage devices that will have 20, 30, 50, 100, 500 NVMe SSDs in a
    single storage system—and they want that now, as SSDs are mature as a technology
    and are widely available.
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Problem number two is more complex. To add insult to injury, the latest generation
    of SSDs (for example, based on Intel Optane) can offer even lower latency and
    higher throughput. That's only going to get *worse* (even lower latencies, higher
    throughput) as technology evolves. For today's services—virtualization, cloud,
    and HPC—it's essential that the storage system is able to handle any load that
    we can throw at it. These technologies are a real game-changer in terms of how
    much faster storage devices can become, only if interconnects can handle it (QPI,
    FC, and many more). Two of these concepts derived from Intel Optane—**Storage
    Class Memory** (**SCM**) and **Persistent Memory** (**PM**) are the latest technologies
    that storage companies and customers want adopted into their storage systems,
    and fast.
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The third problem is how to transfer all of that bandwidth and I/O capability
    to the servers and infrastructures using them. This is why the concept of **NVMe
    over Fabrics** (**NVMe-OF**) was created, to try to work on the storage infrastructure
    stack to make NVMe much more efficient and faster for its consumers.
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you take a look at these advancements from a conceptual point of view, it
    was clear for decades that RAM-like memory is the fastest, lowest latency technology
    that we've had for the past couple of decades. It's also logical that we're moving
    workloads to RAM, as much as possible. Think of in-memory databases (such as Microsoft
    SQL, SAP Hana, and Oracle). They've been around the block for years.
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
- en: These technologies fundamentally change the way we think about storage. Basically,
    no longer are we discussing storage tiering based on technology (SSD versus SAS
    versus SATA), or outright speed, as the speed is unquestionable. The latest storage
    technologies discuss storage tiering in terms of *latency*. The reason is very
    simple—let's say that you're a storage company and that you build a storage system
    that uses 50 SCM SSDs for capacity. For cache, the only reasonable technology
    would be RAM, hundreds of gigabytes of it. The only way you'd be able to work
    with storage tiering on a device like that is by basically *emulating* it in software,
    by creating additional technologies that will produce tiering-like services based
    on queueing, handling priority in cache (RAM), and similar concepts. Why? Because
    if you're using the same SCM SSDs for capacity, and they offer the same speed
    and I/O, you just don't have a way of tiering based on technology or capability.
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s further describe this by using an available storage system to explain.
    The best device to make our point is Dell/EMC''s PowerMax series of storage devices.
    If you load them with NVMe and SCM SSDs, the biggest model (8000) can scale to
    15 million IOPS(!), 350 GB/s throughput at lower than 100 microseconds latency
    and up to 4 PB capacity. Think about those numbers for a second. Then add another
    number—on the frontend, it can have up to 256 FC/FICON/iSCSI ports. Just recently,
    Dell/EMC released new 32 Gbit/s FC modules for it. The smaller PowerMax model
    (2000) can do 7.5 million IOPS, sub-100 microsecond latency, and scale to 1 PB.
    It can also do all of the *usual EMC stuff*—replication, compression, deduplication,
    snapshots, NAS features, and so on. So, this is not just marketing talk; these
    devices are already out there, being used by enterprise customers:'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.30 – PowerMax 2000 – it seems small, but it packs a lot of punch'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_05_30.jpg)'
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.30 – PowerMax 2000 – it seems small, but it packs a lot of punch
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
- en: These are very important concepts for the future, as more and more manufacturers
    produce similar devices (and they are on the way). We fully expect the KVM-based
    world to embrace these concepts in large-scale environments, especially for infrastructures
    with OpenStack and OpenShift.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-472
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we introduced and configured various Open Source storage concepts
    for libvirt. We also discussed industry-standard approaches, such as iSCSI and
    NFS, as they are often used in infrastructures that are not based on KVM. For
    example, VMware vSphere-based environments can use FC, iSCSI, and NFS, while Microsoft-based
    environments can only use FC and iSCSI, from the list of subjects we covered in
    this chapter.
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter will cover subjects related to virtual display devices and
    protocols. We'll provide an in-depth introduction to VNC and SPICE protocols.
    We will also provide a description of other protocols that are used for virtual
    machine connection. All that will help us to understand the complete stack of
    fundamentals that we need to work with our virtual machines, which we covered
    in the past three chapters.
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  id: totrans-475
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What is a storage pool?
  id: totrans-476
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does NFS storage work with libvirt?
  id: totrans-477
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does iSCSI work with libvirt?
  id: totrans-478
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do we achieve redundancy on storage connections?
  id: totrans-479
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What can we use for virtual machine storage except NFS and iSCSI?
  id: totrans-480
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Which storage backend can we use for object-based storage with libvirt?
  id: totrans-481
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How can we create a virtual disk image to use with a KVM virtual machine?
  id: totrans-482
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does using NVMe SSDs and SCM devices change the way that we create storage
    tiers?
  id: totrans-483
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the fundamental problems of delivering tier-0 storage services for
    virtualization, cloud, and HPC environments?
  id: totrans-484
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  id: totrans-485
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Please refer to the following links for more information regarding what was
    covered in this chapter:'
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
- en: 'What''s new with RHEL8 file systems and storage: [https://www.redhat.com/en/blog/whats-new-rhel-8-file-systems-and-storage](https://www.redhat.com/en/blog/whats-new-rhel-8-file-systems-and-storage)'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'oVirt storage: [https://www.ovirt.org/documentation/administration_guide/#chap-Storage](https://www.ovirt.org/documentation/administration_guide/#chap-Storage)'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'RHEL 7 storage administration guide: [https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/storage_administration_guide/index](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/storage_administration_guide/index)'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'RHEL 8 managing storage devices: [https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/managing_storage_devices/index](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/managing_storage_devices/index)'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'OpenFabrics CCIX, Gen-Z, OpenCAPI (overview and comparison): [https://www.openfabrics.org/images/eventpresos/2017presentations/213_CCIXGen-Z_BBenton.pdf](https://www.openfabrics.org/images/eventpresos/2017presentations/213_CCIXGen-Z_BBenton.pdf)'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
