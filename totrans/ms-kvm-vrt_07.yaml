- en: '*Chapter 5*: Libvirt Storage'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter provides you with an insight into the way that KVM uses storage.
    Specifically, we will cover both storage that's internal to the host where we're
    running virtual machines and *shared storage*. Don't let the terminology confuse
    you here – in virtualization and cloud technologies, the term *shared storage*
    means storage space that multiple hypervisors can have access to. As we will explain
    a bit later, the three most common ways of achieving this are by using block-level,
    share-level, or object-level storage. We will use NFS as an example of share-level
    storage, and **Internet Small Computer System Interface** (**iSCSI**) and **Fiber
    Channel** (**FC**) as examples of block-level storage. In terms of object-based
    storage, we will use Ceph. GlusterFS is also commonly used nowadays, so we'll
    make sure that we cover that, too. To wrap everything up in an easy-to-use and
    easy-to-manage box, we will discuss some open source projects that might help
    you while practicing with and creating testing environments.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to storage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Storage pools
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NFS storage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: iSCSI and SAN storage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Storage redundancy and multipathing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gluster and Ceph as a storage backend for KVM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Virtual disk images and formats and basic KVM storage operations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The latest developments in storage – NVMe and NVMeOF
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to storage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Unlike networking, which is something that most IT people have at least a basic
    understanding of, storage tends to be quite different. In short, yes, it tends
    to be a bit more complex. There are loads of parameters involved, different technologies,
    and…let''s be honest, loads of different types of configuration options and people
    enforcing them. And a *lot* of questions. Here are some of them:'
  prefs: []
  type: TYPE_NORMAL
- en: Should we configure one NFS share per storage device or two?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Should we create one iSCSI target per storage device or two?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Should we create one FC target or two?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How many **Logical Unit Numbers** (**LUNs**) per target?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What kind of cluster size should we use?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How should we carry out multipathing?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Should we use block-level or share-level storage?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Should we use block-level or object-level storage?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Which technology or solution should we choose?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How should we configure caching?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How should we configure zoning or masking?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How many switches should we use?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Should we use some kind of clustering technology on a storage level?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As you can see, the questions just keep piling up, and we've barely touched
    the surface, because there are also questions about which filesystem to use, which
    physical controller we will use to access storage, and what type of cabling—it
    just becomes a big mashup of variables that has many potential answers. What makes
    it worse is the fact that many of those answers can be correct—not just one of
    them.
  prefs: []
  type: TYPE_NORMAL
- en: Let's get the basic-level mathematics out of the way. In an enterprise-level
    environment, shared storage is usually *the most expensive* part of the environment
    and can also have *the most significant negative impact* on virtual machine performance,
    while at the same time being *the most oversubscribed resource* in that environment.
    Let's think about this for a second—every powered-on virtual machine is constantly
    going to hammer our storage device with I/O operations. If we have 500 virtual
    machines running on a single storage device, aren't we asking a bit too much from
    that storage device?
  prefs: []
  type: TYPE_NORMAL
- en: At the same time, some kind of shared storage concept is a key pillar of virtualized
    environments. The basic principle is very simple – there are loads of advanced
    functionalities that will work so much better with shared storage. Also, many
    operations are much faster if shared storage is available. Even more so, there
    are so many simple options for high availability when we don't have our virtual
    machines stored in the same place where they are being executed.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a bonus, we can easily avoid **Single Point Of Failure** (**SPOF**) scenarios
    if we design our shared storage environment correctly. In an enterprise-level
    environment, avoiding SPOF is one of the key design principles. But when we start
    adding switches and adapters and controllers to the *to buy* list, our managers''
    or clients'' heads usually starts to hurt. We talk about performance and risk
    management, while they talk about price. We talk about the fact that their databases
    and applications need to be properly fed in terms of I/O and bandwidth, and they
    feel that you can produce that out of thin air. Just wave your magic wand and
    there we are: unlimited storage performance.'
  prefs: []
  type: TYPE_NORMAL
- en: But the best, and our all-time favorite, apples-to-oranges comparison that your
    clients are surely going to try to enforce on you goes something like this…*"the
    shiny new 1 TB NVMe SSD in my laptop has more than 1,000 times more IOPS and more
    than 5 times more performance than your $50,000 storage device, while costing
    100 times less! You have no idea what you're doing!"*
  prefs: []
  type: TYPE_NORMAL
- en: If you've been there, we feel for you. Rarely will you see so many discussions
    and fights about a piece of hardware in a box. But it's such an essential piece
    of hardware in a box that it's a good fight to have. So, let's explain some key
    concepts that libvirt uses in terms of storage access and how to work with it.
    Then, let's use our knowledge to extract as much performance as possible out of
    our storage system and libvirt using it.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we're basically going to cover almost all of these storage
    types via installation and configuration examples. Each and every one of these
    has its own use case, but generally, it's going to be up to you to choose what
    you're going to use.
  prefs: []
  type: TYPE_NORMAL
- en: So, let's start our journey through these supported protocols and learn how
    to configure them. After we cover storage pools, we are going to discuss NFS,
    a typical share-level protocol for virtual machine storage. Then, we're going
    to move to block-level protocols such as iSCSI and FC. Then, we will move to redundancy
    and multipathing to increase the availability and bandwidth of our storage devices.
    We're also going to cover various use cases for not-so-common filesystems (such
    as Ceph, Gluster, and GFS) for KVM virtualization. We're also going to discuss
    the new developments that are de facto trends right now.
  prefs: []
  type: TYPE_NORMAL
- en: Storage pools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When you first start using storage devices—even if they're cheaper boxes—you're
    faced with some choices. They will ask you to do a bit of configuration—select
    the RAID level, configure hot-spares, SSD caching...it's a process. The same process
    applies to a situation in which you're building a data center from scratch or
    extending an existing one. You have to configure the storage to be able to use
    it.
  prefs: []
  type: TYPE_NORMAL
- en: Hypervisors are a bit *picky* when it comes to storage, as there are storage
    types that they support and storage types that they don't support. For example,
    Microsoft's Hyper-V supports SMB shares for virtual machine storage, but it doesn't
    really support NFS storage for virtual machine storage. VMware's vSphere Hypervisor
    supports NFS, but it doesn't support SMB. The reason is simple—a company developing
    a hypervisor chooses and qualifies technologies that its hypervisor is going to
    support. Then, it's up to various HBA/controller vendors (Intel, Mellanox, QLogic,
    and so on) to develop drivers for that hypervisor, and it's up to storage vendor
    to decide which types of storage protocols they're going to support on their storage
    device.
  prefs: []
  type: TYPE_NORMAL
- en: 'From a CentOS perspective, there are many different storage pool types that
    are supported. Here are some of them:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Logical Volume Manager** (**LVM**)-based storage pools'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Directory-based storage pools
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Partition-based storage pools
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GlusterFS-based storage pools
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: iSCSI-based storage pools
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Disk-based storage pools
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: HBA-based storage pools, which use SCSI devices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: From the perspective of libvirt, a storage pool can be a directory, a storage
    device, or a file that libvirt manages. That leads us to 10+ different storage
    pool types, as you're going to see in the next section. From a virtual machine
    perspective, libvirt manages virtual machine storage, which virtual machines use
    so that they have the capacity to store data.
  prefs: []
  type: TYPE_NORMAL
- en: oVirt, on the other hand, sees things a bit differently, as it has its own service
    that works with libvirt to provide centralized storage management from a data
    center perspective. *Data center perspective* might seem like a term that's a
    bit odd. But think about it—a datacenter is some kind of *higher-level* object
    in which you can see all of your resources. A data center uses *storage* and *hypervisors*
    to provide us with all of the services that we need in virtualization—virtual
    machines, virtual networks, storage domains, and so on. Basically, from a data
    center perspective, you can see what's happening on all of your hosts that are
    members of that datacenter. However, from a host level, you can't see what's happening
    on another host. It's a hierarchy that's completely logical from both a management
    and a security perspective.
  prefs: []
  type: TYPE_NORMAL
- en: 'oVirt can centrally manage these different types of storage pools (and the
    list can get bigger or smaller as the years go by):'
  prefs: []
  type: TYPE_NORMAL
- en: '**Network File System** (**NFS**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Parallel NFS** (**pNFS**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: iSCSI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: FC
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Local storage (attached directly to KVM hosts)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GlusterFS exports
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: POSIX-compliant file systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s take care of some terminology first:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Brtfs** is a type of filesystem that supports snapshots, RAID and LVM-like
    functionality, compression, defragmentation, online resizing, and many other advanced
    features. It was deprecated after it was discovered that its RAID5/6 can easily
    lead to a loss of data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ZFS** is a type of filesystem that supports everything that Brtfs does, plus
    read and write caching.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CentOS has a new way of dealing with storage pools. Although still in technology
    preview state, it's worth going through the complete configuration via this new
    tool, called **Stratis**. Basically, a couple of years ago, Red Hat finally deprecated
    the idea of pushing Brtfs for future releases and started working on Stratis.
    If you've ever used ZFS, that's where this is probably going—an easy-to-manage,
    ZFS-like, volume-managing set of utilities that Red Hat can stand behind in their
    future releases. Also, just like ZFS, a Stratis-based pool can use cache; so,
    if you have an SSD that you'd like to dedicate to pool cache, you can actually
    do that, as well. If you have been expecting Red Hat to support ZFS, there's a
    fundamental Red Hat policy that stands in the way. Specifically, ZFS is not a
    part of the Linux kernel, mostly because of licensing reasons. Red Hat has a policy
    for these situations—if it's not a part of the kernel (upstream), then they don't
    provide nor support it. As it stands, that's not going to happen anytime soon.
    These policies are also reflected in CentOS.
  prefs: []
  type: TYPE_NORMAL
- en: Local storage pools
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: On the other hand, Stratis is available right now. We're going to use it to
    manage our local storage by creating storage pools. Creating a pool requires us
    to set up partitions or disks beforehand. After we create a pool, we can create
    a volume on top of it. We only have to be very careful about one thing—although
    Stratis can manage XFS filesystems, we shouldn't make changes to Stratis-managed
    XFS filesystems directly from the filesystem level. For example, do not reconfigure
    or reformat a Stratis-based XFS filesystem directly from XFS-based commands because
    you'll create havoc on your system.
  prefs: []
  type: TYPE_NORMAL
- en: 'Stratis supports various different types of block storage devices:'
  prefs: []
  type: TYPE_NORMAL
- en: Hard disks and SSDs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: iSCSI LUNs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LVM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LUKS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MD RAID
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A device mapper multipath
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NVMe devices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s start from scratch and install Stratis so that we can use it. Let''s
    use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The first command installs the Stratis service and the corresponding command-line
    utilities. The second one will start and enable the Stratis service.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we are going to go through a complete example of how to use Stratis to
    configure your storage devices. We''re going to cover an example of this layered
    approach. So, what we are going to do is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a software RAID10 + spare by using MD RAID.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a Stratis pool out of that MD RAID device.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add a cache device to the pool to use Stratis' cache capability.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a Stratis filesystem and mount it on our local server.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The premise here is simple—the software RAID10+ spare via MD RAID is going
    to approximate the regular production approach, in which you''d have some kind
    of a hardware RAID controller presenting a single block device to the system.
    We''re going to add a cache device to the pool to verify the caching functionality,
    as this is something that we would most probably do if we were using ZFS, as well.
    Then, we are going to create a filesystem on top of that pool and mount it to
    a local directory with the help of the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This mounted filesystem is XFS-formatted. We could then easily use this filesystem
    via NFS export, which is exactly what we're going to do in the NFS storage lesson.
    But for now, this was just an example of how to create a pool by using Stratis.
  prefs: []
  type: TYPE_NORMAL
- en: We've covered some basics of local storage pools, which brings us closer to
    our next subject, which is how to use pools from a libvirt perspective. So, that
    will be our next topic.
  prefs: []
  type: TYPE_NORMAL
- en: Libvirt storage pools
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Libvirt manages its own storage pools, which is done with one thing in mind—to
    provide different pools for virtual machine disks and related data. Keeping in
    mind that libvirt uses what the underlying operating system supports, it''s no
    wonder that it supports loads of different storage pool types. A picture is worth
    a thousand words, so here''s a screenshot of creating a libvirt storage pool from
    virt-manager:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.1 – Different storage pool types supported by libvirt'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_05_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.1 – Different storage pool types supported by libvirt
  prefs: []
  type: TYPE_NORMAL
- en: Out of the box, libvirt already has a predefined default storage pool, which
    is a directory storage pool on the local server. This default pool is located
    in the `/var/lib/libvirt/images` directory. This represents our default location
    where we'll save all the data from locally installed virtual machines.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''re going to create various different types of storage pools in the following
    sections—an NFS-based pool, an iSCSI and FC pool, and Gluster and Ceph pools:
    the whole nine yards. We''re also going to explain when to use each and every
    one of them as there will be different usage models involved.'
  prefs: []
  type: TYPE_NORMAL
- en: NFS storage pool
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As a protocol, NFS has been around since the mid-80s. It was originally developed
    by Sun Microsystems as a protocol for sharing files, which is what it''s been
    used for up to this day. Actually, it''s still being developed, which is quite
    surprising for a technology that''s so *old*. For example, NFS version 4.2 came
    out in 2016\. In this version, NFS received a very big update, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Server-side copy**: A feature that significantly enhances the speed of cloning
    operations between NFS servers by carrying out cloning directly between NFS servers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sparse files** and **space reservation**: Features that enhance the way NFS
    works with files that have unallocated blocks, while keeping an eye on capacity
    so that we can guarantee space availability when we need to write data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Application data block support**: A feature that helps applications that
    work with files as block devices (disks)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Better pNFS implementation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are other bits and pieces that were enhanced in v4.2, but for now, this
    is more than enough. You can find even more information about this in IETF's RFC
    7862 document ([https://tools.ietf.org/html/rfc7862](https://tools.ietf.org/html/rfc7862)).
    We're going to focus our attention on the implementation of NFS v4.2 specifically,
    as it's the best that NFS currently has to offer. It also happens to be the default
    NFS version that CentOS 8 supports.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing that we have to do is install the necessary packages. We''re
    going to achieve that by using the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The first command installs the necessary utilities to run the NFS server. The
    second one is going to start it and permanently enable it so that the NFS service
    is available after reboot.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our next task is to configure what we''re going to share via the NFS server.
    For that, we need to *export* a directory and make it available to our clients
    over the network. NFS uses a configuration file, `/etc/exports`, for that purpose.
    Let''s say that we want to create a directory called `/exports`, and then share
    it to our clients in the `192.168.159.0/255.255.255.0` network, and we want to
    allow them to write data on that share. Our `/etc/exports` file should look like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: These configuration options tell our NFS server which directory to export (`/exports`),
    to which clients (`192.168.159.0/24`), and what options to use (`rw` means read-write).
  prefs: []
  type: TYPE_NORMAL
- en: 'Some other available options include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`ro`: Read-only mode.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sync`: Synchronous I/O operations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`root_squash`: All I/O operations from `UID 0` and `GID 0` are mapped to configurable
    anonymous UIDs and GIDs (the `anonuid` and `anongid` options).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`all_squash`: All I/O operations from any UIDs and GIDs are mapped to anonymous
    UIDs and GIDs (`anonuid` and `anongid` options).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`no_root_squash`: All I/O operations from `UID 0` and `GID 0` are mapped to
    `UID 0` and `GID 0`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If you need to apply multiple options to the exported directory, you add them
    with a comma between them, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'You can use fully qualified domain names or short hostnames (if they''re resolvable
    by DNS or any other mechanism). Also, if you don''t like using prefixes (`24`),
    you can use regular netmasks, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have configured the NFS server, let''s see how we''re going to
    configure libvirt to use that server as a storage pool. As always, there are a
    couple of ways to do this. We could just create an XML file with the pool definition
    and import it to our KVM host by using the `virsh pool-define --file` command.
    Here''s an example of that configuration file:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.2 – Example XML configuration file for NFS pool'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_05_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.2 – Example XML configuration file for NFS pool
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s explain these configuration options:'
  prefs: []
  type: TYPE_NORMAL
- en: '`pool type`: `netfs` means that we are going to use an NFS file share.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`name`: The pool name, as libvirt uses pools as named objects, just like virtual
    networks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`host` : The address of the NFS server that we are connecting to.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dir path`: The NFS export path that we configured on the NFS server via `/etc/exports`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`path`: The local directory on our KVM host where that NFS share is going to
    be mounted to.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`permissions`: The permissions used for mounting this filesystem.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`owner` and `group`: The UID and GID used for mounting purposes (that''s why
    we exported the folder earlier with the `no_root_squash` option).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`label`: The SELinux label for this folder—we''re going to discuss this in
    [*Chapter 16*](B14834_16_Final_ASB_ePub.xhtml#_idTextAnchor302), *Troubleshooting
    Guideline for the KVM Platform*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If we wanted, we could''ve easily done the same thing via the Virtual Machine
    Manager GUI. First, we would have to select the correct type (the NFS pool) and
    give it a name:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.3 – Selecting the NFS pool type and giving it a name'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_05_03.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.3 – Selecting the NFS pool type and giving it a name
  prefs: []
  type: TYPE_NORMAL
- en: 'After we click **Forward**, we can move to the final configuration step, where
    we need to tell the wizard which server we''re mounting our NFS share from:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.4 – Configuring NFS server options'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_05_04.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.4 – Configuring NFS server options
  prefs: []
  type: TYPE_NORMAL
- en: 'When we finish typing in these configuration options (**Host Name** and **Source
    Path**), we can press **Finish**, which will mean exiting the wizard. Also, our
    previous configuration screen, which only contained the **default** storage pool,
    now has our newly configured pool listed as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.5 – Newly configured NFS pool visible on the list'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_05_05.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.5 – Newly configured NFS pool visible on the list
  prefs: []
  type: TYPE_NORMAL
- en: When would we use NFS-based storage pools in libvirt, and for what? Basically,
    we can use them nicely for anything related to the storage of installation images—ISO
    files, virtual floppy disk files, virtual machine files, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Please remember that even though it seemed that NFS is almost gone from enterprise
    environments just a while ago, NFS is still around. Actually, with the introduction
    of NFS 4.1, 4.2, and pNFS, its future on the market actually looks even better
    than a couple of years ago. It's such a familiar protocol with a very long history,
    and it's still quite competitive in many scenarios. If you're familiar with VMware
    virtualization technology, VMware introduced a technology called Virtual Volumes
    in ESXi 6.0\. This is an object-based storage technology that can use both block-
    and NFS-based protocols for its basis, which is a really compelling use case for
    some scenarios. But for now, let's move on to block-level technologies, such as
    iSCSI and FC.
  prefs: []
  type: TYPE_NORMAL
- en: iSCSI and SAN storage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Using iSCSI for virtual machine storage has long been the regular thing to
    do. Even if you take into account the fact that iSCSI isn''t the most efficient
    way to approach storage, it''s still so widely accepted that you''ll find it everywhere.
    Efficiency is compromised for two reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: iSCSI encapsulates SCSI commands into regular IP packages, which means segmentation
    and overhead as IP packages have a pretty large header, which means less efficiency.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Even worse, it's TCP-based, which means that there are sequence numbers and
    retransmissions, which can lead to queueing and latency, and the bigger the environment
    is, the more you usually feel these effects affect your virtual machine performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: That being said, the fact that it's based on an Ethernet stack makes it easier
    to deploy iSCSI-based solutions, while at the same time offering some unique challenges.
    For example, sometimes it's difficult to explain to a customer that using the
    same network switch(es) for virtual machine traffic and iSCSI traffic is not the
    best idea. What makes it even worse is the fact that clients are sometimes so
    blinded by their desire to save money that they don't understand that they're
    working against their own best interest. Especially when it comes to network bandwidth.
    Most of us have been there, trying to work with clients' questions such as *"but
    we already have a Gigabit Ethernet switch, why would you need anything faster
    than that?"*
  prefs: []
  type: TYPE_NORMAL
- en: The fact of the matter is, with iSCSI's intricacies, more is just – more. The
    more speed you have on the disk/cache/controller side and the more bandwidth you
    have on the networking side, the more chance you have of creating a storage system
    that's faster. All of that can have a big impact on our virtual machine performance.
    As you'll see in the *Storage redundancy and multipathing* section, you can actually
    build a very good storage system yourself—both for iSCSI and FC. This might come
    in real handy when you try to create some kind of a testing lab/environment to
    play with as you develop your KVM virtualization skills. You can apply that knowledge
    to other virtualized environments, as well.
  prefs: []
  type: TYPE_NORMAL
- en: The iSCSI and FC architectures are very similar—they both need a target (an
    iSCSI target and an FC target) and an initiator (an iSCS initiator and an FC initiator).
    In this terminology, the target is a *server* component, and the initiator is
    a *client* component. To put it simply, the initiator connects to a target to
    get access to block storage that's presented via that target. Then, we can use
    the initiator's identity to *limit* what the initiator is able to see on the target.
    This is where the terminology starts to get a bit different when comparing iSCSI
    and FC.
  prefs: []
  type: TYPE_NORMAL
- en: 'In iSCSI, the initiator''s identity can be defined by four different properties.
    They are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**iSCSI Qualified Name** (**IQN**): This is a unique name that all initiators
    and targets have in iSCSI communication. We can compare this to a MAC or IP address
    in regular Ethernet-based networks. You can think of it this way—an IQN is for
    iSCSI what a MAC or IP address is for Ethernet-based networks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**IP address**: Every initiator will have a different IP address that it uses
    to connect to the target.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MAC address**: Every initiator has a different MAC address on Layer 2.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fully Qualified Domain Name** (**FQDN**): This represents the name of the
    server as it''s resolved by a DNS service.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: From the iSCSI target perspective—depending on its implementation—you can use
    any one of these properties to create a configuration that's going to tell the
    iSCSI target which IQNs, IP addresses, MAC addresses, or FQDNs can be used to
    connect to it. This is what's called *masking*, as we can *mask* what an initiator
    can *see* on the iSCSI target by using these identities and pairing them with
    LUNs. LUNs are just raw, block capacities that we export via an iSCSI target toward
    initiators. LUNs are *indexed*, or *numbered*, usually from 0 onward. Every LUN
    number represents a different storage capacity that an initiator can connect to.
  prefs: []
  type: TYPE_NORMAL
- en: For example, we can have an iSCSI target with three different LUNs—`LUN0` with
    20 GB, `LUN1` with 40 GB, and `LUN2` with 60 GB. These will all be hosted on the
    same storage system's iSCSI target. We can then configure the iSCSI target to
    accept an IQN to see all the LUNs, another IQN to only see `LUN1`, and another
    IQN to only see `LUN1` and `LUN2`. This is actually what we are going to configure
    right now.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start by configuring the iSCSI target service. For that, we need to
    install the `targetcli` package, and configure the service (called `target`) to
    run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Be careful about the firewall configuration; you might need to configure it
    to allow connectivity on port `3260/tcp`, which is the port that the iSCSI target
    portal uses. So, if your firewall has started, type in the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'There are three possibilities for iSCSI on Linux in terms of what storage backend
    to use. We could use a regular filesystem (such as XFS), a block device (a hard
    drive), or LVM. So, that''s exactly what we''re going to do. Our scenario is going
    to be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`LUN0` (20 GB): XFS-based filesystem, on the `/dev/sdb` device'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`LUN1` (40 GB): Hard drive, on the `/dev/sdc` device'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`LUN2` (60 GB): LVM, on the `/dev/sdd` device'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So, after we install the necessary packages and configure the target service
    and firewall, we should start with configuring our iSCSI target. We''ll just start
    the `targetcli` command and check the state, which should be a blank slate as
    we''re just beginning the process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.6 – The targetcli starting point – empty configuration'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_05_06.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.6 – The targetcli starting point – empty configuration
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start with the step-by-step procedure:'
  prefs: []
  type: TYPE_NORMAL
- en: So, let's configure the XFS-based filesystem and configure the `LUN0` file image
    to be saved there. First, we need to partition the disk (in our case, `/dev/sdb`):![Figure
    5.7 – Partitioning /dev/sdb for the XFS filesystem
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B14834_05_07.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.7 – Partitioning /dev/sdb for the XFS filesystem
  prefs: []
  type: TYPE_NORMAL
- en: The next step is to format this partition, create and use a directory called
    `/LUN0` to mount this filesystem, and serve our `LUN0` image, which we're going
    to configure in the next steps:![Figure 5.8 – Formatting the XFS filesystem, creating
    a directory, and mounting it to that directory
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B14834_05_08.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.8 – Formatting the XFS filesystem, creating a directory, and mounting
    it to that directory
  prefs: []
  type: TYPE_NORMAL
- en: The next step is configuring `targetcli` so that it creates `LUN0` and assigns
    an image file for `LUN0`, which will be saved in the `/LUN0` directory. First,
    we need to start the `targetcli` command:![Figure 5.9 – Creating an iSCSI target,
    LUN0, and hosting it as a file
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B14834_05_09.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.9 – Creating an iSCSI target, LUN0, and hosting it as a file
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let''s configure a block device-based LUN backend— `LUN2`—which is going
    to use `/dev/sdc1` (create the partition using the previous example) and check
    the current state:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.10 – Creating LUN1, hosting it directly from a block device'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_05_10.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.10 – Creating LUN1, hosting it directly from a block device
  prefs: []
  type: TYPE_NORMAL
- en: 'So, `LUN0` and `LUN1` and their respective backends are now configured. Let''s
    finish things off by configuring LVM:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we are going to prepare the physical volume for LVM, create a volume
    group out of that volume, and display all the information about that volume group
    so that we can see how much space we have for `LUN2`:![Figure 5.11 – Configuring
    the physical volume for LVM, building a volume group,
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: and displaying information about that volume group
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_05_11.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.11 – Configuring the physical volume for LVM, building a volume group,
    and displaying information about that volume group
  prefs: []
  type: TYPE_NORMAL
- en: The next step is to actually create the logical volume, which is going to be
    our block storage device backend for `LUN2` in the iSCSI target. We can see from
    the `vgdisplay` output that we have 15,359 4 MB blocks available, so let's use
    that to create our logical volume, called `LUN2`. Go to `targetcli` and configure
    the necessary settings for `LUN2`:![Figure 5.12 – Configuring LUN2 with the LVM
    backend
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B14834_05_12.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.12 – Configuring LUN2 with the LVM backend
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s stop here for a second and switch to the KVM host (the iSCSI initiator)
    configuration. First, we need to install the iSCSI initiator, which is part of
    a package called `iscsi-initiator-utils`. So, let''s use the `yum` command to
    install that:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we need to configure the IQN of our initiator. We usually want this name
    to be reminiscent of the hostname, so, seeing that our host''s FQDN is `PacktStratis01`,
    we''ll use that to configure the IQN. To do that, we need to edit the `/etc/iscsi/initiatorname.iscsi`
    file and configure the `InitiatorName` option. For example, let''s set it to `iqn.2019-12.com.packt:PacktStratis01`.
    The content of the `/etc/iscsi/initiatorname.iscsi` file should be as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Now that this is configured, let's go back to the iSCSI target and create an
    **Access Control List** (**ACL**). The ACL is going to allow our KVM host's initiator
    to connect to the iSCSI target portal:![Figure 5.13 – Creating an ACL so that
    the KVM host's initiator can connect to the iSCSI target
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B14834_05_13.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.13 – Creating an ACL so that the KVM host's initiator can connect to
    the iSCSI target
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we need to publish our pre-created file-based and block-based devices
    to the iSCSI target LUNs. So, we need to do this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.14 – Adding our file-based and block-based devices to the iSCSI
    target LUNs 0, 1, and 2'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_05_14.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.14 – Adding our file-based and block-based devices to the iSCSI target
    LUNs 0, 1, and 2
  prefs: []
  type: TYPE_NORMAL
- en: 'The end result should look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.15 – The end result'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_05_15.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.15 – The end result
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, everything is configured. We need to go back to our KVM host
    and define a storage pool that will use these LUNs. The easiest way to do that
    would be to use an XML configuration file for the pool. So, here''s our sample
    configuration XML file; we''ll call it `iSCSIPool.xml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s explain the file step by step:'
  prefs: []
  type: TYPE_NORMAL
- en: '`pool type= ''iscsi''`: We''re telling libvirt that this is an iSCSI pool.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`name` : The pool name.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`host name`: The IP address of the iSCSI target.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`device path`: The IQN of the iSCSI target.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The IQN name in the initiator section: The IQN of the initiator.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`target path`: The location where iSCSI target''s LUNs will be mounted.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, all that''s left for us to do is to define, start, and autostart our new
    iSCSI-backed KVM storage pool:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The target path part of the configuration can be easily checked via `virsh`.
    If we type the following command into the KVM host, we will get the list of available
    LUNs from the `MyiSCSIPool` pool that we just configured:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following result for this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.16 – Runtime names for our iSCSI pool LUNs'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_05_16.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.16 – Runtime names for our iSCSI pool LUNs
  prefs: []
  type: TYPE_NORMAL
- en: If this output reminds you a bit of the VMware vSphere Hypervisor storage runtime
    names, you are definitely on the right track. We will be able to use these storage
    pools in [*Chapter 7*](B14834_07_Final_ASB_ePub.xhtml#_idTextAnchor125), *Virtual
    Machine – Installation, Configuration, and Life-Cycle Management*, when we start
    deploying our virtual machines.
  prefs: []
  type: TYPE_NORMAL
- en: Storage redundancy and multipathing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Redundancy is one of the keywords of IT, where any single component failure
    could mean big problems for a company or its customers. The general design principle
    of avoiding SPOF is something that we should always stick to. At the end of the
    day, no network adapter, cable, switch, router, or storage controller is going
    to work forever. So, calculating redundancy into our designs helps our IT environment
    during its normal life cycle.
  prefs: []
  type: TYPE_NORMAL
- en: At the same time, redundancy can be combined with multipathing to also ensure
    higher throughput. For example, when we connect our physical host to FC storage
    with two controllers with four FC ports each, we can use four paths (if the storage
    is active-passive) or eight paths (if it's active-active) to the same LUN(s) exported
    from this storage device to a host. This gives us multiple additional options
    for LUN access, on top of the fact that it gives us more availability, even in
    the case of failure.
  prefs: []
  type: TYPE_NORMAL
- en: Getting a regular KVM host to do, for example, iSCSI multipathing is quite a
    bit complex. There are multiple configuration issues and blank spots in terms
    of documentation, and supportability of such a configuration is questionable.
    However, there are products that use KVM that support it out of the box, such
    as oVirt (which we covered before) and **Red Hat Enterprise Virtualization Hypervisor**
    (**RHEV-H**). So, let's use oVirt for this example on iSCSI.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before you do this, make sure that you have done the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Your Hypervisor host is added to the oVirt inventory.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Your Hypervisor host has two additional network cards, independent of the management
    network.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The iSCSI storage has two additional network cards in the same L2 networks as
    the two additional hypervisor network cards.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The iSCSI storage is configured so that it has at least a target and a LUN already
    configured in a way that will enable the hypervisor host to connect to it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So, as we''re doing this in oVirt, there are a couple of things that we need
    to do. First, from a networking perspective, it would be a good idea to create
    some storage networks. In our case, we''re going to assign two networks for iSCSI,
    and we will call them `iSCSI01` and `iSCSI02`. We need to open the oVirt administration
    panel, hover over `iSCSI01` (for the first one), uncheck the `iSCSI02` network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.17 – Configuring networks for iSCSI bond'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_05_17.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.17 – Configuring networks for iSCSI bond
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is assigning these networks to host network adapters. Go to `compute/hosts`,
    double-click on the host that you added to oVirt''s inventory, select the `iSCSI01`
    on the second network interface and `iSCSI02` on the third network interface.
    The first network interface is already taken by the oVirt management network.
    It should look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.18 – Assigning virtual networks to the hypervisor''s physical adapters'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_05_18.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.18 – Assigning virtual networks to the hypervisor's physical adapters
  prefs: []
  type: TYPE_NORMAL
- en: 'Before you close the window down, make sure that you click on the *pencil*
    sign on both `iSCSI01` and `iSCSI02` to set up IP addresses for these two virtual
    networks. Assign network configuration that can connect you to your iSCSI storage
    on the same or different subnets:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.19 – Creating an iSCSI bond on the data center level'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_05_19.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.19 – Creating an iSCSI bond on the data center level
  prefs: []
  type: TYPE_NORMAL
- en: 'There you go, you have just configured an iSCSI bond. The last part of our
    configuration is enabling it. Again, in the oVirt GUI, go to **Compute** | **Data
    Centers**, select your datacenter with a double-click, and go to the **iSCSI Multipathing**
    tab:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.20 – Configuring iSCSI multipathing on the data center level'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_05_20.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.20 – Configuring iSCSI multipathing on the data center level
  prefs: []
  type: TYPE_NORMAL
- en: Click on the `iSCSI01` and `iSCSI02` networks in the top part of the pop-up
    window, and the iSCSI target on the lower side.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have covered the basics of storage pools, NFS, and iSCSI, we can
    move on to a standard open source way of deploying storage infrastructure, which
    would be to use Gluster and/or Ceph.
  prefs: []
  type: TYPE_NORMAL
- en: Gluster and Ceph as a storage backend for KVM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are other advanced types of filesystems that can be used as the libvirt
    storage backend. So, let's now discuss two of them—Gluster and Ceph. Later, we'll
    also check how libvirt works with GFS2.
  prefs: []
  type: TYPE_NORMAL
- en: Gluster
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Gluster is a distributed filesystem that's often used for high-availability
    scenarios. Its main advantages over other filesystems are the fact that it's scalable,
    it can use replication and snapshots, it can work on any server, and it's usable
    as a basis for shared storage—for example, via NFS and SMB. It was developed by
    a company called Gluster Inc., which was acquired by RedHat in 2011\. However,
    unlike Ceph, it's a *file* storage service, while Ceph offers *block* and *object*-based
    storage. Object-based storage for block-based devices means direct, binary storage,
    directly to a LUN. There are no filesystems involved, which theoretically means
    less overhead as there's no filesystem, filesystem tables, and other constructs
    that might slow the I/O process down.
  prefs: []
  type: TYPE_NORMAL
- en: Let's first configure Gluster to show its use case with libvirt. In production,
    that means installing at least three Gluster servers so that we can make high
    availability possible. Gluster configuration is really straightforward, and in
    our example, we are going to create three CentOS 7 machines that we will use to
    host the Gluster filesystem. Then, we will mount that filesystem on our hypervisor
    host and use it as a local directory. We can use GlusterFS directly from libvirt,
    but the implementation is just not as refined as using it via the gluster client
    service, mounting it as a local directory, and using it directly as a directory
    pool in libvirt.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our configuration will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.21 – Basic settings for our Gluster cluster'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_05_21.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.21 – Basic settings for our Gluster cluster
  prefs: []
  type: TYPE_NORMAL
- en: 'So, let''s put that into production. We have to issue a large sequence of commands
    on all of the servers before we configure Gluster and expose it to our KVM host.
    Let''s start with `gluster1`. First, we are going to do a system-wide update and
    reboot to prepare the core operating system for Gluster installation. Type the
    following commands into all three CentOS 7 servers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we can start deploying the necessary repositories and packages, format
    disks, configure the firewall, and so on. Type the following commands into all
    the servers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to do a bit of networking configuration as well. It would be good if
    these three servers can *resolve* each other, which means either configuring a
    DNS server or adding a couple of lines to our `/etc/hosts` file. Let''s do the
    latter. Add the following lines to your `/etc/hosts` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'For the next part of the configuration, we can just log in to the first server
    and use it as the de facto management server for our Gluster infrastructure. Type
    in the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The first three commands should get you the `peer probe: success` status. The
    third one should return an output similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.22 – Confirmation that the Gluster servers peered successfully'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_05_22.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.22 – Confirmation that the Gluster servers peered successfully
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that this part of the configuration is done, we can create a Gluster-distributed
    filesystem. We can do this by typing the following sequence of commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we could mount Gluster as an NFS directory for testing purposes. For
    example, we can create a distributed namespace called `kvmgluster` to all of the
    member hosts (`gluster1`, `gluster2`, and `gluster3`). We can do this by using
    the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The Gluster part is now ready, so we need to go back to our KVM host and mount
    the Gluster filesystem to it by typing in the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: We have to pay close attention to Gluster releases on the server and client,
    which is why we downloaded the Gluster repository information for CentOS 8 (we're
    using it on the KVM server) and installed the necessary Gluster client packages.
    That enabled us to mount the filesystem with the last command.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we''ve finished our configuration, we just need to add this directory
    as a libvirt storage pool. Let''s do that by using an XML file with the storage
    pool definition, which contains the following entries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s say that we saved this file in the current directory, and that the file
    is called `gluster.xml`. We can import and start it in libvirt by using the following
    `virsh` commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'We should mount this pool automatically on boot so that libvirt can use it.
    Therefore, we need to add the following line to `/etc/fstab`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Using a directory-based approach enables us to avoid two problems that libvirt
    (and its GUI interface, `virt-manager`) has with Gluster storage pools:'
  prefs: []
  type: TYPE_NORMAL
- en: We can use Gluster's failover capability, which will be managed automatically
    by the Gluster utilities that we installed directly, as libvirt doesn't support
    them yet.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will avoid creating virtual machine disks *manually*, which is another limitation
    of libvirt's implementation of Gluster support, while directory-based storage
    pools support it without any issues.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It seems weird that we''re mentioning *failover*, as it seems as though we
    didn''t configure it as a part of any of the previous steps. Actually, we have.
    When we issued the last mount command, we used Gluster''s built-in modules to
    establish connectivity to the *first* Gluster server. That, in turn, means that
    after this connection, we got all of the details about the whole Gluster pool,
    which we configured so that it''s hosted on three servers. If any kind of failure
    happens—which we can easily simulate—this connection will continue working. We
    can simulate this scenario by turning off any of the Gluster servers, for example—`gluster1`.
    You''ll see that the local directory where we mounted Gluster directory still
    works, even though `gluster1` is down. Let''s see that in action (the default
    timeout period is 42 seconds):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.23 – Gluster failover working; the first node is down, but we''re
    still able to get our files'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_05_23.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.23 – Gluster failover working; the first node is down, but we're still
    able to get our files
  prefs: []
  type: TYPE_NORMAL
- en: 'If we want to be more aggressive, we can shorten this timeout period to—for
    example—2 seconds by issuing the following command on any of our Gluster servers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The `number` part is in seconds, and by assigning it a lower number, we can
    directly influence how aggressive the failover process is.
  prefs: []
  type: TYPE_NORMAL
- en: So, now that everything is configured, we can start using the Gluster pool to
    deploy virtual machines, which we will discuss further in [*Chapter 7*](B14834_07_Final_ASB_ePub.xhtml#_idTextAnchor125),
    *Virtual Machine – Installation, Configuration, and Life-Cycle Management*.
  prefs: []
  type: TYPE_NORMAL
- en: Seeing as Gluster is a file-based backend that can be used for libvirt, it's
    only natural to describe how to use an advanced block-level and object-level storage
    backend. That's where Ceph comes in, so let's work on that now.
  prefs: []
  type: TYPE_NORMAL
- en: Ceph
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Ceph can act as file-, block-, and object-based storage. But for the most part,
    we're usually using it as either block- or object-based storage. Again, this is
    a piece of open source software that's designed to work on any server (or a virtual
    machine). In its core, Ceph runs an algorithm called **Controlled Replication
    Under Scalable Hashing** (**CRUSH**). This algorithm tries to distribute data
    across object devices in a pseudo-random manner, and in Ceph, it's managed by
    a cluster map (a CRUSH map). We can easily scale Ceph out by adding more nodes,
    which will redistribute data in a minimum fashion to ensure as small amount of
    replication as possible.
  prefs: []
  type: TYPE_NORMAL
- en: An internal Ceph component called **Reliable Autonomic Distributed Object Store**
    (**RADOS**) is used for snapshots, replication, and thin provisioning. It's an
    open source project that was developed by the University of California.
  prefs: []
  type: TYPE_NORMAL
- en: 'Architecture-wise, Ceph has three main services:'
  prefs: []
  type: TYPE_NORMAL
- en: '**ceph-mon** : Used for cluster monitoring, CRUSH maps, and **Object Storage
    Daemon** (**OSD**) maps.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ceph-osd**: This handles actual data storage, replication, and recovery.
    It requires at least two nodes; we''ll use three for clustering reasons.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ceph-mds**: Metadata server, used when Ceph needs filesystem access.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In accordance with best practices, make sure that you always design your Ceph
    environments with the key principles in mind—all of the data nodes need to have
    the same configuration. That means the same amount of memory, the same storage
    controllers (don't use RAID controllers, just plain HBAs without RAID firmware
    if possible), the same disks, and so on. That's the only way to ensure a constant
    level of Ceph performance in your environments.
  prefs: []
  type: TYPE_NORMAL
- en: 'One very important aspect of Ceph is data placement and how placement groups
    work. Placement groups offer us a chance to split the objects that we create and
    place them in OSDs in an optimal fashion. Translation: the bigger the number of
    placement groups we configure, the better balance we''re going to get.'
  prefs: []
  type: TYPE_NORMAL
- en: So, let's configure Ceph from scratch. We're going to follow the best practices
    again and deploy Ceph by using five servers—one for administration, one for monitoring,
    and three OSDs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our configuration will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.24 – Basic Ceph configuration for our infrastructure'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_05_24.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.24 – Basic Ceph configuration for our infrastructure
  prefs: []
  type: TYPE_NORMAL
- en: 'Make sure that these hosts can resolve each other via DNS or `/etc/hosts`,
    and that you configure all of them to use the same NTP source. Make sure that
    you update all of the hosts by using the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Also, make sure that you type the following commands into all of the hosts
    as the *root* user. Let''s start by deploying packages, creating an admin user,
    and giving them rights to `sudo`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Disabling SELinux will make our life easier for this demonstration, as will
    getting rid of the firewall:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s add hostnames to `/etc/hosts` so that administration is easier for us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Change the last `echo` part to suit your environment—hostnames and IP addresses.
    We''re just using this as an example from our environment. The next step is making
    sure that we can use our admin host to connect to all of the hosts. The easiest
    way to do that is by using SSH keys. So, on `ceph-admin`, log in as root and type
    in the `ssh-keygen` command, and then press the *Enter* key all the way through.
    It should look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.25 – Generating an SSH key for root for Ceph setup purposes'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_05_25.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.25 – Generating an SSH key for root for Ceph setup purposes
  prefs: []
  type: TYPE_NORMAL
- en: 'We also need to copy this key to all of the hosts. So, again, on `ceph-admin`,
    use `ssh-copy-id` to copy the keys to all of the hosts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Accept all of the keys when SSH asks you, and use `ceph123` as the password,
    which we selected in one of the earlier steps. After all of this is done, there''s
    one last step that we need to do on `ceph-admin` before we start deploying Ceph—we
    have to configure SSH to use the `cephadmin` user as a default user to log in
    to all of the hosts. We will do this by going to the `.ssh` directory as root
    on `ceph-admin`, and creating a file called `config` with the following content:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'That was a long pre-configuration, wasn''t it? Now it''s time to actually start
    deploying Ceph. The first step is to configure `ceph-monitor`. So, on `ceph-admin`,
    type in the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Because of the fact that we selected a configuration in which we have three
    OSDs, we need to configure Ceph so that it uses these additional two hosts. So,
    in the `cluster` directory, edit the file called `ceph.conf` and add the following
    two lines at the end:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: This will make sure that we can only use our example network (`192.168.159.0/24`)
    for Ceph, and that we have two additional OSDs on top of the original one.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that everything''s ready, we have to issue a sequence of commands to configure
    Ceph. So, again, on `ceph-admin`, type in the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s describe these commands one by one:'
  prefs: []
  type: TYPE_NORMAL
- en: The first command starts the actual deployment process—for the admin, monitor,
    and OSD nodes, with the installation of all the necessary packages.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second and third commands configure the monitor host so that it's ready
    to accept external connections.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The two disk commands are all about disk preparation—Ceph will clear the disks
    that we assigned to it (`/dev/sdb` per OSD host) and create two partitions on
    them, one for Ceph data and one for the Ceph journal.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The last two commands prepare these filesystems for use and activate Ceph. If
    at any time your `ceph-deploy` script stops, check your DNS and `/etc/hosts` and
    `firewalld` configuration, as that's where the problems usually are.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We need to expose Ceph to our KVM host, which means that we have to do a bit
    of extra configuration. We''re going to expose Ceph as an object pool to our KVM
    host, so we need to create a pool. Let''s call it `KVMpool`. Connect to `ceph-admin`,
    and issue the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: This command will create a pool called `KVMpool`, with 128 placement groups.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step involves approaching Ceph from a security perspective. We don''t
    want anyone connecting to this pool, so we''re going to create a key for authentication
    to Ceph, which we''re going to use on the KVM host for authentication purposes.
    We do that by typing the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'It''s going to throw us a status message, something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then switch to the KVM host, where we need to do two things:'
  prefs: []
  type: TYPE_NORMAL
- en: Define a secret—an object that's going to link libvirt to a Ceph user—and by
    doing that, we're going to create a secret object with its **Universally Unique
    Identifier** (**UUID**).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use that secret's UUID to link it to the Ceph key when we define the Ceph storage
    pool.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The easiest way to do these two steps would be by using two XML configuration
    files for libvirt. So, let''s create those two files. Let''s call the first one,
    `secret.xml`, and here are its contents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Make sure that you save and import this XML file by typing in the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'After you press the *Enter* key, this command is going to throw out a UUID.
    Please copy and paste that UUID someplace safe, as we''re going to need it for
    the pool XML file. In our environment, this first `virsh` command threw out the
    following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to assign a value to this secret so that when libvirt tries to use
    this secret, it knows which *password* to use. That''s actually the password that
    we created on the Ceph level, when we used `ceph auth get-create`, which threw
    us the key. So, now that we have both the secret UUID and the Ceph key, we can
    combine them to create a complete authentication object. On the KVM host, we need
    to type in the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can create the Ceph pool file. Let''s call the config file `ceph.xml`,
    and here are its contents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'So, the UUID from the previous step was used in this file to reference which
    secret (identity) is going to be used for Ceph pool access. Now we need to do
    the standard procedure—import the pool, start it, and autostart it—if we want
    to use it permanently (after the KVM host reboot). So, let''s do that with the
    following sequence of commands on the KVM host:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'The last command should produce an output similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.26 – Checking the state of our pools; the Ceph pool is configured
    and ready to be used'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_05_26.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.26 – Checking the state of our pools; the Ceph pool is configured and
    ready to be used
  prefs: []
  type: TYPE_NORMAL
- en: Now that the Ceph object pool is available for our KVM host, we could install
    a virtual machine on it. We're going to work on that – again – in [*Chapter 7*](B14834_07_Final_ASB_ePub.xhtml#_idTextAnchor125),
    *Virtual Machine – Installation, Configuration, and Life-Cycle Management*.
  prefs: []
  type: TYPE_NORMAL
- en: Virtual disk images and formats and basic KVM storage operations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Disk images are standard files stored on the host''s filesystem. They are large
    and act as virtualized hard drives for guests. You can create such files using
    the `dd` command, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the translation of this command for you:'
  prefs: []
  type: TYPE_NORMAL
- en: Duplicate data (`dd`) from the input file (`if`) of `/dev/zero` (virtually limitless
    supply of zeros) into the output file (`of`) of `/vms/dbvm_disk2.img` (disk image)
    using blocks of 1 G size (`bs` = block size) and repeat this (`count`) just once
    (`10`).
  prefs: []
  type: TYPE_NORMAL
- en: 'Important note:'
  prefs: []
  type: TYPE_NORMAL
- en: '`dd` is known to be a resource-hungry command. It may cause I/O problems on
    the host system, so it''s good to first check the available free memory and I/O
    state of the host system, and only then run it. If the system is already loaded,
    lower the block size to MB and increase the count to match the size of the file
    you wanted (use `bs=1M`, `count=10000` instead of `bs=1G`, `count=10`).'
  prefs: []
  type: TYPE_NORMAL
- en: '`/vms/dbvm_disk2.img` is the result of the preceding command. The image now
    has 10 GB preallocated and ready to use with guests either as the boot disk or
    second disk. Similarly, you can also create thin-provisioned disk images. Preallocated
    and thin-provisioned (sparse) are disk allocation methods, or you may also call
    it the format:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Preallocated**: A preallocated virtual disk allocates the space right away
    at the time of creation. This usually means faster write speeds than a thin-provisioned
    virtual disk.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`seek` option with the `dd` command, as shown in the following command:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Each comes with its own advantages and disadvantages. If you are looking for
    I/O performance, go for a preallocated format, but if you have a non-IO-intensive
    load, choose thin-provisioned.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, you might be wondering how you can identify what disk allocation method
    a certain virtual disk uses. There is a good utility for finding this out: `qemu-img`.
    This command allows you to read the metadata of a virtual image. It also supports
    creating a new disk and performing low-level format conversion.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting image information
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `info` parameter of the `qemu-img` command displays information about a
    disk image, including the absolute path of the image, the file format, and the
    virtual and disk size. By looking at the virtual disk size from a QEMU perspective
    and comparing that to the image file size on the disk, you can easily identify
    what disk allocation policy is in use. As an example, let''s look at two of the
    disk images we created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: See the `disk size` line of both the disks. It's showing `10G` for `/vms/dbvm_disk2.img`,
    whereas for `/vms/dbvm_disk2_seek.img`, it's showing `10M` MiB. This difference
    is because the second disk uses a thin-provisioning format. The virtual size is
    what guests see and the disk size is what space the disk reserved on the host.
    If both the sizes are the same, it means the disk is preallocated. A difference
    means that the disk uses the thin-provisioning format. Now, let's attach the disk
    image to a virtual machine; you can attach it using `virt-manager` or the CLI
    alternative, `virsh`.
  prefs: []
  type: TYPE_NORMAL
- en: Attaching a disk using virt-manager
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Start virt-manager from the host system''s graphical desktop environment. It
    can also be started remotely using SSH, as demonstrated in the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'So, let''s use the Virtual Machine Manager to attach the disk to the virtual
    machine:'
  prefs: []
  type: TYPE_NORMAL
- en: In the Virtual Machine Manager main window, select the virtual machine to which
    you want to add the secondary disk.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Go to the virtual hardware details window and click on the **Add Hardware**
    button located at the bottom-left side of the dialog box.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In **Add New Virtual Hardware**, select **Storage** and select the **Create
    a disk image for the virtual machine** button and virtual disk size, as in the
    following screenshot:![Figure 5.27 – Adding a virtual disk in virt-manager
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B14834_05_27.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.27 – Adding a virtual disk in virt-manager
  prefs: []
  type: TYPE_NORMAL
- en: If you want to attach the previously created `dbvm_disk2.img` image, choose
    `dbvm_disk2.img` file from the `/vms` directory or find it in the local storage
    pool, then select it and click `/dev/sdb`) or disk partition (`/dev/sdb1`), or
    LVM logical volume. We could have used any of the previously configured storage
    pools for storing this image either as a file or object or directly to a block
    device.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Clicking on the `virsh` command.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using virt-manager to create a virtual disk was easy enough—just a couple of
    clicks of a mouse and a bit of typing. Now, let's see how we can do that via the
    command line—namely, by using `virsh`.
  prefs: []
  type: TYPE_NORMAL
- en: Attaching a disk using virsh
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`virsh` is a very powerful command-line alternative to virt-manager. You can
    perform an action in a second that would take minutes to perform through a graphical
    interface such as virt-manager. It provides an `attach-disk` option to attach
    a new disk device to a virtual machine. There are lots of switches provided with
    `attach-disk`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'However, in a normal scenario, the following are sufficient to perform hot-add
    disk attachment to a virtual machine:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: Here, `CentOS8` is the virtual machine to which a disk attachment is executed.
    Then, there is the path of the disk image. `vdb` is the target disk name that
    would be visible inside the guest operating system. `--live` means performing
    the action while the virtual machine is running, and `--config` means attaching
    it persistently across reboot. Not adding a `--config` switch will keep the disk
    attached only until reboot.
  prefs: []
  type: TYPE_NORMAL
- en: 'Important note:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Hot plugging support: The `acpiphp` kernel module should be loaded in a Linux
    guest operating system in order to recognize a hot-added disk; `acpiphp` provides
    legacy hot plugging support, whereas `pciehp` provides native hot plugging support
    . `pciehp` is dependent on `acpiphp`. Loading `acpiphp` will automatically load
    `pciehp` as a dependency.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can use the `virsh domblklist <vm_name>` command to quickly identify how
    many vDisks are attached to a virtual machine. Here is an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: This clearly indicates that the two vDisks connected to the virtual machine
    are both file images. They are visible to the guest operating system as `vda`
    and `vdb`, respectively, and in the last column of the disk images path on the
    host system.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we are going to see how to create an ISO library.
  prefs: []
  type: TYPE_NORMAL
- en: Creating an ISO image library
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although a guest operating system on the virtual machine can be installed from
    physical media by carrying out a passthrough the host's CD/DVD drive to the virtual
    machine, it's not the most efficient way. Reading from a DVD drive is slow compared
    to reading ISO from a hard disk, so a better way is to store ISO files (or logical
    CDs) used to install operating systems and applications for the virtual machines
    in a file-based storage pool and create an ISO image library.
  prefs: []
  type: TYPE_NORMAL
- en: 'To create an ISO image library, you can either use virt-manager or a `virsh`
    command. Let''s see how to create an ISO image library using the `virsh` command:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, create a directory on the host system to store the `.iso` images:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Set the correct permissions. It should be owned by a root user with permission
    set to `700`. If SELinux is in enforcing mode, the following context needs to
    be set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the ISO image library using the `virsh` command, as shown in the following
    code block:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Verify that the pool (ISO image library) was created:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Now you can copy or move the `.iso` images to the `/iso_lib` directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Upon copying the `.iso` files into the `/iso_lib` directory, refresh the pool
    and then check its contents:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: This will list all the ISO images stored in the directory, along with their
    path. These ISO images can now be used directly with a virtual machine for guest
    operating system installation, software installation, or upgrades.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Creating an ISO image library is the de facto norm in today's enterprises. It's
    better to have a centralized place where all your ISO images are, and it makes
    it easier to implement some kind of synchronization method (for example, `rsync`)
    if you need to synchronize across different locations.
  prefs: []
  type: TYPE_NORMAL
- en: Deleting a storage pool
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deleting a storage pool is fairly easy. Please note that deleting a storage
    domain will not remove any file/block devices. It just disconnects the storage
    from virt-manager. The file/block device has to be removed manually.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can delete a storage pool via virt-manager or by using the `virsh` command.
    Let''s first check how to do it via virt-manager:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.28 – Deleting a pool'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_05_28.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.28 – Deleting a pool
  prefs: []
  type: TYPE_NORMAL
- en: First, select the red stop button to stop the pool, and then click on the red
    circle with an **X** to delete the pool.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to use `virsh`, it''s even simpler. Let''s say that we want to
    delete the storage pool called `MyNFSpool` in the previous screenshot. Just type
    in the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: The next logical step after creating a storage pool is to create a storage volume.
    From a logical standpoint, the storage volume slices a storage pool into smaller
    parts. Let's learn how to do that now.
  prefs: []
  type: TYPE_NORMAL
- en: Creating storage volumes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Storage volumes are created on top of storage pools and attached as virtual
    disks to virtual machines. In order to create a storage volume, start the Storage
    Management console, navigate to virt-manager, then click **Edit** | **Connection
    Details** | **Storage** and select the storage pool where you want to create a
    new volume. Click on the create new volume button (**+**):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.29 – Creating a storage volume for the virtual machine'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_05_29.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.29 – Creating a storage volume for the virtual machine
  prefs: []
  type: TYPE_NORMAL
- en: Next, provide the name of the new volume, choose the disk allocation format
    for it, and click on the `virsh` command. There are several disk formats that
    are supported by libvirt (`raw`, `cow`, `qcow`, `qcow2`, `qed`, and `vmdk`). Use
    the disk format that suits your environment and set the proper size in the `Max
    Capacity` and `Allocation` fields to decide whether you wish to go with preallocated
    disk allocation or thin-provisioned. If you keep the disk size the same in `qcow2`
    format does not support the thick disk allocation method.
  prefs: []
  type: TYPE_NORMAL
- en: In [*Chapter 8*](B14834_08_Final_ASB_ePub.xhtml#_idTextAnchor143), *Creating
    and Modifying VM Disks, Templates, and Snapshots*, all the disk formats are explained
    in detail. For now, just understand that `qcow2` is a specially designed disk
    format for KVM virtualization. It supports the advanced features needed for creating
    internal snapshots.
  prefs: []
  type: TYPE_NORMAL
- en: Creating volumes using the virsh command
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The syntax to create a volume using the `virsh` command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, `dedicated_storage` is the storage pool, `vm_vol1` is the volume name,
    and 10 GB is the size:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: The `virsh` command and arguments to create a storage volume are almost the
    same regardless of the type of storage pool it is created on. Just enter the appropriate
    input for a `--pool` switch. Now, let's see how to delete a volume using the `virsh`
    command.
  prefs: []
  type: TYPE_NORMAL
- en: Deleting a volume using the virsh command
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The syntax to delete a volume using the `virsh` command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: Executing this command will remove the `vm_vol2` volume from the `dedicated_storage`
    storage pool.
  prefs: []
  type: TYPE_NORMAL
- en: The next step in our storage journey is about looking a bit into the future
    as all of the concepts that we mentioned in this chapter have been well known
    for years, some even for decades. The world of storage is changing and moving
    into new and interesting directions, so let's discuss that for a bit next.
  prefs: []
  type: TYPE_NORMAL
- en: The latest developments in storage – NVMe and NVMeOF
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the past 20 or so years, by far the biggest disruption in the storage world
    in terms of technology has been the introduction of **Solid State Drives** (**SSDs**).
    Now, we know that a lot of people have gotten quite used to having them in their
    computers—laptops, workstations, whichever type of device we use. But again, we're
    discussing storage for virtualization, and enterprise storage concepts overall,
    and that means that our regular SATA SSDs aren't going to make the cut. Although
    a lot of people use them in mid-range storage devices and/or handmade storage
    devices that host ZFS pools (for cache), some of these concepts have a life of
    their own in the latest generations of storage devices. These devices are fundamentally
    changing the way technology is working and redoing parts of modern IT history
    in terms of which protocols are used, how fast they are, how much lower latencies
    they have, and how they approach storage tiering—tiering being a concept that
    differentiates different storage devices or their storage pools based on a capability,
    usually speed.
  prefs: []
  type: TYPE_NORMAL
- en: Let's briefly explain what we're discussing here by using an example of where
    the storage world is heading. Along with that, the storage world is taking the
    virtualization, cloud, and HPC world along for the ride, so these concepts are
    not outlandish. They already exist, in readily available storage devices that
    you can buy today.
  prefs: []
  type: TYPE_NORMAL
- en: The introduction of SSDs brought a significant change in the way we access our
    storage devices. It's all about performance and latency, and older concepts such
    as **Advanced Host Controller Interface** (**AHCI**), which we're still actively
    using with many SSDs on the market today, are just not good enough to handle the
    performance that SSDs have. AHCI is a standard way in which a regular hard disk
    (mechanical disk or regular spindle) talks via software to SATA devices. However,
    the key part of that is *hard disk*, which means cylinders, heads sectors—things
    that SSDs just don't have, as they don't spin around and don't need that kind
    of paradigm. That meant that another standard had to be created so that we can
    use SSDs in a more native fashion. That's what **Non-Volatile Memory Express**
    (**NVMe**) is all about—bridging the gap between what SSDs are capable of doing
    and what they can actually do, without using translations from SATA to AHCI to
    PCI Express (and so on).
  prefs: []
  type: TYPE_NORMAL
- en: The fast development pace of SSDs and the integration of NVMe made huge advancements
    in enterprise storage possible. That means that new controllers, new software,
    and completely new architectures had to be invented to support this paradigm shift.
    As more and more storage devices integrate NVMe for various purposes—primarily
    for caching, then for storage capacity as well—it's becoming clear that there
    are other problems that need to be solved as well. The first of which is the way
    in which we're going to connect storage devices offering such a tremendous amount
    of capability to our virtualized, cloud, or HPC environments.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the past 10 or so years, many people argued that FC is going to disappear
    from the market, and a lot of companies hedged their bets on different standards—iSCSI,
    iSCSI over RDMA, NFS over RDMA, and so on. The reasoning behind that seemed solid
    enough:'
  prefs: []
  type: TYPE_NORMAL
- en: FC is expensive—it requires separate physical switches, separate cabling, and
    separate controllers, all of which cost a lot of money.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There's licensing involved—when you buy, for example, a Brocade switch that
    has 40 FC ports, that doesn't mean that you can use all of them out of the box,
    as there are licenses to get more ports (8-port, 16-port, and so on).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: FC storage devices are expensive and often require more expensive disks (with
    FC connectors).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuring FC requires extensive knowledge and/or training, as you can't simply
    go and configure a stack of FC switches for an enterprise-level company without
    knowing the concepts, and the CLI from the switch vendor, on top of knowing what
    that enterprise's needs are.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ability of FC as a protocol to speed up its development to reach new speeds
    has been really bad. In simple terms, during the time it took FC to go from 8
    Gbit/s to 32 Gbit/s, Ethernet went from 1 Gbit/s to 25, 40, 50, and 100 Gbit/s
    bandwidth. There's already talk about 400 Gbit/s Ethernet, and there are the first
    devices that support that standard as well. That usually makes customers concerned
    as higher numbers mean better throughput, at least in most people's minds.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: But what's happening on the market *now* tells us a completely different story—not
    just that FC is back, but that it's back with a mission. The enterprise storage
    companies have embraced that and started introducing storage devices with *insane*
    levels of performance (with the aid of NVMe SSDs, as a first phase). That performance
    needs to be transferred to our virtualized, cloud, and HPC environments, and that
    requires the best possible protocol, in terms of lowest latency, its design, and
    the quality and reliability, and FC has all of that.
  prefs: []
  type: TYPE_NORMAL
- en: That leads to the second phase, where NVMe SSDs aren't just being used as cache
    devices, but as capacity devices as well.
  prefs: []
  type: TYPE_NORMAL
- en: Take note of the fact that, right now, there's a big fight brewing on the storage
    memory/storage interconnects market. There are multiple different standards trying
    to compete with Intel's **Quick Path Interconnect** (**QPI**), a technology that's
    been used in Intel CPUs for more than a decade. If this is a subject that's interesting
    to you, there is a link at the end of this chapter, in the *Further reading* section,
    where you can find more information. Essentially, QPI is a point-to-point interconnection
    technology with low latency and high bandwidth that's at the core of today's servers.
    Specifically, it handles communication between CPUs, CPUs and memory, CPUs and
    chipsets, and so on. It's a technology that Intel developed after it got rid of
    the **Front Side Bus** (**FSB**) and chipset-integrated memory controllers. FSB
    was a shared bus that was shared between memory and I/O requests. That approach
    had much higher latency, didn't scale well, and had lower bandwidth and problems
    with situations in which there's a large amount of I/O happening on the memory
    and I/O side. After switching to an architecture where the memory controller was
    a part of the CPU (therefore, memory directly connects to it), it was essential
    for Intel to finally move to this kind of concept.
  prefs: []
  type: TYPE_NORMAL
- en: If you're more familiar with AMD CPUs, QPI is to Intel what HyperTransport bus
    on a CPU with built-in memory controller is to AMD CPUs.
  prefs: []
  type: TYPE_NORMAL
- en: 'As NVMe SSDs became faster, the PCI Express standard also needed to be updated,
    which is the reason why the latest version (PCIe 4.0 – the first products started
    shipping recently) was so eagerly anticipated. But now, the focus has switched
    to two other problems that need resolving for storage systems to work. Let''s
    describe them briefly:'
  prefs: []
  type: TYPE_NORMAL
- en: Problem number one is simple. For a regular computer user, one or two NVMe SSDs
    will be enough in 99% of scenarios or more. Realistically, the only real reason
    why regular computer users need a faster PCIe bus is for a faster graphics cards.
    But for storage manufacturers, it's completely different. They want to produce
    enterprise storage devices that will have 20, 30, 50, 100, 500 NVMe SSDs in a
    single storage system—and they want that now, as SSDs are mature as a technology
    and are widely available.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Problem number two is more complex. To add insult to injury, the latest generation
    of SSDs (for example, based on Intel Optane) can offer even lower latency and
    higher throughput. That's only going to get *worse* (even lower latencies, higher
    throughput) as technology evolves. For today's services—virtualization, cloud,
    and HPC—it's essential that the storage system is able to handle any load that
    we can throw at it. These technologies are a real game-changer in terms of how
    much faster storage devices can become, only if interconnects can handle it (QPI,
    FC, and many more). Two of these concepts derived from Intel Optane—**Storage
    Class Memory** (**SCM**) and **Persistent Memory** (**PM**) are the latest technologies
    that storage companies and customers want adopted into their storage systems,
    and fast.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The third problem is how to transfer all of that bandwidth and I/O capability
    to the servers and infrastructures using them. This is why the concept of **NVMe
    over Fabrics** (**NVMe-OF**) was created, to try to work on the storage infrastructure
    stack to make NVMe much more efficient and faster for its consumers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you take a look at these advancements from a conceptual point of view, it
    was clear for decades that RAM-like memory is the fastest, lowest latency technology
    that we've had for the past couple of decades. It's also logical that we're moving
    workloads to RAM, as much as possible. Think of in-memory databases (such as Microsoft
    SQL, SAP Hana, and Oracle). They've been around the block for years.
  prefs: []
  type: TYPE_NORMAL
- en: These technologies fundamentally change the way we think about storage. Basically,
    no longer are we discussing storage tiering based on technology (SSD versus SAS
    versus SATA), or outright speed, as the speed is unquestionable. The latest storage
    technologies discuss storage tiering in terms of *latency*. The reason is very
    simple—let's say that you're a storage company and that you build a storage system
    that uses 50 SCM SSDs for capacity. For cache, the only reasonable technology
    would be RAM, hundreds of gigabytes of it. The only way you'd be able to work
    with storage tiering on a device like that is by basically *emulating* it in software,
    by creating additional technologies that will produce tiering-like services based
    on queueing, handling priority in cache (RAM), and similar concepts. Why? Because
    if you're using the same SCM SSDs for capacity, and they offer the same speed
    and I/O, you just don't have a way of tiering based on technology or capability.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s further describe this by using an available storage system to explain.
    The best device to make our point is Dell/EMC''s PowerMax series of storage devices.
    If you load them with NVMe and SCM SSDs, the biggest model (8000) can scale to
    15 million IOPS(!), 350 GB/s throughput at lower than 100 microseconds latency
    and up to 4 PB capacity. Think about those numbers for a second. Then add another
    number—on the frontend, it can have up to 256 FC/FICON/iSCSI ports. Just recently,
    Dell/EMC released new 32 Gbit/s FC modules for it. The smaller PowerMax model
    (2000) can do 7.5 million IOPS, sub-100 microsecond latency, and scale to 1 PB.
    It can also do all of the *usual EMC stuff*—replication, compression, deduplication,
    snapshots, NAS features, and so on. So, this is not just marketing talk; these
    devices are already out there, being used by enterprise customers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.30 – PowerMax 2000 – it seems small, but it packs a lot of punch'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_05_30.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.30 – PowerMax 2000 – it seems small, but it packs a lot of punch
  prefs: []
  type: TYPE_NORMAL
- en: These are very important concepts for the future, as more and more manufacturers
    produce similar devices (and they are on the way). We fully expect the KVM-based
    world to embrace these concepts in large-scale environments, especially for infrastructures
    with OpenStack and OpenShift.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we introduced and configured various Open Source storage concepts
    for libvirt. We also discussed industry-standard approaches, such as iSCSI and
    NFS, as they are often used in infrastructures that are not based on KVM. For
    example, VMware vSphere-based environments can use FC, iSCSI, and NFS, while Microsoft-based
    environments can only use FC and iSCSI, from the list of subjects we covered in
    this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter will cover subjects related to virtual display devices and
    protocols. We'll provide an in-depth introduction to VNC and SPICE protocols.
    We will also provide a description of other protocols that are used for virtual
    machine connection. All that will help us to understand the complete stack of
    fundamentals that we need to work with our virtual machines, which we covered
    in the past three chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What is a storage pool?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does NFS storage work with libvirt?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does iSCSI work with libvirt?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do we achieve redundancy on storage connections?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What can we use for virtual machine storage except NFS and iSCSI?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Which storage backend can we use for object-based storage with libvirt?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How can we create a virtual disk image to use with a KVM virtual machine?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does using NVMe SSDs and SCM devices change the way that we create storage
    tiers?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the fundamental problems of delivering tier-0 storage services for
    virtualization, cloud, and HPC environments?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Please refer to the following links for more information regarding what was
    covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: 'What''s new with RHEL8 file systems and storage: [https://www.redhat.com/en/blog/whats-new-rhel-8-file-systems-and-storage](https://www.redhat.com/en/blog/whats-new-rhel-8-file-systems-and-storage)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'oVirt storage: [https://www.ovirt.org/documentation/administration_guide/#chap-Storage](https://www.ovirt.org/documentation/administration_guide/#chap-Storage)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'RHEL 7 storage administration guide: [https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/storage_administration_guide/index](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/storage_administration_guide/index)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'RHEL 8 managing storage devices: [https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/managing_storage_devices/index](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/managing_storage_devices/index)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'OpenFabrics CCIX, Gen-Z, OpenCAPI (overview and comparison): [https://www.openfabrics.org/images/eventpresos/2017presentations/213_CCIXGen-Z_BBenton.pdf](https://www.openfabrics.org/images/eventpresos/2017presentations/213_CCIXGen-Z_BBenton.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
