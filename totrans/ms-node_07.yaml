- en: Using Multiple Processes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '"It is a very sad thing that nowadays there is so little useless information."'
  prefs: []
  type: TYPE_NORMAL
- en: – Oscar Wilde
  prefs: []
  type: TYPE_NORMAL
- en: The importance of I/O efficiency is not lost on those witnessing the rapidly
    increasing volume of data being produced within a growing number of applications.
    User-generated content (blogs, videos, tweets, and posts) is becoming the premier
    type of internet content, and this trend has moved in tandem with the rise of
    social software, where mapping the intersections between content generates an
    exponential rise in yet another level of data.
  prefs: []
  type: TYPE_NORMAL
- en: A number of data silos, such as Google, Facebook, and hundreds of others, expose
    their data to the public through an API, often for free. These networks each gather
    astounding volumes of content, opinions, relationships, and so forth from their
    users, data further augmented by market research and various types of traffic
    and usage analysis. Most of these APIs are two-way, gathering and storing data
    uploaded by their members as well as serving that data.
  prefs: []
  type: TYPE_NORMAL
- en: Node has arrived during this period of data expansion. In this chapter, we will
    investigate how Node addresses this need for sorting, merging, searching, and
    otherwise manipulating large amounts of data. Fine-tuning your software so that
    it can process large amounts of data safely and inexpensively is critical when
    building fast and scalable network applications.
  prefs: []
  type: TYPE_NORMAL
- en: We will deal with specific scaling issues in the next chapter. In this chapter,
    we will study some best practices when designing systems where multiple Node processes
    work together on large volumes of data.
  prefs: []
  type: TYPE_NORMAL
- en: As part of that discussion, we will be investigating strategies for parallelism
    when building data-heavy applications, focusing on how to take advantage of multiple
    CPU environments, use multiple workers, and leverage the OS itself to achieve
    the efficiency of parallelism. The process of assembling applications out of these
    contained and efficient processing units will be demonstrated by example.
  prefs: []
  type: TYPE_NORMAL
- en: As noted in [Chapter 5](ae24c7b4-7661-4041-8108-15fe453bf8c8.xhtml)*, Managing
    Many Simultaneous Client Connections*, concurrency is not the same as parallelism.
    The goal of concurrency is good structure for programs, where modeling the complexities
    inherent in juggling multiple simultaneous processes is simplified. The goal of
    parallelism is to increase application performance by sharing parts of a task
    or computation across many workers. It is useful to recall *Clinger's* vision
    of "…dozens, hundreds or even thousands of independent microprocessors, each with
    its own local memory and communications processor, communicating via a high-performance
    communications network."
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ve already discussed how Node helps us reason about non-deterministic control
    flow. Let''s also recall how Node''s designers follow the **Rule Of Modularity**,
    which encourages us to write simple parts connected by clean interfaces. This
    rule leads to a preference for simple networked processes communicating with each
    other using a common protocol. An associated rule is the **Rule of Simplicity**,
    stated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: As [https://en.wikipedia.org/wiki/Unix_philosophy](https://en.wikipedia.org/wiki/Unix_philosophy)
    says, "developers should design for simplicity by looking for ways to break up
    program systems into small, straightforward cooperating pieces. This rule aims
    to discourage developers' affection for writing "intricate and beautiful complexities"
    that are bug prone programs in reality."
  prefs: []
  type: TYPE_NORMAL
- en: It is good to keep this rule in mind as we proceed through this chapter. To
    tame expanding data volume, we can build enormous, complex, and powerful monoliths
    in the hope that they will remain big and powerful enough. Alternatively, we can
    build small and useful units of processing that can be combined into a single
    processing team of any size, not unlike the way that supercomputers can be built
    out of many thousands or millions of cheap commodity processors.
  prefs: []
  type: TYPE_NORMAL
- en: 'A process viewer will be useful while working through this chapter. A good
    one for Unix systems is **htop**, which can be downloaded from: [http://hisham.hm/htop/](http://hisham.hm/htop/).
    This tool provides, among other things, a view into CPU and memory usage; here,
    we see how load is spread across all eight cores:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/54113a36-c425-40c0-9f3f-dd1e012b09af.png)'
  prefs: []
  type: TYPE_IMG
- en: Let's get started by looking into threads and processes.
  prefs: []
  type: TYPE_NORMAL
- en: Node's single-threaded model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Taken in its entirety, the Node environment usefully demonstrates both the efficiency
    of multithreaded parallelism and an expressive syntax amenable to applications
    featuring high concurrency. Using Node does not constrain the developer, the developer's
    access to system resources, or the types of applications the developer might like
    to build.
  prefs: []
  type: TYPE_NORMAL
- en: Nevertheless, a surprising number of persistent criticisms of Node are based
    on this misunderstanding. As we'll see, the belief that Node is not multithreaded
    and is, therefore, slow, or not ready for prime time, simply misses the point.
    JavaScript is single-threaded; the Node stack is not. JavaScript represents the
    language used to coordinate the execution of several multithreaded C++ processes,
    even the bespoke C++ add-ons created by you, the developer. Node provides JavaScript,
    run through V8, primarily as a tool for modeling concurrency. That, additionally,
    where one can write an entire application using just JavaScript is simply another
    benefit of the platform. You are never stuck with JavaScript—you may write the
    bulk of your application in C++ if that is your choice.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will attempt to dismantle these misunderstandings, clearing
    the way for optimistic development with Node. In particular, we will study techniques
    for spreading effort across cores, processes, and threads. For now, this section
    will attempt to clarify how much a single thread is capable of (hint: it''s usually
    all you need).'
  prefs: []
  type: TYPE_NORMAL
- en: The benefits of single-threaded programming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You will be hard-pressed to find any significant number of professional software
    engineers working on enterprise-grade software willing to deny that multithreaded
    software development is painful. However, why is it so hard to do well?
  prefs: []
  type: TYPE_NORMAL
- en: It is not that multithreaded programming is difficult per se—the difficultly
    lies in the complexity of thread synchronization. It is very difficult to build
    high concurrency using the thread model, especially models in which the state
    is shared. Anticipating every way that an action taken in one thread might affect
    all the others is nearly impossible once an application grows beyond the most
    basic of shapes. Entanglements and collisions multiply rapidly, sometimes corrupting
    shared memory, sometimes creating bugs nearly impossible to track down.
  prefs: []
  type: TYPE_NORMAL
- en: 'Node''s designers chose to recognize the speed and parallelization advantages
    of threads without demanding that developers did the same. In particular, Node''s
    designers wanted to save developers from managing the difficulties that accompany
    threaded systems:'
  prefs: []
  type: TYPE_NORMAL
- en: Shared memory and the locking behavior leads to systems that are very difficult
    to reason about as they grow in complexity.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Communication between tasks requires the implementation of a wide range of synchronization
    primitives, such as mutexes and semaphores, condition variables, and so forth.
    An already challenging environment requires highly complex tools, expanding the
    level of expertise necessary to complete even relatively simple systems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Race conditions and deadlocks are common pitfalls in these sorts of systems.
    Contemporaneous read and write operations within a shared program space lead to
    problems of sequencing, where two threads may be in an unpredictable *race* for
    the right to influence a state, event, or other key system characteristic.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As maintaining dependable boundaries between threads and their states is so
    difficult, ensuring that a library (what for Node would be a *module*) is thread-safe
    consumes a great deal of developer time. Can I know that this library will not
    destroy some part of my application? Guaranteeing thread safety requires great
    diligence on the part of a library's developer and these guarantees may be conditional;
    for example, a library may be thread-safe when reading, but not when writing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The primary argument for single-threading is that control flow is difficult
    in concurrent environments, and especially so when memory access or code execution
    order is unpredictable:'
  prefs: []
  type: TYPE_NORMAL
- en: Instead of concerning themselves with arbitrary locking and other collisions,
    developers can focus on constructing execution chains whose ordering is predictable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As parallelization is accomplished through the use of multiple processes, each
    with an individual and distinct memory space, communication between processes
    remains uncomplicated—via the Rule of Simplicity, we achieve not only simple and
    bug-free components, but easier interoperability as well.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As state is not (arbitrarily) shared between individual Node processes; a single
    process is automatically protected from surprise visits from other processes bent
    on memory reallocation or resource monopolization. Communication is through clear
    channels using basic protocols, all of which make it very difficult to write programs
    that make unpredictable changes across processes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Thread-safety is one less concern for developers to waste time worrying about.
    As single-threaded concurrency obviates the collisions present in multithreaded
    concurrency, development can proceed more quickly, on surer ground. In the following
    diagram, we see on the left how sharing state across threads requires diligent
    management to guard against collisions while on the right, a "share-nothing" architecture
    avoids collisions and blocking actions:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/3da79fcb-5688-4fe0-bb98-5a24cd07bf7b.png)'
  prefs: []
  type: TYPE_IMG
- en: A single thread efficiently managed by an event loop brings stability, maintainability,
    readability, and resilience to Node programs. The big news is that Node continues
    to deliver the speed and power of multithreading to its developers—the brilliance
    of Node's design makes such power transparent, reflecting one part of Node's stated
    aim of bringing the most power to the most people with the least difficulty.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following diagram, the differences between two single-threaded models
    and a multithreaded model are shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/39063bd4-4b00-4411-b0ae-c614baa4aa91.png)'
  prefs: []
  type: TYPE_IMG
- en: There is no escape from blocking operations—reading from a file, for example,
    will always take some time. A single-threaded synchronous model forces each task
    to wait for others to finish prior to starting, consuming more time. Several tasks
    can be started in parallel using threads, even at different times, where total
    execution time is no longer than that taken by the longest running thread. When
    using threads, the developer becomes responsible for synchronizing the activity
    of each individual thread, using locks or other scheduling tools. This can become
    very complex when the number of threads increases, and in this complexity live
    very subtle and hard-to-find bugs.
  prefs: []
  type: TYPE_NORMAL
- en: Rather than having the developer struggle with this complexity, Node itself
    manages I/O threads. You need not micromanage I/O threading; one simply designs
    an application to establish data availability points (callbacks) and the instructions
    to be executed once the said data is available. Threads provide the same efficiency
    under the hood, and yet their management is exposed to the developer through an
    easily comprehensible interface.
  prefs: []
  type: TYPE_NORMAL
- en: Multithreading is already native and transparent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Node's I/O thread pool executes within the OS scope, and its work is distributed
    across cores (just as any other job scheduled by the OS would be similarly distributed).
    When you are running Node, you are already taking advantage of its multithreaded
    execution.
  prefs: []
  type: TYPE_NORMAL
- en: In the upcoming discussion of child processes and the Cluster module, we will
    see this style of parallelism—of multiple parallel processes—in action. We will
    see how Node is not denied the full power of an OS.
  prefs: []
  type: TYPE_NORMAL
- en: As we saw earlier, when discussing Node's core architecture, the V8 thread in
    which one executes JavaScript programs is bound to `libuv`, which functions as
    the main, system-level, I/O event dispatcher. In this capacity, `libuv` handles
    the timers, filesystem calls, network calls, and other I/O operations requested
    by the relevant JavaScript process or module commands, such as `fs.readFile` and `http.createServer`.
    Therefore, the main V8 event loop is best understood as a control-flow programming
    interface, supported and powered by the highly-efficient, multithreaded, system
    delegate `libuv`.
  prefs: []
  type: TYPE_NORMAL
- en: '*Bert Belder*, one of Node''s core contributors, is also one of the core contributors
    to `libuv`. In fact, Node''s development has provoked a simultaneous increase
    in `libuv` development, a feedback loop that has only improved the speed and stability
    of both the projects. It has merged and replaced the `libeo` and `libev` libraries
    that formed the original core of Node''s stack.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider another of Raymond''s rules, the **Rule of Separation**: "Separate
    policy from mechanism; separate interfaces from engines." The engine that powers
    Node''s asynchronous, event-driven style of programming is `libuv`; the interface
    to that engine is V8''s JavaScript runtime. Continuing with Raymond, look at this:'
  prefs: []
  type: TYPE_NORMAL
- en: '"One way to effect that separation is, for example, to write your application
    as a library of C service routines that are driven by an embedded scripting language,
    with the application flow of control written in the scripting language rather
    than C."'
  prefs: []
  type: TYPE_NORMAL
- en: The ability to orchestrate hyper-efficient parallel OS processes within the
    abstraction of a single predictable thread exists by design, not as a concession.
  prefs: []
  type: TYPE_NORMAL
- en: It concludes a pragmatic analysis of how the application development process
    can be improved, and it is certainly not a limitation on what is possible.
  prefs: []
  type: TYPE_NORMAL
- en: 'A detailed unpacking of libuv can be found at: [https://github.com/nikhilm/uvbook](https://github.com/nikhilm/uvbook).
    **Burt Belder** also gives an in-depth talk on how libuv, and Node, works under
    the hood at: [https://www.youtube.com/watch?v=PNa9OMajw9w](https://www.youtube.com/watch?v=PNa9OMajw9w).'
  prefs: []
  type: TYPE_NORMAL
- en: Creating child processes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Software development is no longer the realm of monolithic programs. Applications
    running on networks cannot forego interoperability. Modern applications are distributed
    and decoupled. We now build applications that connect users with resources distributed
    across the internet. Many users are accessing shared resources simultaneously.
    A complex system is easier to understand if the whole is understood as a collection
    of interfaces to programs that solve one or a few clearly defined, related problems.
    In such a system, it is expected (and desirable) that processes do not sit idle.
  prefs: []
  type: TYPE_NORMAL
- en: 'An early criticism of Node was that it did not have multicore awareness, that
    is, if a Node server were running on a machine with several cores, it would not
    be able to take advantage of this extra horsepower. Within this seemingly reasonable
    criticism hid an unjustified bias based on a straw man: a program that is unable
    to explicitly allocate memory and execution *threads* in order to implement parallelization
    cannot handle enterprise-grade problems.'
  prefs: []
  type: TYPE_NORMAL
- en: This criticism is a persistent one. It is also not true.
  prefs: []
  type: TYPE_NORMAL
- en: 'While a single Node process runs on a single core, any number of Node processes
    can be *spun up* through use of the `child_process` module. Basic usage of this
    module is straightforward: we fetch a `ChildProcess` object and listen for data
    events. This example will call the `ls` Unix command, listing the current directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we spawn the `ls` process (list directory), and read from the resulting
    `readable` Stream, receiving something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Any number of child processes can be spawned in this way. It is important to
    note here that when a child process is spawned, or otherwise created, the OS itself
    assigns the responsibility for that process to a given CPU. Node is not responsible
    for how an OS allocates resources. The upshot is that on a machine with eight
    cores, it is likely that spawning eight processes will result in each being allocated
    to independent processors. In other words, child processes are automatically spread
    by the OS across CPUs, putting the lie to claims that Node cannot take full advantage
    of multicore environments.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each new Node process (child) is allocated 10 MB of memory, and represents
    a new V8 instance that will take at least 30 milliseconds to start up. While it
    is unlikely that you will be spawning many thousands of these processes, understanding
    how to query and set OS limits on user-created processes is beneficial; htop or
    top will report the number of processes currently running, or you can use `ps
    aux | wc –l` from the command line. The `ulimit` Unix command ([https://ss64.com/bash/ulimit.html](https://ss64.com/bash/ulimit.html))
    provides important information on user limits on an OS. Passing `ulimit`, the
    –u argument will show the maximum number of user processes that can be spawned.
    Changing the limit is accomplished by passing it as an argument: `ulimit –u 8192`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `child_process` module represents a class exposing four main methods: `spawn`,
    `fork`, `exec`, and `execFile`. These methods return a `ChildProcess` object that
    extends `EventEmitter`, exposing an interface to child events and a few functions
    that are helpful in managing child processes. We''ll take a look at its main methods
    and follow up with a discussion of the common `ChildProcess` interface.'
  prefs: []
  type: TYPE_NORMAL
- en: Spawning processes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This powerful command allows a Node program to start and interact with processes
    spawned via system commands. In the preceding example, we used spawn to call a
    native OS process, `ls`, passing the `lh` and `.` arguments to that command. In
    this way, any process can be started just as one might start it via a command
    line. The method takes three arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '**command**: A command to be executed by the OS shell'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**arguments (optional)**: These are command-line arguments, sent as an array'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**options**: An optional map of settings for spawn'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The options for `spawn` allow its behavior to be carefully customized:'
  prefs: []
  type: TYPE_NORMAL
- en: '`cwd` (String): By default, the command will understand its current working
    directory to be the same as that of the Node process calling spawn. Change that
    setting using this directive.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`env` (Object): This is used to pass environment variables to a child process.
    For instance, consider spawning a child with an environment object, such as the
    following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The child process environment will have access to these values:'
  prefs: []
  type: TYPE_NORMAL
- en: '`detached` (Boolean): When a parent spawns a child, both processes form a group,
    and the parent is normally the leader of that group. To make a child the group
    leader, use detached. This will allow the child to continue running even after
    the parent exits. This is because the parent will wait for the child to exit by
    default. You can call `child.unref()` to tell the parent''s event loop that it
    should not count the child reference, and exit if no other work exists.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`uid` (Number): Set the `uid` (user identity) directive for the child process,
    in terms of standard system permissions, such as a UID that has execute privileges
    on the child process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`gid` (Number): Set the `gid` (group identity) directive for the child process,
    in terms of standard system permissions, such as a GID that has execute privileges
    on the child process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stdio` (String or Array): Child processes have file descriptors, the first
    three being the `process.stdin`, `process.stdout` and `process.stderr` standard
    I/O descriptors, in order (fds = 0,1,2). This directive allows those descriptors
    to be redefined, inherited, and so forth.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Consider the output of the following child process program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, a parent would listen on `child.stdout`. Instead, if we want a child
    to inherit its parent''s `stdio`, such that when the child writes to `process.stdout`,
    what is emitted is piped through to the parent''s `process.stdout`, we would pass
    the relevant parent file descriptors to the child, overriding its own:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: In this case, the child's output would pipe straight through to the parent's
    standard output channel. Also, see fork, as follows, for more information on this
    kind of pattern.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each of the three (or more) file descriptors can take one of six values:'
  prefs: []
  type: TYPE_NORMAL
- en: '**pipe**: This creates a pipe between the child and the parent. As the first
    three child file descriptors are already exposed to the parent (`child.stdin`,
    `child.stdout`, and `child.stderr`), this is only necessary in more complex child
    implementations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ipc**: This creates an IPC channel for passing messages between a child and
    parent. A child process may have a maximum of one IPC file descriptor. Once this
    connection is established, the parent may communicate with the child via `child.send`.
    If the child sends JSON messages through this file descriptor, those emissions
    can be caught using `child.on("message")`. If running a Node program as a child,
    it is likely a better choice to use `ChildProcess.fork`, which has this messaging
    channel built in.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ignore**: The file descriptors 0-2 will have `/dev/null` attached to them.
    For others, the referenced file descriptor will not be set on the child.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**A stream object**: This allows the parent to share a stream with the child.
    For demonstration purposes, given a child that will write the same content to
    any provided `WritableStream`, we can do something like this:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The child will now fetch its content and pipe it to whichever output stream
    it has been sent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '**An integer**: A file descriptor ID.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**null and undefined**: These are the default values. For file descriptors
    0-2 (`stdin`, `stdout`, and `stderr`), a pipe is created; others default to `ignore`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition to passing `stdio` settings as an array, certain common groupings
    can
  prefs: []
  type: TYPE_NORMAL
- en: 'be implemented by passing one of these shortcut string values:'
  prefs: []
  type: TYPE_NORMAL
- en: '`''ignore'' = [''ignore'', ''ignore'', ''ignore'']`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''pipe'' = [''pipe'', ''pipe'', ''pipe'']`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''inherit'' = [process.stdin, process.stdout, process.stderr]`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`[0,1,2]`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have shown some examples of using `spawn` to run Node programs as child processes.
    While this is a perfectly valid usage (and a good way to try out the API options),
    `spawn` is primarily for running system commands. Refer to the discussion of `fork`,
    as follows, for more information on running Node processes as children.
  prefs: []
  type: TYPE_NORMAL
- en: 'It should be noted that the ability to spawn any system process means that
    one can use Node to run other application environments installed on the OS. If
    one had the popular PHP language installed, the following would be possible:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Running a more interesting, larger program would be just as easy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Apart from the ease with which one might run Java or Ruby or other programs
    through Node using this technique, asynchronously, we also have a good answer
    to a persistent criticism of Node here: JavaScript is not as fast as other languages
    for crunching numbers, or doing other CPU-heavy tasks. This is true, in the sense
    that Node is primarily optimized for I/O efficiency and helping with the management
    of high-concurrency applications, and JavaScript is an interpreted language without
    a strong focus on heavy computation.'
  prefs: []
  type: TYPE_NORMAL
- en: However, using `spawn`, one can very easily pass off massive computations and
    long-running routines on analytics engines or calculation engines to separate
    processes in other environments. Node's simple event loop will be sure to notify
    the main application when those operations are done, seamlessly integrating the
    resultant data. In the meantime, the main application is free to keep serving
    clients.
  prefs: []
  type: TYPE_NORMAL
- en: Forking processes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Like `spawn`, `fork` starts a child process, but is designed for running Node
    programs with the added benefit of having a communication channel built in. Rather
    than passing a system command to `fork` as its first argument, one passes the
    path to a Node program. As with `spawn`, command-line options can be sent as a
    second argument, accessible via `process.argv` in the forked child process.
  prefs: []
  type: TYPE_NORMAL
- en: 'An optional options object can be passed as its third argument, with the following
    parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`cwd` (String): By default, the command will understand its current working
    directory to be the same as that of the Node process calling `fork`. Change that
    setting using this directive.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`env` (Object): This is used to pass environment variables to a child process.
    Refer to spawn.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`encoding` (String): This sets the encoding of the communication channel.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`execPath` (String): This is the executable used to create the child process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`silent` (Boolean): By default, a forked child will have its `stdio` associated
    with the parent''s (`child.stdout` is identical to `parent.stdout`, for example).
    Setting this option to true disables this behavior.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An important difference between `fork` and `spawn` is that the former's child
    process does not automatically exit when it is finished. Such a child must explicitly
    exit when it is done, easily accomplished via `process.exit()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, we create a child that emits an incrementing number
    every tenth of a second, which its parent then dumps to the system console. First,
    let''s look at the child program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Again, this will simply write a steadily increasing number. Remembering that
    with `fork`, a child will inherit the `stdio` of its parent, we only need to create
    the child in order to get output in a Terminal running the parent process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The silent option can be demonstrated here; `fork(''./emitter.js'', [], { silent:
    true });` turns off any output to the Terminal.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Creating multiple, parallel processes is easy. Let''s multiply the number of
    children created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: It should be clear at this point that by using `fork`, we are creating many
    parallel execution contexts, spread across all machine cores.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is straightforward enough, but the `fork` built-in communication channel provides
    makes communicating with forked children even easier, and cleaner. Consider the
    following file, which spawns a child process and communicates with it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We see that there is a communication channel now available, through which the
    parent can send messages, as well as receiving messages from the child process,
    given below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'By executing the parent script, we will see the following in our console:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: We'll go a little deeper into this important concept of cross-process communication
    shortly.
  prefs: []
  type: TYPE_NORMAL
- en: Buffering process output
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In cases where the complete buffered output of a child process is sufficient,
    with no need to manage data through events, `child_process` offers the `exec`
    method. The method takes three arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '**command:** A command-line string. Unlike `spawn` and `fork`, which pass arguments
    to a command via an array, this first argument accepts a full command string,
    such as `ps aux | grep node`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**options:** This is an optional argument:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cwd` (String): This sets the working directory for the command process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`env` (Object): This is a map of key-value pairs that will be exposed to the
    child process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`encoding` (String): This is the encoding of the child''s data stream. The
    default value is `''utf8''`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`timeout` (Number): This specifies the milliseconds to wait for the process
    to complete, at which point the child process will be sent the `killSignal.maxBuffer`
    value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`killSignal.maxBuffer` (Number): This is the maximum number of bytes allowed
    on `stdout` or `stderr`. When this number is exceeded, the process is killed.
    This default is 200 KB.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`killSignal` (String): The child process receives this signal after a timeout.
    This default is `SIGTERM`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**callback**: This receives three arguments: an `Error` object, if any, `stdout`
    (a `Buffer` object containing the result), `stderr` (a `Buffer` object containing
    error data, if any). If the process was killed, `Error.signal` will contain the
    kill signal.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When you want the buffering behavior of `exec` but are targeting a Node file,
    use `execFile`. Importantly, `execFile` does not spawn a new subshell, which makes
    it slightly less expensive to run.
  prefs: []
  type: TYPE_NORMAL
- en: Communicating with your child
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'All instances of the `ChildProcess` object extend `EventEmitter`, exposing
    events useful for managing child data connections. Additionally, `ChildProcess`
    objects expose some useful methods for interacting with children directly. Let''s
    go through those now, beginning with attributes and methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '`child.connected`: When a child is disconnected from its parent via `child.disconnect()`,
    this flag will be set to `false`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`child.stdin`: This is a `WritableStream` corresponding to the child''s standard
    in.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`child.stdout`: This is a `ReadableStream` corresponding to the child''s standard
    out.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`child.stderr`: This is a `ReadableStream` corresponding to the child''s standard
    error.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`child.pid`: This is an integer representing the process ID (PID) assigned
    to the child process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`child.kill`: This tries to terminate a child process, sending it an optional
    signal. If no signal is specified, the default is `SIGTERM` (for more about signals,
    visit: [https://en.wikipedia.org/wiki/Signal_(IPC)](https://en.wikipedia.org/wiki/Signal_(IPC))).
    While the method name sounds terminal, it is not guaranteed to kill a process—it
    only sends a signal to a process. Dangerously, if `kill` is attempted on a process
    that has already exited, it is possible that another process that has been newly
    assigned the PID of the dead process will receive the signal, with indeterminable
    consequences. This method should fire a `close` event, which the signal used to
    close the process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`child.disconnect()`: This command severs the IPC connection between the child
    and its parent. The child will then die gracefully, as it has no IPC channel to
    keep it alive. You may also call `process.disconnect()` from within the child
    itself. Once a child has disconnected, the `connected` flag on that child reference
    will be set to `false`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sending messages to children
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we saw in our discussion of `fork`, and when using the `ipc` option on `spawn`,
    child processes can be sent messages via `child.send`, with the message passed
    as the first argument. A TCP server, or socket handle, can be passed along with
    the message as a second argument. In this way, a TCP server can spread requests
    across multiple child processes. For example, the following server distributes
    socket handling across a number of child processes equaling the total number of
    CPUs available. Each forked child is given a unique ID, which it reports when
    started. Whenever the TCP server receives a socket, that socket is passed as a
    handle to a random child process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'That child process then sends a unique response, demonstrating that socket
    handling is being distributed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Start the parent server in a Terminal window. In another window, run `telnet
    127.0.0.1 8080`. You should see something similar to the following output, with
    a random child ID being displayed on each connection (assuming that there exist
    multiple cores):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Hit that endpoint a few more times. You should see that your requests are being
    serviced by different child processes.
  prefs: []
  type: TYPE_NORMAL
- en: Parsing a file using multiple processes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the tasks many developers will take on is the building of a logfile processor.
    A logfile can be very large and many megabytes long. Any single program working
    on a very large file can easily run into memory problems or simply run much too
    slowly. It makes sense to process a large file in pieces. We'll build a simple
    log processor that breaks up a big file into pieces and assigns one to each of
    several child workers, running them in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: 'The entire code for this example can be found in the `logproc` folder of the
    code bundle. We will focus on the main routines:'
  prefs: []
  type: TYPE_NORMAL
- en: Determining the number of lines in the logfile
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Breaking those up into equal chunks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating one child for each chunk and passing it parse instructions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assembling and displaying the results
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To get the word count of our file, we use the `wc` command with `child.exec`,
    as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s say that we use `fileChunkLength` of 500,000 lines. This means four
    child processes are to be created, and each will be told to process a range of
    500,000 lines in our file, such as 1 to 500,000:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Each of these workers will themselves use a child process to grab their allotted
    chunk, employing `sed`, the native Stream Editor for Unix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we are executing the `sed –n ''500001,1000001p'' logfile.txt` command, which
    plucks the given range of lines and returns them for processing. Once we''re done
    processing the columns of data (adding them up, and so forth), this child will
    return its data to the master (as described earlier) and the data results will
    be written to a file, otherwise manipulated, or sent to `stdout`, as shown in
    the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/82dec7c0-d77b-4218-9152-3bab484979d4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The full file for this example is much longer, but all that extra code is merely
    formatting and other detail—the Node child process management we have described
    suffices to create a parallelized system for number crunching that will process
    many millions of lines of code in seconds. By using more processes spread across
    more cores, the log parsing speed can be reduced even further.
  prefs: []
  type: TYPE_NORMAL
- en: View  the `README.MD` file in the `/logproc` folder in your code bundle to experiment
    with this example.
  prefs: []
  type: TYPE_NORMAL
- en: Using the cluster module
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we saw when processing large logfiles, the pattern of a master parent controller
    for many child processes is just right for vertical scaling in Node. In response
    to this, the Node API has been augmented by a `cluster` module, which formalizes
    this pattern and helps make its achievement easier. Continuing with Node's core
    purpose of helping make scalable network software easier to build, the particular
    goal of cluster is to facilitate the sharing of network ports among many children.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, the following code creates a `cluster` of worker processes all
    sharing the same HTTP connection:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: We'll dig into the details shortly. For now, note that `cluster.fork` has taken
    zero arguments. What does `fork` without a command or file argument do? Within
    a `cluster`, the default action is to `fork` the current program. We see during
    `cluster.isMaster`, the action is to `fork` children (one for each available CPU).
    When this program is reexecuted in a forking context, `cluster.isWorker` will
    be `true` and a new HTTP server *running on a shared port* is started. Multiple
    processes are sharing the load for a single server.
  prefs: []
  type: TYPE_NORMAL
- en: Start and connect to this server with a browser. You will see something like
    `Hello from 8`, the integer corresponding to the unique `cluster.worker.id` value
    of the worker that assigned responsibility for handling your request. Balancing
    across all workers is handled automatically, such that refreshing your browser
    a few times will result in different worker IDs being displayed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Later on, we''ll go through an example of sharing a socket server across a
    cluster. For now, we''ll lay out the cluster API, which breaks down into two sections:
    the methods, attributes, and events available to the cluster master, and those
    available to the child. As workers in this context are defined using fork, the
    documentation for that method of `child_process` can be applied here as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '`cluster.isMaster`: This is the Boolean value indicating whether the process
    is a master.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cluster.isWorker`: This is the Boolean value indicating whether the process
    was forked from a master.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cluster.worker`: This will bear a reference to the current worker object,
    only available to a child process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cluster.workers`: This is a hash containing references to all active worker
    objects, keyed by the worker ID. Use this to loop through all worker objects.
    This only exists within the master process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cluster.setupMaster([settings])`: This is a convenient way of passing a map
    of default arguments to be used when a child is forked. If all children will fork
    the same file (as is often the case), you will save time by setting it here. The
    available defaults are as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`exec` (String): This is the file path to the process file, defaulting to `__filename`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`args` (Array): This contains Strings sent as arguments to the child process.
    The default is to fetch arguments with `process.argv.slice(2)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`silent` (Boolean): This specifies whether or not to send output to the master''s
    stdio, defaulting to false.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cluster.fork([env])`: This creates a new worker process. Only the master process
    may call this method. To expose a map of key-value pairs to the child''s process
    environment, send an object to `env`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cluster.disconnect([callback])`: This is used to terminate all workers in
    a cluster. Once all the workers have died gracefully, the cluster process will
    itself terminate if it has no further events to wait on. To be notified when all
    children have expired, pass `callback`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cluster events
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The cluster object emits several events, listed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`fork`: This is fired when the master tries to fork a new child. This is not
    the same as `online`. This receives a `worker` object.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`online`: This is fired when the master receives notification that a child
    is fully bound. This differs from the `fork` event and receives a `worker` object.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`listening`: When the worker performs an action that requires a `listen()`
    call (such as starting an HTTP server), this event will be fired in the master.
    The event emits two arguments: a `worker` object, and the address object containing
    the `address`, `port`, and `addressType` values of the connection.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`disconnect`: This is called whenever a child disconnects, which can happen
    either through process exit events or after calling `child.kill()`. This will
    fire prior to the `exit` event—they are not the same. This receives a `worker`
    object.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`exit`: Whenever a child dies, this event is emitted. The event receives three
    arguments: a `worker` object, the exit code number, and the signal string, such
    as `SIGNUP`, which caused the process to be killed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`setup`: This is called after `cluster.setupMaster` has executed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Worker object properties
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Workers have the following attributes and methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '`worker.id`: This is the unique ID assigned to a worker, also representing
    the worker''s key in the `cluster.workers` index.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`worker.process`: This specifies a `ChildProcess` object referencing a worker.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`worker.suicide`: The workers that have recently had `kill` or `disconnect`
    called on them will have their `suicide` attribute set to `true`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`worker.send(message, [sendHandle])`: Refer to `child_process.fork()`, which
    is previously mentioned.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`worker.kill([signal])`: This kills a worker. The master can check this worker''s
    suicide property in order to determine whether the death was intentional or accidental.
    The default signal value that is sent is `SIGTERM`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`worker.disconnect()`: This instructs a worker to disconnect. Importantly,
    the existing connections to the worker are not immediately terminated (as with
    `kill`), but are allowed to exit normally prior to the worker fully disconnecting.
    This is because the existing connections may stay in existence for a very long
    time. It is a good pattern to regularly check whether the worker has actually
    disconnected, perhaps using timeouts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Worker events
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Workers also emit events, such as the ones mentioned in the following list:'
  prefs: []
  type: TYPE_NORMAL
- en: '`message`: Refer to `child_process.fork`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`online`: This is identical to `cluster.online`, except that the check is against
    only the specified worker'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`listening`: This is identical to `cluster.listening`, except that the check
    is against only the specified worker'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`disconnect`: This is identical to `cluster.disconnect`, except that the check
    is against only the specified worker'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`exit`: Refer to the `exit` event for `child_process`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`setup`: This is called after `cluster.setupMaster` has executed'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, using what we now know about the `cluster` module, let's implement a real-time
    tool for analyzing the streams of data emitted by many users simultaneously interacting
    with an application.
  prefs: []
  type: TYPE_NORMAL
- en: Using PM2 to manage multiple processes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: PM2 is designed to be an enterprise-level process manager. As discussed elsewhere,
    Node runs within a Unix process, and its child process and cluster modules are
    used to spawn further processes, typically when scaling an application across
    multiple cores. PM2 can be used to instrument deployment and monitoring of your
    Node processes, both via the command line and programmatically. PM2 spares the
    developer the complexity of configuring clustering boilerplate, handles restarts
    automatically, and provides advanced logging and monitoring tools out of the box.
  prefs: []
  type: TYPE_NORMAL
- en: 'Install PM2 globally: `npm install pm2 -g`'
  prefs: []
  type: TYPE_NORMAL
- en: 'The most straightforward way to use PM2 is as a simple process runner. The
    following program will increment and log a value every second:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we fork a new process from `script.js`, running it in the background
    *forever*, until we stop it. This is a great way to run a daemonized process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the script launches, you should see something like this in your terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c79e0330-beca-4f40-af85-6c6b84c36df8.png)'
  prefs: []
  type: TYPE_IMG
- en: The meaning of most of the values should be clear, such as the amount of memory
    your process is using, whether or not it is online, how long it has been up, and
    so forth (the mode and watching fields will be explained shortly). The process
    will continue to run until it is stopped or deleted.
  prefs: []
  type: TYPE_NORMAL
- en: 'To set a custom name for your process when you start it, pass the `--name`
    argument to PM2: `pm2 start script.js --name ''myProcessName''`.'
  prefs: []
  type: TYPE_NORMAL
- en: This overview of all running PM2 processes can be brought up at any time via
    the command `pm2 list`.
  prefs: []
  type: TYPE_NORMAL
- en: 'PM2 offers other straightforward commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '`pm2 stop <app_name | id | all>` : Stop a process by name, id or stop all processes.
    A stopped process remains in the process list, and can be later restarted.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pm2 restart <app_name | id | all>` : Restart a process. The number of process
    restarts is displayed under restarted in all process lists. To automatically restart
    a process when it reaches some maximum memory limit (say, 15M) use the command
    `pm2 start script.js --max-memory-restart 15M`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pm2 delete <app_name | id | all>` : Deletes a process. This process cannot
    be restarted. pm2 delete all deletes all PM2 processes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pm2 info <app_name | id >` : Provides detailed info on a process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You will be using `pm2 info <processname>` often. Ensure that `script.js` is
    running as a PM2 process using `PM2 list`, then inspect that process info with
    `pm2 info script`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/480ac271-bf14-4d44-9e8f-bd564a647a33.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Note the paths given for error and other logs. Remember that our script increments
    an integer by one every second and logs that count. If you `cat /path/to/script/out/log`
    your terminal will show what has been written to the out log, which should be
    a list of incrementing numbers. Errors are similarly written to a log. Furthermore,
    you can stream the output logs in real time with `pm2 logs`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/97391b02-5a37-4577-bcf7-78b0d0b619ad.png)'
  prefs: []
  type: TYPE_IMG
- en: To clear all logs, use `pm2 flush`.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also use PM2 programmatically. To replicate the steps we took to run
    `scripts.js` with PM2, first create the following script, `programmatic.js`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'This script will use the pm2 module to run `script.js` as a process. Go ahead
    and run it with `node programmatic.js`. Executing a `pm2 list` should show that
    programmed script runner is alive:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2592f665-3ed5-4929-8a02-b014a68ffe9e.png)'
  prefs: []
  type: TYPE_IMG
- en: To make sure, try `pm2 logs` -- you should see numbers being incremented, just
    as before. You can read about the full set of programmatic options here: [http://pm2.keymetrics.io/docs/usage/pm2-api/](http://pm2.keymetrics.io/docs/usage/pm2-api/).
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'PM2 makes process monitoring easy. To view real-time statistics on CPU and
    memory usage for your processes, simply enter the command `pm2 monit`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/205b443c-55ea-47d0-b1be-1526cedfd881.png)'
  prefs: []
  type: TYPE_IMG
- en: Pretty nice, right? On a production server where your Node app is managed via
    PM2, you can use this interface to get a quick look into an application's state,
    including memory usage and a running log.
  prefs: []
  type: TYPE_NORMAL
- en: 'PM2 also makes it easy to create web-based monitoring interfaces – it’s as
    simple as running `pm2 web`. This command will start a monitored process listening
    on port 9615 -- running `pm2 list` will now list a process named `pm2-http-interface`.
    Run the web command and then navigate to `localhost:9615` in your browser. You
    will see a detailed snapshot of your processes, OS, and so forth, as a JSON object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Creating a web-based UI that polls your server every few seconds, fetches process
    information, and then graphs it is made much simpler due to this built-in feature
    of PM2\. PM2 also has an option to set a watcher on all managed scripts, such
    that any changes on the watched script will cause an automatic process restart.
    This is very useful when developing.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a demonstration, let’s create a simple HTTP server and run it through PM2:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: This server will echo “Hello World” whenever `localhost:8080` is hit. Now, lets
    use a PM2 process file to do more involved configuration.
  prefs: []
  type: TYPE_NORMAL
- en: Process files
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Go ahead and kill all running PM2 processes with pm2 delete all. Then, create
    the following `process.json` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'We''re going to use this deployment definition to start our application on
    PM2\. Note that apps is an array, which means you can list several different applications
    with different configurations and start them all at once. We''ll explain the fields
    in a second, but for now, execute this manifest with `pm2 start process.json`.
    You should see something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bff9c69a-8e2a-4a83-97ac-6785f5e67314.png)'
  prefs: []
  type: TYPE_IMG
- en: It was that easy to deploy a multiprocess (clustered) application. PM2 will
    automatically balance load across your instances, set to 4 CPUs in the manifest
    via the `instances` attribute, with `exec_mode` of *cluster* (default mode is
    "fork"). In production, you would likely want to balance across the maximum number
    of cores, which you can flag simply by setting `instances` to `0`. Additionally,
    you see that we've set environment variables via `env:` you can create a *dev*
    and a *prod* (and maybe even a *stage*) configuration for your server, set API
    keys and passwords, and other environment variables here.
  prefs: []
  type: TYPE_NORMAL
- en: 'Open a browser and visit `localhost:8080` to see that the server is running.
    Note that we set `watch` to `true` in our JSON manifest. This tells PM2 to automatically
    restart the application, across all cores, whenever any files are changed in your
    repository. Test it by changing the "Hello" message from your server to something
    else. If you then reload `localhost:8080`, you will see the new message, indicating
    that the servers have been restarted. If you list running PM2 processes, you will
    see the number of restarts:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c0362078-38db-4864-9ce1-1e125a22145d.png)'
  prefs: []
  type: TYPE_IMG
- en: Try it a few times. The restarts are stable, fast, and automatic.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also target specific files for the watcher:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Here, we tell PM2 to watch only `.test` files in `/test`, and the `/app` directory,
    ignoring changes in any .log files. Under the hood PM2 uses Chokidar ([https://github.com/paulmillr/chokidar#api](https://github.com/paulmillr/chokidar#api))
    to watch for file changes, so you can further configure the watcher by setting
    Chokidar options on `watch_options`. Note that you can use glob expressions (and
    regular expressions) in these settings.
  prefs: []
  type: TYPE_NORMAL
- en: You can read the full list of options for PM2 process files here: [http://pm2.keymetrics.io/docs/usage/application-declaration/](http://pm2.keymetrics.io/docs/usage/application-declaration/).
  prefs: []
  type: TYPE_NORMAL
- en: 'Some to note:'
  prefs: []
  type: TYPE_NORMAL
- en: '`max_restarts`: The number of unstable restarts PM2 allows before stopping
    completely.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min_uptime`: The minimum time an app is given to start before being considered
    unstable and triggering a restart.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`autorestart`: Whether to restart at all on a crash.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`node_args`: Pass command-line arguments to the Node process itself. For example: `node_args:
    "--harmony"` is equivalent to `node --harmony server.js`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_memory_restart`: Restart occurs when memory usage breaks this threshold.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`restart_delay`: Particularly in `watch` scenarios, you might want to delay
    restarts on file changes, waiting a bit for further edits before reacting.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Live development of your server applications just got easier, thanks to PM2.
  prefs: []
  type: TYPE_NORMAL
- en: Real-time activity updates of multiple worker results
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Using what we''ve learned, we will construct a multiprocess system to track
    the behavior of all visitors to a sample web page. This will be composed of two
    main segments: a WebSocket-powered client library, which will broadcast each time
    a user moves a mouse, and an administration interface visualizing user interaction
    as well as when a user connects and disconnects from the system. Our goal is to
    show how a more complex system might be designed (such as one that tracks and
    graphs every click, swipe, or other interactions a user might make).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The final administration interface will show activity graphs for several users
    and resemble this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b44a0606-3afa-4d83-8047-faa9831cc5ea.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As this system will be tracking the X and Y positions of each mouse motion made
    by all users, we will spread this continuous stream of data across all available
    machine cores using `cluster`, with each worker in the cluster sharing the burden
    of carrying the large amounts of socket data being fed into a single, shared port.
    Go ahead and visit the code bundle for this chapter, and follow the `README.MD`
    instructions in the `/watcher` folder.
  prefs: []
  type: TYPE_NORMAL
- en: 'A good place to start is in designing the mock client page, which is responsible
    solely for catching all mouse movement events and broadcasting them, through a
    `WebSocket`, to our clustered socket server. We are using the native `WebSocket`
    implementation; you may want to use a library to handle older browsers (such as
    `Socket.IO`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Here, we need to simply turn on the basic `mousemove` tracking, which will broadcast
    the position of a user's mouse on each movement to our socket. Additionally, we
    send along a unique user ID, as a tracking client identity will be important to
    us later on. Note that in a production environment, you will want to implement
    a more intelligent unique ID generator, likely though a server-side authentication
    module.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order for this information to reach other clients, a centralized socket
    server must be set up. As mentioned, we will want this socket server to be clustered.
    Clustered child processes, each duplicates of the following program, will handle
    mouse data sent by clients:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'In this demonstration, we are using *Einar Otto Stangvik''s* very fast and
    well-designed socket server library, `ws`, which is hosted on GitHub at: [https://github.com/websockets/ws](https://github.com/websockets/ws)'
  prefs: []
  type: TYPE_NORMAL
- en: Thankfully, our code remains very simple. We have a socket server listening
    for messages (remember that the client is sending an object with mouse *X* and
    *Y* as well as a user ID). Finally, when data is received (the `message` event),
    we parse the received JSON into an object and pass that back to our cluster master
    via `process.send`.
  prefs: []
  type: TYPE_NORMAL
- en: Note as well how we store the last message (`lastMessage`), done for bookkeeping
    reasons, as when a connection terminates, we will need to pass along the last
    user ID seen on this connection to administrators.
  prefs: []
  type: TYPE_NORMAL
- en: The pieces to catch client data broadcasts are now set up. Once this data is
    received, how is it passed to the administration interface previously pictured?
  prefs: []
  type: TYPE_NORMAL
- en: We've designed this system with scaling in mind, and we want to decouple the
    collection of data from the systems that broadcast data. Our cluster of socket
    servers can accept a constant flow of data from many thousands of clients, and
    should be optimized for doing just that. In other words, the cluster should delegate
    the responsibility for broadcasting mouse activity data to another system, even
    to other servers.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will look at more advanced scaling and messaging tools,
    such as message queues and UDP broadcasting. For our purposes here, we will simply
    create an HTTP server responsible for managing connections from administrators
    and broadcasting mouse activity updates to them. We will use SSE for this, as
    the data flow needs to only be one-way, from server to client.
  prefs: []
  type: TYPE_NORMAL
- en: 'The HTTP server will implement a very basic validation system for administrator
    logins, holding on to successful connections in a way that will allow our socket
    cluster to broadcast mouse activity updates to all. It will also serve as a basic
    static file server, sending both the client and administration HTML when requested,
    though we will focus only on how it handles two routes: `admin/adminname;` and
    `/receive/adminname`. Once the server is understood, we will go into how our socket
    cluster connects to it.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first route—`/admin/adminname`—is mostly responsible for validating administrator
    login, also ensuring that this is not a duplicate login. Once that identity is
    established, we can send back an HTML page to the administration interface. The
    specific client code used to draw the graphs previously pictured won''t be discussed
    here. What we do need is an SSE connection to our server such that the interface''s
    graphing tools receive real-time updates of mouse activity. Some JavaScript on
    the returned administrator''s page establishes such a connection:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'On our server, we implement the `/receive/adminname` route:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: The main purpose of this route is to establish an SSE connection and store the
    administrator's connection, so that we can later broadcast to it.
  prefs: []
  type: TYPE_NORMAL
- en: We will now add the pieces that will pass mouse activity data along to a visualization
    interface. Scaling this subsystem across cores using the cluster module is our
    next step. The cluster master now simply needs to wait for mouse data from its
    socket-serving children, as described earlier.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the same ideas presented in the earlier discussion of cluster,
    simply forking the preceding socket server code across all the available CPUs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Mouse activity data pipes into a cluster worker through a socket and is broadcasted
    via `process.send` to the cluster master described earlier. On each worker message,
    we run through all connected administrators and send mouse data to their visualization
    interfaces using SSE. The administrators can now watch for the arrival and exit
    of clients as well as their individual level of activity.
  prefs: []
  type: TYPE_NORMAL
- en: To test the system, first log in as the default admin with `http://localhost:2112/admin/adminname`.
    You should see a turquoise background, empty for now since there are no connected
    clients. Next, create some clients by opening one or more browser windows and
    navigating to `http://localhost:2112`, where you will see a blank screen. Move
    your mouse around on this screen however you'd like. If you return to the admin
    interface you'll see that your mouse movements (one or many clients) are being
    tracked and graphed.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is the first chapter where we've really begun to test Node's scalability
    goal. Having considered the various arguments for and against different ways of
    thinking about concurrency and parallelism, we arrived at an understanding of
    how Node has successfully maintained the advantages of threading and parallel
    processing while wrapping all that complexity within a concurrency model that
    is both easy to reason about and robust.
  prefs: []
  type: TYPE_NORMAL
- en: Having gone deeper into how processes work, and in particular, how child processes
    can communicate with each other, even spawn further children, we looked at some
    use cases. An example of how to combine native Unix command processes seamlessly
    with custom Node processes led us to a performant and straightforward technique
    for processing large files. The cluster module was then applied to the problem
    of how to share responsibility for handling a busy socket between multiple workers,
    this ability to share socket handles between processes demonstrating a powerful
    aspect of Node's design. And we learned about a production-grade process runner,
    PM2, and how it makes managing both single processes and clusters easier.
  prefs: []
  type: TYPE_NORMAL
- en: Having seen how Node applications might be scaled vertically, we can now look
    into horizontal scaling across many systems and servers. In the next chapter,
    we'll learn how to connect Node with third-party services, such as Amazon and
    Twilio, set up multiple Node servers behind proxies, and more.
  prefs: []
  type: TYPE_NORMAL
