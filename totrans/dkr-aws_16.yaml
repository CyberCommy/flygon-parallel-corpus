- en: Docker Swarm in AWS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Docker Swarm represents Docker's native container management platform that is
    built right into the Docker Engine, and for many people who are using Docker for
    the first time, Docker Swarm is the first container management platform that they
    read and learn about, given that it is an integrated feature of the Docker Engine.
    Docker Swarm is naturally a competitor to the ECS, Fargate, Elastic Beanstalk,
    and recent Elastic Kubernetes Service (EKS) offerings supported by AWS, so you
    might be wondering why a book on Docker in AWS would have a chapter dedicated
    to Docker Swarm. Many organizations prefer to use cloud-agnostic container management
    platforms that they can run on AWS, other cloud providers such as Google Cloud
    and Azure, as well as on premises, and if this is the case for you and your organization,
    then Docker Swarm is certainly an option worth considering.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you will learn how to deploy Docker Swarm to AWS using the
    Docker for AWS solution that makes it very easy to get a Docker Swarm cluster
    up and running in AWS. You will learn the basics of how to manage and access your
    Swarm cluster, how to create and deploy services to Docker Swarm, and how to leverage
    a number of AWS services that are integrated with Swarm in the Docker for AWS
    solution. This will include integrating Docker Swarm with the Elastic Container
    Registry (ECR), publishing your application to the outside world by integrating
    with AWS Elastic Load Balancing (ELB), creating shared volumes using the AWS Elastic
    File System (EFS), and creating persistent volumes using the AWS Elastic Block
    Store (EBS).
  prefs: []
  type: TYPE_NORMAL
- en: Finally, you will learn how to address key operational challenges, including
    running one-shot deployment tasks, performing secrets management using Docker
    secrets, and deploying your application using rolling updates. By the end of this
    chapter, you will know how to deploy a Docker Swarm cluster to AWS, how to integrate
    Docker Swarm with AWS services, and how to deploy your production applications
    to Docker Swarm.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to Docker Swarm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing Docker for AWS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accessing Docker Swarm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying Docker services to Docker Swarm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying Docker stacks to Docker Swarm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integrating Docker Swarm with the ECR
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating shared Docker volumes using EFS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating persistent Docker volumes using EBS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Supporting one-shot deployment tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing rolling updates
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are the technical requirements for this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Administrative access to an AWS account
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Local environment configured as per the instructions in Chapter 1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A local AWS profile, configured as per the instructions in Chapter 3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AWS CLI version 1.15.71 or higher
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Docker 18.06 CE or higher
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Docker Compose 1.22 or higher
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GNU Make 3.82 or higher
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This chapter assumes that you have completed all of the preceding chapters in
    this book
  prefs: []
  type: TYPE_NORMAL
- en: The following GitHub URL contains the code samples that are used in this chapter: [https://github.com/docker-in-aws/docker-in-aws/tree/master/ch16](https://github.com/docker-in-aws/docker-in-aws/tree/master/ch16).
  prefs: []
  type: TYPE_NORMAL
- en: 'Check out the following video to see the Code in Action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://bit.ly/2ogdBpp](http://bit.ly/2ogdBpp)'
  prefs: []
  type: TYPE_NORMAL
- en: Docker Swarm introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Docker Swarm** is a native integrated feature of the Docker Engine, providing
    cluster management and container orchestration features that allow you to run
    Docker containers at scale in production. Every Docker Engine running version
    1.13 or greater includes the ability to operate in swarm mode, which provides
    the following features:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Cluster management**: All nodes operating in swarm mode include native cluster
    features that allow you to quickly establish clusters that you can deploy your
    applications to.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multi-host networking**: Docker supports overlay networking that allows you
    to create virtual networks over which all containers attached to the network can
    communicate privately. This networking layer is completely independent of the
    physical networking topology that connects your Docker Engines, meaning you typically
    don''t have to worry about traditional networking constraints such as IP addressing
    and network segmentation—Docker takes care of all of this for you.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Service discovery and load balancing**: Docker Swarm supports a simple service
    discovery model based upon DNS that allows your applications to discover each
    other without requiring complex service discovery protocols or infrastructure.
    Docker Swarm also supports automatic load balancing of traffic to your applications
    using DNS round robin, and can integrate with an external load balancer such as
    the AWS Elastic Load Balancer service.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Service scaling and rolling updates**: You can easily scale your services
    up and down, and when it''s time to update your services, Docker supports intelligent
    rolling update features with support for rollbacks in the event of a deployment
    failure.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Declarative service model**: Docker Swarm uses the popular Docker Compose
    specification to declaratively define application services, networks, volumes,
    and more in an easy to understand and maintained format.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Desired state**: Docker Swarm continuously monitors application and runtime
    state, ensuring that your services are operating in accordance with the desired
    state you have configured. For example, if you configure a service with an instance
    or replica count of 2, Docker Swarm will always try and maintain this count and
    automatically deploy new replicas to a new node when an existing node fails.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Production-grade operational features such as secrets and configuration management**:
    Some features such as Docker secrets and Docker configurations are exclusive to
    Docker Swarm, and provide solutions for real-world production issues such as the
    ability to securely distribute secrets and configuration data to your applications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When it comes to running Docker Swarm on AWS, Docker provides a community edition
    offering referred to as Docker for AWS CE, which you can find further information
    about at [https://store.docker.com/editions/community/docker-ce-aws](https://store.docker.com/editions/community/docker-ce-aws).
    At present, Docker for AWS CE is deployed via a pre-defined CloudFormation template
    that integrates Docker Swarm with a number of AWS services, including EC2 Auto
    Scaling, Elastic Load Balancing, Elastic File System, and Elastic Block Store.
    As you will soon see, this makes it very easy to stand up a new Docker Swarm cluster
    in AWS.
  prefs: []
  type: TYPE_NORMAL
- en: Docker Swarm versus Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First and foremost, as evidenced by the majority of the content of this book,
    I am an ECS guy, and if you are running your container workloads exclusively on
    AWS, my recommendation, at least at the time of the writing of this book, is almost
    always going to be ECS. However, many organizations don't want to be locked into
    AWS and want a cloud agnostic approach, and this is where Docker Swarm is one
    of the leading solutions available at present.
  prefs: []
  type: TYPE_NORMAL
- en: Right now, Docker Swarm competes head-on with Kubernetes, which we will discuss
    in the next chapter. It's fair to say that Kubernetes looks to have established
    itself as the leading cloud agnostic container management platform of choice,
    but that doesn't mean you should necessarily overlook Docker Swarm.
  prefs: []
  type: TYPE_NORMAL
- en: In general, I personally find Docker Swarm easier to set up and use, and a key
    benefit for me at least, is that it uses familiar tools such as Docker Compose,
    which means you can get up and running very quickly, especially if you have used
    these tools previously. For smaller organizations that just want to get up and
    running fast and ensure that things just work with minimal fuss, Docker Swarm
    is an attractive choice. The Docker for AWS solution makes it very easy to establish
    a Docker Swarm cluster in AWS, although AWS recently made Kubernetes a whole lot
    easier on AWS with the launch of the Elastic Kubernetes Service (EKS)—more on
    this in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Ultimately, I encourage you to try out both with an open mind and make your
    own decisions as to what container management platform works best for you and
    your organization's goals.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Docker for AWS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The recommended and fastest way to get Docker Swarm up and running in AWS is
    to use Docker for AWS, which you can read more about at [https://docs.docker.com/docker-for-aws/](https://docs.docker.com/docker-for-aws/).
    If you browse to this page, in the Setup & prerequisites section, you will be
    presented with links that allow you to install both Docker Enterprise Edition
    (EE) and Docker Community Edition (CE) for AWS.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will be using the free Docker CE for AWS (stable) variant, and notice that
    you can choose to deploy to a brand new VPC or to an existing VPC:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/b3c18990-a854-4275-925f-e86d1f0410e2.png)Selecting a Docker CE for
    AWS option'
  prefs: []
  type: TYPE_NORMAL
- en: 'Given, we already have an existing VPC, if you click on the Deploy Docker CE
    for AWS (stable) users your existing VPC option, you will be redirected to the
    AWS CloudFormation console, where you are prompted to create a new stack from
    a template published by Docker:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/5df1c0f5-b40b-442c-9b7b-2c67dea80f12.png)'
  prefs: []
  type: TYPE_IMG
- en: Creating a Docker for AWS stack
  prefs: []
  type: TYPE_NORMAL
- en: 'After clicking Next, you will be prompted to specify a number of parameters
    that control the configuration of your Docker Swarm Docker installation. I won''t
    describe all of the options available, so assume that you should leave the default
    configuration for any parameters that I do not mention here:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Stack name**: Specify an appropriate name for your stack — for example docker-swarm.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Swarm Size**: Here, you can specify the number of Swarm managers and worker
    nodes. At a minimum, you can specify just one manager, however I recommend also
    configuring a worker node so that you can test deploying your applications to
    a multi-node Swarm cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Swarm Properties**: Here, you should configure the Swarm EC2 instances to
    use your existing admin SSH key (EC2 key pair) and also enable the Create EFS
    prerequisites for Store property, as we will use EFS to provide a shared volume
    later on in this chapter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Swarm Manager Properties**: Change the Manager ephemeral storage volume type
    to gp2 (SSD).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Swarm Worker Properties**: Change the Worker ephemeral storage volume type
    to gp2 (SSD).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**VPC/Network**: Select your existing default VPC and then ensure that you
    specify the VPC CIDR Range that is displayed when you select the VPC (for example,
    `172.31.0.0/16`), and then select appropriate subnets from your default VPC for
    Public Subnets 1 through 3.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After completing the preceding configuration, click on the Next button twice,
    and finally on the Review screen, select the I acknowledge that AWS CloudFormation
    might create IAM resources option and then click on the Create button.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, your new CloudFormation stack will be created, and should be
    complete within 10-15 minutes. Note that if you ever want to increase the number
    of managers and/or worker nodes in your cluster, the recommended way to do this
    is to perform a CloudFormation stack update, modifying the appropriate input parameters
    that define manager and worker count. Also, to upgrade Docker for AWS Swarm Cluster,
    you should apply the latest CloudFormation template that includes updates to Docker
    Swarm and various other resources.
  prefs: []
  type: TYPE_NORMAL
- en: Resources created by the Docker for AWS CloudFormation stack
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you review the Resources tab in the CloudFormation console for your new
    stack, you will notice a variety of resources are created, the most important
    of which are listed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**CloudWatch Logs Group**: This stores all logs for the container''s schedule
    via your Swarm cluster. This resource is only created if you enable the Use Cloudwatch
    for container logging parameter during stack creation (by default, this parameter
    is enabled).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**External Load Balancer**: A classic Elastic Load Balancer is created, which
    is used to publish public access to your Docker applications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Elastic Container Registry IAM Policy**: An IAM policy is created and attached
    to all Swarm manager and worker EC2 instance roles that permit read/pull access
    to ECR. This is required if you store your Docker images in ECR, which is applicable
    to our scenario.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Other resources**: A variety of resources are also created such as a DynamoDB
    table that is used for cluster management operations, and a Simple Queue Service
    (SQS) queue is used for EC2 auto-scaling life cycle hooks during Swarm manager
    upgrade scenarios.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If you click on the Outputs tab, you will notice an output property called
    DefaultDNSTarget, which references the public URL of the external load balancer.
    Take note of this URL, as this is where the sample application will be accessible
    from later on in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/0b5dd42f-03a6-4ac0-ae85-7e7b9d625a70.png)Docker for AWS stack outputs'
  prefs: []
  type: TYPE_NORMAL
- en: Accessing the Swarm cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the CloudFormation stack outputs, there is also a property called Managers,
    which provides a link to the EC2 instance(s) for each of the Swarm managers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/12a015a1-5fcc-4756-abf3-d7312af16cac.png)Swarm Manager Auto Scaling
    group'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can use this information to obtain the public IP address or DNS name of
    one of your Swarm managers. Once you have this IP address, you can establish an
    SSH connection to the manager:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice that you must specify a user name of `docker` when accessing the manager,
    and if you run the `docker ps` command, you can see that there are four system
    containers running on the manager by default:'
  prefs: []
  type: TYPE_NORMAL
- en: '**shell-aws**: This provides SSH access to the manager, meaning the SSH session
    that you establish to the Swarm manager is actually running *inside* this container.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**meta-aws**: Provides general metadata services, including providing tokens
    that allow new members to join the cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**guide-aws**: Performs cluster state management operations such as adding
    each manager to DynamoDB, and other housekeeping tasks such as cleaning up unused
    images and volumes and stopped containers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**l4controller-aws**: Manages integration with the external load balancer for
    the Swarm cluster. This component is responsible for publishing new ports and
    ensuring they are accessible on the elastic load balancer. Note that you should
    never modify the ELB for your cluster directly, and instead rely on the `l4controller-aws`
    component to manage the ELB.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To view and access the other nodes in your cluster, you can use the `docker
    node ls` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that worker nodes do not allow public SSH access, so you can only access
    worker nodes via SSH from a manager. There is a problem, however: you can''t establish
    an SSH session to the worker node, given that the manager node does not have the
    private key of the admin EC2 key pair stored locally.'
  prefs: []
  type: TYPE_NORMAL
- en: Setting up local access to Docker Swarm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although you can run Docker commands remotely via an SSH session to a Swarm
    manager, it is much easier to be able to interact with the remote Swarm manager
    daemon using your local Docker client, where you have access to your local Docker
    service definitions and configurations. We also have the problem of not being
    able to access our worker nodes via SSH, and we can solve both of these problems by
    using a couple of techniques known as SSH agent forwarding and SSH tunneling.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring SSH agent forwarding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To set up SSH agent forwarding, first add your admin SSH key to your local
    SSH agent using the `ssh-add` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The `-K` flag is specific to macOS and adds the passphrase for your SSH key
    to your OS X keychain, meaning that this configuration will persist across reboots.
    If you are not using macOS, you can just omit the `-K` flag.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can now access your Swarm manager using the `-A` flag, which configures
    the SSH client to use your SSH agent identities. Using the SSH agent also enables
    SSH agent forwarding, which means that the SSH key used to establish your SSH
    session with the Swarm manager can be automatically used or forwarded for other
    SSH connections you might establish from within your SSH session:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, using SSH agent forwarding solves the issue of being able to
    access your worker nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring SSH tunneling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**SSH tunneling** is a powerful technique that allows you to tunnel network
    communications securely over an encrypted SSH session to a remote host. SSH tunneling
    works by exposing a local socket or port that is wired to a remote socket or port
    on the remote host. This can provide the illusion that you are communicating with
    a local service, which is particularly useful when working with Docker.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following command demonstrates how you can make the Docker socket running
    on a Swarm manager appear as a port running on your local host:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The `-N` flag passed to the first SSH command instructs the client not to send
    a remote command, while the `-L` or local forwarding flag configures maps TCP
    port `2374` on the localhost to the `/var/run/docker.sock` Docker Engine socket
    on the remote Swarm manager. The ampersand (`&`) character at the end of the command
    causes the command to be run in the background, with the process ID published
    as the output of this command.
  prefs: []
  type: TYPE_NORMAL
- en: With this configuration in place, you can now run the Docker client, locally
    referencing `localhost:2374` as a local endpoint that is wired to your remote
    Swarm manager. Notice that you can specify the host using the `-H` flag, or by
    exporting the environment variable `DOCKER_HOST`. This will allow you to execute
    remote Docker operations while referencing local files in your local environment,
    making it much easier to manage and deploy to your Swarm cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Although Docker does include a client/server model that enables communications
    between a Docker client and remote Docker Engine, to do so securely requires mutual
    transport layer security (TLS) and public key infrastructure (PKI) technologies,
    which are complex to set up and maintain. Using SSH tunneling to expose the remote
    Docker socket is much easier to set up and maintain, and is considered as secure
    as any form of remote SSH access.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying applications to Docker Swarm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that you have installed Docker Swarm using Docker for AWS and established
    management connectivity to the Swarm cluster, we are ready to start deploying
    applications. Deploying applications to Docker Swarm requires use of the `docker
    service` and `docker stack` commands, which we have not covered to date in this
    book, so we will get acquainted with these commands by deploying a few example
    applications before tackling the deployment of our todobackend application.
  prefs: []
  type: TYPE_NORMAL
- en: Docker services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although you can technically deploy a single container to a Swarm cluster, you
    should avoid doing this and always work with Docker *services* as the standard
    unit of deployment to your Swarm clusters. We have actually worked with Docker
    services already using Docker Compose, however when used in conjunction with Docker
    Swarm, they are elevated to a new level.
  prefs: []
  type: TYPE_NORMAL
- en: 'To create a Docker service, you can use the `docker service create` command,
    and the following example demonstrates standing up a very simple web application
    using the popular Nginx web server:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The `--name` flag provides a friendly name for the services, while the `--publish`
    flag allows you to publish an external port the service will be accessible from
    (in this case, port `80`). The `--replicas` flag defines now many containers should
    be deployed for the service, and finally you specify the name of the image (nginx,
    in this case) for the service that you want to run. Note that you can use the
    `docker service ps` command to list the individual containers and nodes that are
    running for the service.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you now attempt to browse to the URL of the external load balancer, you
    should receive the default **Welcome to nginx!** web page:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/13d5f811-507a-4314-8724-213ed904269e.png)Nginx welcome pageTo remove
    a service, you can simply use the `docker service rm` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Docker stacks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A **Docker stack** is defined as a complex, self-contained environment that
    consists of multiple services, networks and/or volumes, and is defined in a Docker
    Compose file.
  prefs: []
  type: TYPE_NORMAL
- en: A good example of a Docker stack that will immediately add some value to our
    Swarm cluster is an open source Swarm management tool called **swarmpit**, which
    you can read more about at [https://swarmpit.io/](https://swarmpit.io/). To get
    started using swarmpit, clone the [https://github.com/swarmpit/swarmpit](https://github.com/swarmpit/swarmpit)
    repository to a local folder, and then open the `docker-compose.yml` file at the
    root of the repository.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: I have highlighted my modifications to the file, which are to update the Docker
    Compose file specification version to 3.6, modify the ports property for the app
    service to publish the management UI externally on port 8888, and ensure that
    the database is only deployed to the Swarm manager in your cluster. The reason
    for pinning the database is to ensure that in the event the database container
    failed for any reason, Docker Swarm will attempt to re-deploy the database container
    to the same node where the local database volume is stored.
  prefs: []
  type: TYPE_NORMAL
- en: In the event that you inadvertently wipe the swarmpit database, be warned that
    the admin password will be reset to the default value of admin, representing a
    significant security risk if you have published the swarmpit management interface
    to the public internet.
  prefs: []
  type: TYPE_NORMAL
- en: 'With these changes in place, you can now run the `docker stack deploy` command
    to deploy the swarmpit management application:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'You can see that the `docker stack deploy` command is much simpler than the
    `docker service create` command, given that the Docker Compose file contains all
    of the service configuration details. Browse to your external URL on port 8888
    and login with the default username and password of `admin`/`admin`, you should
    immediately change the admin password by selecting the admin drop-down in the
    top right-hand corner and selecting **Change Password**. Once you have changed
    the admin password, you can take a look around the swarmpit management UI, which
    provides a lot of information about your Swarm cluster. The following screenshot
    demonstrates the **Infrastructure** | **Nodes** page, which lists the nodes in
    your cluster and displays detailed information about each node:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/77a22746-4832-40ca-b63a-1eefeb5c58d6.png)The swarmkit management
    interface'
  prefs: []
  type: TYPE_NORMAL
- en: Deploying the sample application to Docker Swarm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We are now down to the business end of the chapter, which is to deploy our
    sample todobackend application to our newly created Docker swarm cluster. As you
    might expect, there are a few challenges we are going to encounter, which require
    the following configuration tasks to be performed:'
  prefs: []
  type: TYPE_NORMAL
- en: Integrating Docker Swarm with the Elastic Container Registry
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining a stack
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating shared storage for hosting static content
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a collectstatic service
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating persistent storage for storing the todobackend database
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Secrets management using Docker Swarm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running database migrations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integrating Docker Swarm with the Elastic Container Registry
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The todobackend application is already published in an existing Elastic Container
    Registry (ECR) repository, and ideally we want to be able to integrate our Docker
    swarm cluster so that we can pull private images from ECR. As of the time of writing
    this book, ECR integration is supported in a somewhat limited fashion, in that
    you can pass registry credentials to your Docker swarm manager at the time of
    deployment, which will be shared across all nodes in the cluster. However, these
    credentials expire after 12 hours, and there is currently no native mechanism
    to automatically refresh these credentials.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to periodically refresh ECR credentials so that your Swarm cluster
    can always pull images from ECR, you need to perform the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Ensure that your manager and worker instances have permissions to pull from
    ECR. The Docker for AWS CloudFormation template configures this access by default,
    so you shouldn't have to worry about configuring this.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploy the `docker-swarm-aws-ecr-auth` auto-login system container as a service,
    which is published at [https://github.com/mRoca/docker-swarm-aws-ecr-auth](https://github.com/mRoca/docker-swarm-aws-ecr-auth).
    When installed, this service automatically refreshes ECR credentials on all nodes
    in your cluster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To deploy the `docker-swarm-aws-ecr-auth` service, you use the `docker service
    create` command as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Note that once this service is up and running, you must include the `--with-registry-auth`
    flag for any services that you deploy that use ECR images.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code demonstrates deploying the todobackend application using
    the `docker service create` command, along with the `--with-registry-auth` flag:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'You can verify the todobackend service did indeed deploy by browsing to the
    external load balancer URL:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/c4dd84d5-c966-4f0d-adcc-8539b3ca7ff6.png)Deploying the todobackend
    service'
  prefs: []
  type: TYPE_NORMAL
- en: Note that because we haven't generated any static files the todobackend service
    is missing static content.  We will resolve this later on when we create a Docker
    Compose file and deploy a stack for the todobackend application.
  prefs: []
  type: TYPE_NORMAL
- en: Defining a stack
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although you can deploy services using commands like `docker service create`,
    you can very quickly deploy a complete multi-service environment as we saw earlier
    using the `docker stack deploy` command, referencing a Docker Compose file that
    captures the configuration of the various services, networks, and volumes that
    comprise your stack. Deploying stacks to Docker Swarm requires version 3 of the
    Docker Compose file specification, so we can't use the existing `docker-compose.yml`
    file at the root of the todobackend repository to define our Docker Swarm environments,
    and I recommend keeping your development and test workflow separate, as the Docker
    Compose version 2 specification exclusively supports features that work well for
    continuous delivery workflows.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s get started defining a stack for the todobackend application that
    we can deploy to our Docker Swarm cluster in AWS by creating a file called `stack.yml`
    at the root of the `todobackend` repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The first property we specify is the mandatory `version` property, which we
    define as version 3.6, which was the latest version supported at the time of writing
    this book. Next, we configure the top-level networks property, which specifies
    Docker networks that the stack will use. You will create a network called `net` that
    implements the `overlay` driver, which creates a virtual network segment across
    all nodes in the Swarm cluster over which the various services defined in the
    stack can communicate with each other. In general, each stack that you deploy
    should specify its own overlay network, which provides segmentation between each
    of your stacks and means that you don't need to worry about IP addressing or the
    physical network topology of your cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Next, you must define a single service called `app`, which represents the main
    todobackend web application and via the `image` property specifies the fully qualified
    name of the ECR image for the todobackend application that you published in earlier
    chapters. Note that Docker stacks do not support the `build` property and must
    reference a published Docker image, which is a good reason why you should always
    have a separate Docker Compose specification for your development, test, and build
    workflow.
  prefs: []
  type: TYPE_NORMAL
- en: The `ports` property uses the long style configuration syntax (in previous chapters,
    you have use the short style syntax), which provides access to more configuration
    options, allowing you to specify that the container port 8000 (as specified by
    the `target` property) will be published externally on port 80 (as specified by
    the `published` property), while the `networks` property configures the `app`
    service to be attached to the `net` network you previously defined. Notice that
    the `environment` property does not specify any database configuration settings—the
    focus for now is to just get the application up and running, albeit in a somewhat
    broken state, but we will shall configure database access later on in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the `deploy` property allows you to control how the service should
    be deployed, with the `replica` property specifying to deploy two instances of
    our service, and the `update_config` property configuring rolling updates to update
    one instance at a time (as specified by the `parallelism` property) with a delay
    of 30 seconds between deploying each updated instance.
  prefs: []
  type: TYPE_NORMAL
- en: 'With this configuration in place, you can now deploy your stack using the `docker
    stack deploy` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice that I first login to ECR—this step is not absolutely required, however
    if are not logged into ECR, the Docker client is unable to determine the current
    image hash associated with the latest tag and you will be presented with this
    warning:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: If you now browse the external load balancer URL, the todobackend application
    should load, however you will notice that the application is missing static content
    and if you attempt to access `/todos`, a database configuration error will be
    presented, which is to be expected given we haven't configured any database settings
    or considered how to run the **collectstatic** process in Docker swarm.
  prefs: []
  type: TYPE_NORMAL
- en: Creating shared storage for hosting static content
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Docker for AWS solution includes the Cloudstor volume plugin, which is a
    storage plugin built by Docker and designed to support popular cloud storage mechanisms
    for persistent storage.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the case of AWS, this plugin provides out of the box integration with the following
    types of persistent storage:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Elastic Block Store** (**EBS**): Provides block level storage intended for
    dedicated (non-shared) access. This provides a high level of performance with
    the ability to detach and attach volumes to different instances, and supports
    snapshot and restore operations. EBS storage is suitable for database storage
    or any applications that require high throughput and minimal latency for reading
    and writing local data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Elastic File System** (**EFS**): Provides shared file system access using
    the **Network File System** (**NFS**) version 4 protocol. NFS allows for sharing
    storage at the same time across multiple hosts, however this is much less performant
    than EBS storage. NFS storage is suitable for applications that share common files
    and do not require high performance. Earlier, when you deployed the Docker for
    AWS solution, you selected to create the prerequisites for EFS, which sets up
    an EFS file system for the Swarm cluster that the Cloudstor volume plugin integrates
    with.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As you know from previous chapters, the todobackend application has a specific
    requirement for storing static content, and although I would typically not recommend
    EFS for such a use case, the static content requirement represents a good opportunity
    to demonstrate how to configure and use EFS as a shared volume in a Docker Swarm
    environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: You must first create a volume called `public` and specify a driver of `cloudstor:aws`,
    which ensures that the Cloudstor driver is loaded with AWS support. To create
    an EFS volume, you simply configure a driver option called `backing` with a value
    of `shared`, and then mount the volume at `/public` in the `app` service.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you now deploy your changes using the `docker stack deploy` command, the
    `volume` will be created and the `app` service instances will be updated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: You can use the `docker volume ls` command to view current volumes, and you
    will see that a new volume named according to the convention `<stack name>_<volume
    name>` (for example, `todobackend_public`) is created with a driver of `cloudstor:aws`.
    Notice that the `docker service ps` command output shows that `todobackend.app.1`
    was updated first, and then `todobackend.app.2` was updated 30 seconds later,
    which is based upon the earlier rolling update configuration you applied in the
    `deploy` settings for the `app` service.
  prefs: []
  type: TYPE_NORMAL
- en: 'To verify that the volume was successfully mounted, you can use the `docker
    ps` command to query the Swarm manager for any app service containers running
    locally, and then use `docker exec` to verify that the `/public` mount exists
    and is readable/writable by the `app` user that the todobackend container runs
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'One important point to note is that the `docker volume` and other `docker`
    commands shown in the preceding example are only executed in the context of the
    current Swarm node you are connected to, and won''t display volumes or allow you
    to access containers running on other nodes in the cluster. To verify that the
    volume is indeed shared and accessible by the app service container running on
    the other Swarm node in our cluster, you need to first SSH to the Swarm manager
    and then SSH to the single worker node in the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the volume is available on the worker node, who can see the
    `/public/test` file we created on the other instance, proving the volume is indeed
    shared and accessible to all `app` service instances, regardless of underlying
    nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a collectstatic service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that you have a shared volume in place, we need to consider how we will
    define and execute the collectstatic process to generate static content. To date,
    throughout this book, you have performed the collectstatic process as an imperative
    task that needs to happen at a specific point in time within a defined deployment
    sequence, however Docker Swarm promotes the concept of eventual consistency so
    you should be able to deploy your stack and have a collectstatic process running
    that may fail but will eventually succeed, at which point the desired state of
    your application is reached. This approach is quite different from the imperative
    approach we have taken previously, but is accepted as a best practice for well-architected
    modern cloud native applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'To demonstrate how this works, we first need to tear down the todobackend stack
    so that you can observe failures that will occur in the collectstatic process
    while the Docker storage engine is creating and mounting the EFS backed volume:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: One point to note is that Docker Swarm does not remove volumes when you destroy
    a stack, so you need to manually remove the volume to fully clean up the environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now add a collectstatic service to our stack:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The `collectstatic` service mounts the `public` shared volume and runs the appropriate
    `manage.py` task to generate static content. In the `deploy` section, we configure
    a replica count of 1, given that the `collectstatic` service only needs be run
    once per deployment, and then configure a `restart_policy` that states Docker
    Swarm should attempt to restart the service on failure, with a delay of 30 seconds
    between each restart attempt up to a maximum of 6 attempts. This provides eventual
    consistency behavior as it allows collectstatic to fail initially while EFS volume
    mounting operations are taking place, and then eventually succeed once the volume
    is mounted and ready.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you now deploy the stack and monitor the collectstatic service, you may
    notice some initial failures:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The `docker service ps` command displays not only the current service state,
    but also service history (such as any previous attempts to run the service), and
    you can see that 32 seconds ago the first attempt to run `collectstatic` failed,
    after which Docker Swarm attempted to restart the service. This attempt succeeds,
    and although the `collectstatic` service will eventually complete and exit, because
    the restart policy is set to failure, Docker Swarm will not attempt to start the
    service again, given that the service exited with no error. This supports the
    concept of a "one-shot" service with retry capabilities in the event of a failure,
    and the only time Swarm will attempt to run the service again is in the event
    that a new configuration for the service is deployed to the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: If you now browse to the external load balancer URL, you should find that the
    static content of the todobackend application is now presented correctly, however
    the database configuration error still exists.
  prefs: []
  type: TYPE_NORMAL
- en: Creating persistent storage for storing the application database
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can now shift our attention to the application database, which is an essential
    supporting component of the todobackend application. If you are running in AWS,
    my typical recommendation would be, regardless of container orchestration platform,
    to use the Relational Database Service (RDS) as we have done throughout this book,
    however the application database requirement for the todobackend application provides
    an opportunity to demonstrate how you can support persistent storage using the
    Docker for AWS solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to EFS-backed volumes, the Cloudstor volume plugin also supports
    *relocatable* Elastic Block Store (EBS) volumes. Relocatable means that the plugin
    will automatically relocate the currently assigned EBS volume for a container
    to another node in the event Docker Swarm determines it has to relocate a container
    from one node to another. What actually happens during relocation of the EBS volume
    depends on the scenario:'
  prefs: []
  type: TYPE_NORMAL
- en: '**New node is in the same availability zone**: The plugin simply detaches the
    volume from the EC2 instance of the existing node and reattaches the volume on
    the new node.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**New node is in a different availability zone**: Here, the plugin takes a
    snapshot of the existing volume and then creates a new volume in the new availability
    zone from the snapshot. Once complete, the previous volume is destroyed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is important to note that Docker only supports singular access to a relocatable
    EBS-backed volume—that is, there should only ever be a single container that reads/writes
    to the volume at any given time. If you require shared access to a volume, then
    you must create an EFS-backed shared volume.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s define a volume called `data` to store the todobackend database,
    and create a `db` service that will run MySQL and attach to the `data` volume:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: First, we create a volume called `data` and configure the driver as `cloudstor:aws`.
    In the driver options, we specify a backing of relocatable to create an EBS volume,
    specifying a size of 10 GB and an EBS type of `gp2` (SSD) storage. We then define
    a new service called `db` that runs the official MySQL 5.7 image, attaching the
    `db` service to the previously defined net network and mounting the data volume
    at `/var/lib/mysql`, which is where MySQL stores its database. Note that because
    the Cloudstor plugin formats the mounted volume as `ext4`, a folder called `lost+found`
    is automatically created during the formatting process, which causes the [MySQL
    container to abort](https://github.com/docker-library/mysql/issues/69#issuecomment-365927214)
    as it thinks an existing database called `lost+found` is present.
  prefs: []
  type: TYPE_NORMAL
- en: To overcome this, we pass in a single flag called `--ignore-db-dir` that references
    the `lost+found` folder, which is passed to the MySQL image entrypoint and configures
    the MySQL daemon to ignore this folder.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we define a placement constraint that will force the `db` service to
    be deployed to the Swarm manager, which will allow us to test the relocatable
    features of the data volume by changing this placement constraint to a worker
    later on.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you now deploy the stack and monitor the `db` service, you should observe
    that the service takes some time to come up while the data volume is initializing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'To verify, an EBS volume has actually been created, you can use the AWS CLI
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Note that EBS volumes created by the Cloudstor plugin are tagged with a key
    of `CloudstorVolumeName` and a value of the Docker Swarm volume name. In the preceding
    example, you can also see that the volume has been created in the us-east-1b availability
    zone.
  prefs: []
  type: TYPE_NORMAL
- en: Relocating an EBS volume
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that you have successfully created and attached an EBS-backed data volume,
    let''s test migrating the `db` service from the manager node to the worker node
    by changing its placement constraint:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'If you now deploy your changes, you should be able to observe the EBS relocation
    process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: We first define a `volumes` query that displays the current Cloudstor volume
    status, and a `snapshots` query that displays any EBS snapshots that are in progress.
    After deploying the placement constraint change, we run the volumes query several
    times and observe the current volume located in `us-east-1b`, transition to a
    state of `detaching`, and to a state of `None` (detached).
  prefs: []
  type: TYPE_NORMAL
- en: We then run the snapshot query where you can see that a snapshot is being created
    for the volume that was just detached, and once this snapshot is complete, we
    run the volumes query several times to observe that the old volume is removed
    and a new volume is created in `us-east-1a`, which is then attached. At this point,
    the `todobackend_data` volume has been relocated from the manager in `us-east-1b`
    to `us-east-1a`, and you can verify that the `db` service is now up and running
    again by executing the `docker service ps` command.
  prefs: []
  type: TYPE_NORMAL
- en: Because the Docker for AWS CloudFormation template creates separate auto scaling
    groups for managers and workers, there is a possibility the manager and worker
    are running in the same subnet and availability zone, which will change the behavior
    of the example above.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we proceed to the next section, we actually need to tear down our stack
    as the current password management strategy of using cleartext passwords in our
    stack file is not ideal, and our database has already been initialized with these
    passwords:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Remember that, whenever you tear down a stack, you must remove any volumes you
    may have used in that stack manually.
  prefs: []
  type: TYPE_NORMAL
- en: Secrets management using Docker secrets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the preceding examples when we created the `db` service, we didn't actually
    configure the application to integrate with the `db` service, as although we were
    focusing on how to create persistent storage, another reason I did not integrate
    the `app` service with the `db` service is because we are currently configuring
    passwords for the `db` service in plaintext, which is not ideal.
  prefs: []
  type: TYPE_NORMAL
- en: Docker Swarm includes a feature called Docker secrets, which provide a secure
    secrets management solution for providing secrets to applications running on your
    Docker Swarm clusters. Secrets are stored in an internal encrypted storage mechanism
    called the *raft log*, which is replicated to all nodes in your cluster, ensuring
    that any service and associated containers that are granted access to a secret
    can access the secret securely.
  prefs: []
  type: TYPE_NORMAL
- en: 'To create a Docker secret, you can use the `docker secret create` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding example, we use the `openssl rand` command to generate random
    secrets in Base64 format, which we then pass as standard input to the `docker
    secret create` command. We create 32 character secrets for the todobackend user's
    MySQL password and MySQL root password, and finally create a secret of 50 characters
    for the Django `SECRET_KEY` setting that is required for cryptographic operations
    performed by the todobackend application.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have created several secrets, we can configure our stack to consume
    these secrets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: We first declare the top level `secrets` parameter, specifying the names of
    each of the secrets we previously created and configuring each secret as `external`,
    given we created the secrets outside of the stack. If you don't use external secrets,
    you must define your secrets in a file, which does not solve the issue of storing
    passwords securely outside of your stack definitions and configurations, so creating
    your secrets as separate entities independent of your stack is much more secure.
  prefs: []
  type: TYPE_NORMAL
- en: 'We then reconfigure the `app` service to consume each secret via the `secrets`
    property. Notice that we specify a target of `MYSQL_PASSWORD` and `SECRET_KEY`.
    Whenever you attach a secret to a service, an in-memory tmpfs-backed mount will
    be created at `/run/secrets`, with each secret stored at the location `/run/secrets/<target-name>`,
    so for the `app` service, the following secrets will be mounted:'
  prefs: []
  type: TYPE_NORMAL
- en: '`/run/secrets/MYSQL_PASSWORD`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`/run/secrets/SECRET_KEY`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will learn how to configure our application to consume these secrets later
    on, but also note that we configure the `MYSQL_HOST` and `MYSQL_USER` environment
    variables so that our application knows how to connect to the `db` service and
    which user to authenticate as.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we configure the `db` service to consume the MySQL password and root
    password secrets, and here we configure the targets for each secret so that the
    following secrets are mounted in the `db` service container:'
  prefs: []
  type: TYPE_NORMAL
- en: '`/run/secrets/mysql_password`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`/run/secrets/mysql_root_password`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, we remove the `MYSQL_PASSWORD` and `MYSQL_ROOT_PASSWORD` environment
    variables from the `db` service and replace these with their file-based equivalents,
    referencing the path to each of the configured secrets.
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, if you deploy the newly updated stack (if you haven''t previously
    removed the stack, you will need to do this prior to ensure that you can recreate
    the database with the new credentials), once your todobackend services have started
    successfully, you can determine the container ID of the `app` service instance
    running on the Swarm manager by running the `docker ps` command, after which you
    can examine the contents of the `/run/secrets` directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, the secrets you created earlier are available in the `/run/secrets`
    folder, and if you now browse to the `/todos` path on the external load balancer
    URL where your application is published, unfortunately you will receive an `access
    denied` error:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/bade4a1e-0aa5-4a04-870a-04c2dbed709d.png)'
  prefs: []
  type: TYPE_IMG
- en: Database authentication error
  prefs: []
  type: TYPE_NORMAL
- en: The problem here is that although we have mounted the database secret in the
    `app` service, our todobackend application does not know how to consume these
    secrets, so we need to make some modifications to the todobackend application
    to be able to consume these secrets.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring applications to consume secrets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In previous chapters, we have used an entrypoint script to add support for features
    such as injecting secrets at container startup, however an equally valid (and
    actually better and more secure) approach is to configure your application to
    natively support your secrets management strategy.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the case of Docker secrets, this is very straightforward, given that the
    secrets are mounted at a well-known location (`/run/secrets`) in the local filesystem
    of the container. The following demonstrates modifying the `src/todobackend/settings_release.py`
    file in the `todobackend` repository to support Docker secrets, which, as you
    should recall, are the settings we pass to our `app` service, as specified by
    the environment variable configuration `DJANGO_SETTINGS_MODULE=todobackend.settings_release.`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: We first create a simple function called `secret()`, which takes as input the
    name of the setting or `key`, and an optional default value if the secret cannot
    be found. This function then attempts to look up the path `/run/secrets` (this
    can be overridden by setting the environment variable `SECRETS_ROOT`) and looks
    for a file with the same name as the requested key. If this file is found, the
    contents of the file is read using the `f.read().rstrip()` call, with the `rstrip()`
    function stripping the line break that is returned by the `read()` function. Otherwise,
    the function looks for an environment variable with the same name of key, and
    if all of these lookups fail, it returns the `default` value that was passed to
    the `secret()` function (which itself has a default value of `None`).
  prefs: []
  type: TYPE_NORMAL
- en: 'With this function in place, we can then simply call the secret function, as
    demonstrated for the `SECRET_KEY` and `DATABASES[''PASSWORD'']` settings, and
    using the `SECRET_KEY` setting as an example, the function will return in the
    following order of preference:'
  prefs: []
  type: TYPE_NORMAL
- en: Value of the contents of the `/run/secrets/SECRET_KEY`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Value of the environment variable `SECRET_KEY`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Value of the default value passed to the `secrets()` function (in this case,
    the `SECRET_KEY` setting imported from the base settings file)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now that we have updated the todobackend application to support Docker secrets,
    you need to commit your changes and then test, build, and publish your changes.
    Note that will need to do this in a separate shell that is connected to your local
    Docker Engine (rather than your Docker Swarm cluster):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Once your image has been successfully published, switch back to your Terminal
    session that is connected to your Swarm cluster and redeploy your stack using
    the `docker stack deploy` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: If you run the `docker service ps` command as demonstrated in the preceding
    example, you may notice that your todobackend service is not redeployed (note
    in some cases the service may be redeployed). The reason for this is that we are
    using the latest image by default in our stack file. To ensure that we can continuously
    deliver and deploy our application, we need to reference a specific version or
    build tag, which is the best practice approach you should always take, as it will
    force an explicit version of your image to be deployed on each service update.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do this with our local workflow, we can leverage the `Makefile` that already
    exists in the todobackend application repository and include an `APP_VERSION`
    environment variable that returns the current Git commit hash, which we can subsequently
    reference in our stack file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'With this configuration in place, we now need to add a deploy recipe to the
    `Makefile` in the root of the `todobackend` repository, which will automatically
    make the `APP_VERSION` environment variable available to the Docker client when
    it parses the stack file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'The `deploy` recipe references the `login` recipe, ensuring that we always
    run the equivalent of `make login` first before running the tasks in the `deploy`
    recipe. This recipe simply runs the `docker stack deploy` command so that we can
    now deploy the updates to our stack by running `make deploy`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Because our stack is now configured with a specific image tag, as defined by
    the `APP_VERSION` variable (`3db46c4` in the preceding example), a change is detected
    and the `app` service is updated. You can confirm this using the `docker service
    ps` command, as demonstrated previously, and recall that we have configured this
    service to update a single instance at a time with a 30 second delay between each
    update.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you now browse to the `/todos` path on the external load balancer URL, the
    authentication error should now be replaced with a `table does not exist` error,
    which proves that we are now able to at least connect to the database, but haven''t
    yet dealt with database migrations as part of our Docker Swarm solution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/4421dad8-2e82-412a-9d29-3122345044ae.png)'
  prefs: []
  type: TYPE_IMG
- en: Database error
  prefs: []
  type: TYPE_NORMAL
- en: Running database migrations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have established a mechanism to securely access the db service
    in our stack, the final configuration task we need to perform is to add a service
    that will run database migrations. This is similar to the collectstatic service
    we created earlier, in that it needs to be a "one-shot" task that only executes
    whenever we create the stack or whenever we deploy a new version of the application:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: All of the settings for the new `migrate` service should be self-explanatory,
    as we've configured them previously for other services. The `deploy` configuration
    is especially important and is configured identically to the other one-shot collectstatic
    service, where Docker Swarm will attempt to ensure a single replica of the `migrate`
    service is able to start successfully up to six times with a delay of 30 seconds
    between each attempt.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you now run `make deploy` to deploy your changes, the `migrate` service
    should be able to complete successfully:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'To verify that the migrations actually ran, because we enabled CloudWatch logs
    when we created the Docker Swarm cluster, you can review logs for the `migrate`
    service in the CloudWatch logs console. When using the Docker for AWS solution
    templates to deploy your cluster, a single log group called `<cloudformation-stack-name>-lg`
    is created, which in our case is `docker-swarm-lg`. If you open this log group
    in the CloudWatch logs console, you will see that log streams exist for every
    container that is running or has run in the Swarm cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/54f50ec8-9ee0-4ceb-878e-ac7caf4c352b.png)'
  prefs: []
  type: TYPE_IMG
- en: Deploying the migrate service
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see that the most recent log stream relates to the `migrate` service,
    and if you open this log stream, you can confirm that database migrations ran
    successfully:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/28559836-2823-487e-95d2-492c1db559e8.png)'
  prefs: []
  type: TYPE_IMG
- en: The migrate service log stream
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, your application should be running successfully, and you should
    be able to interact with the application to create, update, view, and delete Todo
    items. One good way to verify this, which could be used as a strategy for automated
    post-deployment testing, is to run the acceptance tests you created in earlier
    chapters that are included in the todobackend release image, ensuring that you
    pass in the external load balancer URL via the `APP_URL` environment variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: You have now successfully deployed the todobackend application to a Docker Swarm
    cluster running in AWS, and I encourage you to further test that your application
    is production ready by tearing down/recreating the stack, and running a few example
    deployments by making test commits and creating new application versions to deploy.
  prefs: []
  type: TYPE_NORMAL
- en: Once complete, you should commit the changes you have made, and don't forget
    to destroy your Docker Swarm cluster by deleting the `docker-swarm` stack in the
    CloudFormation console.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned how to deploy Docker applications using Docker
    Swarm and the Docker for AWS solution. Docker for AWS provides a CloudFormation
    template that allows you to set up a Docker Swarm cluster within minutes, and
    also provides integration with AWS services including the Elastic Load Balancer
    service, Elastic File System, and Elastic Block Store.
  prefs: []
  type: TYPE_NORMAL
- en: After creating a Docker Swarm cluster, you learned how to establish remote access
    to a Swarm manager for your local Docker clients by configuring an SSH tunnel,
    which links to the `/var/run/docker.sock` socket file on your Swarm manager and
    presents it as a local endpoint your Docker client can interact with. This makes
    the experience of managing your Swarm clusters similar to managing your local
    Docker Engine.
  prefs: []
  type: TYPE_NORMAL
- en: You learned how to create and deploy Docker services, which typically represent
    a long-running application, but may also represent one-shot tasks like running
    database migrations or generating static content files. Docker stacks represent
    complex multi-service environments, and are defined using the Docker Compose version
    3 specification and deployed using the `docker stack deploy` command. One advantage
    of using Docker Swarm is access to the Docker secrets feature, which allows you
    to store secrets securely in the encrypted raft log that is automatically replicated
    and shared across all nodes in the cluster. Docker secrets can then be exposed
    to services as in-memory tmpfs mounts at `/run/secrets`. You have already learned
    how easy it is to configure your applications to integrate with the Docker secrets
    feature.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, you learned how to address common operational challenges associated
    with running your containers in production, such as how to provide access to durable,
    persistent storage in the form of EBS volumes that can be automatically relocated
    with your containers, how to provide access to shared volumes using EFS, and how
    to orchestrate deployment of new application features , supporting the ability
    to run one-shot tasks and rolling upgrades of your application services.
  prefs: []
  type: TYPE_NORMAL
- en: In the next and final chapter of this book, you will be introduced to the AWS
    Elastic Kubernetes Service (EKS), which was launched mid 2018 and provides support
    for Kubernetes, the leading open source container management platform that competes
    with Docker Swarm.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'True/false: Docker Swarm is a native feature of the Docker Engine.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What Docker client command do you use to create a service?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'True/false: Docker Swarm includes three node types—manager, worker, and agent.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'True/false: Docker for AWS provides integration with AWS application load balancers.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'True/false: The Cloudstor AWS volume plugin creates an EFS-backed volume when
    the backing is set to relocatable.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'True/false: You create a database service that uses the Cloudstor AWS volume
    plugin to provide an EBS-backed volume that is located in the availability zone
    us-west-1a. A failure occurs and a new database service container is created in
    the availability zone us-west-1b. In this scenario, the original EBS volume will
    be reattached to the new database service container.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the flag you need to append to Docker Stack deploy and Docker service
    create commands to integrate with private Docker registries?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You deploy a stack that downloads an image from ECR. The first deployment succeeds,
    however when you attempt to perform a new deployment the next day, you notice
    that your Docker swarm nodes cannot pull the ECR image. How can you fix this?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Which version of the Docker Compose specification should you use for defining
    Docker Swarm stacks?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'True/false: When configuring a single shot service, you should configure a
    restart policy as always.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can check the following links for more information about the topics covered
    in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Docker Community Edition for AWS: [https://store.docker.com/editions/community/docker-ce-aws](https://store.docker.com/editions/community/docker-ce-aws)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Docker for AWS documentation: [https://docs.docker.com/docker-for-aws](https://docs.docker.com/docker-for-aws)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Docker Compose file version 3 reference: [https://docs.docker.com/compose/compose-file/](https://docs.docker.com/compose/compose-file/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Docker for AWS persistent data volumes: [https://docs.docker.com/docker-for-aws/persistent-data-volumes/](https://docs.docker.com/docker-for-aws/persistent-data-volumes/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Docker for AWS template archive: [https://docs.docker.com/docker-for-aws/archive/](https://docs.docker.com/docker-for-aws/archive/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Managing sensitive data with Docker secrets: [https://docs.docker.com/engine/swarm/secrets/](https://docs.docker.com/engine/swarm/secrets/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Docker command-line reference: [https://docs.docker.com/engine/reference/commandline/cli/](https://docs.docker.com/engine/reference/commandline/cli/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Docker Get Started - Part 4: Swarms: [https://docs.docker.com/get-started/part4/](https://docs.docker.com/get-started/part4/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Docker Get Started - Part 5: Stacks: [https://docs.docker.com/get-started/part5](https://docs.docker.com/get-started/part5/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Docker for AWS Swarm ECR auto login: [https://github.com/mRoca/docker-swarm-aws-ecr-auth](https://github.com/mRoca/docker-swarm-aws-ecr-auth)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SSH agent forwarding: [https://developer.github.com/v3/guides/using-ssh-agent-forwarding/](https://developer.github.com/v3/guides/using-ssh-agent-forwarding/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
