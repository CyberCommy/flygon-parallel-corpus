- en: Chapter 8. Exploring Additional Features of Swarm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we''re going to discuss and deepen our knowledge on two very
    important topics related to Docker and orchestration systems: networking and consensus.
    In particular, we''ll see how to:'
  prefs: []
  type: TYPE_NORMAL
- en: Foundations of Libnetwork
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Basic security of Libnetwork
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Routing mesh
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overlay networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Network Control Plane
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Libkv
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Libnetwork
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Libnetwork is the networking stack designed from the ground-up to work with
    Docker regardless of platforms, environments, operating systems, or infrastructures.
    Libnetwork is not only an interface for the network driver. It's not only a library
    to manage VLAN or VXLAN networks but it does more.
  prefs: []
  type: TYPE_NORMAL
- en: 'Libnetwork is a full networking stack and consists of three planes, the **Management
    Plane**, the **Control Plane**, and the **Data Plane** as shown in the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Libnetwork](images/image_08_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The **Management Plane** allows users, operators, or tools to manage the network
    infrastructure. These operations include network monitoring. The Management Plane
    represents the Docker network user experiences, provides the APIs. It's also extensible
    via management plugins, such as IPAM plugins, which, for example, allows us to
    control how we assign IP addresses to each container.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **Control Plane** is implemented in the -scoped gossip protocol, service-discovery,
    encryption key distribution is added directly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In brief, the **Data Plane** is responsible for moving network packets between
    two Endpoints. Network plugins work for each Data Plane. By default, there are
    a few built-in drivers. For example, the overlay driver we met in the previous
    chapters directly uses the features inside Linux and Windows kernels, so there
    is no driver code for this kind of network. This is also applied for bridge, IPVLAN,
    and MacVLAN drivers. In contrast, other third-party networks need their own implementation
    in the form of plugins.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Following the usual Docker UX, that states the components should just work on
    any environment, also the networking stack must be portable. And to make Docker's
    networking stack portable, its design and implementation must be solid. For example,
    the Management Plane cannot be controlled by any other component. Also, the Control
    Plane cannot be replaced by other components. If we allowed that, the networking
    stack would break when we change our application environment from one to another.
  prefs: []
  type: TYPE_NORMAL
- en: Networking plugins
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Data Plane is designed to be pluggable. In fact, it can be only managed
    by built-in or external plugins. For example, MacVLAN was implemented as a plugin
    into Docker 1.12 without affecting other parts of the system.
  prefs: []
  type: TYPE_NORMAL
- en: The most remarkable thing is that we can have several Drivers and plugins on
    the same networking stack they can work without interfering with one another.
    So typically, in Swarm, we can have an overlay network, a bridge network as well
    as a host driver running on the same cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Container Networking Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Libnetwork is designed and implemented to serve the Docker Swarm requirements
    to run Docker''s distributed applications. That is, Libnetwork is actually the
    Docker Networking Fabric. The foundation of Libnetwork is a model called **Container
    Networking Model** (**CNM**). It is a well-defined basic model that describes
    how containers connect to the given networks. The CNM consists of three components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Sandbox**: This is an isolation containing the configuration of the network
    stack of the container.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Endpoint**: This is a connection point that only belongs to a network and
    a sandbox.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Network**: This is a group of endpoints which allowed to community freely
    among them. A network consists of one or more endpoints.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Drivers represent the Data Plane. Every Driver, being overlay, bridge, or
    MacVLAN are in the form of Plugins. Each plugin works in a Data Plane specific
    to it.
  prefs: []
  type: TYPE_NORMAL
- en: In the system, there is a built-in IPAM by default. This is an important issue
    because each container must have an IP address attached. So it's necessary to
    have an IPAM system built-in, which allows each container to be able to connect
    to each otheras we did in the traditional way and we need an IP address for others
    to talk to the container. We also require to define subnets as well as ranges
    of IP addresses. Also, the system is designed for IPAM to be pluggable. This means
    that it allows us to have our own DHCP drivers or allow plumbing the system to
    an existing DHCP server.
  prefs: []
  type: TYPE_NORMAL
- en: As previously mentioned, Libnetwork supports multihost networking out-of-the-box.
    Components worth to discuss for the multihost networking are its Data and Control
    Planes.
  prefs: []
  type: TYPE_NORMAL
- en: The Control Plane currently included in Docker 1.12 uses the gossip mechanism
    as the general discovery system for nodes. This gossip protocol-based network
    works on another layer in parallel of the Raft consensus system. Basically, we
    have twodifferent membership mechanisms work at the same time. Libnetwork allows
    the driver from other plugins to commonly use the control plane.
  prefs: []
  type: TYPE_NORMAL
- en: 'These are the features of Libnetwork''s Control plane:'
  prefs: []
  type: TYPE_NORMAL
- en: It's secure and encrypted out-of-the-box
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Every single data plane can use it
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It provides native service discovery and load balancing out-of-the-box
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Docker 1.12 implements VIP-based service discovery in Swarm. This service works
    by mapping a Virtual IP address of the container to the DNS records. Then all
    DNS records are shared via gossip. In Docker 1.12, with the introduction of the
    concept of service, this notion fits directly to the concept of discovery.
  prefs: []
  type: TYPE_NORMAL
- en: In Docker 1.11 and previous versions, it was necessary instead to use container
    names and aliases to "simulate" service discovery and do DNS roundrobin to perform
    some kind of primitive load balancing.
  prefs: []
  type: TYPE_NORMAL
- en: Libnetwork carries on the principle of battery included but removable, which
    is implemented as the plugin system. In the future, Libnetwork will gradually
    expand the plugin system to cover other networking parts, for example, load balancing.
  prefs: []
  type: TYPE_NORMAL
- en: Encryption and routing mesh
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The model at the heart of Libnetwork is CNM, as previously mentioned. In Swarm
    mode, libnetwork is built in a cluster-aware mode and supports multi-host networking
    without external key value stores. The overlay network fits naturally in this
    model. And both Data plane and Control plane encryption has been introduced. With
    encrypted Control Plane, routing information on VXLAN, for example, for which
    container has which MAC address and which IP address, is automatically secured.
    Also, with Routing Mesh, CNM provides a decentralized mechanism allowing you to
    access services from any IP of the cluster. When a request comes from the outsideand
    hits any node of the cluster, the traffic will be routed to a working container.
  prefs: []
  type: TYPE_NORMAL
- en: MacVLAN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The new Driver in 1.12 is MacVLAN. MacVLAN is a performant driver designed to
    allow the Docker network to plumb to the existing VLAN, for example, a corporate
    one, letting everything to continue to work. There is a scenario where we will
    gradually migrate workloads from the original VLAN to Docker and MacVLAN will
    help plumb the Docker cluster to the original VLAN. This will make the Docker
    networks integrated with the underlay network and the containers will be able
    to work in the same VLAN.
  prefs: []
  type: TYPE_NORMAL
- en: We could just create a network with the MacVLAN driver and specify the real
    subnet to the network. We can also specify a range of IP addresses only for the
    containers. Also, we can exclude some IP addresses, for example, the gateway,
    from assigning to containers with `--aux-address`. The parent interface of the
    MacVLAN driver is the interface we would like to connect this network to. As previously
    mentioned, MacVLAN yields the best performance of all drivers. Its Linux implementation
    isextremely lightweight. They just enforce the separation between networks and
    connection to the physical parent network, rather than implemented as traditional
    Linux bridge for network isolation. The use of MacVLAN driver requires Linux Kernel
    3.9 - 3.19 or 4.x.
  prefs: []
  type: TYPE_NORMAL
- en: Overlay networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Because Swarm cluster is now a native feature built into the Docker Engine,
    this allows the creation of overlay networks very easy without using external
    key-value stores.
  prefs: []
  type: TYPE_NORMAL
- en: Manager nodes are responsible for managing the state of the networks. All the
    networking states are kept inside the Raft log. The main difference between Raft
    implementation in the Swarm mode and the external key-value store is that the
    embedded Raft has far higher performance than the external ones. Our own experiments
    confirmed that the external key-value store will stick around 100-250 nodes, while
    the embedded Raft helped us scale the system to 4,700 nodes in the Swarm3k event.
    This is because the external Raft store basically has high network latency. When
    we need to agree on some states, we will be incurred from the network round-trips,
    while the embedded Raft store is just there in memory.
  prefs: []
  type: TYPE_NORMAL
- en: In the past, when we wanted to do any network-related action, assigning IP address
    to the containers, for example, significant network latency happened as we always
    talk to the external store. For the embedded Raft, when we would like to have
    a consensus on values, we can do it right away with the in-memory store.
  prefs: []
  type: TYPE_NORMAL
- en: '![Overlay networks](images/image_08_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'When we create a network with the overlay driver, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The command will talk to the allocator. Then there will be a subnet reservation,
    in this case `10.9.0.0/24`, and agree related values right away in the manager
    host in its memory once it''s allocated. We would like to create a service after
    that. Then we will later connect that service to the network. When we create a
    service, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The orchestrator creates a number of tasks (containers) for that service. Then
    each created task will be assigned an IP address. The allocation will be working
    again during this assignment.
  prefs: []
  type: TYPE_NORMAL
- en: 'After the task creation is done:'
  prefs: []
  type: TYPE_NORMAL
- en: The task gets an IP address
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Its network-related information will be committed into the Raft log store
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After the commit is done by the allocation, the scheduler will be moving the
    task to another state
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Dispatcher dispatches each task to one of the worker nodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, the container associated to that task will be running on the Docker
    Engine
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If a task is not able to allocate its network resource, it will be stuck there
    at the allocated state and will not be scheduled. This is the important difference
    from the previous versions of Docker that in the network system of Swarm mode,
    the concept of allocation state is obvious. With this, it improves the overall
    allocation cycle of the system a lot. When we talk about the allocation, we refer
    not only to the allocation of IP addresses, but also to related driver artifacts.
    For an overlay network, it needs to reserve a VXLAN identifier, which is a set
    of global identifiers for each VXLAN. This identifier reservation is done by the
    Network Allocator.
  prefs: []
  type: TYPE_NORMAL
- en: In the future, for a plugin to do the same allocation mechanism, it will be
    enough to implement only some interfaces and make the state being automatically
    managed by Libnetwork and stored into the Raft log. With this, the resource allocation
    is in the centralized way, so we can achieve consistency and consensus. With consensus,
    we need a highly efficient consensus protocol.
  prefs: []
  type: TYPE_NORMAL
- en: Network Control Plane
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Network Control Plane is a subsystem of Libnetwork to manage routing information
    and we need a protocol that converge quickly to do that job. For example, Libnetwork
    does not use BGP as the protocol (despite that BGP is great at scalability to
    support very large number of endpoints), because point BGP won't converge quick
    enough to use in the highly dynamic environment such as the software container
    environment.
  prefs: []
  type: TYPE_NORMAL
- en: In a container-centric world, the networking system is expected to change very
    quickly, especially for the new Docker service model, which requires a massive
    and fast IP assignation. We want the routing information to converge very rapidly
    as well, especially at a big scale, for example, for more than 10,000 containers.
    In Swarm2k and Swarm3k experiments, we really did start 10,000 containers at a
    time. Especially, in Swarm3k, we started 4,000 NGINX containers on the Ingress
    load-balancing network. Without a fine implementation, this number of scale won't
    work correctly.
  prefs: []
  type: TYPE_NORMAL
- en: 'To solve this problem, the Libnetwork team chose to include the gossip protocol
    in the Network Control Plane. The internal algorithm of the protocol works like
    this: It choses 3 neighbors and then propagates the same information; in the case
    of Libnetwork, the routing and other network related information. The Gossip protocol
    will do this process repeatedly, until every node shares the same information.
    With this technique, the whole cluster will receive the information very quickly,
    in a matter of seconds.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Network Control Plane](images/image_08_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Anyway, the whole cluster does not need the same information all the time. Every
    node on the cluster does not need to know information of all the networks. Only
    nodes in a particular network need to know its own networking information. To
    optimize this for Libnetwork, the team implemented two scopes, *Cluster Scoped
    Gossip Communication* and *Network Scoped Gossip Communication*. What we have
    explained so far is the Cluster Scope Gossip Communication, while Network Scoped
    Gossip Communication limits the network information within a particular network.
    When a network expands to cover addition nodes, its gossip scoped broadcast will
    also cover them.
  prefs: []
  type: TYPE_NORMAL
- en: This activity is built on top Docker's CNM and therefore relieson the network
    abstraction. From the Figure, we have node **w1**, **w2**, and **w3** in the Left
    network and also **w3**, **w4**, **w5** in the right network. The left network
    performs gossip and only **w1**, **w2**, **w3** would know its routing information.
    You may observe that w3 is in both the networks. Therefore, it will receive routing
    information of all left and right networks.
  prefs: []
  type: TYPE_NORMAL
- en: Libkv
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`libkv` is a unified library to interact with different key-value store backends.
    `libkv` was originally part of Docker Swarm v1 in the very first versions of the
    development. Later, all code related to key-value store discovery services was
    refactored and moved to [www.github.com/docker/libkv](https://github.com/docker/libkv).'
  prefs: []
  type: TYPE_NORMAL
- en: '`libkv` allows you to execute CRUD operations and also to watch key-value entries
    from different backends, so we can use the same code to work with all HA distributed
    key-value stores, which are **Consul**, **Etcd**, and **ZooKeeper** as shown in
    the following figure. At the time of writing, libkv also supports a local store
    implemented using **BoltDB**.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Libkv](images/image_08_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: How to use libkv
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To start with `libkv,` we need to understand how to call its APIs first. Here''s
    the `libkv Store` interface in Go, for every store implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We need to know how to `Put`, `Get`, `Delete`, and `Watch` to basically interact
    with a store.
  prefs: []
  type: TYPE_NORMAL
- en: 'Make sure you also have Go and Git installed on your machine and the Git executable
    is on your PATH. Then, we need to do a number of go get to install dependencies
    for our program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Here we provide with a skeleton. You need to start a single-node `Consul` before
    you try to run the following program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: You can also test getting your value with curl. The value you've put should
    be there. We should continue playing with the libkv APIs,which are `Get` and `Delete`.
    It's left for the readers as an exercise.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers Libnetwork, one of the most important parts of Docker Swarm.
    We have discussed its Management Plane, Control Plane, and Data Plane. This chapter
    also includes some techniques on how to use `libkv`, a key-value abstraction to
    implement your own service discovery system. In the next chapter, we'll focus
    on security. In the next chapter, we will learn how to secure a swarm cluster.
  prefs: []
  type: TYPE_NORMAL
