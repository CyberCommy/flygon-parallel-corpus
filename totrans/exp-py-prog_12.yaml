- en: Chapter 12. Optimization – Some Powerful Techniques
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第12章 优化-一些强大的技术
- en: 'Optimizing a program is not a magical process. It is done by following a simple
    algorithm, synthesized by Stefan Schwarzer at Europython 2006 in his original
    pseudocode example:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 优化程序并不是一个神奇的过程。它是通过遵循一个简单的算法完成的，由Stefan Schwarzer在Europython 2006中合成的原始伪代码示例：
- en: '[PRE0]'
  id: totrans-2
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This example probably isn''t the neatest and clearest one but captures pretty
    much all the important aspects of an organized optimization procedure. The main
    things we learn from it are:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子可能不是最整洁和最清晰的例子，但基本上涵盖了组织优化过程的所有重要方面。我们从中学到的主要内容是：
- en: Optimization is an iterative process where not every iteration leads to better
    results
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化是一个迭代过程，不是每一次迭代都会带来更好的结果
- en: The main prerequisite is code that is verified to be working properly with tests
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主要的前提是经过测试验证的代码能够正常工作
- en: You should always focus on optimizing the current application bottleneck
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您应该始终专注于优化当前的应用程序瓶颈
- en: Making your code work faster is not an easy task. In case of abstract mathematical
    problems, the solution of course lies in choosing the right algorithm and proper
    data structures. But in that case, it is very hard to provide some generic tips
    and tricks that can be used in any code for solving algorithmic issues. There
    are of course some generic methodologies for designing a new algorithm, or even
    meta-heuristics that can be applied to a large variety of problems but they are
    pretty language-agnostic and thus are rather out of scope of this book.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 使您的代码运行更快并不是一件容易的事情。在抽象数学问题的情况下，解决方案当然在于选择正确的算法和适当的数据结构。但在这种情况下，很难提供一些通用的提示和技巧，可以用于解决算法问题的任何代码。当然，有一些通用的方法论用于设计新算法，甚至可以应用于各种问题的元启发式算法，但它们是相当与语言无关的，因此超出了本书的范围。
- en: 'Anyway, some performance issues are only caused by certain code quality defects
    or application usage context. For instance, the speed of the application might
    be reduced by:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 无论如何，一些性能问题只是由特定的代码质量缺陷或应用程序使用上下文引起的。例如，应用程序的速度可能会因为：
- en: Bad usage of basic built-in types
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基本内置类型的错误使用
- en: Too much complexity
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过于复杂
- en: Hardware resource usage patterns not matching with the execution environment
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 硬件资源使用模式与执行环境不匹配
- en: Waiting too long for responses from third-party APIs or backing services
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 等待第三方API或后台服务的响应时间过长
- en: Doing too much in time-critical parts of the application
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在应用程序的时间关键部分做得太多
- en: More often, the solving of such performance issues does not require advanced
    academic knowledge but only good software craftsmanship. And a big part of craftsmanship
    is knowing when to use the proper tools. Fortunately, there are some well-known
    patterns and solutions for dealing with performance problems.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 更多时候，解决这些性能问题并不需要高级的学术知识，而只需要良好的软件工艺。而工艺的一大部分就是知道何时使用适当的工具。幸运的是，有一些处理性能问题的众所周知的模式和解决方案。
- en: 'In this chapter, we will discuss some popular and reusable solutions that allow
    you to non-algorithmically optimize your program through:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论一些流行且可重复使用的解决方案，使您能够通过非算法优化程序：
- en: Reducing the complexity
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 降低复杂性
- en: Using architectural trade offs
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用架构权衡
- en: Caching
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缓存
- en: Reducing the complexity
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 降低复杂性
- en: Before we dig further into optimization techniques, let's define exactly what
    we are going to deal with. From the chapter's introduction, we know that focusing
    on improving application bottlenecks is critical for successful optimization.
    A bottleneck is a single component that severely limits the capacity of a program
    or computer system. An important characteristic of every piece of code with performance
    issues is that it usually has only a single bottleneck. We discussed some profiling
    techniques in the previous chapter, so you should already be familiar with the
    tools required to locate and isolate such places. If your profiling results show
    that there are few places that need immediate improvement, then you should at
    first try to treat each as a separate component and optimize independently.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们进一步探讨优化技术之前，让我们明确定义我们要处理的内容。从本章的介绍中，我们知道专注于改进应用程序瓶颈对于成功的优化至关重要。瓶颈是严重限制程序或计算机系统容量的单个组件。每个具有性能问题的代码的一个重要特征是它通常只有一个瓶颈。我们在上一章中讨论了一些分析技术，所以您应该已经熟悉了定位和隔离这些地方所需的工具。如果您的分析结果显示有一些地方需要立即改进，那么您应该首先尝试将每个地方视为一个独立的组件并进行独立优化。
- en: Of course, if there is no explicit bottleneck but your application still performs
    under your expectations, then you are really in a bad position. The gains of the
    optimization process are proportional to the performance impact of optimized bottlenecks.
    Optimizing every small component that does not substantially contribute to the
    overall execution time or resource consumption will only give you minimal benefit
    for all the time spent on profiling and optimization. If your application does
    not seem to have real bottlenecks, there is a possibility that you have missed
    something. Try using different profiling strategies or tools or look at it from
    a different perspective (memory, I/O operations, or network throughput). If that
    does not help, you should really consider revising your software architecture.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，如果没有明显的瓶颈，但您的应用程序仍然表现不符合您的期望，那么您真的处于一个糟糕的位置。优化过程的收益与优化瓶颈的性能影响成正比。优化每个不会对整体执行时间或资源消耗产生实质性贡献的小组件，只会让您在分析和优化上花费的时间获益微薄。如果您的应用程序似乎没有真正的瓶颈，有可能是您遗漏了某些东西。尝试使用不同的分析策略或工具，或者从不同的角度（内存、I/O操作或网络吞吐量）来看待它。如果这并没有帮助，您应该真正考虑修改您的软件架构。
- en: But if you have successfully found a single and integral component that limits
    your application performance, then you are really lucky. There is high chance
    that with only minimal code improvement, you will be able to really improve code
    execution time and/or resource usage. And the gain from optimization will, again,
    be proportional to the bottleneck size.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，如果您成功找到了限制应用程序性能的单个完整组件，那么您真的很幸运。很有可能，只需进行最小的代码改进，您就能真正提高代码执行时间和/或资源使用率。而优化的收益将再次与瓶颈的大小成正比。
- en: The first and most obvious aspect to look after when trying to improve application
    performance is complexity. There are many definitions of what makes a program
    complex and many ways to express it. Some complexity metrics can provide objective
    information about how the code behaves and such information can sometimes be extrapolated
    into performance expectations. An experienced programmer can even reliably guess
    how two different implementations will perform in practice knowing their complexities
    and realistic execution contexts.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在尝试提高应用程序性能时，首要和最明显的方面是复杂性。关于程序复杂性有很多定义，也有很多表达方式。一些复杂度度量标准可以提供关于代码行为的客观信息，有时这些信息可以推断出性能期望。有经验的程序员甚至可以可靠地猜测两种不同的实现在实践中的性能，知道它们的复杂性和现实的执行环境。
- en: 'The two popular ways to define application complexity are:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 定义应用程序复杂性的两种流行方法是：
- en: '**Cyclomatic** **complexity** that is very often correlated with application
    performance'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 圈复杂度经常与应用程序性能相关联
- en: The Landau notation, also known as **big O** **notation**, that is an algorithm
    classification method that is very useful in objectively judging performance
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Landau符号，也称为大O符号，是一种非常有用的算法分类方法，可以客观地评判性能。
- en: From there, the optimization process may be sometimes understood as a process
    of reducing the complexity. This section provides simple tips for this work by
    simplifying loops. But first of all, let's learn how to measure complexity.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 从那里，优化过程有时可以理解为降低复杂性的过程。本节提供了简化循环的简单技巧。但首先，让我们学习如何测量复杂性。
- en: Cyclomatic complexity
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 圈复杂度
- en: '**Cyclomatic** **complexity** is a metric developed by Thomas J. McCabe in
    1976\. Because of its author, it is very often called **McCabe''s complexity**.
    It measures the number of linear paths through the code. All `if`, `for`, and
    `while` loops are counted to come up with a measure.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 圈复杂度是由Thomas J. McCabe在1976年开发的一个度量标准。因为它的作者，它经常被称为McCabe的复杂度。它衡量了代码中的线性路径数量。所有的if，for和while循环都被计算出一个度量。
- en: 'The code can then be categorized as follows:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 然后可以将代码分类如下：
- en: '| Cyclomatic Complexity | What it means |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 圈复杂度 | 它的含义 |'
- en: '| --- | --- |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| 1 to 10 | Not complex |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| 1到10 | 不复杂 |'
- en: '| 11 to 20 | Moderately complex |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| 11到20 | 中等复杂 |'
- en: '| 21 to 50 | Really complex |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| 21到50 | 真的很复杂 |'
- en: '| More than 50 | Too complex |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| 大于50 | 太复杂 |'
- en: Cyclomatic complexity is rather the code quality score than a metric that objectively
    judges its performance. It does not replace the need for code profiling for finding
    performance bottlenecks. Anyway, code that has high cyclomatic complexity often
    tends to utilize rather complex algorithms that may not perform well with larger
    inputs.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 圈复杂度更多是代码质量评分，而不是客观评判其性能的度量标准。它不能取代寻找性能瓶颈的代码性能分析的需要。无论如何，具有较高圈复杂度的代码往往倾向于使用相当复杂的算法，这些算法在输入数据较大时可能表现不佳。
- en: Although cyclomatic complexity is not a reliable way to judge application performance,
    it has one very nice advantage. It is a source code metric so it can be measured
    with proper tools. This cannot be said about other ways of expressing complexity—the
    big O notation. Thanks to measurability, cyclomatic complexity may be a useful
    addition to profiling that gives you more information about problematic parts
    of the software. Complex parts of code are the first to review when considering
    radical code architecture redesigns.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管圈复杂度不是判断应用程序性能的可靠方法，但它有一个非常好的优势。它是一个源代码度量标准，因此可以用适当的工具来测量。这不能说是关于表达复杂性的其他方式——大O符号。由于可测量性，圈复杂度可能是对性能分析的有用补充，它可以为您提供有关软件问题部分的更多信息。在考虑根本性的代码架构重设计时，复杂的代码部分是首先要审查的。
- en: Measuring McCabe's complexity is relatively simple in Python because it can
    be deduced from its Abstract Syntax Tree. Of course, you don't need to do that
    by yourself. A popular tool that provides cyclomatic complexity measurement for
    Python is `flake8` (with the `mccabe` plugin), which has already been introduced
    in [Chapter 4](ch04.html "Chapter 4. Choosing Good Names"), *Choosing Good Names*.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中测量McCabe的复杂度相对简单，因为它可以从其抽象语法树中推导出来。当然，你不需要自己做这个。一个为Python提供圈复杂度测量的流行工具是flake8（带有mccabe插件），它已经在第4章“选择良好的名称”中介绍过。
- en: The big O notation
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 大O符号
- en: The most canonical method to define function complexity is the **big O notation**
    (see [http://en.wikipedia.org/wiki/Big_O_notation](http://en.wikipedia.org/wiki/Big_O_notation)).
    This metric defines how an algorithm is affected by the size of the input data.
    For instance, does the algorithm scale linearly with the size of the input data
    or quadratically?
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 定义函数复杂性的最经典方法是大O符号。这个度量标准定义了算法如何受输入数据大小的影响。例如，算法是否与输入数据的大小成线性关系还是二次关系？
- en: Calculating the big O notation manually for an algorithm is the best approach
    to get an overview on how its performance is related with the size of the input
    data. Knowing the complexity of your application components gives you the ability
    to detect and focus on the parts that will really slow down the code.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 手动计算算法的大O符号是获得算法性能与输入数据大小关系概览的最佳方法。了解应用程序组件的复杂度使您能够检测并专注于真正减慢代码的部分。
- en: 'To measure the big O notation, all constants and low-order terms are removed
    in order to focus on the portion that really weighs when the input data grows.
    The idea is to try to categorize the algorithm in one of these categories, even
    if it is an approximation:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 为了衡量大O符号，所有常数和低阶项都被移除，以便专注于当输入数据增长时真正起作用的部分。这个想法是尝试将算法归类为这些类别中的一个，即使它是一个近似值：
- en: '| Notation | Type |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| 符号 | 类型 |'
- en: '| --- | --- |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| O(1) | Constant. Does not depend on the input data. |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| O(1) | 常数。不依赖于输入数据。 |'
- en: '| O(n) | Linear. Will grow as "n" grows. |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| O(n) | 线性。随着“n”的增长而增长。 |'
- en: '| O(n log n) | Quasi linear. |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| O(n log n) | 准线性。 |'
- en: '| O(n²) | Quadratic complexity. |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| O(n²) | 二次复杂度。 |'
- en: '| O(n³) | Cubic complexity. |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| O(n³) | 立方复杂度。 |'
- en: '| O(n!) | Factorial complexity. |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| O(n!) | 阶乘复杂度。 |'
- en: For instance, we already know from [Chapter 2](ch02.html "Chapter 2. Syntax
    Best Practices – below the Class Level"), *Syntax Best Practices – below the Class
    Level*, that a `dict` lookup has an average complexity of *O(1)*. It is considered
    constant regardless of how many elements are in the `dict`, whereas looking through
    a list of items for a particular item is *O(n)*.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们已经从[第2章](ch02.html "第2章。类级别以下的语法最佳实践")中知道，`dict`查找的平均复杂度是*O(1)*。无论`dict`中有多少元素，它都被认为是常数，而查找特定项的列表中的元素是*O(n)*。
- en: 'Let''s take another example:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看另一个例子：
- en: '[PRE1]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In that case, the print statement will be executed *n* times. Loop speed will
    depend on `n`, so its complexity expresses using the big O notation will be *O(n)*.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，打印语句将被执行*n*次。循环速度将取决于`n`，因此它的复杂度使用大O符号表示将是*O(n)*。
- en: 'If the function has conditions, the correct notation to keep is the highest
    one:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如果函数有条件，保留的正确符号是最高的：
- en: '[PRE2]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In this example, the function could be *O(1)* or *O(n)*, depending on the test.
    But the worst case is *O(n)*, so whole function complexity is *O(n)*.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，函数可能是*O(1)*或*O(n)*，取决于测试。但最坏情况是*O(n)*，所以整个函数的复杂度是*O(n)*。
- en: When discussing complexity expressed in big O notation, we usually review the
    worst case scenario. While this is the best method to define complexity when comparing
    two independent algorithms, it may not be the best approach in every practical
    situation. Many algorithms change the runtime performance depending on the statistical
    characteristic of input data or amortize the cost of worst case operations by
    doing clever tricks. This is why, in many cases, it may be better to review your
    implementation in terms *of average complexity* or *amortized complexity*.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论用大O符号表示的复杂度时，我们通常会考虑最坏情况。虽然这是在比较两个独立算法的复杂度时最好的方法，但在每种实际情况下可能不是最佳方法。许多算法会根据输入数据的统计特征改变运行时性能，或者通过巧妙的技巧摊销最坏情况操作的成本。这就是为什么在许多情况下，最好以*平均复杂度*或*摊销复杂度*来审查你的实现。
- en: For example, take a look at the operation of appending a single element to Python's
    `list` type instance. We know that `list` in CPython uses an array with overallocation
    for the internal storage instead of linked lists. In case an array is already
    full, appending a new element requires allocation of the new array and copying
    all existing elements (references) to a new area in the memory. If we look from
    the point of the **worst-case complexity**, it is clear that the `list.append()`
    method has *O(n)* complexity. And this is a bit expensive when compared to a typical
    implementation of the linked list structure.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，看一下将单个元素附加到Python的`list`类型实例的操作。我们知道CPython中的`list`使用具有内部存储的过度分配的数组，而不是链表。如果数组已满，附加新元素需要分配新数组，并将所有现有元素（引用）复制到内存中的新区域。如果从**最坏情况复杂度**的角度来看，很明显`list.append()`方法的复杂度是*O(n)*。与链表结构的典型实现相比，这有点昂贵。
- en: But we also know that the CPython `list` type implementation uses overallocation
    to mitigate the complexity of such occasional reallocation. If we evaluate the
    complexity over a sequence of operations, we will see that the *average complexity*
    of `list.append()` is *O(1)* and this is actually a great result.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们也知道CPython的`list`类型实现使用过度分配来减轻这种偶尔重新分配的复杂性。如果我们评估一系列操作的复杂性，我们会发现`list.append()`的*平均复杂度*是*O(1)*，这实际上是一个很好的结果。
- en: 'When solving problems, we often know a lot of details about our input data
    such as its size or statistical distribution. When optimizing the application,
    it is always worth using every bit of knowledge about your input data. Here, another
    problem of worst-case complexity starts to show up. It is intended to show the
    limiting behavior of the function when the input tends toward large values or
    infinity, rather than give a reliable performance approximation for real-life
    data. Asymptotic notation is great when defining the growth rate of a function
    but it won''t give a reliable answer for the simple question: which implementation
    will take less time? Worst-case complexity dumps all those little details about
    both your implementation and data characteristic to show you how your program
    will behave asymptotically. It works for arbitrarily large inputs that you may
    not even need to consider.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在解决问题时，我们通常对输入数据的许多细节有很多了解，比如它的大小或统计分布。在优化应用程序时，始终值得利用关于输入数据的每一个知识点。在这里，最坏情况复杂度的另一个问题开始显现出来。它旨在显示函数在输入趋向于大值或无穷大时的极限行为，而不是为真实数据提供可靠的性能近似值。渐近符号在定义函数的增长率时非常有用，但它不会对一个简单的问题给出可靠的答案：哪种实现会花费更少的时间？最坏情况复杂度会忽略关于你的实现和数据特征的所有细节，以显示你的程序在渐近上的行为。它适用于可能根本不需要考虑的任意大的输入。
- en: For instance, let's assume that you have a problem to solve regarding data consisting
    of *n* independent elements. Let's suppose also that you know two different ways
    to solve this problem—*program A* and *program B*. You know that *program A* requires
    100n² operations to finish and *program B* requires 5n³ operations to give the
    problem a solution. Which one would you choose? When speaking about very large
    inputs, *program A* is of course the better choice because it behaves better asymptotically.
    It has *O(n²**)* complexity compared to *O(n³**)* complexity that characterizes
    *program B*.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设您有一个关于由*n*个独立元素组成的数据的问题要解决。再假设您知道两种不同的解决这个问题的方法——*程序A*和*程序B*。您知道*程序A*需要100n²次操作才能完成，而*程序B*需要5n³次操作才能给出问题的解决方案。您会选择哪一个？当谈论非常大的输入时，*程序A*当然是更好的选择，因为它在渐近上表现更好。它的复杂度是*O(n²)*，而*程序B*的复杂度是*O(n³)*。
- en: But by solving a simple 100 n² > 5 n³ inequality, we can find that *program
    B* will take fewer operations when *n* is less than 20\. If we know a bit more
    about our input bounds, we can make slightly better decisions.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 但是通过解决一个简单的100 n² > 5 n³不等式，我们可以发现当*n*小于20时，*程序B*将需要更少的操作。如果我们对输入范围有更多了解，我们可以做出稍微更好的决策。
- en: Simplifying
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简化
- en: To reduce the complexity of code, the way data is stored is fundamental. You
    should pick your data structure carefully. This section provides a few examples
    on how the performance of simple code snippets can be improved by the proper datatypes
    for the job.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减少代码的复杂性，数据存储的方式是基础性的。您应该仔细选择数据结构。本节提供了一些简单代码片段的性能如何通过适当的数据类型来提高的示例。
- en: Searching in a list
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在列表中搜索
- en: Due to implementation details of the `list` type in Python, searching for a
    specific value in a list isn't a cheap operation. The complexity of the `list.index()`
    method is *O(n)*, where *n* is the number of list elements. Such linear complexity
    is not especially bad if you don't need to perform many element index lookups,
    but it can have a negative performance impact if there is a need for many such
    operations.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Python中`list`类型的实现细节，搜索列表中特定值不是一个廉价的操作。`list.index()`方法的复杂度是*O(n)*，其中*n*是列表元素的数量。如果不需要执行许多元素索引查找，这种线性复杂度并不特别糟糕，但如果需要执行许多这样的操作，它可能会产生负面的性能影响。
- en: 'If you need fast search over a list, you can try the `bisect` module from the
    Python standard library. The functions in this module are mainly designed for
    inserting or finding insertion indexes for given values in a way that will preserve
    the order of the already sorted sequence. Anyway, they can be used for efficiently
    finding element indexes with a bisection algorithm. Here is the recipe from the
    official documentation of the function that finds the element index using a binary
    search:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您需要在列表上进行快速搜索，可以尝试Python标准库中的`bisect`模块。该模块中的函数主要设计用于以保持已排序序列顺序的方式插入或查找给定值的插入索引。无论如何，它们可以用于使用二分算法有效地查找元素索引。以下是官方文档中使用二分搜索查找元素索引的函数的配方：
- en: '[PRE3]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Note that every function from the bisect module requires a sorted sequence in
    order to work. If your list is not in the correct order, then sorting it is a
    task with at least *O(n log n)* complexity. This is a worse class than *O(n)*,
    so sorting the whole list for performing only a single search will definitely
    not pay off. However, if you need to perform a lot of index searches in a huge
    list that does not need to change often, then using a single sort operation bisect
    may be a very good trade off.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`bisect`模块中的每个函数都需要一个排序好的序列才能工作。如果您的列表没有按正确的顺序排列，那么对其进行排序至少需要*O(n log n)*的复杂度。这是比*O(n)*更糟糕的类别，因此对整个列表进行排序仅进行单个搜索肯定不划算。但是，如果您需要在一个不经常改变的大列表中执行大量索引搜索，那么使用单个排序操作的`bisect`可能是一个非常好的折衷方案。
- en: Also, if you already have a sorted list, you can insert new items into that
    list using `bisect` without needing to re-sort it.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，如果您已经有一个排序好的列表，您可以使用`bisect`插入新的项目到该列表中，而无需重新排序。
- en: Using a set instead of a list
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用`set`而不是列表
- en: 'When you need to build a sequence of distinct values out of a given sequence,
    the first algorithm that might come to your mind is:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 当您需要从给定序列中构建一系列不同值时，可能首先想到的算法是：
- en: '[PRE4]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The complexity is introduced by the lookup in the `result` list with the `in`
    operator that has the time complexity, *O(n)*. It is then used in the loop, which
    costs *O(n*). So, the overall complexity is quadratic—*O(n²**)*.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 复杂度是由在`result`列表中使用`in`运算符引入的，它的时间复杂度是*O(n)*。然后它在循环中使用，这将花费*O(n)*。因此，总体复杂度是二次的—*O(n²)*。
- en: 'Using a `set` type for the same work will be faster because the stored values
    are looked up using hashes same as in the `dict` type. Also, `set` ensures the
    uniqueness of elements, so we don''t need to do anything more but create a new
    set from our `sequence` object. In other words, for each value in `sequence`,
    the time taken to see if it is already in the `set` will be constant:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 对于相同的工作使用`set`类型将更快，因为存储的值使用哈希查找，就像`dict`类型一样。此外，`set`确保元素的唯一性，因此我们不需要做任何额外的工作，只需从我们的`sequence`对象创建一个新的集合。换句话说，对于`sequence`中的每个值，查看它是否已经在`set`中所花费的时间将是恒定的：
- en: '[PRE5]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This lowers the complexity to *O(n),* which is the complexity of the `set` object
    creation. The additional advantage is shorter and more explicit code.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这将复杂度降低到*O(n)*，这是`set`对象创建的复杂度。额外的优势是代码更短更明确。
- en: Note
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: When you try to reduce the complexity of an algorithm, carefully consider your
    data structures. There are a range of built-in types, so pick the right one.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 当您尝试降低算法的复杂度时，要仔细考虑您的数据结构。有各种内置类型，所以要选择合适的类型。
- en: Cut the external calls, reduce the workload
  id: totrans-82
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 减少外部调用，减轻工作量
- en: A part of the complexity is introduced by calls to other functions, methods,
    and classes. In general, get as much of the code out of the loops as possible.
    This is doubly important for nested loops. Don't recalculate over and over those
    things that can be calculated before the loop even begins. Inner loops should
    be tight.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 复杂性的一部分是由于调用其他函数、方法和类引入的。一般来说，尽可能多地将代码从循环中移出。对于嵌套循环来说，这一点尤为重要。不要一遍又一遍地重新计算那些在循环开始之前就可以计算出来的东西。内部循环应该是紧凑的。
- en: Using collections
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 collections
- en: 'The `collections` module provides high-performance alternatives to the built-in
    container types. The main types available in this module are:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '`collections` 模块提供了高性能的替代内置容器类型。该模块中提供的主要类型有：'
- en: '`deque`: A list-like type with extra features'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`deque`：带有额外功能的类似列表的类型'
- en: '`defaultdict`: A dict-like type with a built-in default factory feature'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`defaultdict`：带有内置默认工厂功能的类似字典的类型'
- en: '`namedtuple`: A tuple-like type that assigns keys for members'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`namedtuple`：类似元组的类型，为成员分配键'
- en: deque
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: deque
- en: A `deque` is an alternative implementation for lists. While a list is based
    on arrays, a `deque` is based on a doubly linked list. Hence, a `deque` is much
    faster when you need to insert something into its middle or head but much slower
    when you need to access an arbitrary index.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '`deque` 是列表的另一种实现方式。列表基于数组，而 `deque` 基于双向链表。因此，当需要在中间或头部插入时，`deque` 要快得多，但是当需要访问任意索引时，`deque`
    要慢得多。'
- en: Of course, thanks to the overallocation of an internal array in the Python `list`
    type, not every `list.append()` call requires memory reallocation, and the average
    complexity of this method is *O(1)*. Still, *pops* and *appends* are generally
    faster when performed on linked lists instead of arrays. The situation changes
    dramatically when the element needs to be added on arbitrary point of sequence.
    Because all elements on the right of the new one need to be shifted in an array,
    the complexity of `list.insert()` is *O(n)*. If you need to perform a lot of pops,
    appends, and inserts, the `deque` in place of the list may provide substantial
    performance improvement. But always be sure to profile your code before switching
    from a `list` to the `deque`, because a few things that are fast in arrays (such
    as accessing arbitrary index) are extremely inefficient in linked lists.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，由于 Python `list` 类型中内部数组的过度分配，不是每次 `list.append()` 调用都需要内存重新分配，而这种方法的平均复杂度是
    *O(1)*。但是，*pops* 和 *appends* 在链表上执行时通常比在数组上执行要快。当元素需要添加到序列的任意点时，情况会发生戏剧性的变化。因为数组中新元素右侧的所有元素都需要移动，所以
    `list.insert()` 的复杂度是 *O(n)*。如果需要执行大量的 pops、appends 和 inserts，那么使用 `deque` 而不是列表可能会提供显著的性能改进。但是在从
    `list` 切换到 `deque` 之前，一定要对代码进行分析，因为在数组中快速的一些操作（例如访问任意索引）在链表中非常低效。
- en: 'For example, if we measure the time of appending one element and removing it
    from the sequence with `timeit`, the difference between `list` and `deque` may
    not even be noticeable:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们使用 `timeit` 测量向序列添加一个元素并从中删除的时间，`list` 和 `deque` 之间的差异甚至可能不会被注意到：
- en: '[PRE6]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'But if we do similar comparison for situations when we want to add and remove
    the first element of the sequence, the performance difference is impressive:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，如果我们对想要添加和移除序列的第一个元素的情况进行类似的比较，性能差异是显著的：
- en: '[PRE8]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'And the difference is, it gets bigger when the size of the sequence grows.
    Here is an example of the same test performed on lists containing 10,000 elements:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 而且，当序列的大小增长时，这种差异会变得更大。以下是对包含 10,000 个元素的列表执行相同测试的示例：
- en: '[PRE9]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Thanks to efficient `append()` and `pop()` methods that work at the same speed
    from both ends of the sequence, `deque` makes a perfect type for implementing
    queues. For example, a **FIFO** (**First In First Out**) queue will definitely
    be much more efficient if implemented with a `deque` instead of `list`.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 由于高效的 `append()` 和 `pop()` 方法可以同时从序列的两端以相同的速度工作，`deque` 是实现队列的完美类型。例如，使用 `deque`
    而不是 `list` 来实现 **FIFO**（先进先出）队列将会更加高效。
- en: Note
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '`deque` works great when implementing queues. Anyway, starting from Python
    2.6 there is a separate `queue` module in Python''s standard library that provides
    basic implementation for FIFO, LIFO, and priority queues. If you want to utilize
    queues as a mechanism of interthread communication, you should really use classes
    from the `queue` module instead of `collections.deque`. This is because these
    classes provide all the necessary locking semantics. If you don''t use threading
    and don''t utilize queues as a communication mechanism, then `deque` should be
    enough to provide queue implementation basics.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '`deque` 在实现队列时效果很好。不过，从 Python 2.6 开始，Python 标准库中有一个单独的 `queue` 模块，提供了 FIFO、LIFO
    和优先级队列的基本实现。如果要将队列用作线程间通信的机制，应该使用 `queue` 模块中的类，而不是 `collections.deque`。这是因为这些类提供了所有必要的锁定语义。如果不使用线程和不使用队列作为通信机制，那么
    `deque` 应该足够提供队列实现的基础。'
- en: defaultdict
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: defaultdict
- en: The `defaultdict` type is similar to the `dict` type but adds a default factory
    for new keys. This avoids writing an extra test to initialize the mapping entry
    and is more efficient than the `dict.setdefault` method.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '`defaultdict` 类型类似于 `dict` 类型，但为新键添加了一个默认工厂。这避免了编写额外的测试来初始化映射条目，并且比 `dict.setdefault`
    方法更高效。'
- en: '`defaultdict` seems just like syntactic sugar over `dict` that simply allows
    you to write shorter code. In fact, the fallback to a predefined value on a failed
    key lookup is also slightly faster than the `dict.setdefault()` method:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '`defaultdict` 看起来只是 `dict` 上的语法糖，简单地允许您编写更短的代码。实际上，在失败的键查找时返回预定义值也比 `dict.setdefault()`
    方法稍微快一些：'
- en: '[PRE10]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The difference isn't great because the computational complexity hasn't changed.
    The `dict.setdefault` method consist of two steps (key lookup and key set), both
    of which have a complexity of *O(1)*, as we have seen in the *Dictionaries* section
    in [Chapter 2](ch02.html "Chapter 2. Syntax Best Practices – below the Class Level"),
    *Syntax Best Practices – below the Class Level*. There is no way to have a complexity
    class lower than *O(1)*. But it is indisputably faster in some situations and
    it is worth knowing because every small speed improvement counts when optimizing
    critical code sections.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: 'The `defaultdict` type takes a factory as a parameter and can therefore be
    used with built-in types or classes whose constructor does not take arguments.
    Here is an example from the official documentation that shows how to use `defaultdict`
    for counting:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: namedtuple
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`namedtuple` is a class factory that takes a type name and a list of attributes
    and creates a class out of it. The class can then be used to instantiate a tuple-like
    object and provide accessors for its elements:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: It can be used to create records that are easier to write compared to a custom
    class that would require some boilerplate code to initialize values. On the other
    hand, it is based on tuple, so access to its elements by index is very fast. The
    generated class can be subclassed to add more operations.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: The gain from using `namedtuple` instead of other datatypes may not be obvious
    at first. The main advantage is that it is way more easier to use, understand,
    and interpret than ordinary tuples. Tuple indexes don't carry any semantics, so
    it is great to access tuple elements by attributes too. However, you could get
    the same benefit from dictionaries that have an *O(1)* average complexity of get/set
    operations.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: The first advantage in terms of performance is that `namedtuple` is still the
    flavor of `tuple`. It means that it is immutable, so the underlying array storage
    is allocated exactly for the needed size. Dictionaries, on the other hand, need
    to use overallocation of the internal hash table to ensure low average complexity
    of get/set operations. So, `namedtuple` wins over `dict` in terms of memory efficiency.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: The fact that `namedtuple` is based on a tuple may also be beneficial for performance.
    Its elements may be accessed by an integer index, like in two other simple sequence
    objects—lists and tuples. This operation is both simple and fast. In the case
    of `dict` or custom class instances (that also use dictionaries for storing attributes),
    the element access requires hash table lookup. It is highly optimized to ensure
    good performance independently from collection size, but the mentioned *O(1)*
    complexity is actually only the *average complexity*. The actual, amortized worst
    case complexity for set/get operations in `dict` is *O(n)*. The real amount of
    work when performing such an operation at a given moment is dependent on both
    collection size and its history. So, in sections of code that are critical for
    performance, sometimes it may be wise to use lists or tuples instead of dictionaries.
    This is only because they are more predictable when it comes to performance.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: 'In such a situation, `namedtuple` is a great type that combines the advantages
    of dictionaries and tuples:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: In sections where readability is more important, the attribute access may be
    preferred
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In performance-critical sections, elements may be accessed by their indexes
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Reduced complexity can be achieved by storing the data in an efficient data
    structure that works well with the way the algorithm will use it.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: That said, when the solution is not obvious, you should consider dropping and
    rewriting the incriminated part instead of killing the code readability for the
    sake of performance.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: Often, the Python code can be both readable and fast. So, try to find a good
    way to perform the work instead of trying to work around a flawed design.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: Using architectural trade-offs
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When your code cannot be improved any further by reducing the complexity or
    choosing the proper data structure, a good approach may be to consider doing some
    trade-offs. If we review user problems and define what is really important for
    them, we can relax some of the application requirements. The performance can often
    be improved by:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: Replacing exact solution algorithms with heuristics and approximation algorithms
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deferring some work to delayed task queues
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using probabilistic data structures
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using heuristics and approximation algorithms
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Some algorithmic problems simply don't have *good state of the art* solutions
    that could run in time acceptable to the user. For example, consider a program
    that deals with some complex optimization problems such as **Traveling Salesman
    Problem** (**TSP**) or **Vehicle Routing Problem** (**VRP**). Both problems are
    *NP-hard* problems in combinatorial optimization. The exact algorithms for such
    problems that have low complexity are not known. This means that the size of the
    problems that can be practically solved is greatly limited. For very large inputs,
    it is very unlikely that it will be able to provide the exact solution in a time
    that would be acceptable for any user.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: 'Fortunately, it is very probable that the user is not interested in the best
    possible solution but the one that is good enough and the one that can be obtained
    in a timely manner. So, it really makes sense to use **heuristics** or **approximation
    algorithms** whenever they provide an acceptable quality of results:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: Heuristics solve given problems by trading optimality, completeness, accuracy,
    or precision for speed. They concentrate on the speed, but it may be really hard
    to prove their solution quality compared to the result of exact algorithms.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Approximation algorithms are similar in idea to heuristics, but unlike heuristics
    have provable solution quality and run-time bounds.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For instance, there are known good heuristics and approximation problems that
    can solve extremely large TSP problems within a reasonable time. They also have
    a high probability of producing results just 2-5% from the optimal solution.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: 'Another good thing about heuristics is that they don''t always need to be constructed
    from scratch for every new problem you need to solve. Their higher-level versions,
    called **metaheuristics**, provide strategies for solving mathematical optimization
    problems that are not problem-specific and can thus be applied in many situations.
    Some popular metaheuristic algorithms include:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: Simulated annealing
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Genetic algorithms
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tabu search
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ant colony optimization
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evolutionary computation
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using task queues and delayed processing
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sometimes it's not about doing a lot but about doing things at the right time.
    A good example of that is sending e-mails in web applications. In that case, increased
    response times may not necessarily be the result of your implementation. The response
    time may be dominated by some third-party service, such as an e-mail server. Can
    you optimize your application if it just spends most of its time on waiting for
    other services to reply?
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: 'The answer is both: yes and no. If you don''t have any control over a service
    that is the main contributor to your processing time and there is no other faster
    solution you could use, you, of course, cannot speed it up any further. You cannot
    simply skip in time to get the replies you are waiting for. A simple example of
    processing an HTTP request that results in sending an e-mail is presented in the
    following figure (*Figure* *1*). You cannot reduce the waiting time, but you can
    change the way users will perceive it!'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '![Using task queues and delayed processing](graphics/B05295_12_01.jpg)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
- en: Figure 1 An example of synchronous e-mail delivery in web application
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: The usual pattern for such type of problems is using message/task queues. When
    you need to do something that may take an indefinite amount of time, just add
    this to the queue of work that needs to be done and immediately respond to the
    user whose request was accepted. Here, we come to the reason why sending e-mails
    is such a great example. E-mails are already task queues! If you submit a new
    message to the e-mail server using SMTP protocol, the successful response does
    not mean that your e-mail was delivered to addressee. It means that the e-mail
    was delivered to the e-mail server and it will try later to deliver it further.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这种类型问题的通常模式是使用消息/任务队列。当您需要做一些可能需要不确定时间的事情时，只需将其添加到需要完成的工作队列中，并立即响应接受请求的用户。这里，我们来到为什么发送电子邮件是一个很好的例子的原因。电子邮件已经是任务队列！如果您使用SMTP协议向电子邮件服务器提交新消息，成功的响应并不意味着您的电子邮件已经传递给收件人。这意味着电子邮件已经传递给了电子邮件服务器，并且它将稍后尝试进一步传递。
- en: 'So, if the response from the server does not guarantee that the e-mail was
    delivered at all, you don''t need to wait for it in order to generate an HTTP
    response for the user. The updated flow of processing requests with the usage
    of the task queue is presented in the following figure:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果服务器的响应并不保证电子邮件是否已经传递，您无需等待它以生成用户的HTTP响应。使用任务队列处理请求的更新流程如下图所示：
- en: '![Using task queues and delayed processing](graphics/B05295_12_02.jpg)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![使用任务队列和延迟处理](graphics/B05295_12_02.jpg)'
- en: Figure 2 An example of asynchronous e-mail delivery in web application
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图2 Web应用程序中异步电子邮件传递的示例
- en: Of course, your e-mail server may be responding blazingly fast, but you need
    some more time to generate the message that needs to be sent. Perhaps you are
    generating yearly reports in an XLS format or maybe delivering invoices in PDF
    files. If you use e-mail transport that is already asynchronous, then put the
    whole message generation task to the message processing system too. If you cannot
    guarantee the exact time of delivery, then you should not bother to generate your
    deliverables synchronously.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，您的电子邮件服务器可能响应非常快，但您需要更多时间来生成需要发送的消息。也许您正在生成XLS格式的年度报告，或者在PDF文件中交付发票。如果您使用的是已经是异步的电子邮件传输，那么也将整个消息生成任务放到消息处理系统中。如果无法保证准确的交付时间，那么您不应该打扰同步生成您的交付物。
- en: 'The proper usage of task/message queues in critical sections of the application
    can also give you other benefits:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用程序的关键部分正确使用任务/消息队列还可以给您带来其他好处：
- en: Web workers that serve HTTP requests will be relieved from additional work and
    processing requests faster. This means that you will be able to process more requests
    with the same resources and thus handle greater load.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为服务HTTP请求的Web工作者将从额外的工作中解脱出来，处理请求更快。这意味着您将能够使用相同的资源处理更多的请求，从而处理更大的负载。
- en: Message queues are generally more immune to transient failures of external services.
    For instance, if your database or e-mail server times out from time to time, you
    can always re-queue the currently processed task and retry it later.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 消息队列通常更不容易受到外部服务的瞬态故障的影响。例如，如果您的数据库或电子邮件服务器不时超时，您可以始终重新排队当前处理的任务并稍后重试。
- en: With a good message queue implementation, you can easily distribute the work
    on multiple machines. This approach may improve the scalability of some of your
    application components.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过良好的消息队列实现，您可以轻松地将工作分布在多台机器上。这种方法可能提高应用程序某些组件的可扩展性。
- en: As you can see in *Figure 2*, adding an asynchronous task processing to your
    application inevitably increases the complexity of the whole system's architecture.
    You will need to set up some new backing services (a message queue such as RabbitMQ)
    and create workers that will be able to process these asynchronous jobs. Fortunately,
    there are some popular tools for building distributed task queues. The most popular
    one among Python developers is **Celery** ([http://www.celeryproject.org/](http://www.celeryproject.org/)).
    It is a full-fledged task queue framework with support of multiple message brokers
    that also allows for the scheduled execution of tasks (it can replace your `cron`
    jobs). If you need something simpler, then RQ ([http://python-rq.org/](http://python-rq.org/))
    might be a good alternative. It is a lot simpler than Celery and uses Redis key/value
    storage as its message broker (**RQ** actually stands for **Redis Queue**).
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 如您在*图2*中所见，将异步任务处理添加到应用程序中不可避免地增加了整个系统架构的复杂性。您将需要设置一些新的后备服务（例如RabbitMQ这样的消息队列）并创建能够处理这些异步作业的工作者。幸运的是，有一些流行的工具用于构建分布式任务队列。在Python开发人员中最受欢迎的是**Celery**（[http://www.celeryproject.org/](http://www.celeryproject.org/)）。它是一个完整的任务队列框架，支持多个消息代理，还允许定期执行任务（可以替代您的`cron`作业）。如果您需要更简单的东西，那么RQ（[http://python-rq.org/](http://python-rq.org/)）可能是一个不错的选择。它比Celery简单得多，并使用Redis键/值存储作为其消息代理（**RQ**实际上代表**Redis
    Queue**）。
- en: 'Although there are some good and battle-tested tools, you should always carefully
    consider your approach to the task queues. Definitely not every kind of work should
    be processed in queues. They are good at solving a few types of issues but also
    introduce a load of new problems:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有一些经过良好测试的工具，您应该始终仔细考虑您对任务队列的方法。绝对不是每种工作都应该在队列中处理。它们擅长解决一些问题，但也引入了一大堆新问题：
- en: Increased complexity of system architecture
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 系统架构的复杂性增加
- en: Dealing with *more than once* deliveries
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理“多次”交付
- en: More services to maintain and monitor
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更多需要维护和监控的服务
- en: Larger processing delays
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更长的处理延迟
- en: More difficult logging
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更困难的日志记录
- en: Using probabilistic data structures
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用概率数据结构
- en: Probabilistic data structures are structures that are designed to store collections
    of values in a way that allows you to answer some specific questions within time
    or resource constraints that would not be possible with other data structures.
    The most important fact is that the answer is only probable to be true or is the
    approximation of the real value. However, the probability of the correct answer
    or its accuracy can be easily estimated. So, despite not always giving the correct
    answer, it can be still useful if we accept some level of error.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: There are a lot of data structures with such probabilistic properties. Each
    one of them solves some specific problems, and due to theirs stochastic nature
    cannot be used in every situation. But to give a practical example, let's talk
    about one of them that is especially popular—**HyperLogLog**.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: HyperLogLog (refer to [https://en.wikipedia.org/wiki/HyperLogLog](https://en.wikipedia.org/wiki/HyperLogLog))
    is an algorithm that approximates the number of distinct elements in a multiset.
    With ordinary sets, you need to store every element, and this may be very impractical
    for very large datasets. HLL is distinct from the classical way of implementing
    sets as programming data structures. Without digging into implementation details,
    let's say that it only concentrates on providing an approximation of the set cardinality.
    Thus, real values are never stored. They cannot be retrieved, iterated, and tested
    for membership. HyperLogLog trades accuracy and correctness for time complexity
    and size in memory. For instance, the Redis implementation of HLL takes only 12k
    bytes with a standard error of 0.81% with no practical limit of collection size.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: Using probabilistic data structures is a very interesting way of solving performance
    problems. In most cases, it is about trading off some accuracy or correctness
    for faster processing or better resource usage. But it does not always need to
    be that way. Probabilistic data structures are very often used in key/value storage
    systems to speed up key lookups. One of the popular techniques used in such systems
    is called approximate member query (AMQ). One interesting data structure that
    can be used for that purpose is Bloom filter (refer to [https://en.wikipedia.org/wiki/Bloom_filter](https://en.wikipedia.org/wiki/Bloom_filter)).
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: Caching
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When some of your application function takes too long to compute, the useful
    technique to consider is caching. Caching is nothing but saving a return value
    for future reference. The result of a function or method that is expensive to
    run can be cached as long as:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: The function is deterministic and the results have the same value every time,
    given the same input
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The return value of the function continues to be useful and valid for some period
    of time (nondeterministic)
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In other words, a deterministic function always returns the same result for
    the same set of arguments, whereas a nondeterministic one returns results that
    may vary in time. Such an approach usually greatly reduces the time of computation
    and allows you to save a lot of computer resources.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: 'The most important requirement for any caching solution is to have a storage
    that allows you to retrieve saved values significantly faster than it takes to
    calculate them. Good candidates for caching are usually:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: Results from callables that query databases
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Results from callables that render static values, such as file content, web
    requests, or PDF rendering
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Results from deterministic callables that perform complex calculations
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Global mappings that keep track of values with expiration times, such as web
    session objects
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Results that needs to be accessed often and quickly
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another important use case for caching is saving results from third-party APIs
    served over the Web. This may greatly improve application performance by cutting
    off the network latencies but also allows you to save money if you are billed
    for every request to such API.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: Depending on your application architecture, the cache can be implemented in
    many ways and with various levels of complexity. There are many ways to provide
    caching and complex applications can use different approaches on different levels
    of the application architecture stack. Sometimes a cache may be as simple as a
    single global data structure (usually a `dict`) kept in the process memory. In
    other situations, you may want to set up a dedicated caching service that will
    run on carefully tailored hardware. This section will provide you with basic information
    on the most popular caching approaches and guide you through the usual use cases
    and also the common pitfalls.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: Deterministic caching
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deterministic functions are the easiest and safest use case for caching. Deterministic
    functions always return the same value if given exactly the same input, so generally
    you can store their result indefinitely. The only limitation is the size of storage
    you use for caching. The simplest way to cache such results is to put them into
    process memory because it is usually the fastest place to retrieve data from.
    Such a technique is often called **memoization**.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: 'Memoization is very useful when optimizing recursive functions that may evaluate
    the same inputs multiple times. We already discussed recursive implementation
    for the Fibonacci sequence in [Chapter 7](ch07.html "Chapter 7. Python Extensions
    in Other Languages"), *Python Extensions in Other Languages*. Back then, we tried
    to improve the performance of our program with C and Cython. Now we will try to
    achieve the same goal by simpler means—with the help of caching. But before we
    do that, let''s recall the code for the `fibonacci()` function:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: As we see, `fibonacci()` is a recursive function that calls itself twice if
    the input value is more than two. This makes it highly inefficient. The run time
    complexity is *O(2^n**)* and its execution creates a very deep and vast call tree.
    For the large value, this function will take extremely long to execute and there
    is high chance of quickly exceeding the maximal recursion limit of the Python
    interpreter.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: If you take a closer look at *Figure 3,* which presents an example call tree,
    you will see that it evaluates many of the intermediate results multiple times.
    A lot of time and resources could be saved if we could reuse some of these values.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: '![Deterministic caching](graphics/B05295_12_03.jpg)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
- en: Figure 3 Call tree for fibonacci(5) execution
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: 'A simple memoization attempt would be to store results of the previous runs
    in a dictionary and retrieve them if they are available. Both the recursive calls
    in the `fibonacci()` function are contained in a single line of code:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We know that Python evaluates instructions from left to right. This means that,
    in this situation, the call to the function with a higher argument value will
    be executed before the call to the function with a lower argument. Thanks to this,
    we can provide memoizaton by constructing a very simple decorator:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: We used the dictionary on the closure of the `memoize()` decorator as a simple
    storage from cached values. Saving and retrieving value to that data structure
    has an average *O(1)* complexity, so this greatly reduces the overall complexity
    of the memoized function. Every unique function call will be evaluated only once.
    The call tree of such an updated function is presented in *Figure 4*. Without
    going into mathematical proofs, we can visually deduce that without changing the
    core of the `fibonacci()` function, we reduced the complexity from the very expensive
    *O(2n)* to the linear *O(n)*.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: '![Deterministic caching](graphics/B05295_12_04.jpg)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
- en: Figure 4 A call tree for fibonacci(5) execution with memoization
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: 'The implementation of our `memoize()` decorator is, of course, not perfect.
    It worked well for that simple example, but it definitely isn''t a reusable piece
    of software. If you need to memoize functions with multiple arguments or want
    to limit the size of your cache, you need something more generic. Luckily, the
    Python standard library provides a very simple and reusable utility that may be
    used in most cases when you need to cache in memory the results of deterministic
    functions. It is the `lru_cache(maxsize, typed)` decorator from the `functools`
    module. The name comes from the LRU cache, which stands for *last recently used*.
    The additional parameters allow for finer control over memoization behavior:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '`maxsize`: This sets the maximum size of the cache. The `None` value means
    no limit at all.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`typed`: This defines if the values of different types that compare as equal
    should be cached as giving the same result.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The usage of `lru_cache` in our Fibonacci sequence example would be as follows:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Nondeterministic caching
  id: totrans-199
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The caching of nondeterministic functions is way more trickier that memoization.
    Due to the fact that every execution of such a function may give different results,
    it is usually impossible to use previous values for an arbitrarily long amount
    of time. What you need to do is to decide for how long a cached value can be considered
    valid. After a defined period of time passes, the stored results are considered
    to be stale and the cache needs to be refreshed by a new value.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: 'Nondeterministic functions that are usually a subject of caching very often
    depend on some external state that is hard to track inside of your application
    code. Typical examples of components are:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: Relational databases and generally any type of structured data storage engine
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Third-party services accessible through network connection (web APIs)
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Filesystems
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, in other words, nondeterministic caching is used in any situation when you
    temporarily use precomputed results without being sure if they represent a state
    that is consistent with the state of other system components (usually, the backing
    service).
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: Note that such an implementation of caching is obviously a trade-off. Thus,
    it is somehow related to the techniques we featured in the *Using architectural
    trade-offs* section. If you resign from running part of your code every time and
    instead use the results saved in the past, you are risking using data that becomes
    stale or represents an inconsistent state of your system. This way, you are trading
    the correctness and/or completeness for speed and performance.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: Of course, such caching is efficient as long as the time taken to interact with
    the cache is less than the time taken by the function. If it's faster to simply
    recalculate the value, by all means do so! That's why setting up a cache has to
    be done only if it's worth it; setting it up properly has a cost.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: The actual things that are cached are usually the whole results of interaction
    with other components of your system. If you want to save time and resources when
    communicating with the database, it is worth to cache expensive queries. If you
    want to reduce the number of I/O operations, you may want to cache the content
    of the files that are accessed very often (configuration files, for instance).
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: Techniques for caching non-deterministic functions are actually very similar
    to those used in caching the deterministic ones. The most notable difference is
    that they usually require the option to invalidate cached values by their age.
    This means that the `lru_cache()` decorator from the `functools` module has very
    limited use in such situations. It should not be so hard to extend this function
    to provide the expiration feature, but I will leave it as an exercise for you.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: Cache services
  id: totrans-210
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We said that nondeterministic caching can be implemented using local process
    memory, but actually it is rarely done that way. It's because local process memory
    is very limited in its utility as storage for caching in large applications.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: If you run into a situation where non-deterministic caching is your preferred
    solution to solve performance problems, you usually need something more than that.
    Usually, nondeterministic caching is your *must have* solution when you need to
    serve data or service to multiple users at the same time. If it's true, then sooner
    or later you will need to ensure that users can be served concurrently. While
    local memory provides a way to share data between multiple threads, it may not
    be the best concurrency model for every application. It does not scale well, so
    you will eventually need to run your application as multiple processes.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: If you are lucky enough, you may need to run your application on hundreds or
    thousands of machines. If you would like to store cached values in local memory,
    it means that your cache needs to be duplicated on every process that requires
    it. It isn't only a total waste of resources. If every process has its own cache,
    that is already a trade-off between speed and consistency, how can you guarantee
    that all caches are consistent with each other?
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: Consistency across subsequent request is a serious concern (especially) for
    web applications with distributed backends. In complex distributed systems, it
    is extremely hard to ensure that the user will be always consistently served by
    the same process hosted on the same machine. It is of course doable to some extent,
    but once you solve that problem, ten others will pop up.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: If you are making an application that will need to serve multiple concurrent
    users, then the best way to handle a nondeterministic cache is to use some dedicated
    service for that. With tools such as Redis or Memcached, you allow all your application
    processes to share the same cached results. This both reduces the usage of precious
    computing resources and saves you from problems caused by having multiple independent
    and inconsistent caches.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: Memcached
  id: totrans-216
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you want to be serious about caching, **Memcached** is a very popular and
    battle-tested solution. This cache server is used by big applications such as
    Facebook or Wikipedia to scale their websites. Among simple caching features,
    it has clustering capabilities that makes it possible to set up a highly efficient
    distributed cache system in no time.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: 'The tool is Unix-based but can be driven from any platform and from many languages.
    There are many Python clients that differ slightly from each other but the basic
    usage is usually the same. The simplest interaction with Memcached almost always
    consists of three methods:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: '`set(key, value)`: This saves the value for the given key'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`get(key)`: This gets the value for the given key if it exists'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`delete(key)`: This deletes the value under the given key if it exists'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is an example of integration with Memcached using one of the popular Python
    packages—`pymemcached`:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'One of the downsides of Memcached is that it is designed to store values either
    as strings or a binary blob, and this isn''t compatible with every native Python
    type. Actually, it is compatible with only one—strings. This means that more complex
    types need to be serialized in order to be successfully stored in Memcached. A
    common serialization choice for simple data structures is JSON. Here is an example
    of using JSON serialization with `pymemcached`:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The other problem that is very common when working with every caching service
    that works on the key/value storage principle is how to choose key names.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: For cases when you cache simple function invocations that have basic parameters,
    the problem is usually simple. You can convert the function name and its arguments
    to strings and concatenate them together. The only thing you need to care about
    is to make sure there are no collisions between keys created for different functions
    if you use cache in many parts of your application.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: A more problematic case is when cached functions have complex arguments consisting
    of dictionaries or custom classes. In that case, you need to find a way to convert
    such invocation signatures to cache keys in a consistent manner.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: The last problem is that Memcached, like many other caching services, does not
    tend to like very long key strings. Usually, the shorter the better. Long keys
    may either reduce performance or just not fit the hardcoded service limits. For
    instance, if you cache whole SQL queries, the query strings themselves are generally
    good unique identifiers that could be used as keys. But on the other hand, complex
    queries are generally too long to be stored in typical caching services such as
    Memcached. A common practice is to calculate the **MD5**, **SHA**, or any other
    hash function and use it as a cache key instead. The Python standard library has
    a `hashlib` module that provides implementation for few popular hash algorithms.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: Remember that calculating a hash comes at a price. However, sometimes it is
    the only viable solution. It is also a very useful technique when dealing with
    complex types that need to be used when creating cache keys. One important thing
    to care about when using hashing functions is hash collisions. There is no hash
    function that guarantees that collisions will never occur, so always be sure to
    know the probability and mind such risks.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-231
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, you have learned:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: How to define the complexity of the code and some approaches to reduce it
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to improve performance using some architectural trade-offs
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What caching is and how to use it to improve application performance
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The preceding methods concentrated our optimization efforts inside a single
    process. We tried to reduce the code complexity, choose better datatypes, or reuse
    old function results. If that did not help, we tried to make some trade-offs using
    approximations, doing less, or leaving work for later.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will discuss a few techniques for concurrency and parallel
    processing in Python.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
