- en: Chapter 12. Optimization – Some Powerful Techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Optimizing a program is not a magical process. It is done by following a simple
    algorithm, synthesized by Stefan Schwarzer at Europython 2006 in his original
    pseudocode example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This example probably isn''t the neatest and clearest one but captures pretty
    much all the important aspects of an organized optimization procedure. The main
    things we learn from it are:'
  prefs: []
  type: TYPE_NORMAL
- en: Optimization is an iterative process where not every iteration leads to better
    results
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The main prerequisite is code that is verified to be working properly with tests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You should always focus on optimizing the current application bottleneck
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Making your code work faster is not an easy task. In case of abstract mathematical
    problems, the solution of course lies in choosing the right algorithm and proper
    data structures. But in that case, it is very hard to provide some generic tips
    and tricks that can be used in any code for solving algorithmic issues. There
    are of course some generic methodologies for designing a new algorithm, or even
    meta-heuristics that can be applied to a large variety of problems but they are
    pretty language-agnostic and thus are rather out of scope of this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'Anyway, some performance issues are only caused by certain code quality defects
    or application usage context. For instance, the speed of the application might
    be reduced by:'
  prefs: []
  type: TYPE_NORMAL
- en: Bad usage of basic built-in types
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Too much complexity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hardware resource usage patterns not matching with the execution environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Waiting too long for responses from third-party APIs or backing services
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Doing too much in time-critical parts of the application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More often, the solving of such performance issues does not require advanced
    academic knowledge but only good software craftsmanship. And a big part of craftsmanship
    is knowing when to use the proper tools. Fortunately, there are some well-known
    patterns and solutions for dealing with performance problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will discuss some popular and reusable solutions that allow
    you to non-algorithmically optimize your program through:'
  prefs: []
  type: TYPE_NORMAL
- en: Reducing the complexity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using architectural trade offs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Caching
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reducing the complexity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we dig further into optimization techniques, let's define exactly what
    we are going to deal with. From the chapter's introduction, we know that focusing
    on improving application bottlenecks is critical for successful optimization.
    A bottleneck is a single component that severely limits the capacity of a program
    or computer system. An important characteristic of every piece of code with performance
    issues is that it usually has only a single bottleneck. We discussed some profiling
    techniques in the previous chapter, so you should already be familiar with the
    tools required to locate and isolate such places. If your profiling results show
    that there are few places that need immediate improvement, then you should at
    first try to treat each as a separate component and optimize independently.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, if there is no explicit bottleneck but your application still performs
    under your expectations, then you are really in a bad position. The gains of the
    optimization process are proportional to the performance impact of optimized bottlenecks.
    Optimizing every small component that does not substantially contribute to the
    overall execution time or resource consumption will only give you minimal benefit
    for all the time spent on profiling and optimization. If your application does
    not seem to have real bottlenecks, there is a possibility that you have missed
    something. Try using different profiling strategies or tools or look at it from
    a different perspective (memory, I/O operations, or network throughput). If that
    does not help, you should really consider revising your software architecture.
  prefs: []
  type: TYPE_NORMAL
- en: But if you have successfully found a single and integral component that limits
    your application performance, then you are really lucky. There is high chance
    that with only minimal code improvement, you will be able to really improve code
    execution time and/or resource usage. And the gain from optimization will, again,
    be proportional to the bottleneck size.
  prefs: []
  type: TYPE_NORMAL
- en: The first and most obvious aspect to look after when trying to improve application
    performance is complexity. There are many definitions of what makes a program
    complex and many ways to express it. Some complexity metrics can provide objective
    information about how the code behaves and such information can sometimes be extrapolated
    into performance expectations. An experienced programmer can even reliably guess
    how two different implementations will perform in practice knowing their complexities
    and realistic execution contexts.
  prefs: []
  type: TYPE_NORMAL
- en: 'The two popular ways to define application complexity are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Cyclomatic** **complexity** that is very often correlated with application
    performance'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Landau notation, also known as **big O** **notation**, that is an algorithm
    classification method that is very useful in objectively judging performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: From there, the optimization process may be sometimes understood as a process
    of reducing the complexity. This section provides simple tips for this work by
    simplifying loops. But first of all, let's learn how to measure complexity.
  prefs: []
  type: TYPE_NORMAL
- en: Cyclomatic complexity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Cyclomatic** **complexity** is a metric developed by Thomas J. McCabe in
    1976\. Because of its author, it is very often called **McCabe''s complexity**.
    It measures the number of linear paths through the code. All `if`, `for`, and
    `while` loops are counted to come up with a measure.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The code can then be categorized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Cyclomatic Complexity | What it means |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1 to 10 | Not complex |'
  prefs: []
  type: TYPE_TB
- en: '| 11 to 20 | Moderately complex |'
  prefs: []
  type: TYPE_TB
- en: '| 21 to 50 | Really complex |'
  prefs: []
  type: TYPE_TB
- en: '| More than 50 | Too complex |'
  prefs: []
  type: TYPE_TB
- en: Cyclomatic complexity is rather the code quality score than a metric that objectively
    judges its performance. It does not replace the need for code profiling for finding
    performance bottlenecks. Anyway, code that has high cyclomatic complexity often
    tends to utilize rather complex algorithms that may not perform well with larger
    inputs.
  prefs: []
  type: TYPE_NORMAL
- en: Although cyclomatic complexity is not a reliable way to judge application performance,
    it has one very nice advantage. It is a source code metric so it can be measured
    with proper tools. This cannot be said about other ways of expressing complexity—the
    big O notation. Thanks to measurability, cyclomatic complexity may be a useful
    addition to profiling that gives you more information about problematic parts
    of the software. Complex parts of code are the first to review when considering
    radical code architecture redesigns.
  prefs: []
  type: TYPE_NORMAL
- en: Measuring McCabe's complexity is relatively simple in Python because it can
    be deduced from its Abstract Syntax Tree. Of course, you don't need to do that
    by yourself. A popular tool that provides cyclomatic complexity measurement for
    Python is `flake8` (with the `mccabe` plugin), which has already been introduced
    in [Chapter 4](ch04.html "Chapter 4. Choosing Good Names"), *Choosing Good Names*.
  prefs: []
  type: TYPE_NORMAL
- en: The big O notation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The most canonical method to define function complexity is the **big O notation**
    (see [http://en.wikipedia.org/wiki/Big_O_notation](http://en.wikipedia.org/wiki/Big_O_notation)).
    This metric defines how an algorithm is affected by the size of the input data.
    For instance, does the algorithm scale linearly with the size of the input data
    or quadratically?
  prefs: []
  type: TYPE_NORMAL
- en: Calculating the big O notation manually for an algorithm is the best approach
    to get an overview on how its performance is related with the size of the input
    data. Knowing the complexity of your application components gives you the ability
    to detect and focus on the parts that will really slow down the code.
  prefs: []
  type: TYPE_NORMAL
- en: 'To measure the big O notation, all constants and low-order terms are removed
    in order to focus on the portion that really weighs when the input data grows.
    The idea is to try to categorize the algorithm in one of these categories, even
    if it is an approximation:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Notation | Type |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| O(1) | Constant. Does not depend on the input data. |'
  prefs: []
  type: TYPE_TB
- en: '| O(n) | Linear. Will grow as "n" grows. |'
  prefs: []
  type: TYPE_TB
- en: '| O(n log n) | Quasi linear. |'
  prefs: []
  type: TYPE_TB
- en: '| O(n²) | Quadratic complexity. |'
  prefs: []
  type: TYPE_TB
- en: '| O(n³) | Cubic complexity. |'
  prefs: []
  type: TYPE_TB
- en: '| O(n!) | Factorial complexity. |'
  prefs: []
  type: TYPE_TB
- en: For instance, we already know from [Chapter 2](ch02.html "Chapter 2. Syntax
    Best Practices – below the Class Level"), *Syntax Best Practices – below the Class
    Level*, that a `dict` lookup has an average complexity of *O(1)*. It is considered
    constant regardless of how many elements are in the `dict`, whereas looking through
    a list of items for a particular item is *O(n)*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take another example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: In that case, the print statement will be executed *n* times. Loop speed will
    depend on `n`, so its complexity expresses using the big O notation will be *O(n)*.
  prefs: []
  type: TYPE_NORMAL
- en: 'If the function has conditions, the correct notation to keep is the highest
    one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: In this example, the function could be *O(1)* or *O(n)*, depending on the test.
    But the worst case is *O(n)*, so whole function complexity is *O(n)*.
  prefs: []
  type: TYPE_NORMAL
- en: When discussing complexity expressed in big O notation, we usually review the
    worst case scenario. While this is the best method to define complexity when comparing
    two independent algorithms, it may not be the best approach in every practical
    situation. Many algorithms change the runtime performance depending on the statistical
    characteristic of input data or amortize the cost of worst case operations by
    doing clever tricks. This is why, in many cases, it may be better to review your
    implementation in terms *of average complexity* or *amortized complexity*.
  prefs: []
  type: TYPE_NORMAL
- en: For example, take a look at the operation of appending a single element to Python's
    `list` type instance. We know that `list` in CPython uses an array with overallocation
    for the internal storage instead of linked lists. In case an array is already
    full, appending a new element requires allocation of the new array and copying
    all existing elements (references) to a new area in the memory. If we look from
    the point of the **worst-case complexity**, it is clear that the `list.append()`
    method has *O(n)* complexity. And this is a bit expensive when compared to a typical
    implementation of the linked list structure.
  prefs: []
  type: TYPE_NORMAL
- en: But we also know that the CPython `list` type implementation uses overallocation
    to mitigate the complexity of such occasional reallocation. If we evaluate the
    complexity over a sequence of operations, we will see that the *average complexity*
    of `list.append()` is *O(1)* and this is actually a great result.
  prefs: []
  type: TYPE_NORMAL
- en: 'When solving problems, we often know a lot of details about our input data
    such as its size or statistical distribution. When optimizing the application,
    it is always worth using every bit of knowledge about your input data. Here, another
    problem of worst-case complexity starts to show up. It is intended to show the
    limiting behavior of the function when the input tends toward large values or
    infinity, rather than give a reliable performance approximation for real-life
    data. Asymptotic notation is great when defining the growth rate of a function
    but it won''t give a reliable answer for the simple question: which implementation
    will take less time? Worst-case complexity dumps all those little details about
    both your implementation and data characteristic to show you how your program
    will behave asymptotically. It works for arbitrarily large inputs that you may
    not even need to consider.'
  prefs: []
  type: TYPE_NORMAL
- en: For instance, let's assume that you have a problem to solve regarding data consisting
    of *n* independent elements. Let's suppose also that you know two different ways
    to solve this problem—*program A* and *program B*. You know that *program A* requires
    100n² operations to finish and *program B* requires 5n³ operations to give the
    problem a solution. Which one would you choose? When speaking about very large
    inputs, *program A* is of course the better choice because it behaves better asymptotically.
    It has *O(n²**)* complexity compared to *O(n³**)* complexity that characterizes
    *program B*.
  prefs: []
  type: TYPE_NORMAL
- en: But by solving a simple 100 n² > 5 n³ inequality, we can find that *program
    B* will take fewer operations when *n* is less than 20\. If we know a bit more
    about our input bounds, we can make slightly better decisions.
  prefs: []
  type: TYPE_NORMAL
- en: Simplifying
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To reduce the complexity of code, the way data is stored is fundamental. You
    should pick your data structure carefully. This section provides a few examples
    on how the performance of simple code snippets can be improved by the proper datatypes
    for the job.
  prefs: []
  type: TYPE_NORMAL
- en: Searching in a list
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Due to implementation details of the `list` type in Python, searching for a
    specific value in a list isn't a cheap operation. The complexity of the `list.index()`
    method is *O(n)*, where *n* is the number of list elements. Such linear complexity
    is not especially bad if you don't need to perform many element index lookups,
    but it can have a negative performance impact if there is a need for many such
    operations.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you need fast search over a list, you can try the `bisect` module from the
    Python standard library. The functions in this module are mainly designed for
    inserting or finding insertion indexes for given values in a way that will preserve
    the order of the already sorted sequence. Anyway, they can be used for efficiently
    finding element indexes with a bisection algorithm. Here is the recipe from the
    official documentation of the function that finds the element index using a binary
    search:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Note that every function from the bisect module requires a sorted sequence in
    order to work. If your list is not in the correct order, then sorting it is a
    task with at least *O(n log n)* complexity. This is a worse class than *O(n)*,
    so sorting the whole list for performing only a single search will definitely
    not pay off. However, if you need to perform a lot of index searches in a huge
    list that does not need to change often, then using a single sort operation bisect
    may be a very good trade off.
  prefs: []
  type: TYPE_NORMAL
- en: Also, if you already have a sorted list, you can insert new items into that
    list using `bisect` without needing to re-sort it.
  prefs: []
  type: TYPE_NORMAL
- en: Using a set instead of a list
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When you need to build a sequence of distinct values out of a given sequence,
    the first algorithm that might come to your mind is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The complexity is introduced by the lookup in the `result` list with the `in`
    operator that has the time complexity, *O(n)*. It is then used in the loop, which
    costs *O(n*). So, the overall complexity is quadratic—*O(n²**)*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using a `set` type for the same work will be faster because the stored values
    are looked up using hashes same as in the `dict` type. Also, `set` ensures the
    uniqueness of elements, so we don''t need to do anything more but create a new
    set from our `sequence` object. In other words, for each value in `sequence`,
    the time taken to see if it is already in the `set` will be constant:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This lowers the complexity to *O(n),* which is the complexity of the `set` object
    creation. The additional advantage is shorter and more explicit code.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When you try to reduce the complexity of an algorithm, carefully consider your
    data structures. There are a range of built-in types, so pick the right one.
  prefs: []
  type: TYPE_NORMAL
- en: Cut the external calls, reduce the workload
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A part of the complexity is introduced by calls to other functions, methods,
    and classes. In general, get as much of the code out of the loops as possible.
    This is doubly important for nested loops. Don't recalculate over and over those
    things that can be calculated before the loop even begins. Inner loops should
    be tight.
  prefs: []
  type: TYPE_NORMAL
- en: Using collections
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `collections` module provides high-performance alternatives to the built-in
    container types. The main types available in this module are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`deque`: A list-like type with extra features'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`defaultdict`: A dict-like type with a built-in default factory feature'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`namedtuple`: A tuple-like type that assigns keys for members'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: deque
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A `deque` is an alternative implementation for lists. While a list is based
    on arrays, a `deque` is based on a doubly linked list. Hence, a `deque` is much
    faster when you need to insert something into its middle or head but much slower
    when you need to access an arbitrary index.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, thanks to the overallocation of an internal array in the Python `list`
    type, not every `list.append()` call requires memory reallocation, and the average
    complexity of this method is *O(1)*. Still, *pops* and *appends* are generally
    faster when performed on linked lists instead of arrays. The situation changes
    dramatically when the element needs to be added on arbitrary point of sequence.
    Because all elements on the right of the new one need to be shifted in an array,
    the complexity of `list.insert()` is *O(n)*. If you need to perform a lot of pops,
    appends, and inserts, the `deque` in place of the list may provide substantial
    performance improvement. But always be sure to profile your code before switching
    from a `list` to the `deque`, because a few things that are fast in arrays (such
    as accessing arbitrary index) are extremely inefficient in linked lists.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, if we measure the time of appending one element and removing it
    from the sequence with `timeit`, the difference between `list` and `deque` may
    not even be noticeable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'But if we do similar comparison for situations when we want to add and remove
    the first element of the sequence, the performance difference is impressive:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'And the difference is, it gets bigger when the size of the sequence grows.
    Here is an example of the same test performed on lists containing 10,000 elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Thanks to efficient `append()` and `pop()` methods that work at the same speed
    from both ends of the sequence, `deque` makes a perfect type for implementing
    queues. For example, a **FIFO** (**First In First Out**) queue will definitely
    be much more efficient if implemented with a `deque` instead of `list`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`deque` works great when implementing queues. Anyway, starting from Python
    2.6 there is a separate `queue` module in Python''s standard library that provides
    basic implementation for FIFO, LIFO, and priority queues. If you want to utilize
    queues as a mechanism of interthread communication, you should really use classes
    from the `queue` module instead of `collections.deque`. This is because these
    classes provide all the necessary locking semantics. If you don''t use threading
    and don''t utilize queues as a communication mechanism, then `deque` should be
    enough to provide queue implementation basics.'
  prefs: []
  type: TYPE_NORMAL
- en: defaultdict
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `defaultdict` type is similar to the `dict` type but adds a default factory
    for new keys. This avoids writing an extra test to initialize the mapping entry
    and is more efficient than the `dict.setdefault` method.
  prefs: []
  type: TYPE_NORMAL
- en: '`defaultdict` seems just like syntactic sugar over `dict` that simply allows
    you to write shorter code. In fact, the fallback to a predefined value on a failed
    key lookup is also slightly faster than the `dict.setdefault()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The difference isn't great because the computational complexity hasn't changed.
    The `dict.setdefault` method consist of two steps (key lookup and key set), both
    of which have a complexity of *O(1)*, as we have seen in the *Dictionaries* section
    in [Chapter 2](ch02.html "Chapter 2. Syntax Best Practices – below the Class Level"),
    *Syntax Best Practices – below the Class Level*. There is no way to have a complexity
    class lower than *O(1)*. But it is indisputably faster in some situations and
    it is worth knowing because every small speed improvement counts when optimizing
    critical code sections.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `defaultdict` type takes a factory as a parameter and can therefore be
    used with built-in types or classes whose constructor does not take arguments.
    Here is an example from the official documentation that shows how to use `defaultdict`
    for counting:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: namedtuple
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`namedtuple` is a class factory that takes a type name and a list of attributes
    and creates a class out of it. The class can then be used to instantiate a tuple-like
    object and provide accessors for its elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: It can be used to create records that are easier to write compared to a custom
    class that would require some boilerplate code to initialize values. On the other
    hand, it is based on tuple, so access to its elements by index is very fast. The
    generated class can be subclassed to add more operations.
  prefs: []
  type: TYPE_NORMAL
- en: The gain from using `namedtuple` instead of other datatypes may not be obvious
    at first. The main advantage is that it is way more easier to use, understand,
    and interpret than ordinary tuples. Tuple indexes don't carry any semantics, so
    it is great to access tuple elements by attributes too. However, you could get
    the same benefit from dictionaries that have an *O(1)* average complexity of get/set
    operations.
  prefs: []
  type: TYPE_NORMAL
- en: The first advantage in terms of performance is that `namedtuple` is still the
    flavor of `tuple`. It means that it is immutable, so the underlying array storage
    is allocated exactly for the needed size. Dictionaries, on the other hand, need
    to use overallocation of the internal hash table to ensure low average complexity
    of get/set operations. So, `namedtuple` wins over `dict` in terms of memory efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: The fact that `namedtuple` is based on a tuple may also be beneficial for performance.
    Its elements may be accessed by an integer index, like in two other simple sequence
    objects—lists and tuples. This operation is both simple and fast. In the case
    of `dict` or custom class instances (that also use dictionaries for storing attributes),
    the element access requires hash table lookup. It is highly optimized to ensure
    good performance independently from collection size, but the mentioned *O(1)*
    complexity is actually only the *average complexity*. The actual, amortized worst
    case complexity for set/get operations in `dict` is *O(n)*. The real amount of
    work when performing such an operation at a given moment is dependent on both
    collection size and its history. So, in sections of code that are critical for
    performance, sometimes it may be wise to use lists or tuples instead of dictionaries.
    This is only because they are more predictable when it comes to performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'In such a situation, `namedtuple` is a great type that combines the advantages
    of dictionaries and tuples:'
  prefs: []
  type: TYPE_NORMAL
- en: In sections where readability is more important, the attribute access may be
    preferred
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In performance-critical sections, elements may be accessed by their indexes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Reduced complexity can be achieved by storing the data in an efficient data
    structure that works well with the way the algorithm will use it.
  prefs: []
  type: TYPE_NORMAL
- en: That said, when the solution is not obvious, you should consider dropping and
    rewriting the incriminated part instead of killing the code readability for the
    sake of performance.
  prefs: []
  type: TYPE_NORMAL
- en: Often, the Python code can be both readable and fast. So, try to find a good
    way to perform the work instead of trying to work around a flawed design.
  prefs: []
  type: TYPE_NORMAL
- en: Using architectural trade-offs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When your code cannot be improved any further by reducing the complexity or
    choosing the proper data structure, a good approach may be to consider doing some
    trade-offs. If we review user problems and define what is really important for
    them, we can relax some of the application requirements. The performance can often
    be improved by:'
  prefs: []
  type: TYPE_NORMAL
- en: Replacing exact solution algorithms with heuristics and approximation algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deferring some work to delayed task queues
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using probabilistic data structures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using heuristics and approximation algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Some algorithmic problems simply don't have *good state of the art* solutions
    that could run in time acceptable to the user. For example, consider a program
    that deals with some complex optimization problems such as **Traveling Salesman
    Problem** (**TSP**) or **Vehicle Routing Problem** (**VRP**). Both problems are
    *NP-hard* problems in combinatorial optimization. The exact algorithms for such
    problems that have low complexity are not known. This means that the size of the
    problems that can be practically solved is greatly limited. For very large inputs,
    it is very unlikely that it will be able to provide the exact solution in a time
    that would be acceptable for any user.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fortunately, it is very probable that the user is not interested in the best
    possible solution but the one that is good enough and the one that can be obtained
    in a timely manner. So, it really makes sense to use **heuristics** or **approximation
    algorithms** whenever they provide an acceptable quality of results:'
  prefs: []
  type: TYPE_NORMAL
- en: Heuristics solve given problems by trading optimality, completeness, accuracy,
    or precision for speed. They concentrate on the speed, but it may be really hard
    to prove their solution quality compared to the result of exact algorithms.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Approximation algorithms are similar in idea to heuristics, but unlike heuristics
    have provable solution quality and run-time bounds.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For instance, there are known good heuristics and approximation problems that
    can solve extremely large TSP problems within a reasonable time. They also have
    a high probability of producing results just 2-5% from the optimal solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another good thing about heuristics is that they don''t always need to be constructed
    from scratch for every new problem you need to solve. Their higher-level versions,
    called **metaheuristics**, provide strategies for solving mathematical optimization
    problems that are not problem-specific and can thus be applied in many situations.
    Some popular metaheuristic algorithms include:'
  prefs: []
  type: TYPE_NORMAL
- en: Simulated annealing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Genetic algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tabu search
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ant colony optimization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evolutionary computation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using task queues and delayed processing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sometimes it's not about doing a lot but about doing things at the right time.
    A good example of that is sending e-mails in web applications. In that case, increased
    response times may not necessarily be the result of your implementation. The response
    time may be dominated by some third-party service, such as an e-mail server. Can
    you optimize your application if it just spends most of its time on waiting for
    other services to reply?
  prefs: []
  type: TYPE_NORMAL
- en: 'The answer is both: yes and no. If you don''t have any control over a service
    that is the main contributor to your processing time and there is no other faster
    solution you could use, you, of course, cannot speed it up any further. You cannot
    simply skip in time to get the replies you are waiting for. A simple example of
    processing an HTTP request that results in sending an e-mail is presented in the
    following figure (*Figure* *1*). You cannot reduce the waiting time, but you can
    change the way users will perceive it!'
  prefs: []
  type: TYPE_NORMAL
- en: '![Using task queues and delayed processing](graphics/B05295_12_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1 An example of synchronous e-mail delivery in web application
  prefs: []
  type: TYPE_NORMAL
- en: The usual pattern for such type of problems is using message/task queues. When
    you need to do something that may take an indefinite amount of time, just add
    this to the queue of work that needs to be done and immediately respond to the
    user whose request was accepted. Here, we come to the reason why sending e-mails
    is such a great example. E-mails are already task queues! If you submit a new
    message to the e-mail server using SMTP protocol, the successful response does
    not mean that your e-mail was delivered to addressee. It means that the e-mail
    was delivered to the e-mail server and it will try later to deliver it further.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, if the response from the server does not guarantee that the e-mail was
    delivered at all, you don''t need to wait for it in order to generate an HTTP
    response for the user. The updated flow of processing requests with the usage
    of the task queue is presented in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Using task queues and delayed processing](graphics/B05295_12_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2 An example of asynchronous e-mail delivery in web application
  prefs: []
  type: TYPE_NORMAL
- en: Of course, your e-mail server may be responding blazingly fast, but you need
    some more time to generate the message that needs to be sent. Perhaps you are
    generating yearly reports in an XLS format or maybe delivering invoices in PDF
    files. If you use e-mail transport that is already asynchronous, then put the
    whole message generation task to the message processing system too. If you cannot
    guarantee the exact time of delivery, then you should not bother to generate your
    deliverables synchronously.
  prefs: []
  type: TYPE_NORMAL
- en: 'The proper usage of task/message queues in critical sections of the application
    can also give you other benefits:'
  prefs: []
  type: TYPE_NORMAL
- en: Web workers that serve HTTP requests will be relieved from additional work and
    processing requests faster. This means that you will be able to process more requests
    with the same resources and thus handle greater load.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Message queues are generally more immune to transient failures of external services.
    For instance, if your database or e-mail server times out from time to time, you
    can always re-queue the currently processed task and retry it later.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With a good message queue implementation, you can easily distribute the work
    on multiple machines. This approach may improve the scalability of some of your
    application components.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As you can see in *Figure 2*, adding an asynchronous task processing to your
    application inevitably increases the complexity of the whole system's architecture.
    You will need to set up some new backing services (a message queue such as RabbitMQ)
    and create workers that will be able to process these asynchronous jobs. Fortunately,
    there are some popular tools for building distributed task queues. The most popular
    one among Python developers is **Celery** ([http://www.celeryproject.org/](http://www.celeryproject.org/)).
    It is a full-fledged task queue framework with support of multiple message brokers
    that also allows for the scheduled execution of tasks (it can replace your `cron`
    jobs). If you need something simpler, then RQ ([http://python-rq.org/](http://python-rq.org/))
    might be a good alternative. It is a lot simpler than Celery and uses Redis key/value
    storage as its message broker (**RQ** actually stands for **Redis Queue**).
  prefs: []
  type: TYPE_NORMAL
- en: 'Although there are some good and battle-tested tools, you should always carefully
    consider your approach to the task queues. Definitely not every kind of work should
    be processed in queues. They are good at solving a few types of issues but also
    introduce a load of new problems:'
  prefs: []
  type: TYPE_NORMAL
- en: Increased complexity of system architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dealing with *more than once* deliveries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More services to maintain and monitor
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Larger processing delays
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More difficult logging
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using probabilistic data structures
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Probabilistic data structures are structures that are designed to store collections
    of values in a way that allows you to answer some specific questions within time
    or resource constraints that would not be possible with other data structures.
    The most important fact is that the answer is only probable to be true or is the
    approximation of the real value. However, the probability of the correct answer
    or its accuracy can be easily estimated. So, despite not always giving the correct
    answer, it can be still useful if we accept some level of error.
  prefs: []
  type: TYPE_NORMAL
- en: There are a lot of data structures with such probabilistic properties. Each
    one of them solves some specific problems, and due to theirs stochastic nature
    cannot be used in every situation. But to give a practical example, let's talk
    about one of them that is especially popular—**HyperLogLog**.
  prefs: []
  type: TYPE_NORMAL
- en: HyperLogLog (refer to [https://en.wikipedia.org/wiki/HyperLogLog](https://en.wikipedia.org/wiki/HyperLogLog))
    is an algorithm that approximates the number of distinct elements in a multiset.
    With ordinary sets, you need to store every element, and this may be very impractical
    for very large datasets. HLL is distinct from the classical way of implementing
    sets as programming data structures. Without digging into implementation details,
    let's say that it only concentrates on providing an approximation of the set cardinality.
    Thus, real values are never stored. They cannot be retrieved, iterated, and tested
    for membership. HyperLogLog trades accuracy and correctness for time complexity
    and size in memory. For instance, the Redis implementation of HLL takes only 12k
    bytes with a standard error of 0.81% with no practical limit of collection size.
  prefs: []
  type: TYPE_NORMAL
- en: Using probabilistic data structures is a very interesting way of solving performance
    problems. In most cases, it is about trading off some accuracy or correctness
    for faster processing or better resource usage. But it does not always need to
    be that way. Probabilistic data structures are very often used in key/value storage
    systems to speed up key lookups. One of the popular techniques used in such systems
    is called approximate member query (AMQ). One interesting data structure that
    can be used for that purpose is Bloom filter (refer to [https://en.wikipedia.org/wiki/Bloom_filter](https://en.wikipedia.org/wiki/Bloom_filter)).
  prefs: []
  type: TYPE_NORMAL
- en: Caching
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When some of your application function takes too long to compute, the useful
    technique to consider is caching. Caching is nothing but saving a return value
    for future reference. The result of a function or method that is expensive to
    run can be cached as long as:'
  prefs: []
  type: TYPE_NORMAL
- en: The function is deterministic and the results have the same value every time,
    given the same input
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The return value of the function continues to be useful and valid for some period
    of time (nondeterministic)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In other words, a deterministic function always returns the same result for
    the same set of arguments, whereas a nondeterministic one returns results that
    may vary in time. Such an approach usually greatly reduces the time of computation
    and allows you to save a lot of computer resources.
  prefs: []
  type: TYPE_NORMAL
- en: 'The most important requirement for any caching solution is to have a storage
    that allows you to retrieve saved values significantly faster than it takes to
    calculate them. Good candidates for caching are usually:'
  prefs: []
  type: TYPE_NORMAL
- en: Results from callables that query databases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Results from callables that render static values, such as file content, web
    requests, or PDF rendering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Results from deterministic callables that perform complex calculations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Global mappings that keep track of values with expiration times, such as web
    session objects
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Results that needs to be accessed often and quickly
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another important use case for caching is saving results from third-party APIs
    served over the Web. This may greatly improve application performance by cutting
    off the network latencies but also allows you to save money if you are billed
    for every request to such API.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on your application architecture, the cache can be implemented in
    many ways and with various levels of complexity. There are many ways to provide
    caching and complex applications can use different approaches on different levels
    of the application architecture stack. Sometimes a cache may be as simple as a
    single global data structure (usually a `dict`) kept in the process memory. In
    other situations, you may want to set up a dedicated caching service that will
    run on carefully tailored hardware. This section will provide you with basic information
    on the most popular caching approaches and guide you through the usual use cases
    and also the common pitfalls.
  prefs: []
  type: TYPE_NORMAL
- en: Deterministic caching
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deterministic functions are the easiest and safest use case for caching. Deterministic
    functions always return the same value if given exactly the same input, so generally
    you can store their result indefinitely. The only limitation is the size of storage
    you use for caching. The simplest way to cache such results is to put them into
    process memory because it is usually the fastest place to retrieve data from.
    Such a technique is often called **memoization**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Memoization is very useful when optimizing recursive functions that may evaluate
    the same inputs multiple times. We already discussed recursive implementation
    for the Fibonacci sequence in [Chapter 7](ch07.html "Chapter 7. Python Extensions
    in Other Languages"), *Python Extensions in Other Languages*. Back then, we tried
    to improve the performance of our program with C and Cython. Now we will try to
    achieve the same goal by simpler means—with the help of caching. But before we
    do that, let''s recall the code for the `fibonacci()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: As we see, `fibonacci()` is a recursive function that calls itself twice if
    the input value is more than two. This makes it highly inefficient. The run time
    complexity is *O(2^n**)* and its execution creates a very deep and vast call tree.
    For the large value, this function will take extremely long to execute and there
    is high chance of quickly exceeding the maximal recursion limit of the Python
    interpreter.
  prefs: []
  type: TYPE_NORMAL
- en: If you take a closer look at *Figure 3,* which presents an example call tree,
    you will see that it evaluates many of the intermediate results multiple times.
    A lot of time and resources could be saved if we could reuse some of these values.
  prefs: []
  type: TYPE_NORMAL
- en: '![Deterministic caching](graphics/B05295_12_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3 Call tree for fibonacci(5) execution
  prefs: []
  type: TYPE_NORMAL
- en: 'A simple memoization attempt would be to store results of the previous runs
    in a dictionary and retrieve them if they are available. Both the recursive calls
    in the `fibonacci()` function are contained in a single line of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We know that Python evaluates instructions from left to right. This means that,
    in this situation, the call to the function with a higher argument value will
    be executed before the call to the function with a lower argument. Thanks to this,
    we can provide memoizaton by constructing a very simple decorator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: We used the dictionary on the closure of the `memoize()` decorator as a simple
    storage from cached values. Saving and retrieving value to that data structure
    has an average *O(1)* complexity, so this greatly reduces the overall complexity
    of the memoized function. Every unique function call will be evaluated only once.
    The call tree of such an updated function is presented in *Figure 4*. Without
    going into mathematical proofs, we can visually deduce that without changing the
    core of the `fibonacci()` function, we reduced the complexity from the very expensive
    *O(2n)* to the linear *O(n)*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Deterministic caching](graphics/B05295_12_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4 A call tree for fibonacci(5) execution with memoization
  prefs: []
  type: TYPE_NORMAL
- en: 'The implementation of our `memoize()` decorator is, of course, not perfect.
    It worked well for that simple example, but it definitely isn''t a reusable piece
    of software. If you need to memoize functions with multiple arguments or want
    to limit the size of your cache, you need something more generic. Luckily, the
    Python standard library provides a very simple and reusable utility that may be
    used in most cases when you need to cache in memory the results of deterministic
    functions. It is the `lru_cache(maxsize, typed)` decorator from the `functools`
    module. The name comes from the LRU cache, which stands for *last recently used*.
    The additional parameters allow for finer control over memoization behavior:'
  prefs: []
  type: TYPE_NORMAL
- en: '`maxsize`: This sets the maximum size of the cache. The `None` value means
    no limit at all.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`typed`: This defines if the values of different types that compare as equal
    should be cached as giving the same result.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The usage of `lru_cache` in our Fibonacci sequence example would be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Nondeterministic caching
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The caching of nondeterministic functions is way more trickier that memoization.
    Due to the fact that every execution of such a function may give different results,
    it is usually impossible to use previous values for an arbitrarily long amount
    of time. What you need to do is to decide for how long a cached value can be considered
    valid. After a defined period of time passes, the stored results are considered
    to be stale and the cache needs to be refreshed by a new value.
  prefs: []
  type: TYPE_NORMAL
- en: 'Nondeterministic functions that are usually a subject of caching very often
    depend on some external state that is hard to track inside of your application
    code. Typical examples of components are:'
  prefs: []
  type: TYPE_NORMAL
- en: Relational databases and generally any type of structured data storage engine
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Third-party services accessible through network connection (web APIs)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Filesystems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, in other words, nondeterministic caching is used in any situation when you
    temporarily use precomputed results without being sure if they represent a state
    that is consistent with the state of other system components (usually, the backing
    service).
  prefs: []
  type: TYPE_NORMAL
- en: Note that such an implementation of caching is obviously a trade-off. Thus,
    it is somehow related to the techniques we featured in the *Using architectural
    trade-offs* section. If you resign from running part of your code every time and
    instead use the results saved in the past, you are risking using data that becomes
    stale or represents an inconsistent state of your system. This way, you are trading
    the correctness and/or completeness for speed and performance.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, such caching is efficient as long as the time taken to interact with
    the cache is less than the time taken by the function. If it's faster to simply
    recalculate the value, by all means do so! That's why setting up a cache has to
    be done only if it's worth it; setting it up properly has a cost.
  prefs: []
  type: TYPE_NORMAL
- en: The actual things that are cached are usually the whole results of interaction
    with other components of your system. If you want to save time and resources when
    communicating with the database, it is worth to cache expensive queries. If you
    want to reduce the number of I/O operations, you may want to cache the content
    of the files that are accessed very often (configuration files, for instance).
  prefs: []
  type: TYPE_NORMAL
- en: Techniques for caching non-deterministic functions are actually very similar
    to those used in caching the deterministic ones. The most notable difference is
    that they usually require the option to invalidate cached values by their age.
    This means that the `lru_cache()` decorator from the `functools` module has very
    limited use in such situations. It should not be so hard to extend this function
    to provide the expiration feature, but I will leave it as an exercise for you.
  prefs: []
  type: TYPE_NORMAL
- en: Cache services
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We said that nondeterministic caching can be implemented using local process
    memory, but actually it is rarely done that way. It's because local process memory
    is very limited in its utility as storage for caching in large applications.
  prefs: []
  type: TYPE_NORMAL
- en: If you run into a situation where non-deterministic caching is your preferred
    solution to solve performance problems, you usually need something more than that.
    Usually, nondeterministic caching is your *must have* solution when you need to
    serve data or service to multiple users at the same time. If it's true, then sooner
    or later you will need to ensure that users can be served concurrently. While
    local memory provides a way to share data between multiple threads, it may not
    be the best concurrency model for every application. It does not scale well, so
    you will eventually need to run your application as multiple processes.
  prefs: []
  type: TYPE_NORMAL
- en: If you are lucky enough, you may need to run your application on hundreds or
    thousands of machines. If you would like to store cached values in local memory,
    it means that your cache needs to be duplicated on every process that requires
    it. It isn't only a total waste of resources. If every process has its own cache,
    that is already a trade-off between speed and consistency, how can you guarantee
    that all caches are consistent with each other?
  prefs: []
  type: TYPE_NORMAL
- en: Consistency across subsequent request is a serious concern (especially) for
    web applications with distributed backends. In complex distributed systems, it
    is extremely hard to ensure that the user will be always consistently served by
    the same process hosted on the same machine. It is of course doable to some extent,
    but once you solve that problem, ten others will pop up.
  prefs: []
  type: TYPE_NORMAL
- en: If you are making an application that will need to serve multiple concurrent
    users, then the best way to handle a nondeterministic cache is to use some dedicated
    service for that. With tools such as Redis or Memcached, you allow all your application
    processes to share the same cached results. This both reduces the usage of precious
    computing resources and saves you from problems caused by having multiple independent
    and inconsistent caches.
  prefs: []
  type: TYPE_NORMAL
- en: Memcached
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you want to be serious about caching, **Memcached** is a very popular and
    battle-tested solution. This cache server is used by big applications such as
    Facebook or Wikipedia to scale their websites. Among simple caching features,
    it has clustering capabilities that makes it possible to set up a highly efficient
    distributed cache system in no time.
  prefs: []
  type: TYPE_NORMAL
- en: 'The tool is Unix-based but can be driven from any platform and from many languages.
    There are many Python clients that differ slightly from each other but the basic
    usage is usually the same. The simplest interaction with Memcached almost always
    consists of three methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '`set(key, value)`: This saves the value for the given key'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`get(key)`: This gets the value for the given key if it exists'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`delete(key)`: This deletes the value under the given key if it exists'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is an example of integration with Memcached using one of the popular Python
    packages—`pymemcached`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'One of the downsides of Memcached is that it is designed to store values either
    as strings or a binary blob, and this isn''t compatible with every native Python
    type. Actually, it is compatible with only one—strings. This means that more complex
    types need to be serialized in order to be successfully stored in Memcached. A
    common serialization choice for simple data structures is JSON. Here is an example
    of using JSON serialization with `pymemcached`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The other problem that is very common when working with every caching service
    that works on the key/value storage principle is how to choose key names.
  prefs: []
  type: TYPE_NORMAL
- en: For cases when you cache simple function invocations that have basic parameters,
    the problem is usually simple. You can convert the function name and its arguments
    to strings and concatenate them together. The only thing you need to care about
    is to make sure there are no collisions between keys created for different functions
    if you use cache in many parts of your application.
  prefs: []
  type: TYPE_NORMAL
- en: A more problematic case is when cached functions have complex arguments consisting
    of dictionaries or custom classes. In that case, you need to find a way to convert
    such invocation signatures to cache keys in a consistent manner.
  prefs: []
  type: TYPE_NORMAL
- en: The last problem is that Memcached, like many other caching services, does not
    tend to like very long key strings. Usually, the shorter the better. Long keys
    may either reduce performance or just not fit the hardcoded service limits. For
    instance, if you cache whole SQL queries, the query strings themselves are generally
    good unique identifiers that could be used as keys. But on the other hand, complex
    queries are generally too long to be stored in typical caching services such as
    Memcached. A common practice is to calculate the **MD5**, **SHA**, or any other
    hash function and use it as a cache key instead. The Python standard library has
    a `hashlib` module that provides implementation for few popular hash algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Remember that calculating a hash comes at a price. However, sometimes it is
    the only viable solution. It is also a very useful technique when dealing with
    complex types that need to be used when creating cache keys. One important thing
    to care about when using hashing functions is hash collisions. There is no hash
    function that guarantees that collisions will never occur, so always be sure to
    know the probability and mind such risks.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, you have learned:'
  prefs: []
  type: TYPE_NORMAL
- en: How to define the complexity of the code and some approaches to reduce it
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to improve performance using some architectural trade-offs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What caching is and how to use it to improve application performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The preceding methods concentrated our optimization efforts inside a single
    process. We tried to reduce the code complexity, choose better datatypes, or reuse
    old function results. If that did not help, we tried to make some trade-offs using
    approximations, doing less, or leaving work for later.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will discuss a few techniques for concurrency and parallel
    processing in Python.
  prefs: []
  type: TYPE_NORMAL
