- en: Collecting and Querying Logs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In critical moments, men sometimes see exactly what they wish to see.
  prefs: []
  type: TYPE_NORMAL
- en: '- *Spock*'
  prefs: []
  type: TYPE_NORMAL
- en: So far, our primary focus was on metrics. We used them in different forms and
    for different purposes. In some cases, we used metrics to scale Pods and nodes.
    In others, metrics were used to create alerts that would notify us when there
    is an issue that cannot be fixed automatically. We also created a few dashboards.
  prefs: []
  type: TYPE_NORMAL
- en: However, metrics are often not enough. That is especially true when dealing
    with issues that require manual interventions. When metrics alone are insufficient,
    we usually need to consult logs hoping that they will reveal the cause of the
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: Logging is often misunderstood or, to be more precise, mixed with metrics. For
    many, the line between logs and metrics is blurred. Some are extracting metrics
    from logs. Others are treating metrics and logs as the same source of information.
    Both approaches are wrong. Metrics and logs are separate entities, they serve
    different purposes, and there is a clear distinction between them. We store them
    separately, and we use them to solve different types of issues. We'll park that
    and a few other discussions. Instead of going into details based on theory, we'll
    explore them through hands-on examples. For that, we need a cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You know the drill. We'll move into the directory with the `vfarcic/k8s-specs`
    ([https://github.com/vfarcic/k8s-specs](https://github.com/vfarcic/k8s-specs))
    repository, we'll pull the latest version of the code just in case I pushed something
    recently, and we'll create a new cluster unless you already have one at hand.
  prefs: []
  type: TYPE_NORMAL
- en: All the commands from this chapter are available in the `07-logging.sh` ([https://gist.github.com/vfarcic/74774240545e638b6cf0e01460894f34](https://gist.github.com/vfarcic/74774240545e638b6cf0e01460894f34))
    Gist.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This time, the requirements for the cluster changed. We need much more memory
    than before. The main culprit is ElasticSearch which is very resource hungry.
  prefs: []
  type: TYPE_NORMAL
- en: If you're using **Docker for Desktop** or **minikube**, you'll need to increase
    the memory dedicated to the cluster to **10 GB**. If that's too much for your
    laptop, you might choose to read the *Exploring Centralized Logging Through Elasticsearch,
    Fluentd, and Kibana* without running the examples or you might have to switch
    to one of the Cloud providers (AWS, GCP, or Azure).
  prefs: []
  type: TYPE_NORMAL
- en: In the case of **EKS** and **AKS**, we'll need bigger nodes. For EKS we'll use
    **t2.large** and for AKS **Standard_B2ms**. Both are based on **2 CPUs** and **8
    GB RAM**.
  prefs: []
  type: TYPE_NORMAL
- en: '**GKE** requirements are the same as before.'
  prefs: []
  type: TYPE_NORMAL
- en: On top of new requirements, it should be noted that we do NOT need Prometheus
    in this chapter, so I removed it from the Gists.
  prefs: []
  type: TYPE_NORMAL
- en: Feel free to use one of the Gists that follow to create a new cluster, or to
    validate that the one you're planning to use meets the requirements.
  prefs: []
  type: TYPE_NORMAL
- en: '`gke-monitor.sh`: **GKE** with 3 n1-standard-1 worker nodes, **nginx Ingress**,
    **tiller**, and cluster IP stored in environment variable **LB_IP** ([https://gist.github.com/vfarcic/10e14bfbec466347d70d11a78fe7eec4](https://gist.github.com/vfarcic/10e14bfbec466347d70d11a78fe7eec4)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eks-logging.sh`: **EKS** with 3 t2.large worker nodes, **nginx Ingress**,
    **tiller**, **Metrics Server**, **Cluster Autoscaler**, and cluster IP stored
    in environment variable **LB_IP** ([https://gist.github.com/vfarcic/a783351fc9a3637a291346dd4bc346e7](https://gist.github.com/vfarcic/a783351fc9a3637a291346dd4bc346e7)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`aks-logging.sh`: **AKS** with 3 Standard_B2ms worker nodes, **nginx Ingress**,
    and **tiller**, and cluster IP stored in environment variable **LB_IP** ([https://gist.github.com/vfarcic/c4a63b92c03a0a1c721cb63b07d2ddfc](https://gist.github.com/vfarcic/c4a63b92c03a0a1c721cb63b07d2ddfc)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`docker-logging.sh`: **Docker for Desktop** with **2 CPUs** and **10 GB RAM**,
    **nginx Ingress**, **tiller**, **Metrics Server**, and cluster IP stored in environment
    variable **LB_IP** ([https://gist.github.com/vfarcic/17d4f11ec53eed74e4b5e73debb4a590](https://gist.github.com/vfarcic/17d4f11ec53eed74e4b5e73debb4a590)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`minikube-logging.sh`: **minikube** with **2 CPUs** and **10 GB RAM**, **ingress**,
    **storage-provisioner**, **default-storageclass**, and **metrics-server** addons
    enabled, **tiller**, and cluster IP stored in environment variable **LB_IP** ([https://gist.github.com/vfarcic/9f72c8451e1cca71758c70195c1c9f07](https://gist.github.com/vfarcic/9f72c8451e1cca71758c70195c1c9f07)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we have a working cluster, we'll explore how to use logs through `kubectl`.
    That will provide a base for more comprehensive solutions that follow.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring logs through kubectl
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first contact most people have with logs in Kubernetes is through `kubectl`.
    It is almost unavoidable not to use it.
  prefs: []
  type: TYPE_NORMAL
- en: As we're learning how to tame the Kubernetes beast, we are bound to check logs
    when we get stuck. In Kubernetes, the term "logs" is reserved for the output produced
    by our and third-party applications running inside a cluster. However, those exclude
    the events generated by different Kubernetes resources. Even though many would
    call them logs as well, Kubernetes separates them from logs and calls them events.
    I'm sure that you already know how to retrieve logs from the applications and
    how to see Kubernetes events. Nevertheless, we'll explore them briefly here as
    well since that will add relevance to the discussion we'll have later on. I promise
    to keep it short, and you are free to skip this section if a brief overview of
    logging and events baked into Kubernetes is too basic for you.
  prefs: []
  type: TYPE_NORMAL
- en: We'll install the already familiar `go-demo-5` application. It should generate
    enough logs for us to explore them. Since it consists of a few resources, we are
    bound to create some Kubernetes events as well.
  prefs: []
  type: TYPE_NORMAL
- en: Off we go.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: We rolled out `go-demo-5` and sent a `curl` request to confirm that it is indeed
    working.
  prefs: []
  type: TYPE_NORMAL
- en: The outputs and screenshots in this chapter are taken from minikube, except
    inside the sections dedicated to exclusively GKE, EKS, and AKS. There might be
    slight differences between what you see here and what you can observe on your
    screen.
  prefs: []
  type: TYPE_NORMAL
- en: To see "logs" generated by Kubernetes and limited to a specific resource, we
    need to retrieve the events.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The output, limited to the messages in the `Events` section, is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The events you see in front of you are, in a way, Kubernetes logs generated
    by, in this case, the `go-demo-5-db` StatefulSet.
  prefs: []
  type: TYPE_NORMAL
- en: While those events are useful, they are often insufficient. More often than
    not, we do not know in advance where the problem is. If one of our Pods misbehaves,
    the cause might be in that Pod, but it might also be in the ReplicaSet that created
    it, or it might be in the Deployment that created the ReplicaSet, or maybe the
    node got detached from the cluster, or it might be something completely different.
  prefs: []
  type: TYPE_NORMAL
- en: For any but the smallest systems, going from one resource to another and from
    one node to another to find the cause of an issue is anything but practical, reliable,
    and fast.
  prefs: []
  type: TYPE_NORMAL
- en: Simply put, looking at events by describing a resource is not the way to go
    and we need to find an alternative.
  prefs: []
  type: TYPE_NORMAL
- en: But, before we do that, let's see what happens with logs from applications.
  prefs: []
  type: TYPE_NORMAL
- en: We deployed a few replicas of the `go-demo-5` API and a few replicas of the
    MongoDB. How can we explore their logs if we suspect that there is a problem with
    one of them? We can execute `kubectl logs` command like the one that follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The output shows the logs of the `db` container inside the `go-demo-5-db-0`
    Pod.
  prefs: []
  type: TYPE_NORMAL
- en: While the previous output is limited to a single container and a single Pod,
    we can use labels to retrieve logs from multiple Pods.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This time, the output comes from all the Pods with the label `app` set to `go-demo-5`.
    We broadened our results, and that is often all we need. If we know that there
    is something wrong with, let's say, `go-demo-5` Pods, we need to figure out whether
    the issue is present in multiple Pods or it is limited to a single one. While
    the previous command allowed us to broaden our search, if there were something
    suspicious in those logs, we would not know where that comes from. Retrieving
    logs from multiple Pods does not get us any closer to knowing which Pods are misbehaving.
  prefs: []
  type: TYPE_NORMAL
- en: Using labels is still very limiting. They are by no means a substitute for more
    complex querying. We might need to filter the results based on timestamps, nodes,
    keywords, and so on. While we could accomplish some of those things with additional
    `kubectl logs` arguments and creative usage of `grep`, `sed`, and other Linux
    commands, this approach to retrieving, filtering, and outputting logs is far from
    optimum.
  prefs: []
  type: TYPE_NORMAL
- en: More often than not, `kubectl logs` command does not provide us with enough
    options to perform anything but simplest retrieval of logs.
  prefs: []
  type: TYPE_NORMAL
- en: We need a few things to improve our debugging capabilities. We need a potent
    query language that will allow us to filter log entries, we need sufficient information
    about the origin of those logs, we need queries to be fast, and we need access
    to logs created in any part of the cluster. We'll try to accomplish that, and
    a few other things, by setting up a centralized logging solution.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing a centralized logging solution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first thing we need to do is to find a place where we'll store logs. Given
    that we want to have the ability to filter log entries, storing them in files
    should be discarded from the start. What we need is a database, of sorts. It is
    more important that it is fast than transactional, so we are most likely looking
    into a solution that is an in-memory database. But, before we take a look at the
    choices, we should discuss the location of our database. Should we run it inside
    our cluster, or should we use a service? Instead of making that decision right
    away, we'll explore both options, before we make a choice.
  prefs: []
  type: TYPE_NORMAL
- en: There are two major groups of logging-as-a-service types. If we are running
    our cluster with one of the Cloud providers, an obvious choice might be to use
    a logging solution they provide. EKS has AWS CloudWatch, GKE has GCP Stackdriver,
    and AKS has Azure Log Analytics. If you choose to use one of the Cloud vendors,
    that might make a lot of sense. Why bother with setting up your own solution of
    looking for a third-party service, if everything is already set up and waiting
    for you? We'll explore them soon.
  prefs: []
  type: TYPE_NORMAL
- en: Since my mission is to provide instructions that work for (almost) anyone, we'll
    also explore one of the logging-as-a-service solutions found outside hosting vendors.
    But, which one should we select? There are too many solutions in the market. We
    could, for example, choose *Splunk* ([https://www.splunk.com/](https://www.splunk.com/))
    or *DataDog* ([https://www.datadoghq.com/](https://www.datadoghq.com/)). Both
    are excellent choices, and both are much more than only logging solutions. We
    can use them to collect metrics (like with Prometheus). They provide dashboards
    (like Grafana), and a few other things. Later on, we'll discuss whether we should
    combine logs and metrics in a single tool. For now, our focus is only on logging,
    and that's the main reason we'll skip Splunk, DataDog, and similar comprehensive
    tools that offer much more than what we need. That does not mean that you should
    discard them, but rather that this chapter tries to maintain the focus on logging.
  prefs: []
  type: TYPE_NORMAL
- en: There are many logging services available, with *Scalyr* ([https://www.scalyr.com/pricing](https://www.scalyr.com/pricing)),
    [*logdna*](https://logdna.com/) ([https://logdna.com/](https://logdna.com/)),
    *sumo logic* ([https://www.sumologic.com/](https://www.sumologic.com/)) being
    only a few. We won't go through all of them since would take much more time and
    space than I feel is useful. Given that most services are very similar when logging
    is involved, I'll skip a detailed comparison and jump straight into *Papertrail*
    ([https://papertrailapp.com/](https://papertrailapp.com/)), my favorite logging
    service. Please bear in mind that we'll use it only as an example. I will assume
    that you'll check at least a few others and make your own choice based on your
    needs.
  prefs: []
  type: TYPE_NORMAL
- en: Logging-as-a-service might not be a good fit for all. Some might prefer a self-hosted
    solution, while others might not even be allowed to send data outside their clusters.
    In those cases, a self-hosted logging solution is likely the only choice.
  prefs: []
  type: TYPE_NORMAL
- en: Even if you are not restrained to your own cluster, there might be other reasons
    to keep it inside, latency being only one of many. We'll explore a self-hosted
    solution as well, so let us pick one. Which one will it be?
  prefs: []
  type: TYPE_NORMAL
- en: Given that we need a place to store our logs, we might look into traditional
    databases. However, most of them would not fit our needs. Transactional databases
    like MySQL require fixed schemas so we can discard them immediately. NoSQL is
    a better fit so we might choose something like MongoDB. But, that would be a poor
    choice since we require the ability to perform very fast free-text searches. For
    that we probably need an in-memory database. MongoDB is not one of those. We could
    use Splunk Enterprise, but this book is dedicated to free (mostly open source)
    solutions. The only exception we made so far is with Cloud providers, and I intend
    to keep it that way.
  prefs: []
  type: TYPE_NORMAL
- en: The few requirements we mentioned (fast, free-text, in-memory, and a free solution)
    limit our potential candidates to only a few. *Solr* ([http://lucene.apache.org/solr/](http://lucene.apache.org/solr/))
    is one of those, but its usage has been dropping, and it is rarely used today
    (for a good reason). The solution that sticks from a tiny crowd is *Elasticsearch*
    ([https://www.elastic.co/products/elasticsearch](https://www.elastic.co/products/elasticsearch)).
    If you have a preference for a different on-prem solution, consider the examples
    we'll go through as a set of practices that you should be able to apply to other
    centralized logging solutions.
  prefs: []
  type: TYPE_NORMAL
- en: All in all, we'll explore an example of independent logging-as-a-service offering
    (Papertrail), we'll explore the solutions provided with Cloud hosting vendors
    (AWS CloudWatch, GCP Stackdriver, and Azure Log Analytics), and we'll try to set
    up ElasticSearch with a few friends. Those should provide enough examples for
    you to choose which type of a solution fits your use case the best.
  prefs: []
  type: TYPE_NORMAL
- en: But, before we explore the tools where we will store logs, we need to figure
    out how to collect them and ship them to their final destination.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring logs collection and shipping
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For a long time now, there are two major contestants for the "logs collection
    and shipping" throne. Those are *Logstash* ([https://www.elastic.co/products/logstash](https://www.elastic.co/products/logstash))
    and *Fluentd* ([https://www.fluentd.org/](https://www.fluentd.org/)). Both are
    open source, and both are widely accepted and actively maintained. While both
    have their pros and cons, Fluentd turned up to have an edge with cloud-native
    distributed systems. It consumes fewer resources and, more importantly, it is
    not tied to a single destination (Elasticsearch). While Logstash can push logs
    to many different targets, it is primarily designed to work with Elasticsearch.
    For that reason, other logging solutions adopted Fluentd.
  prefs: []
  type: TYPE_NORMAL
- en: As of today, no matter which logging product you embrace, the chances are that
    it will support Fluentd. The culmination of that adoption can be seen by Fluentd's
    entry into the list of *Cloud Native Computing Foundation* ([https://www.cncf.io/](https://www.cncf.io/))
    projects. Even Elasticsearch users are adopting Fluentd over Logstash. What was
    previously commonly referred to as **ELK** (**Elasticsearch**, **Logstash**, **Kibana**)
    stack, is now called **EFK** (**Elasticsearch**, **Fluentd**, **Kibana**).
  prefs: []
  type: TYPE_NORMAL
- en: We'll follow the trend and adopt Fluentd as the solution for collecting and
    shipping logs, no matter whether the destination is Papertrail, Elasticsearch,
    or something else.
  prefs: []
  type: TYPE_NORMAL
- en: We'll install Fluentd soon. But, since Papertrail is our first target, we need
    to create and set up an account. For now, remember that we need to collect logs
    from all the nodes of the cluster and, as you already know, Kubernetes' DaemonSet
    will ensure that a Fluentd Pod will run in each of our servers.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring centralized logging through Papertrail
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first centralized logging solution we'll explore is *Papertrail* ([https://papertrailapp.com/](https://papertrailapp.com/)).
    We'll use it as a representative of a logging-as-a-service solution that can save
    us from installing and, more importantly, maintaining a self-hosted alternative.
  prefs: []
  type: TYPE_NORMAL
- en: Papertrail features live trailing, filtering by timestamps, powerful search
    queries, pretty colors, and quite a few other things that might (or might not)
    be essential when skimming through logs produced inside our clusters.
  prefs: []
  type: TYPE_NORMAL
- en: The first thing we need to do is to register or, if this is not the first time
    you tried Papertrail, to log in.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Please follow the instructions to register or to log in if you already have
    a user in their system.
  prefs: []
  type: TYPE_NORMAL
- en: You will be glad to find out that Papertrail provides a free plan that allows
    storage of 50 MB of logs searchable for one day, as well as a full year of downloadable
    archives. That should be more than enough for running the examples we are about
    to explore. If you have a relatively small cluster, that should keep you going
    indefinitely. Their prices are reasonable, even if your cluster is bigger and
    you have more monthly logs than 50 MB.
  prefs: []
  type: TYPE_NORMAL
- en: Arguably, they are so cheap that we can say that it provides a better return
    on investment than if we'd run an alternative solution inside our own cluster. After
    all, nothing is free. Even self-hosted solutions based on open source create costs
    in maintenance time as well as in compute power.
  prefs: []
  type: TYPE_NORMAL
- en: For now, what matters is that the examples we'll run with Papertrail will be
    well within their free plan.
  prefs: []
  type: TYPE_NORMAL
- en: If you have a small operation, Papertrail will work well. But, if you're having
    many applications and a bigger cluster, you might be wondering whether Papertrail
    scales to suit your needs. Worry not. One of their customers is GitHub, and they
    are likely bigger than you are. Papertrail can handle (almost) any load. Whether
    it is a good solution for you is yet to be discovered. Read on.
  prefs: []
  type: TYPE_NORMAL
- en: Let's go to the start screen unless you are already there.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: If you were redirected to the welcome screen, you are not authenticated (your
    session might have expired). Login and repeat the previous command to get to the
    start screen.
  prefs: []
  type: TYPE_NORMAL
- en: Click the Add systems button.
  prefs: []
  type: TYPE_NORMAL
- en: If you read the instructions, you'll probably think that the setup is relatively
    easy. It is. However, Kubernetes is not available as one of the options. If you
    change the value of the *from* drop-down list to *something else...*, you'll see
    a fairly big list of log sources that can be plugged into Papertrail. Still, there
    is no sign of Kubernetes. The closest one on that list is *Docker*. Even that
    one will not do. Don't worry. I prepared instructions for you or, to be more precise,
    I extracted them from the documentation buried in Papertrail's site.
  prefs: []
  type: TYPE_NORMAL
- en: Please note the `Your logs will go to logsN.papertrailapp.com:NNNNN and appear
    in Events` message at the top of the screen. We'll need that address soon, so
    we better store the values in environment variables.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Please replace the first `[...]` with the host. It should be something like
    `logsN.papertrailapp.com`, where `N` is the number assigned to you by Papertrail.
    The second `[...]` should be replaced with the port from the before mentioned
    message.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have the host and the port stored in environment variables, we can
    explore the mechanism we'll use to collect and ship the logs to Papertrail.
  prefs: []
  type: TYPE_NORMAL
- en: Since I already claimed that most vendors adopted Fluentd for collecting and
    shipping logs to their solutions, it should come as no surprise that Papertrail
    recommends it as well. Folks from SolarWinds (Papertrail's parent company) created
    an image with customized Fluentd that we can use. In turn, I created a YAML file
    with all the resources we'll need to run their image.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the YAML defines a DaemonSet with ServiceAccount, SolarWind's
    Fluentd, and a ConfigMap that uses a few environment variables to specify the
    host and the port where logs should be shipped.
  prefs: []
  type: TYPE_NORMAL
- en: We'll have to change the `logsN.papertrailapp.com` and `NNNNN` entries in that
    YAML before we apply it. Also, I prefer running all logs-related resources in
    `logging` Namespace, so we'll need to change that as well.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Now that we're running Fluentd in our cluster and that it is configured to forward
    logs to our Papertrail account, we should turn back to its UI.
  prefs: []
  type: TYPE_NORMAL
- en: Please switch back to Papertrail console in your browser. You should see a green
    box stating that logs were received. Click the Events link.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/cbe13042-656d-4b4f-9703-0cc7617f5aed.png)Figure 7-1: Papertrail''s
    Setup Logging screen'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we'll produce a few logs and explore how they appear in Papertrail.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: That Pod uses `chentex/random-logger` image that has a single purpose. It periodically
    outputs random log entries.
  prefs: []
  type: TYPE_NORMAL
- en: Let's create `random-logger`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Please wait for a minute or two to accumulate a few logs entries.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The output should be similar to the one that follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the container is outputting random entries, some of them as
    `ERROR`, and others as `DEBUG`, `WARN`, and `INFO`. Messages are random as well.
    After all, that is not a real application, but a simple image that produces log
    entries we can use to explore our logging solution.
  prefs: []
  type: TYPE_NORMAL
- en: Please go back to Papertrail UI. You should notice that all the logs from our
    system are available. Some are coming from Kubernetes, while others are from system-level
    services.
  prefs: []
  type: TYPE_NORMAL
- en: Those from `go-demo-5` are also there, together with the `random-logger` we
    just installed. We'll focus on the latter.
  prefs: []
  type: TYPE_NORMAL
- en: Let's imagine that we found out through alerts that there is an issue and that
    we limited the scope to the `random-logger` application. Alerts helped us detect
    the problem and we narrowed it down to a single application by digging through
    metrics. We still need to consult logs to find the cause. Given what we know (or
    invented), the logical next step would be to retrieve only the log entries related
    to the `random-logger`.
  prefs: []
  type: TYPE_NORMAL
- en: Please type `random-logger` in the Search field at the bottom of the screen,
    and press the enter key.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/10d4a911-f534-4f12-9cc7-111825990d08.png)Figure 7-2: Papertrail''s
    Events screen'
  prefs: []
  type: TYPE_NORMAL
- en: From now on, we'll see only log entries that contain the word `random-logger`.
    That does not necessarily mean that only the log entries from that application
    are displayed. Instead, any mention of that word is shown on the screen. What
    we did was to instruct Papertrail to perform a free-text search inside all the
    log entries and retrieve only those that contain the beforementioned word.
  prefs: []
  type: TYPE_NORMAL
- en: While free-text search across all the records is probably the most commonly
    used query, there are a few other ways we could filter logs. We won't go through
    all of them. Instead, click the Search tips button in the right-hand side of the
    Search field and explore the syntax yourself. If those few examples are not enough,
    click the Full Syntax Guide link.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/3649c719-cc5c-41d6-8527-a99601654bc3.png)Figure 7-3: Papertrail''s
    Syntax & Examples screen'
  prefs: []
  type: TYPE_NORMAL
- en: There's probably no need to explore Papertrail in more detail. It is intuitive,
    easy to use, and well-documented service. I'm sure you'll figure out the details
    if you choose to use it. For now, we'll remove the DaemonSet and the ConfigMap
    before we move into exploring alternatives.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Next, we'll explore logging solutions available in Cloud providers. Feel free
    to jump directly to *GCP Stackdriver*, *AWS CloudWatch*, or *Azure Log Analytics*. If
    you do not use any of the three providers, you can skip them altogether and go
    directly to the *Exploring centralized logging through Elasticsearch, Fluentd,
    and Kibana* sub-chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Combining GCP Stackdriver with a GKE cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you're using GKE cluster, logging is already set up, even though you might
    not know about it. By default, every GKE cluster comes by default with a Fluentd
    DaemonSet that is configured to forward logs to GCP Stackdriver. It is running
    in the `kube-system` Namespace.
  prefs: []
  type: TYPE_NORMAL
- en: Let's describe GKE's Fluentd DaemonSet and see whether there is any useful information
    we might find.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The output, limited to the relevant parts, is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: We can see that, among others, the DaemonSet's Pod Template has the label `k8s-app=fluentd-gcp`.
    We'll need it soon. Also, we can see that one of the containers is based on the
    `stackdriver-logging-agent` image. Just as Papertrail extended Fluentd, Google
    did the same.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know that Stackdriver-specific Fluentd is running in our cluster
    as a DaemonSet, the logical conclusion would be that there is already a UI we
    can use to explore the logs.
  prefs: []
  type: TYPE_NORMAL
- en: UI is indeed available but, before we see it in action, we'll output the logs
    of the Fluentd containers and verify that everything is working as expected.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Unless you already enabled Stackdriver Logging API, the output should contain
    at least one message similar to the one that follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Fortunately, the warning already tells us not only what the issue is, but also
    what to do. Open the link from the log entry in your favorite browser, and click
    the ENABLE button.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we enabled Stackdriver Logging API, Fluentd will be able to ship log
    entries there. All we have to do is wait for a minute or two until the action
    propagates.
  prefs: []
  type: TYPE_NORMAL
- en: Let's see the Stackdriver UI.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Please type `random-logger` in the Filter by label or text search field and
    select GKE Container from the drop-down list.
  prefs: []
  type: TYPE_NORMAL
- en: The output should display all the logs that contain `random-logger` text.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/6e1b7844-b066-4a75-8bf5-3e131aba34ff.png)Figure 7-4: GCP Stackdriver
    logs screen'
  prefs: []
  type: TYPE_NORMAL
- en: We won't go into details how to use Stackdriver. It is easy and, hopefully,
    intuitive. So, I'll leave it to you to explore it in more detail. What matters
    is that it is very similar to what we experienced with Papertrail. Most of the
    differences are cosmetic.
  prefs: []
  type: TYPE_NORMAL
- en: If you are using GCP, Stackdriver is ready and waiting for you. As such, it
    probably makes sense to use it over any other third-party solution. Stackdriver
    contains not only the logs coming from the cluster but also logs of all GCP services
    (for example, load balancers). That is probably the significant difference between
    the two solutions. It is a massive bonus in favor of Stackdriver. Still, check
    the pricing before making a decision.
  prefs: []
  type: TYPE_NORMAL
- en: Combining AWS CloudWatch with an EKS cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unlike GKE that has a logging solution baked into a cluster, EKS requires us
    to set up a solution. It does provide CloudWatch service, but we need to ensure
    that the logs are shipped there from our cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Just as before, we'll use Fluentd to collect logs and ship them to CloudWatch.
    Or, to be more precise, we'll use a Fluentd tag built specifically for CloudWatch.
    As you probably already know, we'll also need an IAM policy that will allow Fluentd
    to communicate with CloudWatch.
  prefs: []
  type: TYPE_NORMAL
- en: All in all, the setup we are about to make will be very similar to the one we
    did with Papertrail, except that we'll store the logs in CloudWatch, and that
    we'll have to put some effort into creating AWS permissions.
  prefs: []
  type: TYPE_NORMAL
- en: Before we proceed, I'll assume that you still have the environment variables
    `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, and `AWS_DEFAULT_REGION` used in
    the `eks-logging.sh` ([https://gist.github.com/vfarcic/a783351fc9a3637a291346dd4bc346e7](https://gist.github.com/vfarcic/a783351fc9a3637a291346dd4bc346e7))
    Gist. If you don't, please create them.
  prefs: []
  type: TYPE_NORMAL
- en: Off we go.
  prefs: []
  type: TYPE_NORMAL
- en: We need to create a new AWS **Identity and Access Management** (**IAM**) ([https://aws.amazon.com/iam/](https://aws.amazon.com/iam/))
    policy. For that, we need to find out the IAM role, and for that we need the IAM
    profile. If you're confused with that, it might help to know that you're not the
    only one. AWS permissions are anything but straightforward. Nevertheless, that's
    not the subject of this chapter (nor the book), so I will assume at least a basic
    understanding of how IAM works.
  prefs: []
  type: TYPE_NORMAL
- en: If we reverse engineer the route to creating an IAM policy, the first thing
    we need is the profile.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The output should be similar to the one that follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Now that we know the profile, we can use it to retrieve the role.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: With the role at hand, we can finally create the policy. I already created one
    we can use, so let's take a quick look at it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The output is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, there's nothing special about that policy. It defines permissions
    required for interaction with `logs` (CloudWatch) from inside our cluster.
  prefs: []
  type: TYPE_NORMAL
- en: So, let's move on and create it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Finally, to be on the safe side, we'll retrieve the `eks-logs` policy and confirm
    that it was indeed created correctly.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The `PolicyDocument` section of the output should be the same as the JSON file
    we used to create the policy.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have the policy in place, we can turn our attention to Fluentd.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, at this moment (December 2018), there is no CloudWatch-friendly
    Fluentd Helm Chart. So, we'll fall back to good old YAML. I prepared one, so let's
    take a quick look at it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: I won't go into the details of the YAML. You should be able to understand what
    it does by exploring it on your own. The key resources are the `fluentd-cloudwatch`
    ConfigMap that contains the configuration and the DaemonSet with the same name
    that will run Fluentd Pod in each node of your cluster. The only difficulty you
    might have with that YAML is to understand the Fluentd configuration, especially
    if that is the first time you're working with it. Nevertheless, we won't go into
    details, and I'll let you explore Fluentd's documentation on your own. Instead,
    we'll `apply` that YAML hoping that everything works as expected.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Before we move into Cloudwatch UI, we'll retrieve Fluentd Pods and confirm that
    there is one in each node of the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: In my case, the output shows three `fluentd-cloudwatch` Pods matching the number
    of nodes in my EKS cluster.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Now that everything seems to be working inside our cluster, the time has come
    to move into CloudWatch UI.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Please type `random-logger` in the Log Stream Name Prefix field and press the
    enter key. As a result, only one stream should be available. Click it.
  prefs: []
  type: TYPE_NORMAL
- en: Once inside the `random-logger` screen, you should see all the logs generated
    by that Pod. I'll leave it to you to explore the available options (there aren't
    many).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/c6a05184-dcf3-41ce-a1a4-6d2fc4839889.png)Figure 7-5: AWS CloudWatch
    events screen'
  prefs: []
  type: TYPE_NORMAL
- en: Once you're done exploring CloudWatch, we'll proceed by deleting the Fluentd
    resources as well as the policy and the log group. We still have more logging
    solutions to explore. If you choose to use CloudWatch with Fluentd, you should
    be able to replicate the same installation steps in your "real" cluster.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Combining Azure Log Analytics with an AKS cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Just like GKE (and unlike EKS), AKS comes with an integrated logging solution.
    All we have to do is enable one of the AKS addons. To be more precise, we'll enable
    the `monitoring` addon. As the name indicates, the addon does not fulfill only
    the needs to collect logs, but it also handles metrics. However, we are interested
    just in logs. I believe that nothing beats Prometheus for metrics, especially
    since it integrates with HorizontalPodAutoscaler. Still, you should explore AKS
    metrics as well and reach your own conclusion. For now, we'll explore only the
    logging part of the addon.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: The output is a rather big JSON with all the information about the newly enabled
    `monitoring` addon. There's nothing exciting in it.
  prefs: []
  type: TYPE_NORMAL
- en: It's important to note that we could have enabled the addon when we created
    the cluster by adding `-a monitoring` argument to the `az aks create` command.
  prefs: []
  type: TYPE_NORMAL
- en: If you're curious what we got, we can list the Deployments in the `kube-system`
    Namespace.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: The output is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: The new addition is the `omsagent-rs` Deployment that will ship the logs (and
    metrics) to Azure Log Analytics. If you `describe` it, you'll see that it is based
    on `microsoft/oms` image. That makes it the first and the only time we switched
    from Fluentd to a different log shipping solution. We'll use it simply because
    Azure recommends it.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we need to wait for a few minutes until the logs are propagated to Log
    Analytics. This is the perfect moment for you to take a short break. Go fetch
    a cup of coffee.
  prefs: []
  type: TYPE_NORMAL
- en: Let's open Azure portal and see Log Analytics in action.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Please click the All services item from the left-hand menu, type `log analytics`
    in the Filter field, and click the Log Analytics item.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/7fb76fbc-fb0a-4ec2-827b-8e4bbdb12ca8.png)Figure 7-6: Azure portal
    All services screen with log analytics filter'
  prefs: []
  type: TYPE_NORMAL
- en: Unless you are already using Log Analytics, there should be only one active
    workspace. If that's the case, click it. Otherwise, if there are multiple workspaces,
    choose the one with the ID that matches the *id* entry of the `az aks enable-addons`
    output.
  prefs: []
  type: TYPE_NORMAL
- en: Click the menu item Logs in the *General* section.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we'll try to limit the output entries only to those that contain `random-logger`.
    Please type the query that follows in the Type your query here... field.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Click the Run button, and you'll be presented with all the `random-logger` entries.
  prefs: []
  type: TYPE_NORMAL
- en: By default, all the fields are shown in the table, and many of them are either
    not used, or not very useful. The extra columns probably distract us from absorbing
    the logs, so we'll change the output.
  prefs: []
  type: TYPE_NORMAL
- en: It's easier to specify which columns we need, than which ones we don't. Please
    expand the Columns list, and click the SELECT NONE button. Next, select **LogEntry**,
    **Name**, and **TimeGenerated** fields and, once you're finished, contract the
    **Columns** list.
  prefs: []
  type: TYPE_NORMAL
- en: What you see in front of you are logs limited to `random-logger` and presented
    only through the three columns we selected.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/eff379dc-1093-4212-8efa-5731ad7a3ba5.png)Figure 7-7: Azure Log Analytics
    screen with filtered entries'
  prefs: []
  type: TYPE_NORMAL
- en: I'll let you explore Log Analytics features on your own. Even though Azure portal's
    UI is not as intuitive as it could be, I'm sure you'll manage to get your way
    around it. If you choose to adopt AKS integration with Log Analytics, you should
    probably explore *Log Analytics query language* ([https://docs.microsoft.com/en-us/azure/azure-monitor/log-query/query-language](https://docs.microsoft.com/en-us/azure/azure-monitor/log-query/query-language))
    documentation that will help you write more complex queries than the one we used.
  prefs: []
  type: TYPE_NORMAL
- en: Given that there is at least one more solution we should explore before we choose
    the one that fits your needs the best, we'll disable the addon. Later on, if you
    do like Log Analytics more than the alternatives, all you'll have to do is to
    enable it again.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Exploring centralized logging through Elasticsearch, Fluentd, and Kibana
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Elasticsearch is probably the most commonly used in-memory database. At least,
    if we narrow the scope to self-hosted databases. It is designed for many other
    scenarios, and it can be used to store (almost) any type of data. As such, it
    is almost perfect for storing logs which could come in many different formats.
    Given its flexibility, some use it for metrics as well and, as such, Elasticsearch
    competes with Prometheus. We'll leave metrics aside, for now, and focus only on
    logs.
  prefs: []
  type: TYPE_NORMAL
- en: The **EFK** (**Elasticsearch**, **Fluentd**, and **Kibana**) stack consists
    of three components. Data is stored in Elasticsearch, logs are collected, transformed,
    and pushed to the DB by Fluentd, and Kibana is used as UI through which we can
    explore data stored in Elasticsearch. If you are used to ELK (Logstash instead
    of Fluentd), the setup that follows should be familiar.
  prefs: []
  type: TYPE_NORMAL
- en: The first components we'll install is Elasticsearch. Without it, Fluentd would
    not have a destination to ship logs, and Kibana would not have a source of data.
  prefs: []
  type: TYPE_NORMAL
- en: As you might have guessed, we'll continue using Helm and, fortunately, *Elasticsearch
    Chart* ([https://github.com/helm/charts/tree/master/stable/elasticsearch](https://github.com/helm/charts/tree/master/stable/elasticsearch))
    is already available in the stable channel. I'm confident that you know how to
    find the chart and explore all the values you can use. So, we'll jump straight
    into the values I prepared. They are the bare minimum and contain only the `resources`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: The output is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, there are three sections (`client`, `master`, and `data`) that
    correspond with ElasticSearch components that will be installed. All we're doing
    is setting up resource requests and limits, and leaving the rest to the Chart's
    default values.
  prefs: []
  type: TYPE_NORMAL
- en: Before we proceed, please note that you should NOT use those values in production.
    You should know by now that they differ from one case to another and that you
    should adjust resources depending on the actual usage that you can retrieve from
    tools like `kubectl top`, Prometheus, and others.
  prefs: []
  type: TYPE_NORMAL
- en: Let's install Elasticsearch.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: It might take a while until all the resources are created. On top of that, if
    you're using GKE, new nodes might need to be created to accommodate requested
    resources. Be patient.
  prefs: []
  type: TYPE_NORMAL
- en: Now that Elasticsearch is rolled out, we can turn our attention to the second
    component in the EFK stack. We'll install Fluentd. Just as Elasticsearch, Fluentd
    is also available in Helm's stable channel.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: There's no much to say about Fluentd. It is running as DaemonSet and, as the
    name of the chart suggests, it is already preconfigured to work with Elasticsearch.
    I did not even bother showing you the contents of the values file `logging/fluentd-values.yml`
    since it contains only the resources.
  prefs: []
  type: TYPE_NORMAL
- en: To be on the safe side, we'll check Fluentd's logs to confirm that it managed
    to connect to Elasticsearch.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: The output, limited to the messages, is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: A note to Docker for Desktop users You will likely see much more than the few
    log entries presented above. There will be a lot of warnings due to the differences
    in Docker for Desktop API when compared to other Kubernetes flavors. Feel free
    to ignore those warnings since they do not affect the examples we are about to
    explore and you are not going to use Docker for Desktop in production but only
    for practice and local development.
  prefs: []
  type: TYPE_NORMAL
- en: That was simple and beautiful. The only thing left is to install the K from
    EFK.
  prefs: []
  type: TYPE_NORMAL
- en: Let's take a look at the values file we'll use for the Kibana chart.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: The output is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: Again, this is a relatively straightforward set of values. This time, we are
    specifying not only the resources but also the Ingress host, as well as the environment
    variable `ELASTICSEARCH_URL` that'll tell Kibana where to find Elasticsearch.
    As you might have guessed, I did not know in advance what will be your host, so
    we'll need to overwrite `hosts` at runtime. But, before we do that, we need to
    define it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Off we go towards installing the last component in the EFK stack.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: Now we can finally open Kibana and confirm that all three EFK components indeed
    work together and that they are fulfilling our centralized logging objectives.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: If you do not see Kibana just yet, wait for a few moments and refresh the screen.
  prefs: []
  type: TYPE_NORMAL
- en: You should see the *Welcome* screen. Ignore the offer to try their sample data
    by clicking the link to Explore on my own. You'll be presented with the screen
    that allows you to add data.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/f1a8a6ea-94ef-40e8-861b-d7d04f0a6dc5.png)Figure 7-8: Kibana''s home
    screen'
  prefs: []
  type: TYPE_NORMAL
- en: The first thing we need to do is create a new Elasticsearch index that will
    match the one created by Fluentd. The version we're running is already pushing
    data to Elasticsearch, and it's doing that by using LogStash indexing pattern
    as a way to simplify things since that's what Kibana expects to see.
  prefs: []
  type: TYPE_NORMAL
- en: Click the Management item from the left-hand menu, followed with the Index Patterns
    link.
  prefs: []
  type: TYPE_NORMAL
- en: All the logs Fluentd is sending to Elasticsearch are indexes with the *logstash*
    prefix followed with the date. Since we want Kibana to retrieve all the logs,
    type `logstash-*` into the Index pattern field, and click the > Next step button.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we need to specify which field contains timestamps. It is an easy choice.
    Select @timestamp from the Time Filter field name, and click the Create index
    pattern button.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/884ab586-e6e9-4fa6-bc02-d15e6a91b682.png)Figure 7-9: Kibana''s Create
    index pattern screen'
  prefs: []
  type: TYPE_NORMAL
- en: That's it. All we have to do now is wait for a few moments until the index is
    created, and explore the logs collected from across the whole cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Please click the Discover item from the left-hand menu.
  prefs: []
  type: TYPE_NORMAL
- en: What you see in front of you are all the logs generated in the last fifteen
    minutes (can be extended to any period). The list of fields is available on the
    left-hand side.
  prefs: []
  type: TYPE_NORMAL
- en: There is a silly (and useless) graph on the top and the logs themselves are
    in the main body of the screen.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/20ca4884-36f5-465c-8fe2-f1f2af4b5abe.png)Figure 7-10: Kibana''s
    Discover screen'
  prefs: []
  type: TYPE_NORMAL
- en: Just as with Papertrail, we won't go into all the options available in Kibana.
    I trust you can figure them out yourself. We'll just go through a few basic operations
    in case this is your first contact with Kibana.
  prefs: []
  type: TYPE_NORMAL
- en: Our scenario is the same as before. We'll try to find all the log entries generated
    from the `random-logger` application.
  prefs: []
  type: TYPE_NORMAL
- en: 'Please type `kubernetes.pod_name: "random-logger"` into the Search field and
    click the Refresh (or Update) button located on the right.'
  prefs: []
  type: TYPE_NORMAL
- en: More often than not, we want to customize the fields presented by default. For
    example, it would be more useful to see the log entry only, instead of the full
    source.
  prefs: []
  type: TYPE_NORMAL
- en: Click the Add button next to the log field, and it will replace the default
    *_source* column.
  prefs: []
  type: TYPE_NORMAL
- en: If you'd like to see an entry with all the fields, please expand one by clicking
    the arrow on the left side of the row.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/20ca4884-36f5-465c-8fe2-f1f2af4b5abe.png)Figure 7-11: Kibana''s
    Discover screen with filtered entries'
  prefs: []
  type: TYPE_NORMAL
- en: I'll leave you to explore the rest of Kibana on your own. But, before you do
    that, there's a word of warning. Do not get fooled by all the flashy options.
    If all we're having are logs, there's probably no point creating visualizations,
    dashboards, timelines, and other nice looking, but useless things we can do with
    logs. Those might be useful with metrics, but we do not have any. For now, they
    are in Prometheus. Later on, we'll discuss the option of pushing metrics to Elasticsearch
    instead of pulling them from Prometheus.
  prefs: []
  type: TYPE_NORMAL
- en: Now, take your time and see what else you can do in Kibana, at least within
    the *Discover* screen.
  prefs: []
  type: TYPE_NORMAL
- en: We're done with the EFK stack and, given that we did not yet make a decision
    which solution to use, we'll purge it from the system. Later on, if you do choose
    EFK, you should not have any trouble creating it in your "real" cluster.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: Switching to Elasticsearch for storing metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we had Elasticsearch running in our cluster and knowing that it can
    handle almost any data type, a logical question could be whether we can use it
    to store our metrics besides logs. If you explore *elastic.co* ([https://www.elastic.co/](https://www.elastic.co/)),
    you'll see that metrics are indeed something they advertise. If it could replace
    Prometheus, it would undoubtedly be beneficial to have a single tool that can
    handle not only logs but also metrics. On top of that, we could ditch Grafana
    and keep Kibana as a single UI for both data types.
  prefs: []
  type: TYPE_NORMAL
- en: Nevertheless, I would strongly advise against using Elasticsearch for metrics.
    It is a general-purpose free-text no-SQL database. That means that it can handle
    almost any data but, at the same time, it does not excel at any specific format.
    Prometheus, on the other hand, is designed to store time-series data which are
    the preferred way of exposing metrics. As such, it is more limited in what it
    does. But, it handles metrics much better than Elasticsearch. I believe that using
    the right tool for the job is better than having a single tool that does too many
    things, and if you believe the same, Prometheus is a much better choice for metrics.
  prefs: []
  type: TYPE_NORMAL
- en: When compared to Elasticsearch, and focused only on metrics, Prometheus' requires
    much fewer resources (as you already noticed), it is faster, and it has a much
    better query language. That shouldn't come as a surprise given that both tools
    are great, but only Prometheus is designed to work exclusively with metrics. The
    increased cost of maintaining an additional tool is well paid off by having a
    better (and more focused) solution.
  prefs: []
  type: TYPE_NORMAL
- en: Did I mention that notifications generated through Prometheus and Alertmanager
    and better than those through Elasticsearch?
  prefs: []
  type: TYPE_NORMAL
- en: There's one more important thing to note. Prometheus integration with Kubernetes
    is way better than what Elasticsearch offers. That is not a surprise since Prometheus
    is based on the same cloud-native principles as Kubernetes and both belong to
    *Cloud Native Computing Foundation* ([https://www.cncf.io/](https://www.cncf.io/)).
    Elasticsearch, on the other hand, comes from a more traditional background.
  prefs: []
  type: TYPE_NORMAL
- en: Elasticsearch is excellent, but it does too much. Its lack of focus makes it
    inferior to Prometheus for storing and querying metrics, as well as sending alerts
    based on such data.
  prefs: []
  type: TYPE_NORMAL
- en: If replacing Prometheus with Elasticsearch is not a good idea, can we invert
    the question? Can we use Prometheus for logs? The answer is a definite no. As
    already stated, Prometheus is focused only on metrics. If you do adopt it, you
    need a different tool for storing logs. That can be Elasticsearch, Papertrail,
    or any other solution that fits your needs.
  prefs: []
  type: TYPE_NORMAL
- en: How about Kibana? Can we ditch it in favor of Grafana? The answer is yes, but
    don't do that. While we could create a table in Grafana and attach it to Elasticsearch
    as a data source, its capability to display and filter logs is inferior. On the
    other hand, Grafana is much more flexible than Kibana for displaying graphs based
    on metrics. So, the answer is similar to the Elasticsearch vs. Prometheus dilemma.
    Keep Grafana for metrics and use Kibana for logs, if you chose to store them in
    Elasticsearch.
  prefs: []
  type: TYPE_NORMAL
- en: Should you add Elasticsearch as yet another data source in Grafana? If you took
    previous recommendations, the answer is most likely no. There is not much value
    in presenting logs as graphs. Even the pre-defined graph available in Kibana's
    *Explore* section is, in my opinion, a waste of space. There is no point in showing
    how many logs entries we have in total, nor even how many are error entries. We
    use metrics for that.
  prefs: []
  type: TYPE_NORMAL
- en: Logs themselves are too expensive to parse, and most of the time they do not
    provide enough data to act as metrics.
  prefs: []
  type: TYPE_NORMAL
- en: We saw several tools in action, but we did not yet discuss what we truly need
    from a centralized logging solution. We'll explore that in more detail next.
  prefs: []
  type: TYPE_NORMAL
- en: What should we expect from centralized logging?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We explored several products that can be used to centralize logging. As you
    saw, all are very similar, and we can assume that most of the other solutions
    follow the same principles. We need to collect logs across the cluster. We used
    Fluentd for that, which is the most widely accepted solution that you will likely
    use no matter which database receives those logs (Azure being an exception).
  prefs: []
  type: TYPE_NORMAL
- en: Log entries collected with Fluentd are shipped to a database which, in our case,
    is Papertrail, Elasticsearch, or one of the solutions provided by hosting vendors.
    Finally, all solutions offer a UI that allows us to explore the logs.
  prefs: []
  type: TYPE_NORMAL
- en: I usually provide a single solution for a problem but, in this case, there are
    quite a few candidates for your need for centralized logging. Which one should
    you choose? Will it be Papertrail, Elasticsearch-Fluentd-Kibana stack (EFK), AWS
    CloudWatch, GCP Stackdriver, Azure Log Analytics, or something else?
  prefs: []
  type: TYPE_NORMAL
- en: When possible and practical, I prefer a centralized logging solution provided
    as a service, instead of running it inside my clusters. Many things are easier
    when others are making sure that everything works. If we use Helm to install EFK,
    it might seem like an easy setup. However, maintenance is far from trivial. Elasticsearch
    requires a lot of resources. For smaller clusters, compute required to run Elasticsearch
    alone is likely higher than the price of Papertrail or similar solutions. If I
    can get a service managed by others for the same price as running the alternative
    inside my own cluster, service wins most of the time. But, there are a few exceptions.
  prefs: []
  type: TYPE_NORMAL
- en: I do not want to lock my business into a service provider. Or, to be more precise,
    I think it's crucial that core components are controlled by me, while as much
    of the rest is given to others. A good example is VMs. I do not honestly care
    who creates them, as long as the price is competitive and the service is reliable.
    I can easily move my VMs from on-prem to AWS, and from there to, let's say, Azure.
    I can even go back to on-prem. There's not much logic in the creation and maintenance
    of VMs. Or, at least, there shouldn't be.
  prefs: []
  type: TYPE_NORMAL
- en: What I genuinely care are my applications. As long as they are running, they
    are fault-tolerant, they are highly available, and their maintenance is not costly,
    it does not matter where they run. But, I need to make sure that the system is
    done in a way that allows me to switch from one provider to another, without spending
    months in refactoring. That's one of the big reasons why Kubernetes is so widely
    adopted. It abstracts everything below it, thus allowing us to run our applications
    in (almost) any Kubernetes cluster. I believe the same can be applied to logs.
    We need to be clear what we expect, and any solution that meets our requirements
    is as good as any other. So, what do we need from a logging solution?
  prefs: []
  type: TYPE_NORMAL
- en: We need logs centralized in a single location so that we can explore logs from
    any part of the system.We need a query language that will allow us to filter the
    results.We need the solution to be fast.
  prefs: []
  type: TYPE_NORMAL
- en: All of the solutions we explored meets those requirements. Papertrail, EFK,
    AWS CloudWatch, GCP Stackdriver, and Azure Log Analytics all fulfill those requirements.
    Kibana might be a bit prettier, and Elasticsearch's query language might be a
    bit richer than those provided by the other solutions. The importance of prettiness
    is up to you to establish. As for Elasticsearch' query language being more powerful...
    It does not really matter. Most of the time, we need simple operations with logs.
    Find me all the entries that have specific keywords. Return all logs from that
    application. Limit the result to the last thirty minutes.
  prefs: []
  type: TYPE_NORMAL
- en: When possible and practical, logging-as-a-service provided by a third party
    like Papertrail, AWS, GCP, or Azure is a better option than to host it inside
    our clusters.
  prefs: []
  type: TYPE_NORMAL
- en: With a service, we accomplish the same goals, while getting rid of one of the
    things we need to worry about. The reasoning behind that statement is similar
    to the logic that makes me believe that managed Kubernetes services (for example,
    EKS, AKS, GKE) are a better choice than Kubernetes maintained by us. Nevertheless,
    there might be many reasons why using a third-party something-as-a-service solution
    is not possible. Regulations might not allow us to go outside the internal network.
    Latency might be too big. Decision makers are stubborn. No matter the reasons,
    when we can not use something-as-a-service, we have to host that something ourselves.
    In such a case, EFK is likely the best solution, excluding enterprise offerings
    that are out of the scope of this book.
  prefs: []
  type: TYPE_NORMAL
- en: If EFK is likely one of the best solutions for self-hosted centralized logging,
    which one should be our choice when we can use logging-as-a-service? Is Papertrail
    a good choice?
  prefs: []
  type: TYPE_NORMAL
- en: If our cluster is running inside one of the Cloud providers, there is likely
    already a good solution offered by it. For example, EKS has AWS CloudWatch, GKE
    has GCP Stackdriver, and AKS has Azure Log Analytics. Using one of those makes
    perfect sense. It's already there, it is likely already integrated with the cluster
    you are running, and all you have to do is say yes. When a cluster is running
    with one of the Cloud providers, the only reason to choose some other solution
    could be the price.
  prefs: []
  type: TYPE_NORMAL
- en: Use a service provided by your Cloud provider, unless it is more expensive than
    alternatives. If your cluster is on-prem, use a third-party service like Papertrail,
    unless there are rules that prevent you from sending logs outside your internal
    network. If everything else fails, use EFK.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, you might be wondering why do I suggest to use a service for
    logs, while I proposed that our metrics should be hosted inside our clusters.
    Isn't that contradictory? Following that logic, shouldn't we use metrics-as-a-service
    as well?
  prefs: []
  type: TYPE_NORMAL
- en: Our system does not need to interact with our logs storage. The system needs
    to ship logs, but it does not need to retrieve them. As an example, there is no
    need for HorizontalPodAutoscaler to hook into Elasticsearch and use logs to decide
    whether to scale the number of Pods. If the system does not need logs to make
    decisions, can we say the same for humans? What do we need logs for? We need logs
    for debugging. We need them to find the cause of a problem. What we do NOT need
    are alerts based on logs. Logs do not help us discover that there is an issue,
    but to find the cause of a problem detected through alerts based on metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Wait a minute! Shouldn't we create an alert when the number of log entries with
    the word *ERROR* goes over a certain threshold? The answer is no. We can (and
    should) accomplish the same objective through metrics. We already explored how
    to fetch errors from exporters as well as through instrumentation.
  prefs: []
  type: TYPE_NORMAL
- en: What happens when we detect that there is an issue through a metrics-based notification?
    Is that the moment we should start exploring logs? Most of the time, the first
    steps towards finding the cause of a problem does not lie in exploring logs, but
    in querying metrics. Is the application down? Does it have a memory leak? Is there
    a problem with networking? Is there a high number of error responses? Those and
    countless other questions are answered through metrics. Sometimes metrics reveal
    the cause of the problem, and in other cases, they help us narrow it down to a
    specific part of the system. Logs are becoming useful only in the latter case.
  prefs: []
  type: TYPE_NORMAL
- en: We should start exploring logs only when metrics reveal the culprit, but not
    the cause of the issue.
  prefs: []
  type: TYPE_NORMAL
- en: If we do have comprehensive metrics, and they do reveal most (if not all) of
    the information we need to solve an issue, we do not need much from a logging
    solution. We need logs to be centralized so that we can find them all in one place,
    we need to be able to filter them by application or a specific replica, we need
    to be able to narrow the scope to a particular time frame, and we need to be able
    to search for specific keywords.
  prefs: []
  type: TYPE_NORMAL
- en: That's all we need. As it happens, almost all solutions offer those features.
    As such, the choice should be based on simplicity and the cost of ownership.
  prefs: []
  type: TYPE_NORMAL
- en: Whatever you choose, do not fall into the trap of getting impressed with shiny
    features that you are not going to use. I prefer solutions that are simple to
    use and manage. Papertrail fulfills all the requirements, and its cheap. It's
    the perfect choice for both on-prem and Cloud clusters. The same can be said for
    CloudWatch (AWS), Stackdriver (GCP), and Log Analytics (Azure). Even though I
    have a slight preference towards Papertrail, those three do, more or less, the
    same job, and they are already part of the offer.
  prefs: []
  type: TYPE_NORMAL
- en: If you are not allowed to store your data outside your cluster, or you have
    some other impediment towards one of those solutions, EFK is a good choice. Just
    be aware that it'll eat your resources for breakfast, and still complain that
    it's hungry. Elasticsearch alone requires a few GB of RAM as a minimum, and you
    will likely need much more. That, of course, is not that important if you're already
    using Elasticsearch for other purposes. If that's the case, EFK is a no-brainer.
    It's already there, so use it.
  prefs: []
  type: TYPE_NORMAL
- en: What now?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You know what to do. Destroy the cluster if you created it specifically for
    this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Before you leave, you might want to go over the main points of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: For any but the smallest systems, going from one resource to another and from
    one node to another to find the cause of an issue is anything but practical, reliable,
    and fast.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More often than not, `kubectl logs` command does not provide us with enough
    options to perform anything but simplest retrieval of logs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Elasticsearch is excellent, but it does too much. Its lack of focus makes it
    inferior to Prometheus for storing and querying metrics, as well as sending alerts
    based on such data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logs themselves are too expensive to parse, and most of the time they do not
    provide enough data to act as metrics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We need logs centralized in a single location so that we can explore logs from
    any part of the system.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We need a query language that will allow us to filter the results.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We need the solution to be fast.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use a service provided by your Cloud provider, unless it is more expensive than
    alternatives. If your cluster is on-prem, use a third-party service like Papertrail,
    unless there are rules that prevent you from sending logs outside your internal
    network. If everything else fails, use EFK.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We should start exploring logs only when metrics reveal the culprit, but not
    the cause of the issue.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
