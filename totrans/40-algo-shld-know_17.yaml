- en: Practical Considerations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are a bunch of algorithms presented in this book that can be used to solve
    real-world problems. This chapter is about some practical considerations the algorithms
    presented in this book.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter is organized as follows. We will start with an introduction. Then,
    we will present the important topic of the explainability of an algorithm, which
    is the degree to which the internal mechanics of an algorithm can be explained
    in understandable terms. Then, we will present the ethics of using an algorithm
    and the possibility of creating biases when implementing them. Next, the techniques
    for handling NP-hard problems are discussed. Finally, we will look into factors
    that should be considered before choosing an algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you will have learned about the practical considerations
    that are important to keep in mind when using algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing practical considerations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The explainability of an algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding ethics and algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reducing bias in models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tackling NP-hard problems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When to use algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's start with the introduction,
  prefs: []
  type: TYPE_NORMAL
- en: Introducing practical considerations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In addition to designing, developing, and testing an algorithm, in many cases,
    it is important to consider certain practical aspects of starting to rely on a
    machine to solve a real-world problem as this makes the solution more useful.
    For certain algorithms, we may need to consider ways to reliably incorporate new
    important information that is expected to keep changing even after we have deployed
    our algorithm. Will incorporating this new information change the quality of our
    well-tested algorithm in any way? If so, how does our design handle it? And then,
    for some algorithms that use global patterns, we may need to keep an eye on real-time
    parameters that capture changes in the global geopolitical situation. Also, in
    some use cases, we may need to consider regulatory polices enforced at the time
    of use for the solution to be useful.
  prefs: []
  type: TYPE_NORMAL
- en: When we are using algorithms to solve a real-world problem, we are, in a way,
    relying on machines for problem solving. Even the most sophisticated algorithms
    are based on simplification and assumptions and cannot handle surprises. We are
    still not even close to fully handing over critical decision making to our own
    designed algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: For example, Google's designed recommendation engine algorithms have recently
    faced the European Union's regulatory restrictions due to privacy concerns. These
    algorithms may be some of the most advanced in their field. But if banned, these
    algorithms may actually turn out to be useless as they cannot be used to solve
    the problems they were supposed to tackle.
  prefs: []
  type: TYPE_NORMAL
- en: The truth of the matter is that, unfortunately, the practical considerations
    of an algorithm are still afterthoughts that are not usually considered at the
    initial design phase. For many use cases, once an algorithm is deployed and the
    short-term excitement of providing the solution is over, the practical aspects
    and implications of using an algorithm will be discovered over time and will define
    the success or failure of the project.
  prefs: []
  type: TYPE_NORMAL
- en: Let's look into a practical example where not paying attention to the practical
    consideration failed a high-profile project designed by one of the best IT companies
    in the world.
  prefs: []
  type: TYPE_NORMAL
- en: The sad story of an AI Twitter Bot
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's present the classical example of Tay, which was presented as the first-ever
    AI Twitter bot created by Microsoft in 2016\. Being operated by an AI algorithm,
    Tay was supposed to learn from the environment and keep on improving itself. Unfortunately,
    after living in cyberspace for a couple of days, Tay started learning from the
    racism and rudeness of ongoing tweets. It soon started writing offensive tweets
    of its own. Although it exhibited intelligence and quickly learned how to create
    customized tweets based on real-time events, as designed, at the same time, it
    seriously offended people. Microsoft took it offline and tried to re-tool it,
    but that did not work. Microsoft had to eventually kill the project. That was
    the sad end of an ambitious project.
  prefs: []
  type: TYPE_NORMAL
- en: Note that although the intelligence built into it by Microsoft was impressive,
    the company ignored the practical implications of deploying a self-learning Twitter
    bot. The NLP and machine learning algorithms may have been the best in the class
    but due to the obvious shortcomings, it was practically a useless project. Today,
    Tay has become a textbook example of a failure due to ignoring the practical implications
    of allowing algorithms to learn on the fly. The lessons learned by the failure
    of Tay definitely influenced the AI projects of later years. Data scientists also
    started paying more attention to the transparency of algorithms. That brings us
    to the next topic, which explores the need and ways to make algorithms transparent.
  prefs: []
  type: TYPE_NORMAL
- en: The explainability of an algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A black box algorithm is one whose logic of is not interpretable by humans either
    due to its complexity or due to its logic being represented in a convoluted manner.
    On the other hand, a white box algorithm is one whose logic is visible and understandable
    for a human. In other words, explainability helps the human brain to understand
    why an algorithm is giving specific results. The degree of explainability is the
    measure to which a particular algorithm is understandable for the human brain.
    Many classes of algorithms, especially those related to machine learning, are
    classified as black box. If the algorithms are used for critical decision-making,
    it may be important to understand the reasons behind the results generated by
    the algorithm. Converting black box algorithms into white box ones also provides
    better insights into the inner workings of the model. An explainable algorithm
    will guide doctors as to which features were actually used to classify patients
    as sick or not. If the doctor has any doubts about the results, they can go back
    and double-check those particular features for accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning algorithms and explainability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The explainability of an algorithm has special importance for machine learning
    algorithms. In many applications of machine learning, users are asked to trust
    a model to help them to make decisions. Explainability provides transparency when
    needed in such use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Let's look deeper at a specific example. Let's assume that we want to use machine
    learning to predict the prices of houses in the Boston area based on their characteristics.
    Let's also assume that local city regulations will allow us to use machine learning
    algorithms only if we can provide detailed information for the justification of
    any predictions whenever needed. This information is needed for audit purposes
    to make sure that certain segments of the housing market are not artificially
    manipulated. Making our trained model explainable will provide this additional
    information.
  prefs: []
  type: TYPE_NORMAL
- en: Let's look into different options that are available for implementing the explainability
    of our trained model.
  prefs: []
  type: TYPE_NORMAL
- en: Presenting strategies for explainability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For machine learning, there are fundamentally two strategies to provide explainability
    to algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: '**A global explainability strategy:**  This is to provide the details of the
    formulation of a model as a whole.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**A local explainability strategy:**  This is to provide the rationale for
    one or more individual predictions made by our trained model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For global explainability, we have techniques such as **Testing with Concept
    Activation Vectors** (**TCAV**), which is used for providing explainability for
    image classification models. TCAV depends on calculating directional derivatives
    to quantify the degree of the relationship between a user-defined concept and
    the classification of pictures. For example, it will quantify how sensitive a
    prediction of classifying a person as male is to the presence of facial hair in
    the picture. There are other global explainability strategies such as **partial
    dependence plots** and calculating the **permutation importance**, which can help
    to explain the formulations in our trained model. Both global and local explainability
    strategies can either be model-specific or model-agnostic. Model-specific strategies
    apply to certain types of models, whereas model-agnostic strategies can be applied
    to a wide variety of models.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram summarizes the different strategies available for machine
    learning explainability:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/508ea77c-e398-4c0a-a06d-d872d52423d3.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, let's look at how we can implement explainability using one of these strategies.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing explainability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Local Interpretable Model-Agnostic Explanations** (**LIME**) is a model-agnostic
    approach that can explain individual predictions made by a trained model. Being
    model-agnostic, it can explain the predictions of most types of trained  machine
    learning models.'
  prefs: []
  type: TYPE_NORMAL
- en: LIME explains decisions by inducing small changes to the input for each instance.
    It can gather the effects on the local decision boundary for that instance. It
    iterates over the loop to provide details for each variable. Looking at the output,
    we can see which variable has the most influence on that instance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see how we can use LIME to make the individual predictions of our house
    price model  explainable:'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you have never used LIME before, you need to install the package using `pip`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, let''s import the Python packages that we need:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We will train a model that can predict housing prices in a particular city.
    For that we will first import the dataset that is stored in the `housing.pkl`
    file. Then, we will explore the features it has:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/df39d111-43cb-4129-90b9-61e991744bd4.png)'
  prefs: []
  type: TYPE_IMG
- en: Based on these features we need to predict the price of a home.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s train the model. We will be using a random forest regressor to
    train the model. First we divide the data into testing and training partitions
    and then we using it to train the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let us identify the category columns:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s instantiate the LIME explainer with the required configuration
    parameters. Note that we are specifying that our label is `''price''`, representing
    the prices of houses in Boston:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Let us try to look into the details of predictions. For that first let us import
    the pyplot as the plotter from matplotlib
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'As the LIME explainer works on individual predictions, we need to choose the
    predictions we want to analyze. We have asked the explainer for its justification
    of the predictions indexed as `1` and `35`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/68e5f21f-d5d7-4bd4-bd7a-4e78b13b8c87.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s try to analyze the preceding explanation by LIME, which tells us the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**The list of features used in the individual predictions**: They are indicated
    on the *y*-axis in the preceding screenshot.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The relative importance of the features in determining the decision**: The
    larger the bar line, the greater the importance is. The value of the number is
    on the *x*-axis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The positive or negative influence of each of the input features on the label**:
    Red bars show a negative influence and green bars show the positive influence
    of a particular feature.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding ethics and algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The formulation of patterns through algorithms may directly or indirectly result
    in unethical decision making. While designing algorithms, it is difficult to foresee
    the full scope of the potential ethical implications, especially for large-scale
    algorithms, where more than one user may be involved in the design. This makes
    it even more difficult to analyze the effects of human subjectivity.
  prefs: []
  type: TYPE_NORMAL
- en: More and more companies are making the ethical analysis of an algorithm part
    of its design. But the truth is that the problems may not become apparent until
    we find a problematic use case.
  prefs: []
  type: TYPE_NORMAL
- en: Problems with learning algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Algorithms capable of fine-tuning themselves according to changing data patterns
    are called **learning** **algorithms**. They are in learning mode in real time,
    but this real-time learning capability may have ethical implications. This creates
    the possibility that their learning could result in decisions that may have problems
    from an ethical point of view.  As they are created to be in a continuous evolutionary
    phase, it is almost impossible  to continuously perform an ethical analysis of
    them.
  prefs: []
  type: TYPE_NORMAL
- en: As the complexity of algorithms grows, it is becoming more and more difficult
    to fully understand their long-term implications for individuals and groups within
    society.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding ethical considerations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Algorithmic solutions are mathematical formulations without hearts. It is the
    responsibility of the people responsible for developing algorithms to ensure that
    they conform to ethical sensitivities around the problem we are trying to solve.
    These ethical considerations of algorithms depend on the type of the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let''s look into the following algorithms and their ethical considerations.
    Some examples of powerful algorithms for which careful ethical considerations
    are needed are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Classification algorithms, when used on society, determine how individuals and
    groups are shaped and managed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Algorithms, when used in recommendation engines,  can match resumes to job seekers,
    both individuals and groups.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data mining algorithms are used to mine information from users and are provided
    to decision-makers and governments.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Machine learning algorithms are starting to be used by governments to grant
    or deny visas to applicants.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hence, the ethical consideration, of algorithms will depend on the use case
    and the entities they directly or indirectly affect. Careful analysis is needed
    from an ethical point of view before starting to use an algorithm for critical
    decision-making. In the upcoming sections, we shall see the factors that we should
    keep in mind while performing a careful analysis of algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Inconclusive evidence
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The data that is used to train a machine learning algorithm may not have conclusive
    evidence. For example, in clinical trials, the effectiveness of a drug may not
    be proven due to the limited available  evidence. Similarly, there may be limited
    inconclusive evidence that a certain postal code in a certain city is more likely
    to be involved in fraud. We should be careful when we are  judging our decision-making
    based on the mathematical patterns found through algorithms using this limited
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Decisions that are based on inconclusive evidence are prone to lead to unjustified
    actions.
  prefs: []
  type: TYPE_NORMAL
- en: Traceability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The disconnection between the training phase and the testing phase in machine
    learning algorithms means that if there is some harm caused by an algorithm, it
    is very hard to trace and debug. Also, when a problem is found in an algorithm,
    it is difficult to actually determine the people who were affected by it.
  prefs: []
  type: TYPE_NORMAL
- en: Misguided evidence
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Algorithms are data-driven formulations. The  **Garbage-in, Garbage-out** (**GIGO**)
    principle means that results from algorithms can only be as reliable as the data
    on which they are based. If there are biases in the data, they will be reflected
    in the algorithms as well.
  prefs: []
  type: TYPE_NORMAL
- en: Unfair outcomes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The use of algorithms may result in harming vulnerable communities and groups
    that are already at a disadvantage.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, the use of algorithms to distribute research funding has been
    proven on more than one occasion to be biased toward the male population. Algorithms
    used for granting immigration are sometimes unintentionally biased toward vulnerable
    population groups.
  prefs: []
  type: TYPE_NORMAL
- en: Despite using high-quality data and complex mathematical formulations, if the
    result is an unfair outcome, the whole effort may bring more harm than benefit.
  prefs: []
  type: TYPE_NORMAL
- en: Reducing bias in models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the current world, there are known, well-documented general biases based
    on gender, race, and sexual orientation. It means that the data we collect is
    expected to exhibit those biases unless we are dealing with an environment where
    an effort has been made to remove these biases before collecting the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'All bias in algorithms is,  directly or indirectly, due to human bias. Human
    bias can be reflected either in data used by the algorithm or in the formulation
    of the algorithm itself. For a typical machine learning project following the  **CRISP-DM**
    (short for **Cross-Industry Standard Process**) lifecycle, which was explained
    in  [Chapter 5](051e9b32-f15f-4e88-a63a-ae3c14696492.xhtml),  *Graph Algorithms*,
    the bias looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/2ae7ab7a-9d40-4e63-97a0-13ce65e941e8.png)'
  prefs: []
  type: TYPE_IMG
- en: The trickiest part of reducing bias is to first identify and locate unconscious
    bias.
  prefs: []
  type: TYPE_NORMAL
- en: Tackling NP-hard problems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: NP-hard problems were extensively discussed in  [Chapter 4](d8aca545-f465-4ab2-9f26-28e658b90a33.xhtml),
    *Designing Algorithms*. Some NP-hard problems are important and we need to design
    algorithms to solve them.
  prefs: []
  type: TYPE_NORMAL
- en: 'If finding the solution for an NP-hard problem seems out of reach due to its
    complexity or the limitations of the available resources, we can take one of these
    approaches:'
  prefs: []
  type: TYPE_NORMAL
- en: Simplifying the problem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Customizing a well-known solution to a similar problem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using a probabilistic method
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's look into them one by one.
  prefs: []
  type: TYPE_NORMAL
- en: Simplifying the problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can simplify the problem based on certain assumptions. The solved problem
    still gives a solution that is not perfect but is still  insightful and useful.
    For this to work, the chosen assumptions should be as non- restrictive as possible.
  prefs: []
  type: TYPE_NORMAL
- en: Example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The relationship between features and labels in regression problems is seldom
    perfectly linear. But it may be linear within our usual operating range. Approximating
    the relationship as linear greatly simplifies the algorithm and is extensively
    used. But this also introduces some approximations affecting the accuracy of the
    algorithm. The trade-off between approximations and accuracy should be carefully
    studied and the right balance suitable for the stakeholders should be chosen.
  prefs: []
  type: TYPE_NORMAL
- en: Customizing a well-known solution to a similar problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If a solution to a similar problem is known, that solution can be used as a
    starting point. It can be customized to solve the problem we were looking for.
    The concept of  **Transfer Learning**  (**TL**) in machine learning is based on
    this principle. The idea is to use the inference of already pre-trained models
    as the starting point for training the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's assume that we want to train a binary classifier that can differentiate
    between Apple and Windows laptops based on real-time video feed using computer
    vision during corporate training. From the video feed, the first phase of model
    development would be to detect different objects and identify which of the objects
    are laptops. Once done, we can move to the second phase of formulating rules that
    can differentiate between Apple and Windows laptops.
  prefs: []
  type: TYPE_NORMAL
- en: Now, there are already well-trained, well-tested open source models that can
    deal with the first phase of this model training. Why not use them as a starting
    point and use the inference toward the second phase, which is to differentiate
    between Windows and Apple laptops? This will give us a jump start and the solution
    will be less error-prone as phase 1 is already well tested.
  prefs: []
  type: TYPE_NORMAL
- en: Using a probabilistic method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We use a probabilistic method to get a reasonably good solution that is workable,
    but not optimal. When we used decision tree algorithms in  [Chapter 7](e3df232d-9571-4514-a5f1-2789965492e1.xhtml),  *Traditional
    Supervised Learning Algorithms*, to solve the given problem, the solution was
    based on the probabilistic method. We did not prove that it is an optimal solution,
    but it was a reasonably good solution to give us a useful answer to the problem
    that we were trying to solve within the constraints defined in the requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many machine learning algorithms start from a random solution and then iteratively
    improve the solution. The final solution might be efficient but we cannot prove
    that it is the best one. This method is used in complex problems to solve them
    in a reasonable timeframe. That is why, for many machine learning algorithms,
    the only way to get repeatable results is to use the same sequence of random numbers
    by using the same seed.
  prefs: []
  type: TYPE_NORMAL
- en: When to use algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Algorithms are like tools in a practitioner''s toolbox. First, we need to understand
    which tool is the best one to use under the given circumstances. Sometimes, we
    need to ask ourselves whether we even have a solution for the problem we are trying
    to solve and when the right time to deploy our solution is. We need to determine
    whether the use of an algorithm can provide a solution to a real problem that
    is actually useful, rather than the alternative. We need to analyze the effect
    of using the algorithm in terms of three aspects:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Cost**: Can use justify the cost related to the effort of implementing the
    algorithm?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Time**: Does our solution make the overall process more efficient than simpler
    alternatives?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Accuracy**: Does our solution produce more accurate results than simpler
    alternatives?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To choose the right algorithm, we need to find the answers to the following
    questions:'
  prefs: []
  type: TYPE_NORMAL
- en: Can we simplify the problem by making assumptions?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How will we evaluate our algorithm? What are the key metrics?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How will it be deployed and used?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Does it need to be explainable?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do we understand the three important non-functional requirements—security, performance,
    and availability?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is there any expected deadline?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A practical example – black swan events
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Algorithms input data, process and formulate it, and solve a problem. What if
    the data gathered is about an extremely impactful and very rare event? How can
    we use algorithms with the data generated by that event and the events that may
    have led to that Big Bang? Let's look into this aspect in this section.
  prefs: []
  type: TYPE_NORMAL
- en: Such extremely rare events were represented by the *black swan events* metaphor  by
    Nassim Taleb in his book, *Fooled by Randomness*, in 2001\.
  prefs: []
  type: TYPE_NORMAL
- en: Before black swans were first discovered in the wild, for centuries, they were
    used to represent something that cannot happen. After their discovery, the term
    remained popular but there was a change in what it represents. It now represents
    something so rare that it cannot be predicted.
  prefs: []
  type: TYPE_NORMAL
- en: Taleb provided these four criteria to classify an event as a black swan event.
  prefs: []
  type: TYPE_NORMAL
- en: Four criteria to classify an event as a black swan event
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is a bit tricky to decide whether a rare event should be classified as a
    black swan event or not. In general, in order to be classified as a black swan,
    it should meet the following four criteria.
  prefs: []
  type: TYPE_NORMAL
- en: First, once the event has happened, for observers it must be a mind-blowing
    surprise, for example, dropping the atom bomb on Hiroshima.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The event should be a blockbuster—a disruptor and a major one, such as the outbreak
    of the Spanish Flu.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the event has happened and the dust has settled, data scientists who were
    part of the observer group should realize that actually it was not that much of
    a surprise. Observers never paid attention to some important clues. Had they the
    capacity and initiative, the black swan event could have been predicted. For example,
    the Spanish Flu outbreak had some leads that were known to be ignored before it
    became a global outbreak. Also, the Manhattan Project was run for years before
    the atomic bomb was actually dropped on Hiroshima. People in the observer group
    just could not connect the dots.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When it happened, while the observers of the black swan event got the surprise
    of their lifetime, there may be some people for whom it may not have been a surprise
    at all. For example, for scientists working for years to develop the atomic bomb,
    the use of atomic power was never a surprise but an expected event.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Applying algorithms to black swan events
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are majors aspects of black swan events that are related to algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: There are many sophisticated forecasting algorithms available. But if we hope
    to use standard forecasting techniques to predict a black swan event as a precaution,
    it will not work. Using such forecasting algorithms will only offer false security.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once the black swan event has happened, predicting its exact implications on
    broader areas of society including the economy, the public, and governmental issues
    is not usually possible. First, being a rare event, we do not have the right data
    to feed  to the algorithms, and we do not have a grasp of the correlation and
    interactions between broader areas of society that we may have never explored
    and understood.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An important thing to note is that black swan events are not random events.
    We just did not have the capacity to pay attention to the complex events that
    eventually led to these events. This is an area where algorithms can play an important
    role. We should make sure that, in the future, we have a strategy to predict and
    detect these minor events, which combined over time to generate the black swan
    event.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The COVID-19 outbreak of early 2020 is the best example of a black swan event
    of our times.
  prefs: []
  type: TYPE_NORMAL
- en: The preceding example shows how important it is to first consider and understand
    the details of the problem we are trying to solve and then come up with the areas
    where we can contribute toward a solution by implementing an algorithm-based solution.
    Without a comprehensive analysis, as presented earlier, the use of algorithms
    may only solve part of a complex problem, falling short of expectations.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about the practical aspects that should be considered
    while designing algorithms. We looked into the concept of algorithmic explainability
    and the various ways we can provide it at different levels. We also looked into
    the potential ethical issues in algorithms. Finally, we described which factors
    to consider while choosing an algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithms are engines in the new automated world that we are witnessing today.
    It is important to learn about, experiment with, and understand the implications
    of using algorithms. Understanding their strengths and limitations and the ethical
    implications of using algorithms will go a long way in making this world a better
    place to live in. And this book is an effort to achieve this important goal in
    this ever-changing and evolving world.
  prefs: []
  type: TYPE_NORMAL
