- en: '*Chapter 4*: Libvirt Networking'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第4章*：Libvirt网络'
- en: Understanding how virtual networking works is really essential for virtualization.
    It would be very hard to justify the costs associated with a scenario in which
    we didn't have virtual networking. Just imagine having multiple virtual machines
    on a virtualization host and buying network cards so that every single one of
    those virtual machines can have their own dedicated, physical network port. By
    implementing virtual networking, we're also consolidating networking in a much
    more manageable way, both from an administration and cost perspective.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 了解虚拟网络如何工作对于虚拟化非常重要。很难证明在没有虚拟网络的情况下，我们可以承担与拥有多个虚拟机的虚拟化主机相关的成本。想象一下，在虚拟化网络中有多个虚拟机，并购买网络卡，以便每个虚拟机都可以拥有自己专用的物理网络端口。通过实施虚拟网络，我们也以更可管理的方式整合了网络，无论是从管理还是成本的角度来看。
- en: This chapter provides you with an insight into the overall concept of virtualized
    networking and Linux-based networking concepts. We will also discuss physical
    and virtual networking concepts, try to compare them, and find similarities and
    differences between them. Also covered in this chapter is the concept of virtual
    switching for a per-host concept and spanned-across-hosts concept, as well as
    some more advanced topics. These topics include single-root input/output virtualization,
    which allows for a much more direct approach to hardware for certain scenarios.
    We will come back to some of the networking concepts later in this book as we
    start discussing cloud overlay networks. This is because the basic networking
    concepts aren't scalable enough for large cloud environments.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章为您提供了对虚拟化网络和基于Linux的网络概念的整体概念。我们还将讨论物理和虚拟网络概念，尝试比较它们，并找出它们之间的相似之处和不同之处。本章还涵盖了虚拟交换的概念，用于主机概念和跨主机概念，以及一些更高级的主题。这些主题包括单根输入/输出虚拟化，它允许对某些场景的硬件采用更直接的方法。随着我们开始讨论云覆盖网络，我们将在本书的后面回顾一些网络概念。这是因为基本的网络概念对于大型云环境来说并不够可扩展。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Understanding physical and virtual networking
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解物理和虚拟网络
- en: Using TAP/TUN
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用TAP/TUN
- en: Implementing Linux bridging
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实施Linux桥接
- en: Configuring Open vSwitch
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置Open vSwitch
- en: Understanding and configuring SR-IOV
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解和配置SR-IOV
- en: Understanding macvtap
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解macvtap
- en: Let's get started!
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: Understanding physical and virtual networking
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解物理和虚拟网络
- en: Let's think about networking for a second. This is a subject that most system
    administrators nowadays understand pretty well. This might not up to the level
    many of us think we do, but still – if we were to try to find an area of system
    administration where we'd find the biggest common level of knowledge, it would
    be networking.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们思考一下网络。这是当今大多数系统管理员都相当了解的一个主题。这可能不是我们认为的那么高的水平，但是-如果我们试图找到一个系统管理领域，我们会发现最大的共同知识水平，那就是网络。
- en: So, what's the problem with that?
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，问题出在哪里呢？
- en: 'Actually, nothing much. If we really understand physical networking, virtual
    networking is going to be a piece of cake for us. Spoiler alert: *it''s the same
    thing*. If we don''t, it''s going to be exposed rather quickly, because there''s
    no way around it. And the problems are going to get bigger and bigger as time
    goes by because environments evolve and – usually – grow. The bigger they are,
    the more problems they''re going to create, and the more time you''re going to
    spend in debugging mode.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，没有什么。如果我们真正理解物理网络，那么虚拟网络对我们来说将是小菜一碟。剧透警告：*它是一样的*。如果我们不理解，它将很快暴露出来，因为没有绕过它的办法。随着环境的发展和通常的增长，问题会越来越大，因为它们变得越大，它们将产生越多的问题，您将花费更多的时间处于调试模式。
- en: That being said, if you have a firm grasp of VMware or Microsoft-based virtual
    networking purely at a technological level, you're in the clear here as all of
    these concepts are very similar.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，如果您对基于VMware或Microsoft的虚拟网络在技术层面上有很好的掌握，那么这些概念对您来说都是非常相似的。
- en: With that out of the way, what's the whole hoopla about virtual networking?
    It's actually about understanding where things happen, how, and why. This is because,
    physically speaking, virtual networking is literally the same as physical networking.
    Logically speaking, there are some differences that relate more to the *topology*
    of things than to the principle or engineering side of things. And that's what
    usually throws people off a little bit – the fact that there are some weird, software-based
    objects that do the same job as the physical objects that most of us have grown
    used to managing via our favorite CLI-based or GUI-based utilities.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 说到这一点，虚拟网络到底是怎么回事？实际上，这是关于理解事情发生的地方，方式和原因。这是因为从物理上讲，虚拟网络与物理网络完全相同。从逻辑上讲，有一些差异更多地与事物的*拓扑*有关，而不是原则或工程方面的事物。这通常会让人们有点困惑-有一些奇怪的基于软件的对象，它们与大多数人已经习惯通过我们喜爱的基于CLI或GUI的实用程序来管理的物理对象做着相同的工作。
- en: 'First, let''s introduce the basic building block of virtualized networking
    – a virtual switch. A virtual switch is basically a software-based Layer 2 switch
    that you use to do two things:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们介绍虚拟化网络的基本构建块-虚拟交换机。虚拟交换机基本上是一个基于软件的第2层交换机，您可以使用它来做两件事：
- en: Hook up your virtual machines to it.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将您的虚拟机连接到它。
- en: Use its uplinks to connect them to physical server cards so that you can hook
    these physical network cards to a physical switch.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用其上行将它们连接到物理服务器卡，以便您可以将这些物理网络卡连接到物理交换机。
- en: So, let's deal with why we need these virtual switches from the virtual machine
    perspective. As we mentioned earlier, we use a virtual switch to connect virtual
    machines to it. Why? Well, if we didn't have some kind of software object that
    sits in-between our physical network card and our virtual machine, we'd have a
    big problem – we could only connect virtual machines for which we have physical
    network ports to our physical network, and that would be intolerable. First, it
    goes against some of the basic principles of virtualization, such as efficiency
    and consolidation, and secondly, it would cost a lot. Imagine having 20 virtual
    machines on your server. This means that, without a virtual switch, you'd have
    to have at least 20 physical network ports to connect to the physical network.
    On top of that, you'd actually use 20 physical ports on your physical switch as
    well, which would be a disaster.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: So, by introducing a virtual switch between a virtual machine and a physical
    network port, we're solving two problems at the same time – we're reducing the
    number of physical network adapters that we need per server, and we're reducing
    the number of physical switch ports that we need to use to connect our virtual
    machines to the network. We can actually argue that we're solving a third problem
    as well – efficiency – as there are many scenarios where one physical network
    card can handle being an uplink for 20 virtual machines connected to a virtual
    switch. Specifically, there are large parts of our environments that don't consume
    a lot of network traffic and for those scenarios, virtual networking is just amazingly
    efficient.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: Virtual networking
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, in order for that virtual switch to be able to connect to something on
    a virtual machine, we have to have an object to connect to – and that object is
    called a virtual network interface card, often referred to as a vNIC. Every time
    you configure a virtual machine with a virtual network card, you're giving it
    the ability to connect to a virtual switch that uses a physical network card as
    an uplink to a physical switch.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: Of course, there are some potential drawbacks to this approach. For example,
    if you have 50 virtual machines connected to the same virtual switch that uses
    the same physical network card as an uplink and that uplink fails (due to a network
    card issue, cable issue, switch port issue, or switch issue), your 50 virtual
    machines won't have access to the physical network. How do we get around this
    problem? By implementing a better design and following the basic design principles
    that we'd use on a physical network as well. Specifically, we'd use more than
    one physical uplink to the same virtual switch.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: 'Linux has *a lot* of different types of networking interfaces, something like
    20 different types, some of which are as follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: '**Bridge**: Layer 2 interface for (virtual machine) networking.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bond**: For combining network interfaces to a single interface (for balancing
    and failover reasons) into one logical interface.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Team**: Different to bonding, teaming doesn''t create one logical interface,
    but can still do balancing and failover.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MACVLAN**: Creates multiple MAC addresses on a single physical interface
    (creates subinterfaces) on Layer 2.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**IPVLAN**: Unlike MACVLAN, IPVLAN uses the same MAC address and multiplexes
    on Layer 3.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MACVTAP/IPVTAP**: Newer drivers that should simplify virtual networking by
    combining TUN, TAP, and bridge as a single module.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**VXLAN**: A commonly used cloud overlay network concept that we will describe
    in detail in [*Chapter 12*](B14834_12_Final_ASB_ePub.xhtml#_idTextAnchor209),
    *Scaling Out KVM with OpenStack*.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**VETH**: A virtual Ethernet interface that can be used in a variety of ways
    for local tunneling.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**IPOIB**: IP over Infiniband. As Infiniband gains traction in HPC/low latency
    networks, this type of networking is also supported by the Linux kernel.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are a whole host of others. Then, on top of these network interface types,
    there are some 10 types of tunneling interfaces, some of which are as follows:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: '**GRETAP**, **GRE**: Generic Routing Encapsulation protocols for encapsulating
    Layer 2 and Layer 3 protocols, respectively.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**GENEVE**: A convergence protocol for cloud overlay networking that''s meant
    to fuse VXLAN, GRE, and others into one. This is why it''s supported in Open vSwitch,
    VMware NSX, and other products.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**IPIP**: IP over IP tunnel for connecting internal IPv4 subnets via a public
    network.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SIT**: Simple Internet Translation for interconnecting isolated IPv6 networks
    over IPv4.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ip6tnl**: IPv4/6 tunnel over IPv6 tunnel interface.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**IP6GRE**, **IP6GRETAP**, and others.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting your head around all of them is quite a complex and tedious process,
    so, in this book, we're only going to focus on the types of interfaces that are
    really important to us for virtualization and (later in this book) the cloud.
    This is why we will discuss VXLAN and GENEVE overlay networks in [*Chapter 12*](B14834_12_Final_ASB_ePub.xhtml#_idTextAnchor209),
    *Scaling Out KVM with OpenStack*, as we need to have a firm grip on **Software-Defined
    Networking** (**SDN**) as well.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: So, specifically, as part of this chapter, we're going to cover TAP/TUN, bridging,
    Open vSwitch, and macvtap interfaces as these are fundamentally the most important
    networking concepts for KVM virtualization.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: 'But before we dig deep into that, let''s explain a couple of basic virtual
    network concepts that apply to KVM/libvirt networking and other virtualization
    products (for example, VMware''s hosted virtualization products such as Workstation
    or Player use the same concept). When you start configuring libvirt networking,
    you can choose between three basic types: NAT, routed, and isolated. Let''s discuss
    what these networking modes do.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: Libvirt NAT network
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In a `192.168.0.0/24` or something like that) for all the devices that we want
    to connect to the internet.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's convert that into a virtualized network example. In our virtual machine
    scenario, this means that our virtual machine can communicate with anything that's
    connected to the physical network via host's IP address, but not the other way
    around. For something to communicate to our virtual machine behind a NAT'd switch,
    our virtual machine has to initiate that communication (or we have to set up some
    kind of port forwarding, but that's beside the point).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram might explain what we''re talking about a bit better:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.1 – libvirt networking in NAT mode'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_04_01.jpg)'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.1 – libvirt networking in NAT mode
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: From the virtual machine perspective, it's happily sitting in a completely separate
    network segment (hence the `192.168.122.210` and `220` IP addresses) and using
    a virtual network switch as its gateway to access external networks. It doesn't
    have to be concerned with any kind of additional routing as that's one of the
    reasons why we use NAT – to simplify endpoint routing.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: Libvirt routed network
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The second network type is a routed network, which basically means that our
    virtual machine is directly connected to the physical network via a virtual switch.
    This means that our virtual machine is in the same Layer 2/3 network as the physical
    host. This type of network connection is used very often as, oftentimes, there
    is no need to have a separate NAT network to access your virtual machines in your
    environments. In a way, it just makes everything more complicated, especially
    because you have to configure routing to be aware of the NAT network that you''re
    using for your virtual machines. When using routed mode, the virtual machine sits
    *in the same* network segment as the next physical device. The following diagram
    tells a thousand words about routed networks:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.2 – libvirt networking in routed mode'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_04_02.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.2 – libvirt networking in routed mode
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: Now that we've covered the two most commonly used types of virtual machine networking
    scenarios, it's time for the third one, which will seem a bit obscure. If we configure
    a virtual switch without any *uplinks* (which means it has no physical network
    cards attached to it), then that virtual switch can't send traffic to the physical
    network at all. All that's left is communication within the limits of that switch
    itself, hence the name *isolated*. Let's create that elusive isolated network
    now.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: Libvirt isolated network
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this scenario, virtual machines attached to the same isolated switch can
    communicate with each other, but they cannot communicate with anything outside
    the host that they're running on. We used the word *obscure* to describe this
    scenario earlier, but it really isn't – in some ways, it's actually an ideal way
    of *isolating* specific types of traffic so that it doesn't even get to the physical
    network.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: 'Think of it this way – let''s say that you have a virtual machine that hosts
    a web server, for example, running WordPress. You create two virtual switches:
    one running routed network (direct connection to the physical network) and another
    that''s isolated. Then, you can configure your WordPress virtual machine with
    two virtual network cards, with the first one connected to the routed virtual
    switch and the second one connected to the isolated virtual switch. WordPress
    needs a database, so you create another virtual machine and configure it to use
    an internal virtual switch only. Then, you use that isolated virtual switch to
    *isolate* traffic between the web server and the database server so that WordPress
    connects to the database server via that switch. What did you get by configuring
    your virtual machine infrastructure like this? You have a two-tier application,
    and the most important part of that web application (database) is inaccessible
    from the outside world. Doesn''t seem like that bad of an idea, right?'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: Isolated virtual networks are used in many other security-related scenarios,
    but this is just an example scenario that we can easily identify with.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s describe our isolated network with a diagram:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.3 – libvirt networking in isolated mode'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_04_03.jpg)'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.3 – libvirt networking in isolated mode
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: The previous chapter ([*Chapter 3*](B14834_03_Final_ASB_ePub.xhtml#_idTextAnchor049),
    *Installing KVM Hypervisor, libvirt, and ovirt*) of this book mentioned the *default*
    network, and we said that we're going to talk about that a bit later. This seems
    like an opportune moment to do so because now, we have more than enough information
    to describe what the default network configuration is.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: When we install all the necessary KVM libraries and utilities like we did in
    [*Chapter 3*](B14834_03_Final_ASB_ePub.xhtml#_idTextAnchor049), *Installing KVM
    Hypervisor, libvirt, and oVirt*, a default virtual switch gets configured out
    of the box. The reason for this is simple – it's more user-friendly to pre-configure
    something so that users can just start creating virtual machines and connecting
    them to the default network than expect users to configure that as well. VMware's
    vSphere hypervisor does the same thing (the default switch is called vSwitch0),
    and Hyper-V asks us during the deployment process to configure the first virtual
    switch (which we can actually skip and configure later). So, this is just a well-known,
    standardized, established scenario that enables us to start creating our virtual
    machines faster.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: The default virtual switch works in NAT mode with the DHCP server active, and
    again, there's a simple reason for that – guest operating systems are, by default
    pre-configured with DHCP networking configuration, which means that the virtual
    machine that we just created is going to poll the network for necessary IP configuration.
    This way, the VM gets all the necessary network configuration and we can start
    using it right away.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows what the default KVM network does:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.4 – libvirt default network in NAT mode'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_04_04.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.4 – libvirt default network in NAT mode
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s learn how to configure these types of virtual networking concepts
    from the shell and from the GUI. We will treat this procedure as a procedure that
    needs to be done sequentially:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: Let's start by exporting the default network configuration to XML so that we
    can use it as a template to create a new network:![Figure 4.5 – Exporting the
    default virtual network configuration
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B14834_04_05.jpg)'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.5 – Exporting the default virtual network configuration
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s copy that file to a new file called `packtnat.xml`, edit it, and
    then use it to create a new NAT virtual network. Before we do that, however, we
    need to generate two things – a new object UUID (for our new network) and a unique
    MAC address. A new UUID can be generated from the shell by using the `uuidgen`
    command, but generating a MAC address is a bit trickier. So, we can use the standard
    Red Hat-proposed method available on the Red Hat website: [https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/virtualization_administration_guide/sect-virtualization-tips_and_tricks-generating_a_new_unique_mac_address](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/virtualization_administration_guide/sect-virtualization-tips_and_tricks-generating_a_new_unique_mac_address).
    By using the first snippet of code available at that URL, create a new MAC address
    (for example, `00:16:3e:27:21:c1`).'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'By using `yum` command, install python2:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We can now use the `virsh` command to import that configuration and create
    our new virtual network, start that network and make it available permanently,
    and check if everything loaded correctly:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Given that we didn''t delete our default virtual network, the last command
    should give us the following output:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.7 – Using virsh net-list to check which virtual networks we have
    on the KVM host'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_04_07.jpg)'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.7 – Using virsh net-list to check which virtual networks we have on
    the KVM host
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s create two more virtual networks – a bridged network and an isolated
    network. Again, let''s use files as templates to create both of these networks.
    Keep in mind that, in order to be able to create a bridged network, we are going
    to need a physical network adapter, so we need to have an available physical adapter
    in the server for that purpose. On our server, that interface is called `ens224`,
    while the interface called `ens192` is being used by the default libvirt network.
    So, let''s create two configuration files called `packtro.xml` (for our routed
    network) and `packtiso.xml` (for our isolated network):'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.8 – libvirt routed network definition'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_04_08.jpg)'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.8 – libvirt routed network definition
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: 'In this specific configuration, we''re using `ens224` as an uplink to the routed
    virtual network, which would use the same subnet (`192.168.2.0/24`) as the physical
    network that `ens224` is connected to:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.9 – libvirt isolated network definition'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_04_09.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.9 – libvirt isolated network definition
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: Just to cover our bases, we could have easily configured all of this by using
    the Virtual Machine Manager GUI, as that application has a wizard for creating
    virtual networks as well. But when we're talking about larger environments, importing
    XML is a much simpler process, even when we forget about the fact that a lot of
    KVM virtualization hosts don't have a GUI installed at all.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: 'So far, we''ve discussed virtual networking from an overall host-level. However,
    there''s also a different approach to the subject – using a virtual machine as
    an object to which we can add a virtual network card and connect it to a virtual
    network. We can use `virsh` for that purpose. So, just as an example, we can connect
    our virtual machine called `MasteringKVM01` to an isolated virtual network:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: There are other concepts that allow virtual machine connectivity to a physical
    network, and some of them we will discuss later in this chapter (such as SR-IOV).
    However, now that we've covered the basic approaches to connecting virtual machines
    to a physical network by using a virtual switch/bridge, we need to get a bit more
    technical. The thing is, there are more concepts involved in connecting a virtual
    machine to a virtual switch, such as TAP and TUN, which we will be covering in
    the following section.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他概念允许虚拟机连接到物理网络，其中一些我们将在本章后面讨论（如SR-IOV）。然而，现在我们已经介绍了通过虚拟交换/桥接将虚拟机连接到物理网络的基本方法，我们需要变得更加技术化。问题是，在连接虚拟机到虚拟交换中涉及更多的概念，比如TAP和TUN，我们将在接下来的部分中进行介绍。
- en: Using userspace networking with TAP and TUN devices
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用TAP和TUN设备进行用户空间网络连接
- en: 'In [*Chapter 1*](B14834_01_Final_ASB_ePub.xhtml#_idTextAnchor016), *Understanding
    Linux Virtualization*, we used the `virt-host-validate` command to do some pre-flight
    checks in terms of the host''s preparedness for KVM virtualization. As a part
    of that process, some of the checks include checking if the following devices
    exist:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第1章*](B14834_01_Final_ASB_ePub.xhtml#_idTextAnchor016)，*理解Linux虚拟化*中，我们使用`virt-host-validate`命令对主机的KVM虚拟化准备情况进行了一些预检查。作为该过程的一部分，一些检查包括检查以下设备是否存在：
- en: '`/dev/kvm`: The KVM drivers create a `/dev/kvm` character device on the host
    to facilitate direct hardware access for virtual machines. Not having this device
    means that the VMs won''t be able to access physical hardware, although it''s
    enabled in the BIOS and this will reduce the VM''s performance significantly.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`/dev/kvm`：KVM驱动程序在主机上创建了一个`/dev/kvm`字符设备，以便为虚拟机提供直接硬件访问。没有这个设备意味着虚拟机将无法访问物理硬件，尽管它在BIOS中已启用，这将显著降低虚拟机的性能。'
- en: '`/dev/vhost-net`: The `/dev/vhost-net` character device will be created on
    the host. This device serves as the interface for configuring the `vhost-net`
    instance. Not having this device significantly reduces the virtual machine''s
    network performance.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`/dev/vhost-net`：在主机上将创建`/dev/vhost-net`字符设备。该设备用作配置`vhost-net`实例的接口。没有这个设备会显著降低虚拟机的网络性能。'
- en: '`/dev/net/tun`: This is another character special device used for creating
    TUN/TAP devices to facilitate network connectivity for a virtual machine. The
    TUN/TAP device will be explained in detail in future chapters. For now, just understand
    that having a character device is important for KVM virtualization to work properly.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`/dev/net/tun`：这是另一个用于创建TUN/TAP设备以为虚拟机提供网络连接的字符特殊设备。TUN/TAP设备将在以后的章节中详细解释。现在只需理解，拥有一个字符设备对于KVM虚拟化正常工作是很重要的。'
- en: Let's focus on the last device, the TUN device, which is usually accompanied
    by a TAP device.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们专注于最后一个设备，TUN设备，通常会伴随着一个TAP设备。
- en: So far, all the concepts that we've covered include some kind of connectivity
    to a physical network card, with isolated virtual networks being an exception.
    But even an isolated virtual network is just a virtual network for our virtual
    machines. What happens when we have a situation where we need our communication
    to happen in the user space, such as between applications running on a server?
    It would be useless to patch them through some kind of virtual switch concept,
    or a regular bridge, as that would just bring additional overhead. This is where
    TUN/TAP devices come in, providing packet flow for user space programs. Easily
    enough, an application can open `/dev/net/tun` and use an `ioctl()` function to
    register a network device in the kernel, which, in turn, presents itself as a
    tunXX or tapXX device. When the application closes the file, the network devices
    and routes created by it disappear (as described in the kernel `tuntap.txt` documentation).
    So, it's just a type of virtual network interface for the Linux operating system
    supported by the Linux kernel – you can add an IP address and routes to it so
    that traffic from your application can route through it, and not via a regular
    network device.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们所涵盖的所有概念都包括与物理网络卡的某种连接，隔离的虚拟网络是一个例外。但即使是隔离的虚拟网络对于我们的虚拟机来说也只是一个虚拟网络。当我们需要在用户空间进行通信时会发生什么，比如在服务器上运行的应用之间？将它们通过某种虚拟交换概念或常规桥接连接起来将会带来额外的开销。这就是TUN/TAP设备的作用，为用户空间程序提供数据包流。很容易，应用程序可以打开`/dev/net/tun`并使用`ioctl()`函数在内核中注册一个网络设备，然后它会呈现为一个tunXX或tapXX设备。当应用程序关闭文件时，它创建的网络设备和路由会消失（如内核`tuntap.txt`文档中所述）。因此，这只是Linux操作系统支持的一种虚拟网络接口类型，可以向其添加IP地址和路由，以便应用程序的流量可以通过它路由，而不是通过常规网络设备。
- en: TUN emulates an L3 device by creating a communication tunnel, something like
    a point-to-point tunnel. It gets activated when the tuntap driver gets configured
    in tun mode. When you activate it, any data that you receive from a descriptor
    (the application that configured it) will be data in the form of regular IP packages
    (as the most commonly used case). Also, when you send data, it gets written to
    the TUN device as regular IP packages. This type of interface is sometimes used
    in testing, development, and debugging for simulation purposes.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: TUN通过创建通信隧道来模拟L3设备，类似于点对点隧道。当tuntap驱动程序配置为tun模式时，它会被激活。激活后，从描述符（配置它的应用程序）接收到的任何数据都将以常规IP数据包的形式传输（作为最常用的情况）。同样，当发送数据时，它会被写入TUN设备作为常规IP数据包。这种类型的接口有时用于测试、开发和模拟调试目的。
- en: The TAP interface basically emulates an L2 Ethernet device. It gets activated
    when the tuntap driver gets configured in tap mode. When you activate it, unlike
    what happens with the TUN interface (Layer 3), you get Layer 2 raw Ethernet packages,
    including ARP/RARP packages and everything else. Basically, we're talking about
    a virtualized Layer 2 Ethernet connection.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: TAP接口基本上模拟L2以太网设备。当tuntap驱动程序以tap模式配置时，它会被激活。当您激活它时，与TUN接口（第3层）不同，您会获得第2层原始以太网数据包，包括ARP/RARP数据包和其他所有内容。基本上，我们谈论的是虚拟化的第2层以太网连接。
- en: These concepts (especially TAP) are usable on libvirt/QEMU as well because by
    using these types of configurations, we can create connections from the host to
    a virtual machine – without the libvirt bridge/switch, just as an example. We
    can actually configure all of the necessary details for the TUN/TAP interface
    and then start deploying virtual machines that are hooked up directly to those
    interfaces by using `kvm-qemu` options. So, it's a rather interesting concept
    that has its place in the virtualization world as well. This is especially interesting
    when we start creating Linux bridges.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这些概念（特别是TAP）也可用于libvirt/QEMU，因为通过使用这些类型的配置，我们可以从主机到虚拟机创建连接 - 例如，没有libvirt桥/交换机。我们实际上可以配置TUN/TAP接口的所有必要细节，然后通过使用`kvm-qemu`选项将虚拟机连接到这些接口。因此，这是一个在虚拟化世界中有其位置的相当有趣的概念。当我们开始创建Linux桥接时，这尤其有趣。
- en: Implementing Linux bridging
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实施Linux桥接
- en: 'Let''s create a bridge and then add a TAP device to it. Before we do that,
    we must make sure the bridge module is loaded into the kernel. Let''s get started:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个桥接，然后将TAP设备添加到其中。在这样做之前，我们必须确保桥接模块已加载到内核中。让我们开始吧：
- en: 'If it is not loaded, use `modprobe bridge` to load the module:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果未加载，请使用`modprobe bridge`加载模块：
- en: '[PRE3]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: brctl show command will list all the available bridges on the server, along
    with some basic information, such as the ID of the bridge, Spanning Tree Protocol
    (STP) status, and the interfaces attached to it. Here, the tester bridge does
    not have any interfaces attached to its virtual ports.
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: brctl show命令将列出服务器上所有可用的桥接以及一些基本信息，例如桥接的ID、生成树协议（STP）状态以及连接到其上的接口。在这里，测试器桥没有任何接口连接到其虚拟端口。
- en: '[PRE4]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'A Linux bridge will also be shown as a network device. To see the network details
    of the bridge tester, use the `ip` command:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Linux桥接也将显示为网络设备。要查看桥接测试器的网络详细信息，请使用`ip`命令：
- en: '[PRE5]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ifconfig tester
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ifconfig tester
- en: 'tester: flags=4098<BROADCAST,MULTICAST>mtu 1500'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 测试器：flags=4098<BROADCAST,MULTICAST>mtu 1500
- en: ether26:84:f2:f8:09:e0txqueuelen 1000 (Ethernet)
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: ether26:84:f2:f8:09:e0txqueuelen 1000（以太网）
- en: RX packets 0 bytes 0 (0.0 B)
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: RX数据包0 字节0（0.0 B）
- en: RX errors 0 dropped 0 overruns 0 frame 0
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: RX错误0 丢弃0 超限0 帧0
- en: TX packets 0 bytes 0 (0.0 B)
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: TX数据包0 字节0（0.0 B）
- en: TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: TX错误0 丢弃0 超限0 载波0 冲突0
- en: '[PRE6]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'First, check if the TUN/TAP device module is loaded into the kernel. If not,
    you already know the drill:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，检查TUN/TAP设备模块是否加载到内核中。如果没有，您已经知道该怎么做：
- en: '[PRE7]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'tester and a tap device named vm-vnic. Let''s add vm-vnic to tester:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 测试器和名为vm-vnic的tap设备。让我们将vm-vnic添加到tester：
- en: '[PRE8]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Here, you can see that `vm-vnic` is an interface that was added to the `tester`
    bridge. Now, `vm-vnic` can act as the interface between your virtual machine and
    the `tester` bridge, which, in turn, enables the virtual machine to communicate
    with other virtual machines that are added to this bridge:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，您可以看到`vm-vnic`是添加到`tester`桥的接口。现在，`vm-vnic`可以作为您的虚拟机和`tester`桥之间的接口，从而使虚拟机能够与添加到此桥的其他虚拟机进行通信：
- en: '![Figure 4.10 – Virtual machines connected to a virtual switch (bridge)'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.10 - 连接到虚拟交换机（桥接）的虚拟机
- en: '](img/B14834_04_10.jpg)'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_04_10.jpg)'
- en: Figure 4.10 – Virtual machines connected to a virtual switch (bridge)
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.10 - 连接到虚拟交换机（桥接）的虚拟机
- en: 'You might also need to remove all the objects and configurations that were
    created in the previous procedure. Let''s do this step by step via the command
    line:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能还需要删除在上一个过程中创建的所有对象和配置。让我们通过命令行逐步进行：
- en: 'First, we need to remove the `vm-vnic` tap device from the `tester` bridge:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们需要从`tester`桥中删除`vm-vnic` tap设备：
- en: '[PRE10]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ip tuntap del dev vm-vnic mode tap
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ip tuntap del dev vm-vnic mode tap
- en: '[PRE11]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Then, remove the tester bridge:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，删除测试器桥：
- en: '[PRE12]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: These are the same steps that libvirt carried out in the backend while enabling
    or disabling networking for a virtual machine. We want you to understand this
    procedure thoroughly before moving ahead. Now that we've covered Linux bridging,
    it's time to move on to a more advanced concept called Open vSwitch.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是libvirt在后端执行的相同步骤，用于启用或禁用虚拟机的网络。在继续之前，我们希望您彻底了解此过程。现在我们已经介绍了Linux桥接，是时候转向一个更高级的概念，称为Open
    vSwitch。
- en: Configuring Open vSwitch
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配置Open vSwitch
- en: Imagine for a second that you're working for a small company that has three
    to four KVM hosts, a couple of network-attached storage devices to host their
    15 virtual machines, and that you've been employed by the company from the very
    start. So, you've seen it all – the company buying some servers, network switches,
    cables, and storage devices, and you were a part of a small team of people that
    built that environment. After 2 years of that process, you're aware of the fact
    that everything works, it's simple to maintain, and doesn't give you an awful
    lot of grief.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你在一家小公司工作，有三到四个KVM主机，几个网络附加存储设备来托管他们的15台虚拟机，并且你从一开始就被公司雇佣。因此，您已经见证了一切 -
    公司购买了一些服务器、网络交换机、电缆和存储设备，并且您是建立该环境的一小部分人员团队。经过2年的过程，您已经意识到一切都运作正常，维护简单，并且没有给您带来太多烦恼。
- en: Now, imagine the life of a friend of yours working for a bigger enterprise company
    that has 400 KVM hosts and close to 2,000 virtual machines to manage, doing the
    same job as you're doing in a comfy chair of your office in your small company.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，想象一下，你的一个朋友在一家拥有400个KVM主机和近2000台虚拟机的大型企业公司工作，他们需要管理的工作与你在你的小公司的舒适椅子上所做的工作相同。
- en: Do you think that your friend can manage his or her environment by using the
    very same tools that you're using? XML files for network switch configuration,
    deploying servers from a bootable USB drive, manually configuring everything,
    and having the time to do so? Does that seem like a possibility to you?
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 你认为你的朋友能否通过使用与你相同的工具来管理他或她的环境？使用XML文件进行网络交换机配置，从可引导的USB驱动器部署服务器，手动配置一切，并有时间这样做？这对你来说可能吗？
- en: 'There are two basic problems in this second situation:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二种情况中有两个基本问题：
- en: 'The scale of the environment: This one is more obvious. Because of the environment
    size, you need some kind of concept that''s going to be managed centrally, instead
    of on a host-per-host level, such as the virtual switches we''ve discussed so
    far.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 环境的规模：这一点更为明显。由于环境的规模，您需要一种在中央进行管理的概念，而不是在主机级别进行管理，比如我们迄今讨论过的虚拟交换机。
- en: 'Company policies: These usually dictate some kind of compliance that comes
    from configuration standardization as much as possible. Now, we can agree that
    we could script some configuration updates via Ansible, Puppet, or something like
    that, but what''s the use? We''re going to have to create new config files, new
    procedures, and new workbooks every single time we need to introduce a change
    to KVM networking. And big companies frown upon that.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 公司政策：这些通常规定尽可能从配置标准化中获得的一些合规性。现在，我们可以同意我们可以通过Ansible，Puppet或类似工具脚本化一些配置更新，但有什么用呢？每次我们需要对KVM网络进行更改时，我们都必须创建新的配置文件，新的流程和新的工作簿。大公司对此持负面态度。
- en: So, what we need is a centralized networking object that can span across multiple
    hosts and offer configuration consistency. In this context, configuration consistency
    offers us a huge advantage – every change that we introduce in this type of object
    will be replicated to all the hosts that are members of this centralized networking
    object. In other words, what we need is **Open vSwitch** (**OVS**). For those
    who are more versed in VMware-based networking, we can use an approximate metaphor
    – Open vSwitch is for KVM-based environments similar to what vSphere Distributed
    Switch is for VMware-based environments.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们需要的是一个可以跨越多个主机并提供配置一致性的集中式网络对象。在这种情况下，配置一致性为我们带来了巨大的优势 - 我们在这种类型的对象中引入的每个更改都将被复制到所有属于这个集中式网络对象的主机。换句话说，我们需要的是**Open
    vSwitch**（**OVS**）。对于那些更熟悉基于VMware的网络的人来说，我们可以使用一个近似的隐喻 - 对于基于KVM的环境，Open vSwitch类似于vSphere分布式交换机对于基于VMware的环境。
- en: 'In terms of technology, OVS supports the following:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在技术方面，OVS支持以下内容：
- en: VLAN isolation (IEEE 802.1Q)
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: VLAN隔离（IEEE 802.1Q）
- en: Traffic filtering
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流量过滤
- en: NIC bonding with or without LACP
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有或不具有LACP的NIC绑定
- en: Various overlay networks – VXLAN, GENEVE, GRE, STT, and so on
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 各种覆盖网络 - VXLAN，GENEVE，GRE，STT等
- en: 802.1ag support
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 802.1ag支持
- en: Netflow, sFlow, and so on
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Netflow，sFlow等
- en: (R)SPAN
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: （R）SPAN
- en: OpenFlow
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenFlow
- en: OVSDB
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OVSDB
- en: Traffic queuing and shaping
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流量排队和整形
- en: Linux, FreeBSD, NetBSD, Windows, and Citrix support (and a host of others)
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Linux，FreeBSD，NetBSD，Windows和Citrix支持（以及其他许多）
- en: Now that we've listed some of the supported technologies, let's discuss the
    way in which Open vSwitch works.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经列出了一些支持的技术，让我们讨论一下Open vSwitch的工作方式。
- en: 'First, let''s talk about the Open vSwitch architecture. The implementation
    of Open vSwitch is broken down into two parts: the Open vSwitch kernel module
    (the data plane) and the user space tools (the control pane). Since the incoming
    data packets must be processed as fast as possible, the data plane of Open vSwitch
    was pushed to the kernel space:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们谈谈Open vSwitch的架构。 Open vSwitch的实现分为两部分：Open vSwitch内核模块（数据平面）和用户空间工具（控制平面）。由于传入的数据包必须尽快处理，因此Open
    vSwitch的数据平面被推到了内核空间：
- en: '![Figure 4.11 – Open vSwitch architecture'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.11 - Open vSwitch架构'
- en: '](img/B14834_04_11.jpg)'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_04_11.jpg)'
- en: Figure 4.11 – Open vSwitch architecture
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.11 - Open vSwitch架构
- en: The data path (OVS kernel module) uses the netlink socket to interact with the
    vswitchd daemon, which implements and manages any number of OVS switches on the
    local system.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 数据路径（OVS内核模块）使用netlink套接字与vswitchd守护程序进行交互，在本地系统上实现和管理任意数量的OVS交换机。
- en: Open vSwitch doesn't have a specific SDN controller that it uses for management
    purposes, in a similar fashion to VMware's vSphere distributed switch and NSX,
    which have vCenter and various NSX components to manage their capabilities. In
    OVS, the point is to use someone else's SDN controller, which then interacts with
    ovs-vswitchd using the OpenFlow protocol. The ovsdb-server maintains the switch
    table database and external clients can talk to the ovsdb-server using JSON-RPC;
    JSON is the data format. The ovsdb database currently contains around 13 tables
    and this database is persistent across restarts.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: Open vSwitch没有特定的SDN控制器用于管理目的，类似于VMware的vSphere分布式交换机和NSX，它们有vCenter和各种NSX组件来管理它们的功能。在OVS中，重点是使用其他人的SDN控制器，然后使用OpenFlow协议与ovs-vswitchd进行交互。ovsdb-server维护交换机表数据库，外部客户端可以使用JSON-RPC与ovsdb-server进行通信；JSON是数据格式。ovsdb数据库目前包含大约13个表，并且此数据库在重新启动时是持久的。
- en: 'Open vSwitch works in two modes: normal and flow mode. This chapter will primarily
    concentrate on how to bring up a KVM VM connected to Open vSwitch''s bridge in
    standalone/normal mode and will a give brief introduction to flow mode using the
    OpenDaylight controller:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: Open vSwitch有两种模式：正常模式和流模式。本章将主要集中讨论如何在独立/正常模式下启动连接到Open vSwitch桥的KVM VM，并简要介绍使用OpenDaylight控制器的流模式：
- en: '**Normal Mode**: Switching and forwarding are handled by OVS bridge. In this
    modem OVS acts as an L2 learning switch. This mode is specifically useful when
    configuring several overlay networks for your target rather than manipulating
    the switch''s flow.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**正常模式**：交换和转发由OVS桥处理。在这种模式下，OVS充当L2学习交换机。当为目标配置多个覆盖网络而不是操纵交换机流时，此模式特别有用。'
- en: '`ctl` command. This mode allows a greater level of abstraction and automation;
    the SDN controller exposes the REST API. Our applications can make use of this
    API to directly manipulate the bridge''s flows to meet network needs.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s move on to the practical aspect and learn how to install Open vSwitch
    on CentOS 8:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing that we must do is tell our system to use the appropriate repositories.
    In this case, we need to enable the repositories called `epel` and `centos-release-openstack-train`.
    We can do that by using a couple of `yum` commands:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The next step will be installing `openvswitch` from Red Hat''s repository:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'After the installation process, we need to check if everything is working by
    starting and enabling the Open vSwitch service and running the `ovs-vsctl -V`
    command:'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now that we''ve successfully installed and started Open vSwitch, it''s time
    to configure it. Let''s choose a deployment scenario in which we''re going to
    use Open vSwitch as a new virtual switch for our virtual machines. In our server,
    we have another physical interface called `ens256`, which we''re going to use
    as an uplink for our Open vSwitch virtual switch. We''re also going to clear ens256
    configuration, configure an IP address for our OVS, and start the OVS by using
    the following commands:'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now that everything has been configured but not persistently, we need to make
    the configuration persistent. This means configuring some network interface configuration
    files. So, go to `/etc/sysconfig/network-scripts` and create two files. Call one
    of them `ifcfg-ens256` (for our uplink interface):'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: DEVICE=ovs-br0
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: DEVICETYPE=ovs
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: TYPE=OVSBridge
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: BOOTPROTO=static
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: IPADDR=10.10.10.1
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: NETMASK=255.255.255.0
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: GATEWAY=10.10.10.254
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: ONBOOT=yes
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We didn''t configure all of this just for show, so we need to make sure that
    our KVM virtual machines are also able to use it. This means – again – that we
    need to create a KVM virtual network that''s going to use OVS. Luckily, we''ve
    dealt with KVM virtual network XML files before (check the *Libvirt isolated network*
    section), so this one isn''t going to be a problem. Let''s call our network `packtovs`
    and its corresponding XML file `packtovs.xml`. It should contain the following
    content:'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'So, now, we can perform our usual operations when we have a virtual network
    definition in an XML file, which is to define, start, and autostart the network:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'If we left everything as it was when we created our virtual networks, the output
    from `virsh net-list` should look something like this:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.12 – Successful OVS configuration, and OVS+KVM configuration'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_04_12.jpg)'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.12 – Successful OVS configuration, and OVS+KVM configuration
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: 'So, all that''s left now is to hook up a VM to our newly defined OVS-based
    network called `packtovs` and we''re home free. Alternatively, we could just create
    a new one and pre-connect it to that specific interface using the knowledge we
    gained in [*Chapter 3*](B14834_03_Final_ASB_ePub.xhtml#_idTextAnchor049), *Installing
    KVM Hypervisor, libvirt, and oVirt*. So, let''s issue the following command, which
    has just two changed parameters (`--name` and `--network`):'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'After the virtual machine installation completes, we''re connected to the OVS-based
    `packtovs` virtual network, and our virtual machine can use it. Let''s say that
    additional configuration is needed and that we got a request to tag traffic coming
    from this virtual machine with `VLAN ID 5`. Start your virtual machine and use
    the following set of commands:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'This command tells us that we''re using the `ens256` port as an uplink and
    that our virtual machine, `MasteringKVM03`, is using the virtual `vnet0` network
    port. We can apply VLAN tagging to that port by using the following command:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We need to take note of some additional commands related to OVS administration
    and management since this is done via the CLI. So, here are some commonly used
    OVS CLI administration commands:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: '`#ovs-vsctl show`: A very handy and frequently used command. It tells us what
    the current running configuration of the switch is.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`#ovs-vsctl list-br`: Lists bridges that were configured on Open vSwitch.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`#ovs-vsctl list-ports <bridge>`: Shows the names of all the ports on `BRIDGE`.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`#ovs-vsctl list interface <bridge>`: Shows the names of all the interfaces
    on `BRIDGE`.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`#ovs-vsctl add-br <bridge>`: Creates a bridge in the switch database.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`#ovs-vsctl add-port <bridge> : <interface>`: Binds an interface (physical
    or virtual) to the Open vSwitch bridge.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`#ovs-ofctl and ovs-dpctl`: These two commands are used for administering and
    monitoring flow entries. You learned that OVS manages two kinds of flows: OpenFlows
    and Datapath. The first is managed in the control plane, while the second one
    is a kernel-based flow.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`#ovs-ofctl`: This speaks to the OpenFlow module, whereas `ovs-dpctl` speaks
    to the Kernel module.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following examples are the most used options for each of these commands:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: '`#ovs-ofctl show <BRIDGE>`: Shows brief information about the switch, including
    the port number to port name mapping.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`#ovs-ofctl dump-flows <Bridge>`: Examines OpenFlow tables.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`#ovs-dpctl show`: Prints basic information about all the logical datapaths,
    referred to as *bridges*, present on the switch.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`#ovs-dpctl dump-flows`: It shows the flow cached in datapath.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ovs-appctl`: This command offers a way to send commands to a running Open
    vSwitch and gathers information that is not directly exposed to the `ovs-ofctl`
    command. This is the Swiss Army knife of OpenFlow troubleshooting.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`#ovs-appctl bridge/dumpflows <br>`: Examines flow tables and offers direct
    connectivity for VMs on the same hosts.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`#ovs-appctl fdb/show <br>`: Lists MAC/VLAN pairs learned.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Also, you can always use the `ovs-vsctl show` command to get information about
    the configuration of your OVS switch:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.13 – ovs-vsctl show output'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_04_13.jpg)'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.13 – ovs-vsctl show output
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: We are going to come back to the subject of Open vSwitch in [*Chapter 12*](B14834_12_Final_ASB_ePub.xhtml#_idTextAnchor209),
    *Scaling Out KVM with OpenStack* , as we go deeper into our discussion about spanning
    Open vSwitch across multiple hosts, especially while keeping in mind the fact
    that we want to be able to span our cloud overlay networks (based on GENEVE, VXLAN,
    GRE, or similar protocols) across multiple hosts and sites.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: Other Open vSwitch use cases
  id: totrans-228
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As you might imagine, Open vSwitch isn't just a handy concept for libvirt or
    OpenStack – it can be used for a variety of other scenarios as well. Let's describe
    one of them as it might be important for people looking into VMware NSX or NSX-T
    integration.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s just describe a few basic terms and relationships here. VMware''s NSX
    is an SDN-based technology that can be used for a variety of use cases:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: Connecting data centers and extending cloud overlay networks across data center
    boundaries.
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A variety of disaster recover scenarios. NSX can be a big help for disaster
    recover, for multi-site environments, and for integration with a variety of external
    services and devices that can be a part of the scenario (Palo Alto PANs).
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consistent micro-segmentation, across sites, done *the right way* on the virtual
    machine network card level.
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For security purposes, varying from different types of supported VPN technologies
    to connect sites and end users, to distributed firewalls, guest introspection
    options (antivirus and anti-malware), network introspection options (IDS/IPS),
    and more.
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For load balancing, up to Layer 7, with SSL offload, session persistence, high
    availablity, application rules, and more.
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yes, VMware's take on SDN (NSX) and Open vSwitch seem like *competing technologies*
    on the market, but realistically, there are loads of clients who want to use both.
    This is where VMware's integration with OpenStack and NSX's integration with Linux-based
    KVM hosts (by using Open vSwitch and additional agents) comes in really handy.
    Just to further explain these points – there are things that NSX does that take
    *extensive* usage of Open vSwitch-based technologies – hardware VTEP integration
    via Open vSwitch Database, extending GENEVE networks to KVM hosts by using Open
    vSwitch/NSX integration, and much more.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: Imagine that you're working for a service provider – a cloud service provider,
    an ISP; basically, any type of company that has large networks with a lot of network
    segmentation. There are loads of service providers using VMware's vCloud Director
    to provide cloud services to end users and companies. However, because of market
    needs, these environments often need to be extended to include AWS (for additional
    infrastructure growth scenarios via the public cloud) or OpenStack (to create
    hybrid cloud scenarios). If we didn't have a possibility to have interoperability
    between these solutions, there would be no way to use both of these offerings
    at the same time. But from a networking perspective, the network background for
    that is NSX or NSX-T (which actually *uses* Open vSwitch).
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: It's been clear for years that the future is all about multi-cloud environments,
    and these types of integrations will bring in more customers; they will want to
    take advantage of these options in their cloud service design. Future developments
    will also most probably include (and already partially include) integration with
    Docker, Kubernetes, and/or OpenShift to be able to manage containers in the same
    environment.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: There are also some more extreme examples of using hardware – in our example,
    we are talking about network cards on a PCI Express bus – in a *partitioned* way.
    For the time being, our explanation of this concept, called SR-IOV, is going to
    be limited to network cards, but we will expand on the same concept in [*Chapter
    6*](B14834_06_Final_ASB_ePub.xhtml#_idTextAnchor108), *Virtual Display Devices
    and Protocols*, when we start talking about partitioning GPUs for use in virtual
    machines. So, let's discuss a practical example of using SR-IOV on an Intel network
    card that supports it.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: Understanding and using SR-IOV
  id: totrans-240
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The SR-IOV concept is something that we already mentioned in [*Chapter 2*](B14834_02_Final_ASB_ePub.xhtml#_idTextAnchor029),
    *KVM as a Virtualization Solution*. By utilizing SR-IOV, we can *partition* PCI
    resources (for example, network cards) into virtual PCI functions and inject them
    into a virtual machine. If we''re using this concept for network cards, we''re
    usually doing this with a single purpose – so that we can avoid using the operating
    system kernel and network stack while accessing a network interface card from
    our virtual machine. In order for us to be able to do this, we need to have hardware
    support, so we need to check if our network card actually supports it. On a physical
    server, we could use the `lspci` command to extract attribute information about
    our PCI devices and then `grep` out *Single Root I/O Virtualization* as a string
    to try to see if we have a device that''s compatible. Here''s an example from
    our server:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.14 – Checking if our system is SR-IOV compatible'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_04_14.jpg)'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.14 – Checking if our system is SR-IOV compatible
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: 'Be careful when configuring SR-IOV. You need to have a server that supports
    it, a device that supports it, and you must make sure that you turn on SR-IOV
    functionality in BIOS. Then, you need to keep in mind that there are servers that
    only have specific slots assigned for SR-IOV. The server that we used (HP Proliant
    DL380p G8) has three PCI-Express slots assigned to CPU1, but SR-IOV worked only
    in slot #1\. When we connected our card to slot #2 or #3, we got a BIOS message
    that SR-IOV will not work in that slot and that we should move our card to a slot
    that supports SR-IOV. So, please, make sure that you read the documentation of
    your server thoroughly and connect a SR-IOV compatible device to a correct PCI-Express
    slot.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: 'In this specific case, it''s an Intel 10 Gigabit network adapter with two ports,
    which we could use to do SR-IOV. The procedure isn''t all that difficult, and
    it requires us to complete the following steps:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: Unbind from the previous module.
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Register it to the vfio-pci module, which is available in the Linux kernel stack.
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Configure a guest that's going to use it.
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'So, what you would do is unload the module that the network card is currently
    using by using `modprobe -r`. Then, you would load it again, but by assigning
    an additional parameter. On our specific server, the Intel dual-port adapter that
    we''re using (X540-AT2) was assigned to the `ens1f0` and `ens1f1` network devices.
    So, let''s use `ens1f0` as an example for SR-IOV configuration at boot time:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing that we need to do (as a general concept) is find out which
    kernel module our network card is using. To do that, we need to issue the following
    command:'
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'ixgbe module here, and we can do the following:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Then, we can use the `modprobe` system to make these changes permanent across
    reboots by creating a file in `/etc/modprobe.d` called (for example) `ixgbe.conf`
    and adding the following line to it:'
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'This would give us up to four virtual functions that we can use inside our
    virtual machines. Now, the next issue that we need to solve is how to boot our
    server with SR-IOV active at boot time. There are quite a few steps involved here,
    so, let''s get started:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: We need to add the `iommu` and `vfs` parameters to the default kernel boot line
    and the default kernel configuration. So, first, open `/etc/default/grub` and
    edit the `GRUB_CMDLINE_LINUX` line and add `intel_iommu=on` (or `amd_iommu=on`
    if you're using an AMD system) and `ixgbe.max_vfs=4` to it.
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We need to reconfigure `grub` to use this change, so we need to use the following
    command:'
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Sometimes, even that isn''t enough, so we need to configure the necessary kernel
    parameters, such as the maximum number of virtual functions and the `iommu` parameter
    to be used on the server. That leads us to the following command:'
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'After reboot, we should be able to see our virtual functions. Type in the following
    command:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We should get an output that looks like this:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.15 –  Checking for virtual function visibility'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_04_15.jpg)'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.15 – Checking for virtual function visibility
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: 'We should be able to see these virtual functions from libvirt, and we can check
    that via the `virsh` command. Let''s try this (we''re using `grep 04` because
    our device IDs start with 04, which is visible from the preceding image; we''ll
    shrink the output to important entries only):'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The first two devices are our physical functions. The remaining eight devices
    (two ports times four functions) are our virtual devices (from `pci_0000_04_10_0`
    to `pci_0000_04_10_7`). Now, let''s dump that device''s information by using the
    `virsh nodedev-dumpxml pci_0000_04_10_0` command:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.16 – Virtual function information from the perspective of virsh'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_04_16.jpg)'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.16 – Virtual function information from the perspective of virsh
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: 'So, if we have a running virtual machine that we''d like to reconfigure to
    use this, we''d have to create an XML file with definition that looks something
    like this (let''s call it `packtsriov.xml`):'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Of course, the domain, bus, slot, and function need to point exactly to our
    VF. Then, we can use the `virsh` command to attach that device to our virtual
    machine (for example, `MasteringKVM03`):'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: When we use `virsh dumpxml`, we should now see a part of the output that starts
    with `<driver name='vfio'/>`, along with all the information that we configured
    in the previous step (address type, domain, bus, slot, function). Our virtual
    machine should have no problems using this virtual function as a network card.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, it''s time to cover another concept that''s very much useful in KVM networking:
    macvtap. It''s a newer driver that should simplify our virtualized networking
    by completely removing tun/tap and bridge drivers with a single module.'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: Understanding macvtap
  id: totrans-283
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This module works like a combination of the tap and macvlan modules. We already
    explained what the tap module does. The macvlan module enables us to create virtual
    networks that are pinned to a physical network interface (usually, we call this
    interface a *lower* interface or device). Combining tap and macvlan enables us
    to choose between four different modes of operation, called **Virtual Ethernet
    Port Aggregator** (**VEPA**), bridge, private, and passthru.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: 'If we''re using the VEPA mode (default mode), the physical switch has to support
    VEPA by supporting `hairpin` mode (also called reflective relay). When a *lower*
    device receives data from a VEPA mode macvlan, this traffic is always sent out
    to the upstream device, which means that traffic is always going through an external
    switch. The advantage of this mode is the fact that network traffic between virtual
    machines becomes visible on the external network, which can be useful for a variety
    of reasons. You can check how network flow works in the following sequence of
    diagrams:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.17 – macvtap VEPA mode, where traffic is forced to the external
    network'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_04_17.jpg)'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.17 – macvtap VEPA mode, where traffic is forced to the external network
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: 'In private mode, it''s similar to VEPA in that everything goes to an external
    switch, but unlike VEPA, traffic only gets delivered if it''s sent via an external
    router or switch. You can use this mode if you want to isolate virtual machines
    connected to the endpoints from one another, but not from the external network.
    If this sounds very much like a private VLAN scenario, you''re completely correct:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.18 – macvtap in private mode, using it for internal network isolation'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_04_18.jpg)'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.18 – macvtap in private mode, using it for internal network isolation
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: 'In bridge mode, data received on your macvlan that''s supposed to go to another
    macvlan on the same lower device is sent directly to the target, not externally,
    and then routed back. This is very similar to what VMware NSX does when communication
    is supposed to happen between virtual machines on different VXLAN networks, but
    on the same host:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.19 – macvtap in bridge mode, providing a kind of internal routing'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_04_19.jpg)'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.19 – macvtap in bridge mode, providing a kind of internal routing
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: 'In passthrough mode, we''re basically talking about the SR-IOV scenario, where
    we''re using a VF or a physical device directly to the macvtap interface. The
    key difference is that a single network interface can only be passed to a single
    guest (1:1 relationship):'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.20 – macvtap in passthrough mode'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_04_20.jpg)'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.20 – macvtap in passthrough mode
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: In [*Chapter 12*](B14834_12_Final_ASB_ePub.xhtml#_idTextAnchor209), *Scaling
    Out KVM with OpenStack* and [*Chapter 13*](B14834_13_Final_ASB_ePub.xhtml#_idTextAnchor238),
    *Scaling Out KVM with AWS,* we'll describe why virtualized and *overlay* networking
    (VXLAN, GRE, GENEVE) is even more important for cloud networking as we extend
    our local KVM-based environment to the cloud either via OpenStack or AWS.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-302
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered the basics of virtualized networking in KVM and
    explained why virtualized networking is such a huge part of virtualization. We
    went knee-deep into configuration files and their options as this will be the
    preferred method for administration in larger environments, especially when talking
    about virtualized networks.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: Pay close attention to all the configuration steps that we discussed through
    this chapter, especially the part related to using virsh commands to manipulate
    network configuration and to configure Open vSwitch and SR-IOV. SR-IOV-based concepts
    are heavily used in latency-sensitive environments to provide networking services
    with the lowest possible overhead and latency, which is why this principle is
    very important for various enterprise environments related to the financial and
    banking sector.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we''ve covered all the necessary networking scenarios (some of which
    will be revisited later in this book), it''s time to start thinking about the
    next big topic of the virtualized world. We''ve already talked about CPU and memory,
    as well as networks, which means we''re left with the fourth pillar of virtualization:
    storage. We will tackle that subject in the next chapter.'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  id: totrans-306
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Why is it important that virtual switches accept connectivity from multiple
    virtual machines at the same time?
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does a virtual switch work in NAT mode?
  id: totrans-308
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does a virtual switch work in routed mode?
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is Open vSwitch and for what purpose can we use it in virtualized and cloud
    environments?
  id: totrans-310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Describe the differences between TAP and TUN interfaces.
  id: totrans-311
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  id: totrans-312
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Please refer to the following links for more information regarding what was
    covered in this chapter:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: 'Libvirt networking: [https://wiki.libvirt.org/page/VirtualNetworking](https://wiki.libvirt.org/page/VirtualNetworking)'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Network XML format: [https://libvirt.org/formatnetwork.html](https://libvirt.org/formatnetwork.html)'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Open vSwitch: [https://www.openvswitch.org/](https://www.openvswitch.org/)'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Open vSwitch and libvirt: [http://docs.openvswitch.org/en/latest/howto/libvirt/](http://docs.openvswitch.org/en/latest/howto/libvirt/)'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Open vSwitch Cheat Sheet: [https://adhioutlined.github.io/virtual/Openvswitch-Cheat-Sheet/](https://adhioutlined.github.io/virtual/Openvswitch-Cheat-Sheet/)'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
