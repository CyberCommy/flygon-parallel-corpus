- en: '*Chapter 4*: Libvirt Networking'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Understanding how virtual networking works is really essential for virtualization.
    It would be very hard to justify the costs associated with a scenario in which
    we didn't have virtual networking. Just imagine having multiple virtual machines
    on a virtualization host and buying network cards so that every single one of
    those virtual machines can have their own dedicated, physical network port. By
    implementing virtual networking, we're also consolidating networking in a much
    more manageable way, both from an administration and cost perspective.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter provides you with an insight into the overall concept of virtualized
    networking and Linux-based networking concepts. We will also discuss physical
    and virtual networking concepts, try to compare them, and find similarities and
    differences between them. Also covered in this chapter is the concept of virtual
    switching for a per-host concept and spanned-across-hosts concept, as well as
    some more advanced topics. These topics include single-root input/output virtualization,
    which allows for a much more direct approach to hardware for certain scenarios.
    We will come back to some of the networking concepts later in this book as we
    start discussing cloud overlay networks. This is because the basic networking
    concepts aren't scalable enough for large cloud environments.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding physical and virtual networking
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using TAP/TUN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing Linux bridging
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuring Open vSwitch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding and configuring SR-IOV
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding macvtap
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's get started!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding physical and virtual networking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's think about networking for a second. This is a subject that most system
    administrators nowadays understand pretty well. This might not up to the level
    many of us think we do, but still – if we were to try to find an area of system
    administration where we'd find the biggest common level of knowledge, it would
    be networking.
  prefs: []
  type: TYPE_NORMAL
- en: So, what's the problem with that?
  prefs: []
  type: TYPE_NORMAL
- en: 'Actually, nothing much. If we really understand physical networking, virtual
    networking is going to be a piece of cake for us. Spoiler alert: *it''s the same
    thing*. If we don''t, it''s going to be exposed rather quickly, because there''s
    no way around it. And the problems are going to get bigger and bigger as time
    goes by because environments evolve and – usually – grow. The bigger they are,
    the more problems they''re going to create, and the more time you''re going to
    spend in debugging mode.'
  prefs: []
  type: TYPE_NORMAL
- en: That being said, if you have a firm grasp of VMware or Microsoft-based virtual
    networking purely at a technological level, you're in the clear here as all of
    these concepts are very similar.
  prefs: []
  type: TYPE_NORMAL
- en: With that out of the way, what's the whole hoopla about virtual networking?
    It's actually about understanding where things happen, how, and why. This is because,
    physically speaking, virtual networking is literally the same as physical networking.
    Logically speaking, there are some differences that relate more to the *topology*
    of things than to the principle or engineering side of things. And that's what
    usually throws people off a little bit – the fact that there are some weird, software-based
    objects that do the same job as the physical objects that most of us have grown
    used to managing via our favorite CLI-based or GUI-based utilities.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s introduce the basic building block of virtualized networking
    – a virtual switch. A virtual switch is basically a software-based Layer 2 switch
    that you use to do two things:'
  prefs: []
  type: TYPE_NORMAL
- en: Hook up your virtual machines to it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use its uplinks to connect them to physical server cards so that you can hook
    these physical network cards to a physical switch.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, let's deal with why we need these virtual switches from the virtual machine
    perspective. As we mentioned earlier, we use a virtual switch to connect virtual
    machines to it. Why? Well, if we didn't have some kind of software object that
    sits in-between our physical network card and our virtual machine, we'd have a
    big problem – we could only connect virtual machines for which we have physical
    network ports to our physical network, and that would be intolerable. First, it
    goes against some of the basic principles of virtualization, such as efficiency
    and consolidation, and secondly, it would cost a lot. Imagine having 20 virtual
    machines on your server. This means that, without a virtual switch, you'd have
    to have at least 20 physical network ports to connect to the physical network.
    On top of that, you'd actually use 20 physical ports on your physical switch as
    well, which would be a disaster.
  prefs: []
  type: TYPE_NORMAL
- en: So, by introducing a virtual switch between a virtual machine and a physical
    network port, we're solving two problems at the same time – we're reducing the
    number of physical network adapters that we need per server, and we're reducing
    the number of physical switch ports that we need to use to connect our virtual
    machines to the network. We can actually argue that we're solving a third problem
    as well – efficiency – as there are many scenarios where one physical network
    card can handle being an uplink for 20 virtual machines connected to a virtual
    switch. Specifically, there are large parts of our environments that don't consume
    a lot of network traffic and for those scenarios, virtual networking is just amazingly
    efficient.
  prefs: []
  type: TYPE_NORMAL
- en: Virtual networking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, in order for that virtual switch to be able to connect to something on
    a virtual machine, we have to have an object to connect to – and that object is
    called a virtual network interface card, often referred to as a vNIC. Every time
    you configure a virtual machine with a virtual network card, you're giving it
    the ability to connect to a virtual switch that uses a physical network card as
    an uplink to a physical switch.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, there are some potential drawbacks to this approach. For example,
    if you have 50 virtual machines connected to the same virtual switch that uses
    the same physical network card as an uplink and that uplink fails (due to a network
    card issue, cable issue, switch port issue, or switch issue), your 50 virtual
    machines won't have access to the physical network. How do we get around this
    problem? By implementing a better design and following the basic design principles
    that we'd use on a physical network as well. Specifically, we'd use more than
    one physical uplink to the same virtual switch.
  prefs: []
  type: TYPE_NORMAL
- en: 'Linux has *a lot* of different types of networking interfaces, something like
    20 different types, some of which are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Bridge**: Layer 2 interface for (virtual machine) networking.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bond**: For combining network interfaces to a single interface (for balancing
    and failover reasons) into one logical interface.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Team**: Different to bonding, teaming doesn''t create one logical interface,
    but can still do balancing and failover.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MACVLAN**: Creates multiple MAC addresses on a single physical interface
    (creates subinterfaces) on Layer 2.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**IPVLAN**: Unlike MACVLAN, IPVLAN uses the same MAC address and multiplexes
    on Layer 3.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MACVTAP/IPVTAP**: Newer drivers that should simplify virtual networking by
    combining TUN, TAP, and bridge as a single module.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**VXLAN**: A commonly used cloud overlay network concept that we will describe
    in detail in [*Chapter 12*](B14834_12_Final_ASB_ePub.xhtml#_idTextAnchor209),
    *Scaling Out KVM with OpenStack*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**VETH**: A virtual Ethernet interface that can be used in a variety of ways
    for local tunneling.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**IPOIB**: IP over Infiniband. As Infiniband gains traction in HPC/low latency
    networks, this type of networking is also supported by the Linux kernel.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are a whole host of others. Then, on top of these network interface types,
    there are some 10 types of tunneling interfaces, some of which are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**GRETAP**, **GRE**: Generic Routing Encapsulation protocols for encapsulating
    Layer 2 and Layer 3 protocols, respectively.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**GENEVE**: A convergence protocol for cloud overlay networking that''s meant
    to fuse VXLAN, GRE, and others into one. This is why it''s supported in Open vSwitch,
    VMware NSX, and other products.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**IPIP**: IP over IP tunnel for connecting internal IPv4 subnets via a public
    network.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SIT**: Simple Internet Translation for interconnecting isolated IPv6 networks
    over IPv4.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ip6tnl**: IPv4/6 tunnel over IPv6 tunnel interface.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**IP6GRE**, **IP6GRETAP**, and others.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting your head around all of them is quite a complex and tedious process,
    so, in this book, we're only going to focus on the types of interfaces that are
    really important to us for virtualization and (later in this book) the cloud.
    This is why we will discuss VXLAN and GENEVE overlay networks in [*Chapter 12*](B14834_12_Final_ASB_ePub.xhtml#_idTextAnchor209),
    *Scaling Out KVM with OpenStack*, as we need to have a firm grip on **Software-Defined
    Networking** (**SDN**) as well.
  prefs: []
  type: TYPE_NORMAL
- en: So, specifically, as part of this chapter, we're going to cover TAP/TUN, bridging,
    Open vSwitch, and macvtap interfaces as these are fundamentally the most important
    networking concepts for KVM virtualization.
  prefs: []
  type: TYPE_NORMAL
- en: 'But before we dig deep into that, let''s explain a couple of basic virtual
    network concepts that apply to KVM/libvirt networking and other virtualization
    products (for example, VMware''s hosted virtualization products such as Workstation
    or Player use the same concept). When you start configuring libvirt networking,
    you can choose between three basic types: NAT, routed, and isolated. Let''s discuss
    what these networking modes do.'
  prefs: []
  type: TYPE_NORMAL
- en: Libvirt NAT network
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In a `192.168.0.0/24` or something like that) for all the devices that we want
    to connect to the internet.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's convert that into a virtualized network example. In our virtual machine
    scenario, this means that our virtual machine can communicate with anything that's
    connected to the physical network via host's IP address, but not the other way
    around. For something to communicate to our virtual machine behind a NAT'd switch,
    our virtual machine has to initiate that communication (or we have to set up some
    kind of port forwarding, but that's beside the point).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram might explain what we''re talking about a bit better:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.1 – libvirt networking in NAT mode'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_04_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.1 – libvirt networking in NAT mode
  prefs: []
  type: TYPE_NORMAL
- en: From the virtual machine perspective, it's happily sitting in a completely separate
    network segment (hence the `192.168.122.210` and `220` IP addresses) and using
    a virtual network switch as its gateway to access external networks. It doesn't
    have to be concerned with any kind of additional routing as that's one of the
    reasons why we use NAT – to simplify endpoint routing.
  prefs: []
  type: TYPE_NORMAL
- en: Libvirt routed network
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The second network type is a routed network, which basically means that our
    virtual machine is directly connected to the physical network via a virtual switch.
    This means that our virtual machine is in the same Layer 2/3 network as the physical
    host. This type of network connection is used very often as, oftentimes, there
    is no need to have a separate NAT network to access your virtual machines in your
    environments. In a way, it just makes everything more complicated, especially
    because you have to configure routing to be aware of the NAT network that you''re
    using for your virtual machines. When using routed mode, the virtual machine sits
    *in the same* network segment as the next physical device. The following diagram
    tells a thousand words about routed networks:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.2 – libvirt networking in routed mode'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_04_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.2 – libvirt networking in routed mode
  prefs: []
  type: TYPE_NORMAL
- en: Now that we've covered the two most commonly used types of virtual machine networking
    scenarios, it's time for the third one, which will seem a bit obscure. If we configure
    a virtual switch without any *uplinks* (which means it has no physical network
    cards attached to it), then that virtual switch can't send traffic to the physical
    network at all. All that's left is communication within the limits of that switch
    itself, hence the name *isolated*. Let's create that elusive isolated network
    now.
  prefs: []
  type: TYPE_NORMAL
- en: Libvirt isolated network
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this scenario, virtual machines attached to the same isolated switch can
    communicate with each other, but they cannot communicate with anything outside
    the host that they're running on. We used the word *obscure* to describe this
    scenario earlier, but it really isn't – in some ways, it's actually an ideal way
    of *isolating* specific types of traffic so that it doesn't even get to the physical
    network.
  prefs: []
  type: TYPE_NORMAL
- en: 'Think of it this way – let''s say that you have a virtual machine that hosts
    a web server, for example, running WordPress. You create two virtual switches:
    one running routed network (direct connection to the physical network) and another
    that''s isolated. Then, you can configure your WordPress virtual machine with
    two virtual network cards, with the first one connected to the routed virtual
    switch and the second one connected to the isolated virtual switch. WordPress
    needs a database, so you create another virtual machine and configure it to use
    an internal virtual switch only. Then, you use that isolated virtual switch to
    *isolate* traffic between the web server and the database server so that WordPress
    connects to the database server via that switch. What did you get by configuring
    your virtual machine infrastructure like this? You have a two-tier application,
    and the most important part of that web application (database) is inaccessible
    from the outside world. Doesn''t seem like that bad of an idea, right?'
  prefs: []
  type: TYPE_NORMAL
- en: Isolated virtual networks are used in many other security-related scenarios,
    but this is just an example scenario that we can easily identify with.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s describe our isolated network with a diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.3 – libvirt networking in isolated mode'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_04_03.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.3 – libvirt networking in isolated mode
  prefs: []
  type: TYPE_NORMAL
- en: The previous chapter ([*Chapter 3*](B14834_03_Final_ASB_ePub.xhtml#_idTextAnchor049),
    *Installing KVM Hypervisor, libvirt, and ovirt*) of this book mentioned the *default*
    network, and we said that we're going to talk about that a bit later. This seems
    like an opportune moment to do so because now, we have more than enough information
    to describe what the default network configuration is.
  prefs: []
  type: TYPE_NORMAL
- en: When we install all the necessary KVM libraries and utilities like we did in
    [*Chapter 3*](B14834_03_Final_ASB_ePub.xhtml#_idTextAnchor049), *Installing KVM
    Hypervisor, libvirt, and oVirt*, a default virtual switch gets configured out
    of the box. The reason for this is simple – it's more user-friendly to pre-configure
    something so that users can just start creating virtual machines and connecting
    them to the default network than expect users to configure that as well. VMware's
    vSphere hypervisor does the same thing (the default switch is called vSwitch0),
    and Hyper-V asks us during the deployment process to configure the first virtual
    switch (which we can actually skip and configure later). So, this is just a well-known,
    standardized, established scenario that enables us to start creating our virtual
    machines faster.
  prefs: []
  type: TYPE_NORMAL
- en: The default virtual switch works in NAT mode with the DHCP server active, and
    again, there's a simple reason for that – guest operating systems are, by default
    pre-configured with DHCP networking configuration, which means that the virtual
    machine that we just created is going to poll the network for necessary IP configuration.
    This way, the VM gets all the necessary network configuration and we can start
    using it right away.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows what the default KVM network does:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.4 – libvirt default network in NAT mode'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_04_04.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.4 – libvirt default network in NAT mode
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s learn how to configure these types of virtual networking concepts
    from the shell and from the GUI. We will treat this procedure as a procedure that
    needs to be done sequentially:'
  prefs: []
  type: TYPE_NORMAL
- en: Let's start by exporting the default network configuration to XML so that we
    can use it as a template to create a new network:![Figure 4.5 – Exporting the
    default virtual network configuration
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B14834_04_05.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.5 – Exporting the default virtual network configuration
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s copy that file to a new file called `packtnat.xml`, edit it, and
    then use it to create a new NAT virtual network. Before we do that, however, we
    need to generate two things – a new object UUID (for our new network) and a unique
    MAC address. A new UUID can be generated from the shell by using the `uuidgen`
    command, but generating a MAC address is a bit trickier. So, we can use the standard
    Red Hat-proposed method available on the Red Hat website: [https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/virtualization_administration_guide/sect-virtualization-tips_and_tricks-generating_a_new_unique_mac_address](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/virtualization_administration_guide/sect-virtualization-tips_and_tricks-generating_a_new_unique_mac_address).
    By using the first snippet of code available at that URL, create a new MAC address
    (for example, `00:16:3e:27:21:c1`).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'By using `yum` command, install python2:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now use the `virsh` command to import that configuration and create
    our new virtual network, start that network and make it available permanently,
    and check if everything loaded correctly:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Given that we didn''t delete our default virtual network, the last command
    should give us the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.7 – Using virsh net-list to check which virtual networks we have
    on the KVM host'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_04_07.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.7 – Using virsh net-list to check which virtual networks we have on
    the KVM host
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s create two more virtual networks – a bridged network and an isolated
    network. Again, let''s use files as templates to create both of these networks.
    Keep in mind that, in order to be able to create a bridged network, we are going
    to need a physical network adapter, so we need to have an available physical adapter
    in the server for that purpose. On our server, that interface is called `ens224`,
    while the interface called `ens192` is being used by the default libvirt network.
    So, let''s create two configuration files called `packtro.xml` (for our routed
    network) and `packtiso.xml` (for our isolated network):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.8 – libvirt routed network definition'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_04_08.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.8 – libvirt routed network definition
  prefs: []
  type: TYPE_NORMAL
- en: 'In this specific configuration, we''re using `ens224` as an uplink to the routed
    virtual network, which would use the same subnet (`192.168.2.0/24`) as the physical
    network that `ens224` is connected to:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.9 – libvirt isolated network definition'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_04_09.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.9 – libvirt isolated network definition
  prefs: []
  type: TYPE_NORMAL
- en: Just to cover our bases, we could have easily configured all of this by using
    the Virtual Machine Manager GUI, as that application has a wizard for creating
    virtual networks as well. But when we're talking about larger environments, importing
    XML is a much simpler process, even when we forget about the fact that a lot of
    KVM virtualization hosts don't have a GUI installed at all.
  prefs: []
  type: TYPE_NORMAL
- en: 'So far, we''ve discussed virtual networking from an overall host-level. However,
    there''s also a different approach to the subject – using a virtual machine as
    an object to which we can add a virtual network card and connect it to a virtual
    network. We can use `virsh` for that purpose. So, just as an example, we can connect
    our virtual machine called `MasteringKVM01` to an isolated virtual network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: There are other concepts that allow virtual machine connectivity to a physical
    network, and some of them we will discuss later in this chapter (such as SR-IOV).
    However, now that we've covered the basic approaches to connecting virtual machines
    to a physical network by using a virtual switch/bridge, we need to get a bit more
    technical. The thing is, there are more concepts involved in connecting a virtual
    machine to a virtual switch, such as TAP and TUN, which we will be covering in
    the following section.
  prefs: []
  type: TYPE_NORMAL
- en: Using userspace networking with TAP and TUN devices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In [*Chapter 1*](B14834_01_Final_ASB_ePub.xhtml#_idTextAnchor016), *Understanding
    Linux Virtualization*, we used the `virt-host-validate` command to do some pre-flight
    checks in terms of the host''s preparedness for KVM virtualization. As a part
    of that process, some of the checks include checking if the following devices
    exist:'
  prefs: []
  type: TYPE_NORMAL
- en: '`/dev/kvm`: The KVM drivers create a `/dev/kvm` character device on the host
    to facilitate direct hardware access for virtual machines. Not having this device
    means that the VMs won''t be able to access physical hardware, although it''s
    enabled in the BIOS and this will reduce the VM''s performance significantly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`/dev/vhost-net`: The `/dev/vhost-net` character device will be created on
    the host. This device serves as the interface for configuring the `vhost-net`
    instance. Not having this device significantly reduces the virtual machine''s
    network performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`/dev/net/tun`: This is another character special device used for creating
    TUN/TAP devices to facilitate network connectivity for a virtual machine. The
    TUN/TAP device will be explained in detail in future chapters. For now, just understand
    that having a character device is important for KVM virtualization to work properly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's focus on the last device, the TUN device, which is usually accompanied
    by a TAP device.
  prefs: []
  type: TYPE_NORMAL
- en: So far, all the concepts that we've covered include some kind of connectivity
    to a physical network card, with isolated virtual networks being an exception.
    But even an isolated virtual network is just a virtual network for our virtual
    machines. What happens when we have a situation where we need our communication
    to happen in the user space, such as between applications running on a server?
    It would be useless to patch them through some kind of virtual switch concept,
    or a regular bridge, as that would just bring additional overhead. This is where
    TUN/TAP devices come in, providing packet flow for user space programs. Easily
    enough, an application can open `/dev/net/tun` and use an `ioctl()` function to
    register a network device in the kernel, which, in turn, presents itself as a
    tunXX or tapXX device. When the application closes the file, the network devices
    and routes created by it disappear (as described in the kernel `tuntap.txt` documentation).
    So, it's just a type of virtual network interface for the Linux operating system
    supported by the Linux kernel – you can add an IP address and routes to it so
    that traffic from your application can route through it, and not via a regular
    network device.
  prefs: []
  type: TYPE_NORMAL
- en: TUN emulates an L3 device by creating a communication tunnel, something like
    a point-to-point tunnel. It gets activated when the tuntap driver gets configured
    in tun mode. When you activate it, any data that you receive from a descriptor
    (the application that configured it) will be data in the form of regular IP packages
    (as the most commonly used case). Also, when you send data, it gets written to
    the TUN device as regular IP packages. This type of interface is sometimes used
    in testing, development, and debugging for simulation purposes.
  prefs: []
  type: TYPE_NORMAL
- en: The TAP interface basically emulates an L2 Ethernet device. It gets activated
    when the tuntap driver gets configured in tap mode. When you activate it, unlike
    what happens with the TUN interface (Layer 3), you get Layer 2 raw Ethernet packages,
    including ARP/RARP packages and everything else. Basically, we're talking about
    a virtualized Layer 2 Ethernet connection.
  prefs: []
  type: TYPE_NORMAL
- en: These concepts (especially TAP) are usable on libvirt/QEMU as well because by
    using these types of configurations, we can create connections from the host to
    a virtual machine – without the libvirt bridge/switch, just as an example. We
    can actually configure all of the necessary details for the TUN/TAP interface
    and then start deploying virtual machines that are hooked up directly to those
    interfaces by using `kvm-qemu` options. So, it's a rather interesting concept
    that has its place in the virtualization world as well. This is especially interesting
    when we start creating Linux bridges.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing Linux bridging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s create a bridge and then add a TAP device to it. Before we do that,
    we must make sure the bridge module is loaded into the kernel. Let''s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'If it is not loaded, use `modprobe bridge` to load the module:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: brctl show command will list all the available bridges on the server, along
    with some basic information, such as the ID of the bridge, Spanning Tree Protocol
    (STP) status, and the interfaces attached to it. Here, the tester bridge does
    not have any interfaces attached to its virtual ports.
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'A Linux bridge will also be shown as a network device. To see the network details
    of the bridge tester, use the `ip` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: ifconfig tester
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'tester: flags=4098<BROADCAST,MULTICAST>mtu 1500'
  prefs: []
  type: TYPE_NORMAL
- en: ether26:84:f2:f8:09:e0txqueuelen 1000 (Ethernet)
  prefs: []
  type: TYPE_NORMAL
- en: RX packets 0 bytes 0 (0.0 B)
  prefs: []
  type: TYPE_NORMAL
- en: RX errors 0 dropped 0 overruns 0 frame 0
  prefs: []
  type: TYPE_NORMAL
- en: TX packets 0 bytes 0 (0.0 B)
  prefs: []
  type: TYPE_NORMAL
- en: TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'First, check if the TUN/TAP device module is loaded into the kernel. If not,
    you already know the drill:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'tester and a tap device named vm-vnic. Let''s add vm-vnic to tester:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, you can see that `vm-vnic` is an interface that was added to the `tester`
    bridge. Now, `vm-vnic` can act as the interface between your virtual machine and
    the `tester` bridge, which, in turn, enables the virtual machine to communicate
    with other virtual machines that are added to this bridge:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.10 – Virtual machines connected to a virtual switch (bridge)'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_04_10.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.10 – Virtual machines connected to a virtual switch (bridge)
  prefs: []
  type: TYPE_NORMAL
- en: 'You might also need to remove all the objects and configurations that were
    created in the previous procedure. Let''s do this step by step via the command
    line:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to remove the `vm-vnic` tap device from the `tester` bridge:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: ip tuntap del dev vm-vnic mode tap
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, remove the tester bridge:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: These are the same steps that libvirt carried out in the backend while enabling
    or disabling networking for a virtual machine. We want you to understand this
    procedure thoroughly before moving ahead. Now that we've covered Linux bridging,
    it's time to move on to a more advanced concept called Open vSwitch.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring Open vSwitch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Imagine for a second that you're working for a small company that has three
    to four KVM hosts, a couple of network-attached storage devices to host their
    15 virtual machines, and that you've been employed by the company from the very
    start. So, you've seen it all – the company buying some servers, network switches,
    cables, and storage devices, and you were a part of a small team of people that
    built that environment. After 2 years of that process, you're aware of the fact
    that everything works, it's simple to maintain, and doesn't give you an awful
    lot of grief.
  prefs: []
  type: TYPE_NORMAL
- en: Now, imagine the life of a friend of yours working for a bigger enterprise company
    that has 400 KVM hosts and close to 2,000 virtual machines to manage, doing the
    same job as you're doing in a comfy chair of your office in your small company.
  prefs: []
  type: TYPE_NORMAL
- en: Do you think that your friend can manage his or her environment by using the
    very same tools that you're using? XML files for network switch configuration,
    deploying servers from a bootable USB drive, manually configuring everything,
    and having the time to do so? Does that seem like a possibility to you?
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two basic problems in this second situation:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The scale of the environment: This one is more obvious. Because of the environment
    size, you need some kind of concept that''s going to be managed centrally, instead
    of on a host-per-host level, such as the virtual switches we''ve discussed so
    far.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Company policies: These usually dictate some kind of compliance that comes
    from configuration standardization as much as possible. Now, we can agree that
    we could script some configuration updates via Ansible, Puppet, or something like
    that, but what''s the use? We''re going to have to create new config files, new
    procedures, and new workbooks every single time we need to introduce a change
    to KVM networking. And big companies frown upon that.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, what we need is a centralized networking object that can span across multiple
    hosts and offer configuration consistency. In this context, configuration consistency
    offers us a huge advantage – every change that we introduce in this type of object
    will be replicated to all the hosts that are members of this centralized networking
    object. In other words, what we need is **Open vSwitch** (**OVS**). For those
    who are more versed in VMware-based networking, we can use an approximate metaphor
    – Open vSwitch is for KVM-based environments similar to what vSphere Distributed
    Switch is for VMware-based environments.
  prefs: []
  type: TYPE_NORMAL
- en: 'In terms of technology, OVS supports the following:'
  prefs: []
  type: TYPE_NORMAL
- en: VLAN isolation (IEEE 802.1Q)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Traffic filtering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NIC bonding with or without LACP
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Various overlay networks – VXLAN, GENEVE, GRE, STT, and so on
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 802.1ag support
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Netflow, sFlow, and so on
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (R)SPAN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenFlow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OVSDB
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Traffic queuing and shaping
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linux, FreeBSD, NetBSD, Windows, and Citrix support (and a host of others)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we've listed some of the supported technologies, let's discuss the
    way in which Open vSwitch works.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s talk about the Open vSwitch architecture. The implementation
    of Open vSwitch is broken down into two parts: the Open vSwitch kernel module
    (the data plane) and the user space tools (the control pane). Since the incoming
    data packets must be processed as fast as possible, the data plane of Open vSwitch
    was pushed to the kernel space:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.11 – Open vSwitch architecture'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_04_11.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.11 – Open vSwitch architecture
  prefs: []
  type: TYPE_NORMAL
- en: The data path (OVS kernel module) uses the netlink socket to interact with the
    vswitchd daemon, which implements and manages any number of OVS switches on the
    local system.
  prefs: []
  type: TYPE_NORMAL
- en: Open vSwitch doesn't have a specific SDN controller that it uses for management
    purposes, in a similar fashion to VMware's vSphere distributed switch and NSX,
    which have vCenter and various NSX components to manage their capabilities. In
    OVS, the point is to use someone else's SDN controller, which then interacts with
    ovs-vswitchd using the OpenFlow protocol. The ovsdb-server maintains the switch
    table database and external clients can talk to the ovsdb-server using JSON-RPC;
    JSON is the data format. The ovsdb database currently contains around 13 tables
    and this database is persistent across restarts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Open vSwitch works in two modes: normal and flow mode. This chapter will primarily
    concentrate on how to bring up a KVM VM connected to Open vSwitch''s bridge in
    standalone/normal mode and will a give brief introduction to flow mode using the
    OpenDaylight controller:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Normal Mode**: Switching and forwarding are handled by OVS bridge. In this
    modem OVS acts as an L2 learning switch. This mode is specifically useful when
    configuring several overlay networks for your target rather than manipulating
    the switch''s flow.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ctl` command. This mode allows a greater level of abstraction and automation;
    the SDN controller exposes the REST API. Our applications can make use of this
    API to directly manipulate the bridge''s flows to meet network needs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s move on to the practical aspect and learn how to install Open vSwitch
    on CentOS 8:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing that we must do is tell our system to use the appropriate repositories.
    In this case, we need to enable the repositories called `epel` and `centos-release-openstack-train`.
    We can do that by using a couple of `yum` commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step will be installing `openvswitch` from Red Hat''s repository:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'After the installation process, we need to check if everything is working by
    starting and enabling the Open vSwitch service and running the `ovs-vsctl -V`
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we''ve successfully installed and started Open vSwitch, it''s time
    to configure it. Let''s choose a deployment scenario in which we''re going to
    use Open vSwitch as a new virtual switch for our virtual machines. In our server,
    we have another physical interface called `ens256`, which we''re going to use
    as an uplink for our Open vSwitch virtual switch. We''re also going to clear ens256
    configuration, configure an IP address for our OVS, and start the OVS by using
    the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that everything has been configured but not persistently, we need to make
    the configuration persistent. This means configuring some network interface configuration
    files. So, go to `/etc/sysconfig/network-scripts` and create two files. Call one
    of them `ifcfg-ens256` (for our uplink interface):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: DEVICE=ovs-br0
  prefs: []
  type: TYPE_NORMAL
- en: DEVICETYPE=ovs
  prefs: []
  type: TYPE_NORMAL
- en: TYPE=OVSBridge
  prefs: []
  type: TYPE_NORMAL
- en: BOOTPROTO=static
  prefs: []
  type: TYPE_NORMAL
- en: IPADDR=10.10.10.1
  prefs: []
  type: TYPE_NORMAL
- en: NETMASK=255.255.255.0
  prefs: []
  type: TYPE_NORMAL
- en: GATEWAY=10.10.10.254
  prefs: []
  type: TYPE_NORMAL
- en: ONBOOT=yes
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'We didn''t configure all of this just for show, so we need to make sure that
    our KVM virtual machines are also able to use it. This means – again – that we
    need to create a KVM virtual network that''s going to use OVS. Luckily, we''ve
    dealt with KVM virtual network XML files before (check the *Libvirt isolated network*
    section), so this one isn''t going to be a problem. Let''s call our network `packtovs`
    and its corresponding XML file `packtovs.xml`. It should contain the following
    content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'So, now, we can perform our usual operations when we have a virtual network
    definition in an XML file, which is to define, start, and autostart the network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'If we left everything as it was when we created our virtual networks, the output
    from `virsh net-list` should look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.12 – Successful OVS configuration, and OVS+KVM configuration'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_04_12.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.12 – Successful OVS configuration, and OVS+KVM configuration
  prefs: []
  type: TYPE_NORMAL
- en: 'So, all that''s left now is to hook up a VM to our newly defined OVS-based
    network called `packtovs` and we''re home free. Alternatively, we could just create
    a new one and pre-connect it to that specific interface using the knowledge we
    gained in [*Chapter 3*](B14834_03_Final_ASB_ePub.xhtml#_idTextAnchor049), *Installing
    KVM Hypervisor, libvirt, and oVirt*. So, let''s issue the following command, which
    has just two changed parameters (`--name` and `--network`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'After the virtual machine installation completes, we''re connected to the OVS-based
    `packtovs` virtual network, and our virtual machine can use it. Let''s say that
    additional configuration is needed and that we got a request to tag traffic coming
    from this virtual machine with `VLAN ID 5`. Start your virtual machine and use
    the following set of commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'This command tells us that we''re using the `ens256` port as an uplink and
    that our virtual machine, `MasteringKVM03`, is using the virtual `vnet0` network
    port. We can apply VLAN tagging to that port by using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to take note of some additional commands related to OVS administration
    and management since this is done via the CLI. So, here are some commonly used
    OVS CLI administration commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '`#ovs-vsctl show`: A very handy and frequently used command. It tells us what
    the current running configuration of the switch is.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`#ovs-vsctl list-br`: Lists bridges that were configured on Open vSwitch.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`#ovs-vsctl list-ports <bridge>`: Shows the names of all the ports on `BRIDGE`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`#ovs-vsctl list interface <bridge>`: Shows the names of all the interfaces
    on `BRIDGE`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`#ovs-vsctl add-br <bridge>`: Creates a bridge in the switch database.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`#ovs-vsctl add-port <bridge> : <interface>`: Binds an interface (physical
    or virtual) to the Open vSwitch bridge.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`#ovs-ofctl and ovs-dpctl`: These two commands are used for administering and
    monitoring flow entries. You learned that OVS manages two kinds of flows: OpenFlows
    and Datapath. The first is managed in the control plane, while the second one
    is a kernel-based flow.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`#ovs-ofctl`: This speaks to the OpenFlow module, whereas `ovs-dpctl` speaks
    to the Kernel module.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following examples are the most used options for each of these commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '`#ovs-ofctl show <BRIDGE>`: Shows brief information about the switch, including
    the port number to port name mapping.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`#ovs-ofctl dump-flows <Bridge>`: Examines OpenFlow tables.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`#ovs-dpctl show`: Prints basic information about all the logical datapaths,
    referred to as *bridges*, present on the switch.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`#ovs-dpctl dump-flows`: It shows the flow cached in datapath.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ovs-appctl`: This command offers a way to send commands to a running Open
    vSwitch and gathers information that is not directly exposed to the `ovs-ofctl`
    command. This is the Swiss Army knife of OpenFlow troubleshooting.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`#ovs-appctl bridge/dumpflows <br>`: Examines flow tables and offers direct
    connectivity for VMs on the same hosts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`#ovs-appctl fdb/show <br>`: Lists MAC/VLAN pairs learned.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Also, you can always use the `ovs-vsctl show` command to get information about
    the configuration of your OVS switch:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.13 – ovs-vsctl show output'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_04_13.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.13 – ovs-vsctl show output
  prefs: []
  type: TYPE_NORMAL
- en: We are going to come back to the subject of Open vSwitch in [*Chapter 12*](B14834_12_Final_ASB_ePub.xhtml#_idTextAnchor209),
    *Scaling Out KVM with OpenStack* , as we go deeper into our discussion about spanning
    Open vSwitch across multiple hosts, especially while keeping in mind the fact
    that we want to be able to span our cloud overlay networks (based on GENEVE, VXLAN,
    GRE, or similar protocols) across multiple hosts and sites.
  prefs: []
  type: TYPE_NORMAL
- en: Other Open vSwitch use cases
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As you might imagine, Open vSwitch isn't just a handy concept for libvirt or
    OpenStack – it can be used for a variety of other scenarios as well. Let's describe
    one of them as it might be important for people looking into VMware NSX or NSX-T
    integration.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s just describe a few basic terms and relationships here. VMware''s NSX
    is an SDN-based technology that can be used for a variety of use cases:'
  prefs: []
  type: TYPE_NORMAL
- en: Connecting data centers and extending cloud overlay networks across data center
    boundaries.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A variety of disaster recover scenarios. NSX can be a big help for disaster
    recover, for multi-site environments, and for integration with a variety of external
    services and devices that can be a part of the scenario (Palo Alto PANs).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consistent micro-segmentation, across sites, done *the right way* on the virtual
    machine network card level.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For security purposes, varying from different types of supported VPN technologies
    to connect sites and end users, to distributed firewalls, guest introspection
    options (antivirus and anti-malware), network introspection options (IDS/IPS),
    and more.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For load balancing, up to Layer 7, with SSL offload, session persistence, high
    availablity, application rules, and more.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yes, VMware's take on SDN (NSX) and Open vSwitch seem like *competing technologies*
    on the market, but realistically, there are loads of clients who want to use both.
    This is where VMware's integration with OpenStack and NSX's integration with Linux-based
    KVM hosts (by using Open vSwitch and additional agents) comes in really handy.
    Just to further explain these points – there are things that NSX does that take
    *extensive* usage of Open vSwitch-based technologies – hardware VTEP integration
    via Open vSwitch Database, extending GENEVE networks to KVM hosts by using Open
    vSwitch/NSX integration, and much more.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine that you're working for a service provider – a cloud service provider,
    an ISP; basically, any type of company that has large networks with a lot of network
    segmentation. There are loads of service providers using VMware's vCloud Director
    to provide cloud services to end users and companies. However, because of market
    needs, these environments often need to be extended to include AWS (for additional
    infrastructure growth scenarios via the public cloud) or OpenStack (to create
    hybrid cloud scenarios). If we didn't have a possibility to have interoperability
    between these solutions, there would be no way to use both of these offerings
    at the same time. But from a networking perspective, the network background for
    that is NSX or NSX-T (which actually *uses* Open vSwitch).
  prefs: []
  type: TYPE_NORMAL
- en: It's been clear for years that the future is all about multi-cloud environments,
    and these types of integrations will bring in more customers; they will want to
    take advantage of these options in their cloud service design. Future developments
    will also most probably include (and already partially include) integration with
    Docker, Kubernetes, and/or OpenShift to be able to manage containers in the same
    environment.
  prefs: []
  type: TYPE_NORMAL
- en: There are also some more extreme examples of using hardware – in our example,
    we are talking about network cards on a PCI Express bus – in a *partitioned* way.
    For the time being, our explanation of this concept, called SR-IOV, is going to
    be limited to network cards, but we will expand on the same concept in [*Chapter
    6*](B14834_06_Final_ASB_ePub.xhtml#_idTextAnchor108), *Virtual Display Devices
    and Protocols*, when we start talking about partitioning GPUs for use in virtual
    machines. So, let's discuss a practical example of using SR-IOV on an Intel network
    card that supports it.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding and using SR-IOV
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The SR-IOV concept is something that we already mentioned in [*Chapter 2*](B14834_02_Final_ASB_ePub.xhtml#_idTextAnchor029),
    *KVM as a Virtualization Solution*. By utilizing SR-IOV, we can *partition* PCI
    resources (for example, network cards) into virtual PCI functions and inject them
    into a virtual machine. If we''re using this concept for network cards, we''re
    usually doing this with a single purpose – so that we can avoid using the operating
    system kernel and network stack while accessing a network interface card from
    our virtual machine. In order for us to be able to do this, we need to have hardware
    support, so we need to check if our network card actually supports it. On a physical
    server, we could use the `lspci` command to extract attribute information about
    our PCI devices and then `grep` out *Single Root I/O Virtualization* as a string
    to try to see if we have a device that''s compatible. Here''s an example from
    our server:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.14 – Checking if our system is SR-IOV compatible'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_04_14.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.14 – Checking if our system is SR-IOV compatible
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: 'Be careful when configuring SR-IOV. You need to have a server that supports
    it, a device that supports it, and you must make sure that you turn on SR-IOV
    functionality in BIOS. Then, you need to keep in mind that there are servers that
    only have specific slots assigned for SR-IOV. The server that we used (HP Proliant
    DL380p G8) has three PCI-Express slots assigned to CPU1, but SR-IOV worked only
    in slot #1\. When we connected our card to slot #2 or #3, we got a BIOS message
    that SR-IOV will not work in that slot and that we should move our card to a slot
    that supports SR-IOV. So, please, make sure that you read the documentation of
    your server thoroughly and connect a SR-IOV compatible device to a correct PCI-Express
    slot.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this specific case, it''s an Intel 10 Gigabit network adapter with two ports,
    which we could use to do SR-IOV. The procedure isn''t all that difficult, and
    it requires us to complete the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Unbind from the previous module.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Register it to the vfio-pci module, which is available in the Linux kernel stack.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Configure a guest that's going to use it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'So, what you would do is unload the module that the network card is currently
    using by using `modprobe -r`. Then, you would load it again, but by assigning
    an additional parameter. On our specific server, the Intel dual-port adapter that
    we''re using (X540-AT2) was assigned to the `ens1f0` and `ens1f1` network devices.
    So, let''s use `ens1f0` as an example for SR-IOV configuration at boot time:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing that we need to do (as a general concept) is find out which
    kernel module our network card is using. To do that, we need to issue the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'ixgbe module here, and we can do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we can use the `modprobe` system to make these changes permanent across
    reboots by creating a file in `/etc/modprobe.d` called (for example) `ixgbe.conf`
    and adding the following line to it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'This would give us up to four virtual functions that we can use inside our
    virtual machines. Now, the next issue that we need to solve is how to boot our
    server with SR-IOV active at boot time. There are quite a few steps involved here,
    so, let''s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: We need to add the `iommu` and `vfs` parameters to the default kernel boot line
    and the default kernel configuration. So, first, open `/etc/default/grub` and
    edit the `GRUB_CMDLINE_LINUX` line and add `intel_iommu=on` (or `amd_iommu=on`
    if you're using an AMD system) and `ixgbe.max_vfs=4` to it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We need to reconfigure `grub` to use this change, so we need to use the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Sometimes, even that isn''t enough, so we need to configure the necessary kernel
    parameters, such as the maximum number of virtual functions and the `iommu` parameter
    to be used on the server. That leads us to the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'After reboot, we should be able to see our virtual functions. Type in the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'We should get an output that looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.15 –  Checking for virtual function visibility'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_04_15.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.15 – Checking for virtual function visibility
  prefs: []
  type: TYPE_NORMAL
- en: 'We should be able to see these virtual functions from libvirt, and we can check
    that via the `virsh` command. Let''s try this (we''re using `grep 04` because
    our device IDs start with 04, which is visible from the preceding image; we''ll
    shrink the output to important entries only):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The first two devices are our physical functions. The remaining eight devices
    (two ports times four functions) are our virtual devices (from `pci_0000_04_10_0`
    to `pci_0000_04_10_7`). Now, let''s dump that device''s information by using the
    `virsh nodedev-dumpxml pci_0000_04_10_0` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.16 – Virtual function information from the perspective of virsh'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_04_16.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.16 – Virtual function information from the perspective of virsh
  prefs: []
  type: TYPE_NORMAL
- en: 'So, if we have a running virtual machine that we''d like to reconfigure to
    use this, we''d have to create an XML file with definition that looks something
    like this (let''s call it `packtsriov.xml`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Of course, the domain, bus, slot, and function need to point exactly to our
    VF. Then, we can use the `virsh` command to attach that device to our virtual
    machine (for example, `MasteringKVM03`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: When we use `virsh dumpxml`, we should now see a part of the output that starts
    with `<driver name='vfio'/>`, along with all the information that we configured
    in the previous step (address type, domain, bus, slot, function). Our virtual
    machine should have no problems using this virtual function as a network card.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, it''s time to cover another concept that''s very much useful in KVM networking:
    macvtap. It''s a newer driver that should simplify our virtualized networking
    by completely removing tun/tap and bridge drivers with a single module.'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding macvtap
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This module works like a combination of the tap and macvlan modules. We already
    explained what the tap module does. The macvlan module enables us to create virtual
    networks that are pinned to a physical network interface (usually, we call this
    interface a *lower* interface or device). Combining tap and macvlan enables us
    to choose between four different modes of operation, called **Virtual Ethernet
    Port Aggregator** (**VEPA**), bridge, private, and passthru.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we''re using the VEPA mode (default mode), the physical switch has to support
    VEPA by supporting `hairpin` mode (also called reflective relay). When a *lower*
    device receives data from a VEPA mode macvlan, this traffic is always sent out
    to the upstream device, which means that traffic is always going through an external
    switch. The advantage of this mode is the fact that network traffic between virtual
    machines becomes visible on the external network, which can be useful for a variety
    of reasons. You can check how network flow works in the following sequence of
    diagrams:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.17 – macvtap VEPA mode, where traffic is forced to the external
    network'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_04_17.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.17 – macvtap VEPA mode, where traffic is forced to the external network
  prefs: []
  type: TYPE_NORMAL
- en: 'In private mode, it''s similar to VEPA in that everything goes to an external
    switch, but unlike VEPA, traffic only gets delivered if it''s sent via an external
    router or switch. You can use this mode if you want to isolate virtual machines
    connected to the endpoints from one another, but not from the external network.
    If this sounds very much like a private VLAN scenario, you''re completely correct:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.18 – macvtap in private mode, using it for internal network isolation'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_04_18.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.18 – macvtap in private mode, using it for internal network isolation
  prefs: []
  type: TYPE_NORMAL
- en: 'In bridge mode, data received on your macvlan that''s supposed to go to another
    macvlan on the same lower device is sent directly to the target, not externally,
    and then routed back. This is very similar to what VMware NSX does when communication
    is supposed to happen between virtual machines on different VXLAN networks, but
    on the same host:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.19 – macvtap in bridge mode, providing a kind of internal routing'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_04_19.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.19 – macvtap in bridge mode, providing a kind of internal routing
  prefs: []
  type: TYPE_NORMAL
- en: 'In passthrough mode, we''re basically talking about the SR-IOV scenario, where
    we''re using a VF or a physical device directly to the macvtap interface. The
    key difference is that a single network interface can only be passed to a single
    guest (1:1 relationship):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.20 – macvtap in passthrough mode'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_04_20.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.20 – macvtap in passthrough mode
  prefs: []
  type: TYPE_NORMAL
- en: In [*Chapter 12*](B14834_12_Final_ASB_ePub.xhtml#_idTextAnchor209), *Scaling
    Out KVM with OpenStack* and [*Chapter 13*](B14834_13_Final_ASB_ePub.xhtml#_idTextAnchor238),
    *Scaling Out KVM with AWS,* we'll describe why virtualized and *overlay* networking
    (VXLAN, GRE, GENEVE) is even more important for cloud networking as we extend
    our local KVM-based environment to the cloud either via OpenStack or AWS.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered the basics of virtualized networking in KVM and
    explained why virtualized networking is such a huge part of virtualization. We
    went knee-deep into configuration files and their options as this will be the
    preferred method for administration in larger environments, especially when talking
    about virtualized networks.
  prefs: []
  type: TYPE_NORMAL
- en: Pay close attention to all the configuration steps that we discussed through
    this chapter, especially the part related to using virsh commands to manipulate
    network configuration and to configure Open vSwitch and SR-IOV. SR-IOV-based concepts
    are heavily used in latency-sensitive environments to provide networking services
    with the lowest possible overhead and latency, which is why this principle is
    very important for various enterprise environments related to the financial and
    banking sector.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we''ve covered all the necessary networking scenarios (some of which
    will be revisited later in this book), it''s time to start thinking about the
    next big topic of the virtualized world. We''ve already talked about CPU and memory,
    as well as networks, which means we''re left with the fourth pillar of virtualization:
    storage. We will tackle that subject in the next chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Why is it important that virtual switches accept connectivity from multiple
    virtual machines at the same time?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does a virtual switch work in NAT mode?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does a virtual switch work in routed mode?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is Open vSwitch and for what purpose can we use it in virtualized and cloud
    environments?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Describe the differences between TAP and TUN interfaces.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Please refer to the following links for more information regarding what was
    covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Libvirt networking: [https://wiki.libvirt.org/page/VirtualNetworking](https://wiki.libvirt.org/page/VirtualNetworking)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Network XML format: [https://libvirt.org/formatnetwork.html](https://libvirt.org/formatnetwork.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Open vSwitch: [https://www.openvswitch.org/](https://www.openvswitch.org/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Open vSwitch and libvirt: [http://docs.openvswitch.org/en/latest/howto/libvirt/](http://docs.openvswitch.org/en/latest/howto/libvirt/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Open vSwitch Cheat Sheet: [https://adhioutlined.github.io/virtual/Openvswitch-Cheat-Sheet/](https://adhioutlined.github.io/virtual/Openvswitch-Cheat-Sheet/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
