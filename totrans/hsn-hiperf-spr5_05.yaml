- en: Understanding Spring Database Interactions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In previous chapters, we learned about Spring core features, such as **dependency
    injection** (DI) and its configuration. We also saw how we can implement reusable
    code with the help of Spring **Aspect-Oriented Programming** (**AOP**). We learned
    how we can develop loosely coupled web applications with the help of Spring **Model-View-Controller**
    (**MVC**), and how we can optimize Spring MVC implementation to achieve better
    results using asynchronous features, multithreading, and authentication caching.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will learn about database interaction with Spring Framework.
    Database interaction is the biggest bottleneck in application performance. Spring
    Framework supports all major data access technologies, such as **Java Database
    Connectivity** (**JDBC**) directly, any **object-relational mapping** (**ORM)**
    framework (such as Hibernate), the **Java Persistence API** (**JPA**), and others.
    We can choose any of the data access technologies to persist our application data.
    Here, we will explore database interaction with Spring JDBC. We will also learn
    about common performance traps with Spring JDBC and the best practices of database
    design. We will then take a look at Spring transaction management and optimal
    connection pooling configuration.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Spring JDBC configuration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Database design for optimal performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transaction management
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Declarative ACID using `@Transactional`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimal isolation levels
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimal fetch size
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimal connection pooling configuration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tomcat JDBC connection pool versus HikariCP
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Database design best practices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spring JDBC configuration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Without using JDBC, we cannot connect to a database using Java only. JDBC will
    be involved, either directly or indirectly, in connecting a database. But there
    are problems faced by Java programmers if they are working directly with core
    JDBC. Let's see what those problems are.
  prefs: []
  type: TYPE_NORMAL
- en: Problems with core JDBC
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following illustrates the problems that we have to face when we work with
    the core JDBC API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding example, I have highlighted some code. Only the code in bold
    format is important; the rest is plumbing code. So, we have to write that redundant
    code every time, to perform a database operation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see some other problems with core JDBC:'
  prefs: []
  type: TYPE_NORMAL
- en: JDBC API exceptions are checked that forces the developers to handle errors,
    which increases the code, as well as the complexity, of the application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In JDBC, we have to close the database connection; if the developer forgets
    to close the connection, then we get some connection issues in our application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Solving problems with Spring JDBC
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To overcome the preceding problems with core JDBC, Spring Framework provides
    excellent database integration with the Spring JDBC module. Spring JDBC provides
    the `JdbcTemplate` class, which helps us to remove the plumbing code, and also
    helps the developer to concentrate only on the SQL query and parameters. We just
    need to configure the `JdbcTemplate` with a `dataSource` and write code like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'As we saw in the previous example, Spring provides a simplification for handling
    database access by using the JDBC template. The JDBC template uses core JDBC code
    internally and provides a new and efficient way to deal with the database. The
    Spring JDBC template has the following advantages, as compared to core JDBC:'
  prefs: []
  type: TYPE_NORMAL
- en: The JDBC template cleans up the resources automatically, by releasing database
    connections
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It converts the core JDBC `SQLException` into `RuntimeExceptions`, which provides
    a better error detection mechanism
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The JDBC template provides various methods to write the SQL queries directly, so
    it saves a lot of work and time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following diagram shows a high-level overview of the Spring JDBC template:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b5608665-5489-4ffc-b7cd-fef724395459.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The various approaches provided by the Spring JDBC for accessing the database
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`JdbcTemplate`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`NamedParameterJdbcTemplate`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`SimpleJdbcTemplate`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`SimpleJdbcInsert`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`SimpleJdbcCall`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spring JDBC dependencies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are the Spring JDBC dependencies available in the `pom.xml` file:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code is for the Spring JDBC dependency:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code is for the PostgreSQL dependency:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we have specified the dependency for Spring JDBC and
    PostgreSQL, respectively. The rest of the dependencies will be automatically resolved
    by Maven. Here, I am using the PostgreSQL database for our testing purposes, so
    I have added a PostgreSQL dependency. If you are using some other RDBMS, then
    you should make changes in the dependencies accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: Spring JDBC example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this example, we are using the PostgreSQL database. The table schema is
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We will use the DAO pattern for JDBC operations, so let''s create a Java bean
    that will model our `Account` table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The following `AccountDao` interface declares the operation that we want to
    implement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The Spring bean configuration class is as follows. For bean configuration,
    simply annotate a method with the `@Bean` annotation. When `JavaConfig` finds
    such a method, it will execute that method and register the return value as a
    bean within `BeanFactory`. Here, we have registered `JdbcTemplate`, `dataSource`,
    and `AccountDao` beans:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: In the previous configuration file, we created a `DataSource` object of the
    class `DriverManagerDataSource`. This class provides a basic implementation of
    `DataSource` that we can use. We have also passed the PostgreSQL database URL,
    username, and password as properties to the `dataSource` bean. Also, the `dataSource`
    bean is set to the `AccountDaoImpl` bean, and we are ready with our Spring JDBC
    implementation. The implementation is loosely coupled, and if we want to switch
    to some other implementation or move to another database server, then we need
    to make changes only in the bean configuration. This is one of the major advantages
    provided by the Spring JDBC framework.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the implementation of `AccountDAO` where we use the Spring `JdbcTemplate`
    class to insert data into the table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: In the previous example, we used the `org.springframework.jdbc.core.JdbcTemplate`
    class to access a persistence resource. The Spring `JdbcTemplate` is the central
    class in the Spring JDBC core package and provides a lot of methods to execute
    queries and automatically parse `ResultSet` to get the object or list of objects.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the test class for the preceding implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'When we run the previous program, we get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Database design for optimal performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Nowadays, it is very easy to design a database with the help of modern tools
    and processes, but we must know that it is a very crucial part of our application
    and it directly impacts the application performance. Once an application has been
    implemented with an inaccurate database design, it's too late to fix it. We have
    no other option but to buy expensive hardware to cope with the problem. So, we
    should be aware of some of the basic concepts and best practices of database table
    design, database partitioning, and good indexing, which improve our application's
    performance. Let's see the fundamental rules and best practices for developing
    high-performance database applications.
  prefs: []
  type: TYPE_NORMAL
- en: Table design
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The table design type can be normalized or denormalized, but each type has its
    own benefits. If the table design is normalized, it means that redundant data
    is eliminated, and data is logically stored with the primary key/foreign key relationship,
    which improves data integrity. If the table design is denormalized, it means increased
    data redundancy and creates inconsistent dependencies between tables. In the denormalized
    type, all of the data for a query is usually stored in a single row in the table;
    that's why it is faster to retrieve data and improves query performance. In the
    normalized type, we have to use joins in our queries to fetch the data from the
    database and, due to the joins, the performance of the query is impacted. Whether
    we should use normalization or denormalization totally depends on our application's
    nature and the business requirements. Normally, databases planned for **online
    transaction processing** (**OLTP**) are typically more normalized than databases
    planned for **online analytical processing** (**OLAP**). From a performance point
    of view, normalization is generally used where more `INSERT`/`UPDATE`/`DELETE`
    operations are required, while denormalization is used where more `READ` operations
    are required.
  prefs: []
  type: TYPE_NORMAL
- en: Vertical partitioning of a table
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the use of vertical partitioning, we split a table with many columns into
    multiple tables with particular columns. For example, we must not define very
    wide text or **binary large object** (**BLOB**) data columns infrequently queried
    tables because of performance issues. This data must be placed in a separate table
    structure, and a pointer can be used in queried tables.
  prefs: []
  type: TYPE_NORMAL
- en: 'What follows is a simple example of how we can use vertical partitioning on
    a `customer` table and move a binary data type column, `customer_Image`, into
    a separate table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Partition data vertically, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: In JPA/Hibernate, we can easily map the previous example with a lazy one-to-many
    relationship between the tables. The data usages of the `customer_Image` table
    are not frequent, so we can set it as lazily loaded. Its data is retrieved when
    a client requests the specific columns of the relationship.
  prefs: []
  type: TYPE_NORMAL
- en: Use indexing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We should use indexes for frequently used queries on big tables because index
    functionality is one of the best ways to improve the read performance of a database
    schema. Index entries are stored in a sorted order, which helps when processing
    the `GROUP BY` and `ORDER BY` clauses. Without an index, a database has to perform
    sort operations at the time of query executions. Through indexes, we can minimize
    the query execution time and improve query performance, but we should take care
    when creating indexes on a table; there are certain drawbacks, as well.
  prefs: []
  type: TYPE_NORMAL
- en: We should not create too many indexes on tables that update frequently because,
    at the time of any data modification on the table, indexing is also changed. We
    should use a maximum of four to five indexes on a table. If the table is read-only,
    then we can add more indexes without worrying.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the guidelines to build the most effective indexes for your application,
    which are valid for every database:'
  prefs: []
  type: TYPE_NORMAL
- en: In order to achieve the maximum benefits of indexes, we should use indexes on
    appropriate columns. Indexes should be used on those columns that are frequently
    used in `WHERE`, `ORDER BY`, or `GROUP BY` clauses in queries.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Always choose integer data type columns for indexing because they provide better
    performance than other data type columns. Keep indexes small because short indexes
    are processed faster in terms of I/O.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clustered indexes are usually better for queries retrieving a range of rows.
    Non-clustered indexes are usually better for point queries.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the correct data type
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A data type determines the type of data that can be stored in a database table
    column. When we create a table, we should define the proper data type for each
    column, as per its storage requirement. For example, a `SMALLINT` occupies 2 bytes
    of space, while an `INT` occupies 4 bytes of space. When we define the `INT` data
    type, it means that we must store all 4 bytes into that column every time. If
    we are storing a number like 10 or 20, then it's a waste of bytes. This will eventually
    make your reads slower because the database must read over multiple sectors of
    the disk. Also, choosing the right data type helps us store the right data into
    the column. For example, if we use the date data type for a column, then the database
    does not allow any string and numeric data in a column that does not represent
    a date.
  prefs: []
  type: TYPE_NORMAL
- en: Defining column constraints
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Column constraints enforce limits on the data or types of data that can be inserted/updated/deleted
    from a table. The whole purpose of constraints is to maintain the data integrity
    during an `UPDATE`/`DELETE`/`INSERT` into a table. However, we should only define
    constraints where appropriate; otherwise, we will create a negative impact on
    performance. For example, defining `NOT NULL` constraints does not impose noticeable
    overhead during query processing, but defining `CHECK` constraints might create
    a negative effect on performance.
  prefs: []
  type: TYPE_NORMAL
- en: Using stored procedures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data access performance can be tuned by using stored procedures to process data
    in the database server to reduce the network overhead, and also by caching data
    within your application to reduce the number of accesses.
  prefs: []
  type: TYPE_NORMAL
- en: Transaction management
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A database transaction is a critical part of any application. A database transaction
    is a sequence of actions that are treated as a single unit of work. These actions
    should either be completed entirely or take no effect at all. The management of
    the sequence of actions is known as **transaction management**. Transaction management
    is an important part of any RDBMS-oriented enterprise applications, to ensure
    data integrity and consistency. The concept of transactions can be described with
    four key properties: **atomicity**, **consistency**, **isolation**, **and durability**
    (**ACID**).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Transactions are described as ACID, which stands for the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Atomicity: A transaction should be treated as a single unit of operation, which
    means that either the entire sequence of operations is completed, or it takes
    no effect at all'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Consistency: Once a transaction is completed and committed, then your data
    and resources will be in a consistent state that conforms to business rules'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Isolation: If many transactions are being processed with the same dataset at
    the same time, then each transaction should be isolated from others to prevent
    data corruption'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Durability: Once a transaction has completed, the results of the transaction
    are written to persistent storage and cannot be erased from the database due to
    system failure'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choosing a transaction manager in Spring
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spring provides different transaction managers, based on different platforms.
    Here, a different platform means a different persistence framework, such as JDBC,
    MyBatis, Hibernate, and **Java Transaction API** (**JTA**). So, we have to choose
    the transaction manager provided by Spring accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram describes platform-specific transaction management provided
    by Spring:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dc325c62-bd48-4502-8b86-fc0af3e09694.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Spring supports two types of transaction management:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Programmatic: This means that we can write our transactions using Java source
    code directly. This gives us extreme flexibility, but it is difficult to maintain.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Declarative: This means that we can manage transactions in either a centralized
    way, by using XML, or in a distributed way, by using annotations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Declarative ACID using @Transactional
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Declarative transactions are highly recommended because they keep transaction
    management out of business logic and are easy to configure. Let's see an example
    of annotation-based declarative transaction management.
  prefs: []
  type: TYPE_NORMAL
- en: Let's use the same example that was used in the Spring JDBC section. In our
    example, we are using `JdbcTemplate` for database interaction. So, we need to
    add `DataSourceTransactionManager` in our Spring configuration file.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the Spring bean configuration class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: In the previous code, we created a `dataSource` bean. It is used to create the
    `DataSource` object. Here, we need to provide the database configuration properties,
    such as `DriverClassName`, `Url`, `Username`, and `Password`. You can change these
    values based on your local settings.
  prefs: []
  type: TYPE_NORMAL
- en: We are using JDBC to interact with the database; that is why we have created
    a `transactionManager` bean of the type `org.springframework.jdbc.datasource.DataSourceTransactionManager`.
  prefs: []
  type: TYPE_NORMAL
- en: The `@EnableTransactionManagement` annotation is used to turn on transaction
    support in our Spring application.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is an `AccountDao` implementation class to create a record in
    an `Account` table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: In the previous code, we provided declarative transaction management through
    annotating the `insertAccountWithJdbcTemplate()` method with the `@Transactional`
    annotation. The `@Transactional` annotation can be used with a method, as well
    as at the class level. In the previous code, I threw the `RuntimeException` exception
    after inserting `Account`, to check how the transaction will be rolled back after
    an exception is generated.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the `main` class to check our transaction management implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, when we run the preceding code, we get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: In the previous log, data is inserted into the `Account` table successfully.
    But, if you check the `Account` table, you won't find a row there, which means
    that the transaction is rolled back completely after `RuntimeException`. Spring
    Framework is committing the transaction only if the method returns successfully.
    If there is an exception, it rolls back the whole transaction.
  prefs: []
  type: TYPE_NORMAL
- en: Optimal isolation levels
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we learned in the previous section, the concept of a transaction is described
    with ACID. Transaction isolation level is a concept that is not limited to Spring
    Framework but is applicable to any application that interacts with a database.
    The isolation level defines how the changes made to some data repository by one
    transaction affect other concurrent transactions, and also how and when that changed
    data becomes available to other transactions. In Spring Framework, we define the
    isolation level of a transaction along with the `@Transaction` annotation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following snippet is an example of how we can define the `isolation` level
    in a transactional method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we defined a method with a transaction `isolation` level
    of `READ_UNCOMMITTED`. This means that the transaction in this method is executed
    with that `isolation` level.
  prefs: []
  type: TYPE_NORMAL
- en: Let's see each `isolation` level in detail in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Read uncommitted
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Read uncommitted is the lowest isolation level. This isolation level defines
    that a transaction may read data that is still uncommitted by other transactions,
    which means that the data is not consistent with other parts of the table or the
    query. This isolation level ensures the quickest performance because data is read
    from a table block directly, and there is no need for further processing, validation,
    or other verification; but it may lead to some issues, such as dirty reads.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ec6551f4-f0be-4a36-b21d-0bce9c32033e.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding diagram, **Transaction A** writes data; meanwhile, **Transaction
    B** reads that same data before **Transaction A** commits it. Later, **Transaction
    A** decides to **rollback**, due to some exception. Now, the data in **Transaction
    B** is inconsistent. Here, **Transaction B** was running in a `READ_UNCOMMITTED`
    isolation level, so it was able to read data from **Transaction A** before a commit
    occurred.
  prefs: []
  type: TYPE_NORMAL
- en: Note that `READ_UNCOMMITTED` can also create issues like **non-repeatable reads**
    and **phantom reads**. A non-repeatable read occurs when a transaction isolation
    selected as `READ_COMMITTED`.
  prefs: []
  type: TYPE_NORMAL
- en: Let's see the `READ_COMMITTED` isolation level in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Read committed
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The read committed isolation level defines that a transaction can't read data
    that is not committed by other transactions. This means that the dirty read is
    no longer an issue, but other issues may occur.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9d62954e-612a-4509-9657-0faac6ad332c.png)'
  prefs: []
  type: TYPE_IMG
- en: In this example, **Transaction A** reads some data. Then, **Transaction B**
    writes that same data and commits. Later, **Transaction A** reads that same data
    again and may get different values, because **Transaction B** already made changes
    to that data and committed. This is a non-repeatable read.
  prefs: []
  type: TYPE_NORMAL
- en: Note that `READ_COMMITTED` can also create issues like **phantom reads**. A
    phantom reads occurs when a transaction isolation is selected as `REPEATABLE_READ`.
  prefs: []
  type: TYPE_NORMAL
- en: Let's see the `REPEATABLE_READ` isolation level in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Repeatable read
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `REPEATABLE_READ` isolation level defines that if a transaction reads one
    record from the database multiple times, the results of all of those reading operations
    must be the same. This isolation helps us to prevent issues like dirty reads and
    non-repeatable reads, but it may create another issue.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d654c888-d00c-4dbe-b0cd-22b71cb75d56.png)'
  prefs: []
  type: TYPE_IMG
- en: In this example, **Transaction A** reads a range of data. Simultaneously, **Transaction
    B** inserts new data in the same range that **Transaction A** initially fetched
    and commits. Later, **Transaction A** reads the same range again and will also
    get the record that **Transaction B** just inserted. This is a phantom read. Here,
    **Transaction A** fetched a range of records from the database multiple times and
    got different result sets.
  prefs: []
  type: TYPE_NORMAL
- en: Serializable
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The serializable isolation level is the highest and more restrictive of all
    isolation levels. It protects against dirty, non-repeatable reads and phantom
    reads. Transactions are executed with locking at all levels (**read**, **range**,
    and **write** locking), so they appear as if they were executed in a serialized
    way. In serializable isolation, we will ensure that no issues will happen, but
    concurrently executing transactions occurs to be serially executing that degrade
    the performance of the application.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a summary of the relationships between isolation levels and
    read phenomena:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Levels** | **Dirty reads** | **Non-repeatable reads** | **Phantom reads**
    |'
  prefs: []
  type: TYPE_TB
- en: '| `READ_UNCOMMITTED` | Yes | Yes | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| `READ_COMMITTED` | No | Yes | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| `REPEATABLE_READ` | No | No | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| `SERIALIZABLE` | No | No | No |'
  prefs: []
  type: TYPE_TB
- en: If the isolation level is not explicitly set, then the transaction uses a default
    isolation level, as per the related database.
  prefs: []
  type: TYPE_NORMAL
- en: Optimal fetch size
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The network traffic between the application and the database server is one of
    the key factors of your application performance. If we can reduce the traffic,
    it will help us improve the performance of the application. The fetch size is
    the number of rows retrieved from the database at one time. It depends on the
    JDBC driver. The default fetch size of most of the JDBC drivers is 10\. In normal
    JDBC programming, if you want to retrieve 1,000 rows, then you will need 100 network
    round-trips between the application and database server to retrieve all of the
    rows. It will increase the network traffic, and also impact performance. But if
    we set the fetch size to 100, then the number of network round-trips will be 10\.
    This will greatly improve the performance of your application.
  prefs: []
  type: TYPE_NORMAL
- en: Many frameworks, such as Spring or Hibernate, give you very convenient APIs
    to do this. If we do not set the fetch size, then it will take the default value
    and provide poor performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following sets the `FetchSize`, with standard JDBC calls:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: In the previous code, we can set the fetch size on each `Statement` or `PreparedStatement`,
    or even on `ResultSet`. By default, `ResultSet` uses the fetch size of `Statement`; `Statement`
    and `PreparedStatement` use the fetch size of a specific JDBC driver.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also set the `FetchSize` in the Spring `JdbcTemplate`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The following points should be considered when setting the fetch size:'
  prefs: []
  type: TYPE_NORMAL
- en: Make sure that your JDBC driver supports configuring the fetch size
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The fetch size should not be hardcoded; keep it configurable because it depends
    on JVM heap memory size, which varies with different environments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the fetch size is large, the application might encounter an out of memory
    issue
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimal connection pooling configuration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: JDBC uses a connection pool when accessing a database. **Connection pooling**
    is similar to any other form of object pooling. Connection pooling usually involves
    little or no code modification, but it can provide significant benefits in terms
    of application performance. Database connection performs the various task while
    creating, such as initializing a session in the database, performing user authentication
    and establishing transaction contexts. Creating a connection is not a zero cost
    process; therefore, we should create connections in an optimal way and reduce
    impacts on performance. A connection pool allows for the reuse of physical connections
    and minimizes expensive operations in the creation and closure of sessions. Also,
    maintaining many idle connections is expensive for a database management system,
    and the pool can optimize the usage of idle connections or disconnect those no
    longer used.
  prefs: []
  type: TYPE_NORMAL
- en: 'Why is pooling useful? Here are a few reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: Frequently opening and closing connections can be expensive; it is better to
    cache and reuse.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can limit the number of connections to the database. This will stop from
    accessing a connection until it is available. This is especially helpful in distributed
    environments.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can use multiple connection pools for common operations, based on our requirements.
    We can design one connection pool for OLAP and another for OLAP, each with different
    configurations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, we will see what the optimal connection pooling configuration
    is, to help improve the performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a simple connection pool configuration for PostgreSQL:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Sizing the connection pool
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We need to work with the following attributes to size the connection pool:'
  prefs: []
  type: TYPE_NORMAL
- en: '`initialSize`: The `initialSize` attribute defines the number of connections
    that will be established when the connection pool is started.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`maxActive`: The `maxActive` attribute can be used to limit the maximum number
    of established connections to the database.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`maxIdle`: The `maxIdeal` attribute is used to maintain the maximum number
    of idle connections in the pool at all times.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`minIdle`: The `minIdeal` attribute is used to maintain the minimum number
    of idle connections in the pool at all times.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`timeBetweenEvictionRunsMillis`: The validation/cleaner thread runs every `timeBetweenEvictionRunsMillis`
    milliseconds. It''s a background thread that can test idle, abandoned connections,
    and resize the pool while the pool is active. The thread is also responsible for
    connection leak detection. This value should not be set below 1 second.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`minEvictableIdleTimeMillis`: The minimum amount of time an object may sit
    idle in the pool before it is eligible for eviction.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Validate connections
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The advantage of setting this configuration is that an invalid connection will
    never be used, and it helps us to prevent client errors. The disadvantage of this
    configuration is a small performance penalty because to validate the connection,
    one round-trip to the database is required to check whether the session is still
    active. The validation is accomplished via sending a small query to the server,
    but the cost of this query could be lower.
  prefs: []
  type: TYPE_NORMAL
- en: 'The configuration parameters for validating connections are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`testOnBorrow`: When the `testOnBorrow` attribute is defined as true, the connection
    object is validated before use. If it fails to validate, it will be dropped into
    the pool, and another connection object will be chosen. Here, we need to make
    sure that the `validationQuery` attribute is not null; otherwise, there is no
    effect on configuration.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`validationInterval`: The `validationInterval` attribute defines the frequency
    of the validating connection. It should not be more than 34 seconds. If we set
    a larger value, it will improve the application performance, but will also increase
    the chances of a stale connection being present in our application.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`validationQuery`: The `SELECT 1` PostgreSQL query is used to validate connections
    from the pool before sending them to serve a request.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Connection leaks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following configuration settings can help us to detect connection leaks:'
  prefs: []
  type: TYPE_NORMAL
- en: '`removeAbandoned`: This flag should be true. It means that abandoned connections
    are removed if they exceed `removeAbandonedTimeout`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`removeAbandonedTimeout`: This is in seconds. A connection is considered abandoned
    if it''s running more than `removeAbandonedTimeout`. The value depends on the
    longest running query in your applications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So, in order to get optimal pool sizing, we need to modify our configuration
    to meet one of following conditions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Tomcat JDBC connection pool versus HikariCP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are many open source connection pool libraries available, such as C3P0,
    Apache Commons DBCP, BoneCP, Tomcat, Vibur, and Hikari. But which one to use depends
    on certain criteria. The following criteria will help to decide which connection
    pool to go with.
  prefs: []
  type: TYPE_NORMAL
- en: Reliability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Performance is always good, but the reliability of a library is always more
    important than performance. We should not go with a library that gives a higher
    performance but is not reliable. The following things should be considered when
    selecting a library:'
  prefs: []
  type: TYPE_NORMAL
- en: How widely it is used?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How is the code maintained?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of outstanding bugs open in the library.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It's community of developers and users.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How active is the library development?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Performance is also considered important criteria. The performance of the library
    depends on how it is configured, and the environment in which it is tested. We
    need to make sure that the library that we choose will have good performance in
    our own environment, with our own configurations.
  prefs: []
  type: TYPE_NORMAL
- en: Features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is also important to look at the features provided by the library. We should
    check all parameters, and also check the default value of the parameters if we
    don't provide them. Also, we need to look at some of the connection strategies,
    such as auto-commit, isolation level, and statement caching.
  prefs: []
  type: TYPE_NORMAL
- en: Ease of use
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is important how easily we can configure the connection pool with the use
    of a library. Also, it should be well documented and frequently updated.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table lists the differences between a Tomcat JDBC connection
    pool and HikariCP:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Tomcat JDBC** | **HikariCP** |'
  prefs: []
  type: TYPE_TB
- en: '| Does not test connections on `getConnection()` by default. | Tests connections
    on `getConnection()`. |'
  prefs: []
  type: TYPE_TB
- en: '| Does not close abandoned open statements. | Tracks and closes abandoned connections.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Does not by default reset auto-commit and transaction levels for connections
    in the pool; users must configure custom interceptors to do this. | Resets auto-commit,
    transaction isolation, and read-only status. |'
  prefs: []
  type: TYPE_TB
- en: '| Pool prepared statement properties are not used. | We can use pool prepared
    statement properties. |'
  prefs: []
  type: TYPE_TB
- en: '| Does not, by default, execute a `rollback()` on connections returned to the
    pool. | By default, executes a `rollback()` on connections returned to the pool.
    |'
  prefs: []
  type: TYPE_TB
- en: Database interaction best practices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section lists some basic rules that developers should be aware of when
    developing any applications. A failure to follow the rules will result in a poorly
    performing application.
  prefs: []
  type: TYPE_NORMAL
- en: Using Statement versus PreparedStatement versus CallableStatement
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Choose between the `Statement`, `PreparedStatement`, and `CallableStatement`
    interfaces; it depends on how you plan to use the interface. The `Statement` interface
    is optimized for a single execution of an SQL statement, while the `PreparedStatement`
    object is optimized for SQL statements that will be executed multiple times, and
    `CallableStatement` is generally preferred for executing stored procedures:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Statement`: The `PreparedStatement` is used to execute normal SQL queries.
    It is preferred when a particular SQL query is to be executed only once. The performance
    of this interface is very low.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PreparedStatement`: The `PreparedStatement` interface is used to execute parametrized
    or dynamic SQL queries. It is preferred when a particular query is to be executed
    multiple times. The performance of this interface is better than the `Statement`
    interface (when used for multiple executions of the same query).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CallableStatement`: The `CallableStatement` interface is preferred when the
    stored procedures are to be executed. The performance of this interface is high.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Batch instead of PreparedStatement
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Inserting a large amount of data into a database is typically done by preparing
    an `INSERT` statement and executing that statement multiple times. This increases
    the number of JDBC calls and impacts the performance. To reduce the number of
    JDBC calls and improve the performance, you can send multiple queries to the database
    at a time by using the `addBatch` method of the `PreparedStatement` object.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding example, `PreparedStatement` is used to execute an `INSERT`
    statement multiple times. For executing the preceding `INSERT` operation, 101
    network round-trips are required: one to prepare the statement, and the remaining
    100 to execute the `INSERT` SQL statement. So, inserting and updating a large
    amount of data actually increases network traffic and, due to that, impacts performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see how we can reduce the network traffic and improve performance with
    the use of `Batch`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'In the previous example, I used the `addBatch()` method. It consolidates all
    100 `INSERT` SQLs and executes the entire operation with only two network round-trips:
    one for preparing the statement and another for executing the batch of consolidated
    SQLs.'
  prefs: []
  type: TYPE_NORMAL
- en: Minimizing the use of database metadata methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Although almost no JDBC application can be written without database metadata
    methods, compared to other JDBC methods, database metadata methods are slow. When
    we use a metadata method, a `SELECT` statement makes two round-trips to the database:
    one for metadata, and the second for the data. This is very performance expensive.
    We can improve the performance by minimizing the use of metadata methods.'
  prefs: []
  type: TYPE_NORMAL
- en: An application should cache all metadata, as they will not change, so multiple
    executions are not needed.
  prefs: []
  type: TYPE_NORMAL
- en: Using get methods effectively
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: JDBC provides different types of methods to retrieve data from a result set,
    such as `getInt`, `getString`, and `getObject`; the `getObject` method is the
    generic one, and you can use it for all data types. But, we should always avoid
    the use of `getObject` because it gives worse performance than others. When we
    get data with the use of `getObject`, the JDBC driver must perform extra processing
    to determine the type of value being fetched and generate the appropriate mapping.
    We should always use the specific method for the data type; this provides better
    performance than using a generic one like `getObject`.
  prefs: []
  type: TYPE_NORMAL
- en: We can also improve the performance by using a column number instead of a column
    name; for example, `getInt(1)`, `getString(2)`, and `getLong(3)`. If we are using
    a column name instead of a column number (for example, `getString("accountName")`),
    then the database driver first converts the column name to uppercase (if required),
    then compares `accountName` with all columns available in the result set. This
    processing time directly impacts performance. We should reduce that processing
    time with the use of column numbers.
  prefs: []
  type: TYPE_NORMAL
- en: When to avoid connection pooling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The use of connection pooling on certain kinds of applications can definitely
    degrade the performance. If your application has any of the following characteristics,
    it is not a suitable candidate for connection pooling:'
  prefs: []
  type: TYPE_NORMAL
- en: If an application restarts many times daily, we should avoid connection pooling
    because, based on the configuration of the connection pool, it may be populated
    with connections each time the application is started, which would cause a performance
    penalty up front.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you have single-user applications, such as applications only generating reports
    (in this type of application, the user only uses the application three to four
    times daily, for generating reports), then avoid connection pooling. The memory
    utilization for establishing a database connection three to four times daily is
    low, compared to the database connection associated with a connection pool. In
    such cases, configuring a connection pool degrades the overall performance of
    the application.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If an application runs only batch jobs, there is no advantage to using connection
    pooling. Normally, a batch job is run at the end of the day or month or year,
    during the off hours, when performance is not as much of a concern.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choose commit mode carefully
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we commit a transaction, the database server must write the changes made
    by the transaction to the database. This involves expensive disk input/output
    and the driver need to send requests over the socket.
  prefs: []
  type: TYPE_NORMAL
- en: In most standard APIs, the default commit mode is auto-commit. In the auto-commit
    mode, the database performs a commit for each SQL statement, such as `INSERT`,
    `UPDATE`, `DELETE`, and `SELECT` statements. The database driver sends a commit
    request to the database after each SQL statement operation. This request requires
    one network round-trip. The round-trip to the database occurs even though the
    SQL statement execution made no changes to the database. For example, the driver
    makes a network round-trip even when a `SELECT` statement is executed. The auto-commit
    mode usually impacts performance because of the significant amount of disk input/output
    needed to commit every operation.
  prefs: []
  type: TYPE_NORMAL
- en: So, we set the auto-commit mode to off to improve the performance of the application,
    but leaving the transaction active is also not advisable. Leaving transactions
    active can reduce throughput by holding locks on rows for longer than necessary
    and preventing other users from accessing the rows. Commit transactions in intervals
    that allow maximum concurrency.
  prefs: []
  type: TYPE_NORMAL
- en: Setting the auto-commit mode to off and doing a manual commit is also not advisable
    for certain applications. For example, consider a banking application that allows
    users to transfer money from one account to another. To protect the data integrity
    of that work, it is required to commit the transaction after both accounts are
    updated with the new amounts.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we got a clear idea of the Spring JDBC module and learned how
    Spring JDBC helps to remove the boilerplate code that we use in core JDBC. We
    also learned how to design our database for optimal performance. We saw the various
    benefits of transaction management in Spring. We learned about various configuration
    techniques, such as isolation level, fetch size, and connection pooling, which
    improves the performance of our application. At the end, we looked at the best
    practices for database interaction, which can help us to improve our application's
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will see database interaction using ORM frameworks (such
    as Hibernate), and we will learn about Hibernate configurations in Spring, common
    Hibernate traps, and Hibernate performance tuning.
  prefs: []
  type: TYPE_NORMAL
