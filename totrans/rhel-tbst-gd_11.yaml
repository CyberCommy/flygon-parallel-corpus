- en: Chapter 11. Recovering from Common Failures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we explored the user and system limitations that exist
    on Linux servers. We looked at what limits are in place and how to change the
    values for applications that require more than default.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will put our troubleshooting skills to use with a system
    that has had its resources exhausted.
  prefs: []
  type: TYPE_NORMAL
- en: The reported problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Today''s chapter, much like the other chapters, will start with someone reporting
    an issue. The issue being reported is that Apache is no longer running on the
    server, which serves the company''s blog: `blog.example.com`.'
  prefs: []
  type: TYPE_NORMAL
- en: A fellow systems administrator who is reporting the issue has explained that
    someone reported that the blog was down and when he logged into the server he
    could see Apache was no longer running. At that point, our peer was unsure what
    to do to continue and asked for our help.
  prefs: []
  type: TYPE_NORMAL
- en: Is Apache really down?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first thing that we should do when a service is reported as down is to validate
    that it really is down. This is essentially our *duplicate it for ourselves* step
    from our troubleshooting process. With a service such as Apache, we should also
    validate that it is in fact down fairly quickly.
  prefs: []
  type: TYPE_NORMAL
- en: In my experience, I have often been told that a service is down when it really
    was not. The server may have been having an issue but it was not technically down.
    The difference between up or down can change the troubleshooting steps that we
    need to perform to resolve the issue.
  prefs: []
  type: TYPE_NORMAL
- en: This said, the first step that I always perform for issues like this is to validate
    whether the service really is down or whether the service simply is not responding.
  prefs: []
  type: TYPE_NORMAL
- en: 'To validate that Apache is really down, we will use the `ps` command. As we
    learned earlier, this command will print a list of the currently running processes.
    We will redirect this output to the `grep` command to check whether there are
    any instances of the `httpd` (Apache) service running:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'From the output of the abovementioned `ps` command, we can see that there are
    no processes running with the name `httpd`. Under normal circumstances, we would
    expect to see at least a few lines that look similar to the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Since there are no `httpd` processes found in the process list, we can conclude
    that Apache is in fact down on this system. The question now is, why?
  prefs: []
  type: TYPE_NORMAL
- en: Why is it down?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before simply resolving the issue by starting the Apache service, we are going
    to first figure out why the Apache service is not running. This is a process called
    **Root Cause Analysis** (**RCA**), which is a formal process that is used to understand
    what first caused the issue.
  prefs: []
  type: TYPE_NORMAL
- en: We will get very familiar with this process in the next chapter. In this chapter,
    we will keep it simple and focus specifically on why Apache is not running.
  prefs: []
  type: TYPE_NORMAL
- en: One of the first places for us to look is in the Apache logs in `/var/log/httpd`.
    We learned of these logs in the previous chapters while troubleshooting other
    webserver-related issues. As we saw in these earlier chapters, application and
    service logs can be very helpful in determining what has happened to the service.
  prefs: []
  type: TYPE_NORMAL
- en: Since Apache is no longer running, we are more interested in the last few events
    that happened. If the service experienced a fatal error or was stopped, there
    should be a message at the end of the log file showing this.
  prefs: []
  type: TYPE_NORMAL
- en: 'Because we are only interested in the last few events, we will use the `tail`
    command to show the last 10 lines of the `error_log` file. The `error_log` file
    is the first log to check as it is the most likely place for anything unusual:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: From the `error_log` file contents, we can see quite a few interesting messages.
    Let's take a quick look at some of the more informational log entries.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding line shows that the Apache process was shut down on `Sunday,
    Jun 21` at `20:53`. We can see this as the error message clearly states `shutting
    down gracefully`. The next few lines, however, seem to indicate that the Apache
    service was back up only `2` seconds later:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The shutdown log entry shows a process id of `2218`, whereas the preceding five
    lines show a process id of `2249`. The 5th line also states `resuming normal operations`.
    These four messages seem to indicate that the Apache process simply restarted.
    Most likely, this was a graceful restart of Apache.
  prefs: []
  type: TYPE_NORMAL
- en: A graceful restart of Apache is a fairly common task performed during the modification
    of its configuration. This is a way to restart the Apache process without taking
    it fully down and impacting the web service.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The most interesting thing that these 10 lines tell us, however, is that the
    last log Apache printed was nothing more than a notification. When Apache was
    stopped gracefully, it logged a message in the `error_log` file to show that it
    was being stopped.
  prefs: []
  type: TYPE_NORMAL
- en: Since the Apache processes are no longer running and there are no log entries
    showing that it was shut down gracefully or even ungracefully, we conclude that
    irrespective of the reason why Apache was not running, it did not shut down normally.
  prefs: []
  type: TYPE_NORMAL
- en: If a person shut down the service by using `apachectl` or the `systemctl` command,
    we would expect to see a message similar to that discussed in the earlier example.
    Since the last line of the log file shows no shutdown message, we can only assume
    that this process was killed or terminated under abnormal circumstances.
  prefs: []
  type: TYPE_NORMAL
- en: Now, the question is *What might have caused the Apache process to terminate
    in an abnormal manner like this?*
  prefs: []
  type: TYPE_NORMAL
- en: One place that may provide a clue as to what happened with Apache is the systemd
    facility as Red Hat Enterprise Linux 7 services, such as Apache, have been moved
    to systemd. Upon booting, the `systemd` facility starts up any service that it
    has been configured to start.
  prefs: []
  type: TYPE_NORMAL
- en: 'When a process that `systemd` starts is terminated, that activity is captured
    by `systemd`. Depending on what has happened since the process was terminated,
    we can see whether `systemd` captured this event by using the `systemctl` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The output of the `systemctl status` command shows quite a bit of information.
    Since we covered this quite a bit in the previous chapters, I am going to skip
    to just the parts of this output that will tell us what happened to the Apache
    service.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first two lines that look interesting are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'In these two lines, we can see process id `2249`, which we also saw in the
    `error_log` file. This is the process id of the Apache instance started on `Sunday,
    June 21`. We can also see from these lines that process `2249` was killed. This
    seems to indicate that someone or something killed our Apache service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: If we look at the last few lines in the `systemctl` status output, we can see
    events that the `systemd` facility captured. The first event that we can see is
    that the Apache service was started on `June 21` at `20:53`. This isn't much of
    a surprise as it correlates with the information we saw in `error_log`.
  prefs: []
  type: TYPE_NORMAL
- en: The last three lines, however, show that the Apache process was subsequently
    killed on `June 26` at `21:21`. Unfortunately these events do not show exactly
    why the Apache process was killed or who killed it. What it does tell us is the
    exact time that Apache was killed. This also shows that it was not likely that
    the `systemd` facility stopped the Apache service.
  prefs: []
  type: TYPE_NORMAL
- en: What else was happening at that time?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since we were not able to determine the cause from the Apache logs or `systemctl
    status`, we will need to keep digging to understand what else may have killed
    this service.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Since the 26th was several days ago, we have a somewhat limited set of places
    to look for additional information. One place that we can look is the `/var/log/messages`
    log file. As we discovered in the earlier chapters, the `messages` log contains
    quite a lot of diverse information from many of the different facilities within
    the system. If there were a place that could tell us what was happening with the
    system at that time, it would be there.
  prefs: []
  type: TYPE_NORMAL
- en: Searching the messages log
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `messages` log is quite large and has many log entries within it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Therefore, we need to filter log messages that are either not relevant to our
    issue or not during the time of our issue. The first thing that we can do is search
    the log for messages from the day Apache was stopped: `June 26`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: From the previously mentioned `tail` command, we can see that messages within
    the `/var/log/messages` file have the format of date, hostname, process, and then
    message. The date field is a three-letter month followed by the day number and
    a 24-h timestamp.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since our issue occurred on June 26th, we can search this log file for any
    instance of the string "`Jun 26`". This should provide all messages that were
    written on the 26th:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'This evidently is still quite a few log messages, far too many to read them
    all. Given this number, we need to filter the messages even more, maybe by the
    process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code is commonly called a **bash** one-liner. This is often a
    series of commands that redirect their output to another command to provide a
    function or output that one command by itself cannot perform or generate. In this
    case, we have a one-liner that shows us which processes were logging the most
    on June 26th.
  prefs: []
  type: TYPE_NORMAL
- en: Breaking down this useful one-liner
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The above mentioned one-liner is somewhat complicated at first but once we break
    down this one-liner, it becomes much easier to understand. This is a useful one-liner
    as it makes identifying trends within log files a lot easier.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s break down this one-liner to get a better understanding of what it is
    doing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: We already know what the first command does; it simply searches the `/var/log/messages`
    file for any instance of the string "`Jun 26`". The other commands are ones that
    we haven't covered before, but they can be useful commands to know.
  prefs: []
  type: TYPE_NORMAL
- en: The cut command
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The `cut` command in this one-liner is used to read the output of the `grep`
    command and print only specific parts of each line. To understand how this works,
    we should first run the one-liner ending at the `cut` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The preceding `cut` command works by specifying a delimiter and cutting the
    output by that delimiter.
  prefs: []
  type: TYPE_NORMAL
- en: A delimiter is a character used to break down the line into multiple fields;
    we can specify it with the `–d` flag. In the preceding example, the `–d` flag
    is followed by "`\`"; the backslash is an escape character and is followed by
    a single space. This tells the `cut` command to use a single space character as
    the delimiter.
  prefs: []
  type: TYPE_NORMAL
- en: The `–f` flag is used to specify the `fields` that should be displayed. These
    fields are the strings of text between the delimiter.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let''s take a look at the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Here, we specified that the "`:`" character is the delimiter for `cut`. We also
    specified that it should print the first, second, and fourth fields. This had
    the effect of printing Apples (the first field), Bananas (the second field), and
    Dried Cherries (the fourth field). The third field, Carrots, was omitted from
    the output. This is because we didn't specifically tell the `cut` command to print
    the third field.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know how `cut` works, let's look at how it processes the `messages`
    log entries.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s a sample of a log message:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'When we executed the `cut` command in our one-liner, we specifically told it
    to only print the first, second, and fifth fields:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: By specifying a single space character to be the delimiter in our `cut` command,
    we can see that this causes `cut` to only print the month, day, and program from
    each log entry. By itself, this may not seem very useful, but as we continue looking
    through this one-liner, the functionality provided by cut will be critical.
  prefs: []
  type: TYPE_NORMAL
- en: The sort command
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The next command `sort` is actually used twice within this one-liner:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: This command is actually pretty simple in what it does. The `sort` command in
    this one-liner takes the output of the `cut` command and orders (sorts) it.
  prefs: []
  type: TYPE_NORMAL
- en: 'To explain this better, let''s look at the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The above file again has several fruits, and this time, they are not in alphabetical
    order. If we use the `sort` command to read this file, however, the order of these
    fruits will change:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see, the order is now alphabetical despite how the fruits are listed
    in the file itself. The nice thing about `sort` is that it can be used to order
    text in several different ways. In fact, in the second instance of `sort` within
    our one-liner, we use the `–n` flag to sort the text numerically as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The uniq command
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The reason that our one-liner contains the `sort` command is simply to order
    the input sent to `uniq -c`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The `uniq` command can be used to identify lines that match and display these
    lines in a single unique line. To understand this better, let''s look at the following
    example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Our example file "`duplicates.txt`" contains multiple duplicate lines. When
    we read this file with `uniq`, we will only see each unique line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'This can be somewhat useful; however, I find that with the `–c` flag, the output
    can be even more useful:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'With the `–c` flag, the `uniq` command will count the number of times that
    it finds each line. Here, we can see that there are four lines with the word Apple.
    Therefore, the `uniq` command printed the number 4 before the word Apple to show
    that there were four instances of this line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: One caveat to the `uniq` command is that in order to get an accurate count,
    each instance needs to be right after the other. You can see what happens when
    we add the word Orange between the groups of Apple lines.
  prefs: []
  type: TYPE_NORMAL
- en: Tying it all together
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If we look at our command again, we can now better understand what it is doing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The above command will filter and print all of the log messages in `/var/log/messages`
    that match the string "`Jun 26`". The output will then be sent to the `cut` command,
    which prints the month, day, and process of each line. This output is then sent
    to the `sort` command to order the output into groups that match each other. The
    sorted output is then sent to `uniq –c` that counts the number of occurrences
    of each line and prints one unique line with the count.
  prefs: []
  type: TYPE_NORMAL
- en: From there, we add another `sort` to order the output by the number added by
    `uniq`, and add `tail` to shorten the output to the last 10 lines.
  prefs: []
  type: TYPE_NORMAL
- en: So, what exactly does this fancy one-liner tell us? Well, it tells us that the
    `kernel` facility and the `systemd` process are logging quite a bit. In fact,
    in comparison to the other items listed, we can see that these two have more log
    messages than the others.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, it may not be unusual for `systemd` and `kernel` to have more log
    messages in `/var/log/messages`. If there was another process that wrote many
    logs, we would be able to see this in the one-liner''s output. However, since
    our first run did not yield anything useful, we can modify the one-liner to narrow
    down the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: If we look at the format of a `messages` log entry, we can see that after the
    process, the log message can be found. To narrow down our search a little bit
    more, we can add a little bit of the message to our output.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can do this by changing the `cut` command''s field list to "`1,2,5-8`".
    By adding the "`-8`" after `5`, we find that the `cut` command displays all fields
    from 5 to 8\. This has the effect of including the first three words of each log
    message in our one-liner:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'If we also increase the `tail` command to display the last 30 lines, we can
    see some interesting trends. The first line that is very interesting is the fourth
    line in the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: It seems that the `kernel` printed `79` log messages that start with the term
    "`Out of memory`". While it may seem a bit obvious to say, it seems that this
    server may have run out of memory at some point.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next two interesting lines seem to support this theory as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: The first line seems to suggest that the kernel killed a process; the second
    line once again indicates that there is an *out of memory* situation. Could this
    system have run out of memory and in doing so killed the Apache process? This
    seems very likely.
  prefs: []
  type: TYPE_NORMAL
- en: What happens when a Linux system runs out of memory?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: On Linux, memory is managed a bit differently from that on other operating systems.
    When a system is running low on memory, the kernel has a process that is designed
    to reclaim the used memory; this process is called **out of memory killer** (**oom-kill**).
  prefs: []
  type: TYPE_NORMAL
- en: The `oom-kill` process is designed to kill processes that utilize a large amount
    of memory in order to free this memory for critical system processes. We will
    cover `oom-kill` in a bit, but first, we should understand how Linux defines out
    of memory.
  prefs: []
  type: TYPE_NORMAL
- en: Minimum free memory
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: On Linux, the oom-kill process will be initiated when the amount of free memory
    is lower than a defined minimum. This minimum is of course a kernel tunable parameter
    named `vm.min_free_kbytes`. This parameter allows you to set the amount of memory
    in kilobytes that the system ensures is always available.
  prefs: []
  type: TYPE_NORMAL
- en: When the available memory is below the value of this parameter, the system starts
    to take action. Before going too far, let's first look at what this value is set
    at on our system and refresh how memory is managed in Linux.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can view the current `vm.min_free_kbytes` value with the same `sysctl` command
    that we used in the previous chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Currently, the value is `11424` kilobytes or approximately 11 megabytes. This
    means that our system''s free memory must always be greater than 11 megabytes
    or the system will kick off the oom-kill process. This seems pretty straightforward,
    but as we know from [Chapter 4](part0028_split_000.html#QMFO1-8ae10833f0c4428b9e1482c7fee089b4
    "Chapter 4. Troubleshooting Performance Issues"), *Troubleshooting Performance
    Issues*, the way Linux manages memory is not necessarily that easy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: If we run the `free` command on this system, we can see the current memory usage
    and how much is available. Before going too far, we will break down this output
    to refresh our understanding of how Linux uses memory.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: In the first line, we can see that the system has a total of 243MB of physical
    memory. We can see in the second column that 230MB of that is currently used,
    and the third column shows that 13MB is unused. It is this unused value that the
    system is measuring in order to determine whether or not the minimum required
    memory is currently free.
  prefs: []
  type: TYPE_NORMAL
- en: This is important because if we remember from [Chapter 4](part0028_split_000.html#QMFO1-8ae10833f0c4428b9e1482c7fee089b4
    "Chapter 4. Troubleshooting Performance Issues"), *Troubleshooting Performance
    Issues*, there is a second "memory free" value that we use to determine how much
    memory is available.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: On the second line of `free`, we can see the amount of used and free memory
    when the system accounts for the memory used by the cache. As we learned earlier,
    the Linux system very aggressively caches files and filesystem attributes. All
    of this cache is stored in memory, and we can see that in the instant when this
    `free` command was run, we had 2,272 KB of memory used by cache.
  prefs: []
  type: TYPE_NORMAL
- en: 'When the free memory (not including cache) starts to get close to the `min_free_kbytes`
    value, the system will start reclaiming some of the memory used for the cache.
    This is designed to allow the system to cache what it can, but during low memory
    conditions, this cache becomes disposable in order to prevent the oom-kill process
    from starting:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The third line of the `free` command brings us to another important step in
    Linux''s memory management: swapping. As we can see from the preceding line, when
    this `free` command was executed, the system swapped roughly 231MB of data from
    the physical memory to the swap device.'
  prefs: []
  type: TYPE_NORMAL
- en: This is what we would expect to see on a system that has been running low on
    available memory. When `free` memory starts to become scarce, the system will
    start taking memory objects that are in the physical memory and push them to the
    swap memory.
  prefs: []
  type: TYPE_NORMAL
- en: 'How aggressively the system starts to perform these swapping activities depends
    greatly on the value defined in the kernel parameter called `vm.swappiness`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: On our system, the `swappiness` value is currently set to `30`. This tunable
    parameter accepts values between 0 and 100, with 100 allowing for the most aggressive
    swapping policy.
  prefs: []
  type: TYPE_NORMAL
- en: When the `swappiness` value is lower, the system will prefer to retain memory
    objects in the physical memory for as long as possible before moving them to the
    swap device.
  prefs: []
  type: TYPE_NORMAL
- en: A quick recap
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Before going into oom-kill, let's recap what happens when the memory starts
    to get low on a Linux system. The system will first try to free memory objects
    used for disk cache and move the used memory to the swap device. If the system
    is unable to free an adequate amount of memory through the two previously mentioned
    processes, the kernel kicks off the oom-kill process.
  prefs: []
  type: TYPE_NORMAL
- en: How oom-kill works
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As mentioned earlier, the oom-kill process is a process launched when free memory
    is low. This process is designed to identify processes that are utilizing large
    amounts of memory and are not critical to the system operation.
  prefs: []
  type: TYPE_NORMAL
- en: So, how does oom-kill determine this? Well, it's actually determined by the
    kernel and is constantly updated.
  prefs: []
  type: TYPE_NORMAL
- en: We discussed in the earlier chapters how every running process on a system has
    a folder within the `/proc` file system. The `kernel` maintains this folder, and
    within it, there are many interesting files.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'The three previously mentioned files are specifically relevant to the oom-kill
    process and how likely each process is to be killed. The first file that we are
    going to look at is the `oom_score` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: If we `cat` this file, we see that it simply contains a number. However, this
    number is very important to the oom-kill process as this number is process 6689's
    OOM Score.
  prefs: []
  type: TYPE_NORMAL
- en: The OOM Score is a value that the `kernel` assigns to a process that determines
    whether the corresponding process is a high or low priority for oom-kill. The
    higher the score, the more likely the process is to be killed. When the kernel
    assigns this process a value, it bases the value on the amount of memory and swap
    that the process uses as well as its criticality to the system.
  prefs: []
  type: TYPE_NORMAL
- en: You may be asking yourself, *I wonder if there is a way to adjust the oom score
    for my processes.* The answer to this question is yes, there is! This is where
    the other two files `oom_adj` and `oom_score_adj` come into play. These two files
    allow you to adjust the oom score of a process, allowing you to control the likelihood
    of the process being killed.
  prefs: []
  type: TYPE_NORMAL
- en: Currently, the `oom_adj` file is to be depreciated in lieu of `oom_score_adj`.
    For this reason, we will simply focus on the `oom_score_adj` file.
  prefs: []
  type: TYPE_NORMAL
- en: Adjusting the oom score
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The `oom_score_adj` file supports values from -1000 to 1000, where the higher
    value will increase the likelihood of oom-kill selecting the process. Let''s see
    what happens to our oom score when we add an adjustment of 800 to our process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Simply by changing the contents to 800, the kernel detected this adjustment
    and added 800 to the oom score for this process. If this system were to run out
    of memory in the near future, this process would absolutely be killed by oom-kill.
  prefs: []
  type: TYPE_NORMAL
- en: If we were to change this value to -1000, this would essentially exclude the
    process from oom-kill.
  prefs: []
  type: TYPE_NORMAL
- en: Determining whether our process was killed by oom-kill
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we know what happens when a system runs low on memory, let''s take
    a closer look at what exactly happened to our system. To do this, we will use
    `less` to read the `/var/log/messages` file and look for the first instance of
    the "`kernel: Out of memory`" message:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Interestingly enough, the first instance of an "`Out of memory`" log message
    is 20 hours before our Apache process was killed. To add to this, the process
    killed is a very familiar process, the "`processor`" cronjob from the previous
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: This single log entry can actually tell us quite a bit about that process and
    why oom-kill selected this process. On the first line, we can see that the kernel
    has given the processor process a score of `265`. While not the highest score,
    we have seen that the score of 265 is very likely to be higher than that of most
    processes running at this time.
  prefs: []
  type: TYPE_NORMAL
- en: 'This seems to suggest that the processor job was utilizing quite a bit of memory
    at this time. Let''s keep looking through this file to see what else may have
    been happening on this system:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Just a bit further down the log file, we can see yet another instance of the
    processor process being killed. It seems that every time this job runs, this system
    is running out of memory.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the interest of time, let''s jump down to the 21st hour to take a closer
    look at the time that our Apache process being killed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'It seems that the `messages` log had our answer all along. From the preceding
    few lines, we can see process `2249`, which happens to be our Apache server process
    id:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Here, we see that `systemd` detected that the process was killed at `21:12:55`.
    Further, we can see from the messages log that oom-kill targeted this process
    at `21:12:54`. At this point, there is no doubt that the process was killed by
    oom-kill.
  prefs: []
  type: TYPE_NORMAL
- en: Why did the system run out of memory?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At this point, we were able to determine that the Apache service was killed
    by the system when it ran out of memory. Unfortunately, oom-kill is not the root
    cause of the issue, but rather a symptom. While it is the reason that the Apache
    service is down, if we simply restarted the process and did nothing else, the
    issue may reoccur.
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, we need to identify what caused the system to run out of memory
    in the first place. To do this, let''s take a look at the entire list of `Out
    of memory` messages in the messages log file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Using the `cut` and `uniq –c` commands again, we can see an interesting trend
    in the messages log. We can see that the kernel has invoked oom-kill quite a few
    times. We can see that even today the system kicked off the oom-kill process.
  prefs: []
  type: TYPE_NORMAL
- en: The first thing that we should do now is to figure out just how much memory
    this system has.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: Using the `free` command, we can see that the system has `238` MB of physical
    memory and `1055` MB of swap. However, we can also see that only `34` MB of memory
    is free and that the system has swapped `428` MB of physical memory.
  prefs: []
  type: TYPE_NORMAL
- en: It's very obvious that for the current workload that this system is under, it
    simply does not have enough memory allocated.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we look back at the processes that are targeted by oom-kill, we can see
    an interesting trend:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Here, it is very obvious that the two processes that were killed the most often
    were `httpd` and `processor`. What we learned earlier is that oom-kill identifies
    which processes to kill on the basis of the amount of memory that they are using.
    This means that these two processes are using the most memory on the system, but
    just how much memory are they using?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: Using the `ps` command to specifically display the **rss** and **size** fields,
    which we learned in [Chapter 4](part0028_split_000.html#QMFO1-8ae10833f0c4428b9e1482c7fee089b4
    "Chapter 4. Troubleshooting Performance Issues"), *Troubleshooting Performance
    Issues*, we can see that the `processor` job is using `130` MB of resident memory
    and `240` MB of virtual memory.
  prefs: []
  type: TYPE_NORMAL
- en: If the system only has `238` MB of physical memory and the process is using
    `240` MB of virtual memory, eventually, this system is going to run low on physical
    memory.
  prefs: []
  type: TYPE_NORMAL
- en: Resolving the issue in the long-term and short-term
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Issues such as the one discussed in this chapter can be a bit tricky, as they
    generally have two paths to resolution. There is a long-term fix and a short-term
    fix; both are necessary, but one is only temporary.
  prefs: []
  type: TYPE_NORMAL
- en: Long-term resolution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For the long-term resolution of this issue, we really have two options. We could
    increase the server's physical memory to provide both Apache and Processor adequate
    memory for their tasks. Alternatively, we could move the processor to another
    server.
  prefs: []
  type: TYPE_NORMAL
- en: Since we know that this server has frequently killed the Apache service and
    the `processor` job, it is likely that the memory on the system is simply too
    low for it to perform both these roles. By moving the `processor` job (and likely
    the custom app that it is part of) to another system, we would be moving the workload
    to a dedicated server.
  prefs: []
  type: TYPE_NORMAL
- en: On the basis of the memory usage of the processor, it may also be worth increasing
    the memory on the new server as well. As it seems, the `processor` job utilizes
    enough memory to cause out of memory conditions on a low memory server such as
    the one that it is on now.
  prefs: []
  type: TYPE_NORMAL
- en: Determining which long-term solution is best frankly depends on the environment
    and the applications causing the system to run out of memory. In some cases, it
    may be better to simply increase the server's memory and call it a day.
  prefs: []
  type: TYPE_NORMAL
- en: This task is very easy in virtual and cloud environments, but it may not always
    be the best answer. Determining which answer is better truly depends on the environment
    that you are working with.
  prefs: []
  type: TYPE_NORMAL
- en: Short-term resolution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's say hypothetically that both the long-term resolutions would take several
    days to implement. As of right now, the Apache service is still down on our system.
    This means that our company blog is also still down; to resolve the issue momentarily,
    we need to bring Apache back up.
  prefs: []
  type: TYPE_NORMAL
- en: However, we shouldn't just simply restart Apache with the `systemctl` command.
    Before bringing anything up, we should actually first reboot the server.
  prefs: []
  type: TYPE_NORMAL
- en: When most Linux administrators hear the words "let's reboot" they get a sinking
    feeling in their stomach. This is because, as Linux systems administrators, we
    very rarely need to reboot our systems. We have been told that rebooting Linux
    servers outside of updating the kernel is a naughty thing to do.
  prefs: []
  type: TYPE_NORMAL
- en: For the most part, we are correct in believing that rebooting a server is not
    the right solution. However, I consider the system running out of memory to be
    a special case.
  prefs: []
  type: TYPE_NORMAL
- en: It is my opinion that when oom-kill is launched the system in question should
    be rebooted before being fully restored to its normal state.
  prefs: []
  type: TYPE_NORMAL
- en: The reason I say this is that the oom-kill process can kill any process, including
    critical system processes. While the oom-kill process does log via syslog what
    processes were killed, the syslog daemon is just another process on the system
    that can be killed by oom-kill.
  prefs: []
  type: TYPE_NORMAL
- en: Even if oom-kill did not kill the syslog process in situations where oom-kill
    has killed many different processes, it can be tricky to ensure each one is up
    and running as it should be. This is particularly true when the person working
    on the issue is less experienced.
  prefs: []
  type: TYPE_NORMAL
- en: While you can spend time determining what processes are running and ensure that
    you restart each process, it is much faster and arguably safer to simply reboot
    the server. As you know that upon booting, every process that is defined to start
    will be started.
  prefs: []
  type: TYPE_NORMAL
- en: While not every system administrator would agree with this opinion, I believe
    that it is the best approach to ensure that the system is in a stable state. It's
    important to remember though that this is only a short-term solution, upon rebooting,
    unless something changes, the system can simply run out of memory again.
  prefs: []
  type: TYPE_NORMAL
- en: For our situation, it would be best to disable the `processor` job until the
    server's memory can be increased or the job can be moved to a dedicated system.
    However, that may not be acceptable in all situations. Like the long-term resolution,
    preventing this from happening again is situational and depends on the environment
    that you are managing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we assume that the short-term solution is the right solution for our
    example, we will proceed to reboot the system:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the system is back online, we can validate that Apache is running with
    the `systemctl` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'If we run `free` again on this system, we can see that the memory utilization
    is much lower, at least until now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we used our troubleshooting skills to identify both the issue
    affecting the company blog and the root cause of this issue. We were able to use
    the skills and techniques that we learned in earlier chapters to determine that
    the Apache service was down. We also identified that the root cause of this issue
    was the system running out of memory.
  prefs: []
  type: TYPE_NORMAL
- en: We could see by investigating the log files that the two processes using the
    most memory on the system were Apache and a custom application named `processor`.
    Furthermore, by identifying these processes, we were able to make a long-term
    recommendation to prevent this issue from re-occurring.
  prefs: []
  type: TYPE_NORMAL
- en: On top of all this, we learned quite a bit about what happens when Linux systems
    run out of memory.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will put everything you have learned this far to the
    test by performing a root cause analysis of an unresponsive system.
  prefs: []
  type: TYPE_NORMAL
