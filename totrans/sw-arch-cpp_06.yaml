- en: Architectural and System Design
  prefs: []
  type: TYPE_NORMAL
- en: 'Patterns help us deal with complexity. At the level of a single software component,
    you can use software patterns such as the ones described by the four authors of
    the book (better known as the *Gang of Four*) *Design Patterns: Elements of Reusable
    Object-Oriented Software*. When we move higher up and start looking at the architecture
    between different components, knowing when and how to apply architectural patterns
    can go a long way.'
  prefs: []
  type: TYPE_NORMAL
- en: There are countless such patterns that are useful for different scenarios. In
    fact, to even get to know all of them, you would need to read more than just one
    book. That being said, we selected several patterns for this book, suited for
    achieving various architectural goals.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we'll introduce you to a few concepts and fallacies related
    to architectural design; we'll show when to use the aforementioned patterns and
    how to design high-quality components that are easy to deploy.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: The different service models and when to use each of them
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to avoid the fallacies of distributed computing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The outcomes of the CAP theorem and how to achieve eventual consistency
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Making your system fault-tolerant and available
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integrating your system
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Achieving performance at scale
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying your system
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Managing your APIs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you'll know how to design your architecture to provide
    several important qualities, such as fault tolerance, scalability, and deployability.
    Before that, let's first learn about two inherent aspects of distributed architectures.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The code from this chapter requires the following tools to build and run:'
  prefs: []
  type: TYPE_NORMAL
- en: Docker
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Docker Compose
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The source code snippets from the chapter can be found at [https://github.com/PacktPublishing/Software-Architecture-with-Cpp/tree/master/Chapter04](https://github.com/PacktPublishing/Software-Architecture-with-Cpp/tree/master/Chapter04).
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the peculiarities of distributed systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are many types of different software systems, each of them suited for
    different scenarios, built for different needs, and using different sets of assumptions.
    Writing and deploying a classical, standalone desktop application is nothing like
    writing and deploying a microservice that needs to communicate with many others
    over a network.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we'll go through the various models that you can use to deploy
    your software, the common mistakes that people should avoid when creating distributed
    systems, and some of the compromises people need to make to create such systems
    successfully.
  prefs: []
  type: TYPE_NORMAL
- en: Different service models and when to use them
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's first start with service models. When designing a bigger system, you need
    to decide how much of the infrastructure you will manage versus how much you can
    build upon existing building blocks. Sometimes, you might want to leverage existing
    software without the need to manually deploy an app or back up data, for example,
    by using Google Drive through its API as storage for your app. Other times, you
    can rely on an existing cloud platform such as Google's App Engine to deploy your
    solution without the need to worry about providing a language runtime or databases.
    If you can decide to deploy everything in your own way, you can either leverage
    an infrastructure from a cloud provider or use your company's one.
  prefs: []
  type: TYPE_NORMAL
- en: Let's discuss the different models and where each can be useful.
  prefs: []
  type: TYPE_NORMAL
- en: On-premises model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The classical way, and the only way available in the pre-cloud era, is to just
    deploy everything on your own premises. You need to buy all the hardware and software
    required and make sure it will provide enough capacity for your needs. If you're
    working for a start-up company, this may be a big upfront cost. Along with the
    growth of your userbase, you need to buy and set up more resources so that your
    service can deal even with the occasional spikes in load. All this means you need
    to predict the growth of your solution and act proactively, as there's no way
    you could just automatically scale depending on the current load.
  prefs: []
  type: TYPE_NORMAL
- en: Even in the cloud era, deploying on-premises is still useful and often spotted
    in the wild. Sometimes you're dealing with data that shouldn't, or even can't,
    leave your company's premises, either due to data privacy issues or compliance
    ones. Other times, you need to have as little latency as possible and you need
    your own data center to do so. Sometimes you may calculate the costs and decide
    that in your case, on-premises will be cheaper than a cloud solution. Last, but
    not least, your company might just already have an existing data center that you
    can use.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying on-premises doesn't mean you need to have a monolith system. Often,
    companies have their own private clouds deployed on-premises. This helps to cut
    costs by better utilization of the available infrastructure. You can also mix
    a private cloud solution with one of the other service models, which can be useful
    when you need that extra capacity from time to time. This is called a **hybrid
    deployment** and is offered by all major cloud providers as well as provided by
    OpenStack's Omni project.
  prefs: []
  type: TYPE_NORMAL
- en: Infrastructure as a Service (IaaS) model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Speaking of other models, the most basic cloud service model is called **Infrastructure
    as a Service** (**IaaS**). It''s also the most similar to on-premises: you can
    think of IaaS as a way to have a virtual data center. As the name suggests, the
    cloud provider offers you a slice of the infrastructure they host, which consists
    of three types of resources:'
  prefs: []
  type: TYPE_NORMAL
- en: Compute, such as virtual machines, containers, or bare-metal machines (excluding
    operating systems)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Networking, which aside from the network itself includes DNS servers, routing,
    and firewalls
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Storage, including backup and recovery capabilities
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It''s still up to you to provide all the software: operating systems, middleware,
    and your applications.'
  prefs: []
  type: TYPE_NORMAL
- en: IaaS can be used in scenarios ranging from hosting websites (might be cheaper
    than traditional web hosting), through storage (for example, Amazon's S3 and Glacier
    services), to high-performance computing and big data analysis (requires huge
    computing power). Some companies use it to quickly set up and purge test and development
    environments when needed.
  prefs: []
  type: TYPE_NORMAL
- en: Using IaaS instead of on-premises infrastructure can be a cheap way to test
    new ideas while saving you the time needed for configuration.
  prefs: []
  type: TYPE_NORMAL
- en: 'If your service observes spikes in usage, for example, during the weekends,
    you might want to leverage your cloud''s automatic scaling capabilities: scale
    up when needed and scale back down later to save money.'
  prefs: []
  type: TYPE_NORMAL
- en: IaaS solutions are offered by all the popular cloud service providers.
  prefs: []
  type: TYPE_NORMAL
- en: A similar concept, sometimes thought of as a subset of IaaS, is **Containers
    as a Service** (**CaaS**). In CaaS, instead of bare-metal systems and virtual
    machines, the service provides you with containers and orchestration capabilities
    that you can use to build your own container clusters. CaaS offerings can be found
    with Google Cloud Platform and AWS, among others.
  prefs: []
  type: TYPE_NORMAL
- en: Platform as a Service (PaaS) model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If the infrastructure itself is not enough for your needs, you can use the **Platform
    as a Service** (**PaaS**) model instead. In this model, the cloud service provider
    manages not only the infrastructure (just like in IaaS), but also the operating
    systems, any required middleware, and the runtime – the platform that you will
    deploy your software on.
  prefs: []
  type: TYPE_NORMAL
- en: Often a PaaS solution will provide you with app versioning capabilities, service
    monitoring and discovery, database management, business intelligence, and even
    development tools.
  prefs: []
  type: TYPE_NORMAL
- en: 'With PaaS, you''re covered throughout the whole development pipeline: from
    building and testing to deploying, updating, and managing your service. However,
    PaaS solutions are more costly than IaaS offerings. On the other hand, with the
    whole platform provided, you can cut the costs and time to develop parts of your
    software and easily provide the same setup for development teams scattered around
    the globe.'
  prefs: []
  type: TYPE_NORMAL
- en: All main cloud providers have their own offerings, for example, Google App Engine
    or Azure App Service. There are also independent ones, such as Heroku.
  prefs: []
  type: TYPE_NORMAL
- en: Aside from the more generic PaaS, there's also **Communications Platform as
    a Service** (**CPaaS**), in which you're provided with the whole communications
    backend, including audio and video, which you can integrate into your solution.
    This technology allows you to easily provide video-enabled help desks or just
    integrate live chats into your apps.
  prefs: []
  type: TYPE_NORMAL
- en: Software as a Service (SaaS) model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Sometimes you might not want to develop a software component on your own and
    just want to use an existing one. **Software as a Service** (**SaaS**) basically
    gives you a hosted application. With SaaS, you don't need to worry about either
    the infrastructure or the platform built upon it, and not even about the software
    itself. The provider is responsible for installing, running, updating, and maintaining
    the whole software stack, as well as backups, licensing, and scaling.
  prefs: []
  type: TYPE_NORMAL
- en: There's quite a variety to what software you can get in the SaaS model. Examples
    vary from office suites such as Office 365 and Google Docs to messaging software
    such as Slack, through **Customer Relationship Management** (**CRM**) systems,
    and span even to gaming solutions such as cloud gaming services, allowing you
    to play resource-hungry video games hosted on the cloud.
  prefs: []
  type: TYPE_NORMAL
- en: Usually, to access such services, all you need is a browser, so this can be
    a great step in providing remote work capabilities for your employees.
  prefs: []
  type: TYPE_NORMAL
- en: You can create your own SaaS applications and provide them to users either by
    deploying them however you like, or through means such as AWS Marketplace.
  prefs: []
  type: TYPE_NORMAL
- en: '**Function as a Service (**FaaS) model and serverless architecture'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With the advent of cloud-native, another model that is growing in popularity
    is **Function as a Service** (**FaaS**). It can be helpful if you want to achieve
    a serverless architecture. With FaaS, you get a platform (similarly to PaaS) on
    which you can run short-lived applications, or functions.
  prefs: []
  type: TYPE_NORMAL
- en: With PaaS, you typically always need to have at least one instance of your service
    running, while in FaaS you can run them only when they're actually needed. Running
    your function can make the time to handle requests longer (measured in seconds;
    you need to launch the function after all). However, some of those requests can
    be cached to reduce both the latency and costs. Speaking about costs, FaaS can
    get way more expensive than PaaS if you run the functions for a long time, so
    you must do the math when designing your system.
  prefs: []
  type: TYPE_NORMAL
- en: If used correctly, FaaS abstracts away the servers from the developers, can
    reduce your costs, and can provide you with better scalability, as it can be based
    on events, not resources. This model is commonly used for running prescheduled
    or manually triggered tasks, processing batches or streams of data, and handling
    incoming, not-so-urgent requests. A few popular providers of FaaS are AWS Lambda,
    Azure Functions, and Google Cloud Functions.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we've covered the common service models in the cloud, let's discuss
    some of the wrong assumptions people make when designing distributed systems.
  prefs: []
  type: TYPE_NORMAL
- en: Avoiding the fallacies of distributed computing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When people new to distributed computing begin their journey with designing
    such systems, they tend to forget or ignore a few aspects of such systems. Although
    they were first noticed back in the 90s, they remain current today.
  prefs: []
  type: TYPE_NORMAL
- en: The fallacies are discussed in the following sub-sections. Let's have a quick
    rundown on each of them.
  prefs: []
  type: TYPE_NORMAL
- en: The network is reliable
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Networking equipment is designed for long years of flawless operation. Despite
    that, many things can still cause packet loss, ranging from power outages through
    poor wireless networking signal, configuration errors, someone tripping over a
    cable, or even animals biting through wires. For instance, Google had to protect
    their underwater cables with Kevlar because they were being bitten by sharks (yes,
    really). You should always assume that data can get lost somewhere over the network.
    Even if that doesn't happen, software issues can still occur on the other side
    of the wire.
  prefs: []
  type: TYPE_NORMAL
- en: To fend off such issues, be sure you have a policy for automatically retrying
    failed network requests and a way to handle common networking issues. When retrying,
    try to not overload the other party and not commit the same transaction multiple
    times. You can use a message queue to store and retry sending for you.
  prefs: []
  type: TYPE_NORMAL
- en: Patterns such as circuit breaker, which we'll show later in this chapter, can
    also help. Oh, and be sure to not just wait infinitely, hogging up resources with
    each failed request.
  prefs: []
  type: TYPE_NORMAL
- en: Latency is zero
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Both the network and the services you're running have to take some time to respond
    even under normal conditions. Occasionally they'll have to take longer, especially
    when being under a bigger-than-average load. Sometimes instead of a few milliseconds,
    your requests can take seconds to complete.
  prefs: []
  type: TYPE_NORMAL
- en: Try to design your system so it doesn't wait on too many fine-grained remote
    calls, as each such call can add to your total processing time. Even in a local
    network, 10,000 requests for 1 record will be much slower than 1 request for 10,000
    records. To reduce network latency, consider sending and handling requests in
    bulk. You can also try to hide the cost of small calls by doing other processing
    tasks while waiting for their results.
  prefs: []
  type: TYPE_NORMAL
- en: Other ways to deal with latency are to introduce caches, push the data in a
    publisher-subscriber model instead of waiting for requests, or deploy closer to
    the customers, for example, by using **Content Delivery Networks** (**CDN**s).
  prefs: []
  type: TYPE_NORMAL
- en: Bandwidth is infinite
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When adding a new service to your architecture, make sure you take note of how
    much traffic it's going to use. Sometimes you might want to reduce the bandwidth
    by compressing the data or by introducing a throttling policy.
  prefs: []
  type: TYPE_NORMAL
- en: This fallacy also has to do with mobile devices. If the signal is weak, often
    the network will become the bottleneck. This means the amount of data a mobile
    app uses should generally be kept low. Using the *Backends for Frontends* pattern
    described in [Chapter 2](6fbde08c-f8a2-475d-9984-2a9882a761b4.xhtml)*, Architectural
    Styles,* can often help save precious bandwidth.
  prefs: []
  type: TYPE_NORMAL
- en: 'If your backend needs to transfer lots of data between some components, try
    to make sure such components are close together: don''t run them in separate data
    centers. With databases, this often boils down to better replication. Patterns
    such as CQRS (discussed later in this chapter) are also handy.'
  prefs: []
  type: TYPE_NORMAL
- en: The network is secure
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This is a dangerous fallacy. A chain is only as strong as its weakest link,
    and unfortunately, there are many links in distributed systems. Here are a few
    ways to make those links stronger:'
  prefs: []
  type: TYPE_NORMAL
- en: Be sure to always apply security patches to every component that you use, to
    your infrastructure, operating systems, and other components.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Train your personnel and try to protect your system from the human factor; sometimes
    it's a rogue employee that compromises a system.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If your system will be online, it will get attacked, and it's possible that
    a breach will happen at one point. Be sure to have a written plan on how to react
    to such events.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You might have heard about the defense in depth principle. It boils down to
    having different checks for different parts of your system (your infrastructure,
    your applications, and so on) so that when a breach happens, its range, and the
    associated damage, will be limited.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use firewalls, certificates, encryption, and proper authentication.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For more on security, refer to [Chapter 10](7b56cb2d-e7f9-4cba-b24b-4a875cf8d520.xhtml),
    *Security in Code and Deployment*.
  prefs: []
  type: TYPE_NORMAL
- en: Topology doesn't change
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This one became especially true in the microservices era. Autoscaling and the
    emergence of the *cattle, not pets* approach to managing infrastructure mean that
    the topology will constantly change. This can affect latency and bandwidth, so
    some of this fallacy's outcomes are the same as the ones described earlier.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, the mentioned approach also comes with guidelines on how to effectively
    manage your *herd* of servers. Relying on hostnames and DNS instead of hardcoding
    IPs is a step in the right direction, and service discovery, described later in
    this book, is another one. A third, even bigger, step is to always assume your
    instances can fail and automate reacting to such scenarios. Netflix's *Chaos Monkey
    tool* can also help you test your preparedness.
  prefs: []
  type: TYPE_NORMAL
- en: There is one administrator
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The knowledge about distributed systems, due to their nature, is often distributed
    itself. Different people are responsible for the development, configuration, deployment,
    and administration of such systems and their infrastructure. Different components
    are often upgraded by different people, not necessarily in sync. There's also
    the so-called bus factor, which in short is the risk factor for a key project
    member being hit by a bus.
  prefs: []
  type: TYPE_NORMAL
- en: How do we deal with all of this? The answer consists of a few parts. One of
    them is the DevOps culture. By facilitating close collaboration between development
    and operations, people share the knowledge about the system, thus reducing the
    bus factor. Introducing continuous delivery can help with upgrading the project
    and keeping it always up.
  prefs: []
  type: TYPE_NORMAL
- en: Try to model your system to be loosely coupled and backward compatible, so upgrades
    of components don't require other components to be upgraded too. An easy way to
    decouple is by introducing messaging between them, so consider adding a queue
    or two. It will help you with downtime during upgrades as well.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, try to monitor your system and gather logs in a centralized place.
    Decentralization of your system shouldn't mean you now need to manually look at
    logs at a dozen different machines. The **ELK** (**Elasticsearch, Logstash, Kibana**)
    stack is invaluable for this. Grafana, Prometheus, Loki, and Jaeger are also very
    popular, especially with Kubernetes. If you're looking for something more lightweight
    than Logstash, consider Fluentd and Filebeat, especially if you're dealing with
    containers.
  prefs: []
  type: TYPE_NORMAL
- en: Transport cost is zero
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This fallacy is important for planning your project and its budget. Building
    and maintaining a network for a distributed system costs both time and money,
    regardless of whether you deploy on-premises or in the cloud – it's just a matter
    of when you pay the cost. Try to estimate the costs of the equipment, the data
    to be transferred (cloud providers charge for this), and the required manpower.
  prefs: []
  type: TYPE_NORMAL
- en: If you're relying on compression, be wary that while this reduces networking
    costs, it can increase the price for your compute. In general, using binary APIs
    such as gRPC-based will be cheaper (and faster) than JSON-based ones, and those
    are still cheaper than XML. If you send images, audio, or video, it's a must to
    estimate how much this will cost you.
  prefs: []
  type: TYPE_NORMAL
- en: The network is homogeneous
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Even if you plan what hardware to have and what software to run on your network,
    it's easy to end up with at least some heterogeneity. A slightly different configuration
    on some of the machines, a different communication protocol used by that legacy
    system that you need to integrate with, or different mobile phones sending requests
    to your system are just a few examples of this. Another one is extending your
    on-premises solution by using additional workers in the cloud.
  prefs: []
  type: TYPE_NORMAL
- en: Try to limit the number of protocols and formats used, strive to use standard
    ones, and avoid vendor lock-in to ensure your system can still communicate properly
    in such heterogeneous environments. Heterogeneity can also mean differences in
    resiliency. Try to use the circuit breaker pattern along with retries to handle
    this.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we've discussed all the fallacies, let's discuss yet another pretty
    important aspect of distributed architectures.
  prefs: []
  type: TYPE_NORMAL
- en: CAP theorem and eventual consistency
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To design successful systems that spread across more than one node, you need
    to know and use certain principles. One of them is the **CAP theorem**. It''s
    about one of the most important choices you need to make when designing a distributed
    system and owes its name to the three properties a distributed system can have.
    They are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Consistency**: Every read would get you the data after the most recent write
    (or an error).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Availability**: Every request will get a non-error response (without the
    guarantee that you''ll get the most recent data).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Partition tolerance**: Even if a network failure occurs between two nodes,
    the system as a whole will continue working.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In essence, the theorem states that you can pick at most two of those three
    properties for a distributed system.
  prefs: []
  type: TYPE_NORMAL
- en: As long as the system operates properly, it looks like all three of the properties
    can be satisfied. However, as we know from looking at the fallacies, the network
    is unreliable, so partitions will occur. In such cases, a distributed system should
    still operate properly. This means the theorem actually makes you choose between
    delivering partition tolerance and consistency (that is CP), or partition tolerance
    and availability (that is AP). Usually, the latter is the better choice. If you
    want to choose CA, you have to remove the network entirely and be left with a
    single-node system.
  prefs: []
  type: TYPE_NORMAL
- en: If under a partition, you decide to deliver consistency, you will have to either
    return an error or risk timeouts when waiting for the data to be consistent. If
    you choose availability over consistency, you risk returning stale data – the
    latest writes might be unable to propagate across the partition.
  prefs: []
  type: TYPE_NORMAL
- en: Both those approaches are suited for different needs. If your system requires
    atomic reads and writes, for instance, because a customer could lose their money,
    go with CP. If your system must continue operating under partitions, or you can
    allow eventual consistency, go with AP.
  prefs: []
  type: TYPE_NORMAL
- en: Okay, but what is eventual consistency? Let's discuss the different levels of
    consistency to understand this.
  prefs: []
  type: TYPE_NORMAL
- en: In a system offering strong consistency, each write is synchronously propagated.
    This means all reads will always see the latest writes, even at the cost of higher
    latency or lower availability. This is the type that relational DBMSes offer (based
    on ACID guarantees) and is best suited for systems that require transactions.
  prefs: []
  type: TYPE_NORMAL
- en: In a system offering eventual consistency, on the other hand, you only guarantee
    that after a write, reads will eventually see the change. Usually, *eventually*
    means in a couple of milliseconds. This is due to the asynchronous nature of data
    replication in such systems, as opposed to the synchronous propagation from the
    previous paragraph. Instead of providing ACID guarantees, for example, using an
    RDBMS, here we have BASE semantics, often provided by NoSQL databases.
  prefs: []
  type: TYPE_NORMAL
- en: For a system to be asynchronous and eventually consistent (as AP systems often
    are), it's needed to have a way to solve state conflicts. A common way to do so
    is to exchange updates between instances and choose either the first or the last
    write as the accepted one.
  prefs: []
  type: TYPE_NORMAL
- en: Let's now discuss two related patterns that can help in achieving eventual consistency.
  prefs: []
  type: TYPE_NORMAL
- en: Sagas and compensating transactions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The saga pattern is useful when you need to perform distributed transactions.
  prefs: []
  type: TYPE_NORMAL
- en: Before the microservice era, if you had one host with one database, you could
    rely on the database engine to do the transaction for you. With multiple databases
    on one host, you could use **Two-Phase Commits** (**2PCs**) to do so. With 2PCs,
    you would have a coordinator, who would first tell all the databases to prepare,
    and once they all report being ready, it would tell them all to commit the transaction.
  prefs: []
  type: TYPE_NORMAL
- en: Now, as each microservice likely has its own database (and it should if you
    want scalability), and they're spanned all over your infrastructure, you can no
    longer rely on simple transactions and 2PCs (losing this ability often means you
    no longer want an RDBMS, as NoSQL databases can be much faster).
  prefs: []
  type: TYPE_NORMAL
- en: Instead, you can use the saga pattern. Let's demonstrate it in an example.
  prefs: []
  type: TYPE_NORMAL
- en: 'Imagine you want to create an online warehouse that tracks how much supply
    it has and allows payment by credit cards. To process an order, above all other
    services, you need three: one for processing the order, one for reserving the
    supplies, and one for charging the card.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, there are two ways the saga pattern can be implemented: **choreography-based**
    (also called **event-based**) and **orchestration-based** (also called **command-based**).'
  prefs: []
  type: TYPE_NORMAL
- en: Choreography-based sagas
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In the first case, the first part of the saga would be the order processing
    service sending an event to the supply service. This one would do its part and
    send another event to the payment service. The payment service would then send
    yet another event back to the order service. This would complete the transaction
    (the saga), and the order could now be happily shipped.
  prefs: []
  type: TYPE_NORMAL
- en: If the order service would want to track the state of the transaction, it would
    simply need to listen to all those events as well.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, sometimes the order would be impossible to complete, and a rollback
    would need to happen. In this case, each step of the saga would need to be rolled
    back separately and carefully, as other transactions could run in parallel, for
    example, modifying the supply state. Such rollbacks are called **compensating
    transactions**.
  prefs: []
  type: TYPE_NORMAL
- en: This way of implementing the saga pattern is pretty straightforward, but if
    there any many dependencies between the involved services it might be better to
    use the *orchestration* approach. Speaking of which, let's now say a few words
    about this second approach to sagas.
  prefs: []
  type: TYPE_NORMAL
- en: Orchestration-based sagas
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In this case, we'll need a message broker to handle communication between our
    services, and an orchestrator that would coordinate the saga. Our order service
    would send a request to the orchestrator, which would then send commands to both
    the supply and payment services. Each of those would then do their part and send
    replies back to the orchestrator, through a reply channel available at the broker.
  prefs: []
  type: TYPE_NORMAL
- en: In this scenario, the orchestrator has all the logic needed to, well, orchestrate
    the transaction, and the services themselves don't need to be aware of any other
    services taking part in the saga.
  prefs: []
  type: TYPE_NORMAL
- en: If the orchestrator is sent a message that one of the services failed, for example,
    if the credit card has expired, it would then need to start the rollback. In our
    case, it would again use the broker to send an appropriate rollback command to
    specific services.
  prefs: []
  type: TYPE_NORMAL
- en: Okay, that's enough about eventual consistency for now. Let's now switch to
    other topics related to availability.
  prefs: []
  type: TYPE_NORMAL
- en: Making your system fault tolerant and available
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Availability and fault tolerance are software qualities that are at least somewhat
    important for every architecture. What's the point of creating a software system
    if the system can't be reached? In this section, we'll learn what exactly those
    terms mean and a few techniques to provide them in your solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Calculating your system's availability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Availability is the percentage of the time that a system is up, functional,
    and reachable. Crashes, network failures, or extremely high load (for example,
    from a DDoS attack) that prevents the system from responding can all affect its
    availability.
  prefs: []
  type: TYPE_NORMAL
- en: 'Usually, it''s a good idea to strive for as high a level of availability as
    possible. You may stumble upon the term *counting the nines*, as availability
    is often specified as 99% (two nines), 99.9% (three), and so on. Each additional
    nine is much harder to obtain, so be careful when making promises. Take a look
    at the following table to see how much downtime you could afford if you specified
    it on a monthly basis:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Downtime/month | Uptime |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 7 hours 18 minutes | 99% (“two nines”) |'
  prefs: []
  type: TYPE_TB
- en: '| 43 minutes 48 seconds | 99.9% (“three nines”) |'
  prefs: []
  type: TYPE_TB
- en: '| 4 minutes 22.8 seconds | 99.99% (“four nines”) |'
  prefs: []
  type: TYPE_TB
- en: '| 26.28 seconds | 99.999% (“five nines”) |'
  prefs: []
  type: TYPE_TB
- en: '| 2.628 seconds | 99.9999% (“six nines”) |'
  prefs: []
  type: TYPE_TB
- en: '| 262.8 ms | 99.99999% (“seven nines”) |'
  prefs: []
  type: TYPE_TB
- en: '| 26.28 ms | 99.999999% (“eight nines”) |'
  prefs: []
  type: TYPE_TB
- en: '| 2.628 ms | 99.9999999% (“nine nines”) |'
  prefs: []
  type: TYPE_TB
- en: A common practice for cloud applications is to provide a **Service-Level Agreement**
    (**SLA**), which specifies how much downtime can occur per a given period of time
    (for example, a year). An SLA for your cloud service will strongly depend on the
    SLAs of the cloud services you build upon.
  prefs: []
  type: TYPE_NORMAL
- en: To calculate a compound availability between two services that need to cooperate,
    you should just multiply their uptimes. This means if you have two services with
    99.99% availability, their compound availability will be 99.99% * 99.99% = 99.98%.
    To calculate the availability of redundant services (such as two independent regions),
    you should multiply their unavailability. For instance, if two regions have 99.99%
    availability, their total unavailability will be (100% – 99.99%) * (100% – 99.99%)
    = 0.01% * 0.01% = 0.0001%, so their compound availability is 99.9999%.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, it's impossible to provide 100% availability. Failures do occur
    from time to time, so let's learn how to make your system tolerate them.
  prefs: []
  type: TYPE_NORMAL
- en: Building fault-tolerant systems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Fault tolerance is a system's ability to detect such failures and to handle
    them gracefully. It's essential that your cloud-based services are resilient,
    as due to the nature of the cloud, many different things can suddenly go south.
    Good fault tolerance can help your service's availability.
  prefs: []
  type: TYPE_NORMAL
- en: 'Different types of issues require different handling: from prevention, through
    detection, to minimizing the impact. Let''s start with common ways to avoid having
    a single point of failure.'
  prefs: []
  type: TYPE_NORMAL
- en: Redundancy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the most basic preventions is introducing **redundancy**. Similar to
    how you can have a spare tire for your car, you can have a backup service that
    takes over when your primary server goes down. This stepping-in is also known
    as **failover**.
  prefs: []
  type: TYPE_NORMAL
- en: How does the backup server know when to step in? One way to implement this is
    by using the heartbeat mechanism described in the *Detecting faults* section.
  prefs: []
  type: TYPE_NORMAL
- en: To make the switch faster, you can send all the messages that are going into
    the primary server also to the backup one. This is called a **hot standby**, as
    opposed to a cold one – initializing from zero. A good idea in such a case is
    to stay one message behind, so if a *poisoned* message kills the primary server,
    the backup one can simply reject it.
  prefs: []
  type: TYPE_NORMAL
- en: The preceding mechanism is called an **active-passive** (or **master-slave**)
    failover, as the backup server doesn't handle incoming traffic. If it did, we
    would have an **active-active** (or **master-master**) failover. For more on active-active
    architectures, refer to the last link in the *Further reading* section.
  prefs: []
  type: TYPE_NORMAL
- en: Be sure you don't lose any data when the failover happens. Using a message queue
    with backing storage may help with this.
  prefs: []
  type: TYPE_NORMAL
- en: Leader election
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: It's also important for both the servers to know which one is which – if both
    start behaving as primary instances, you'll likely be in trouble. Choosing the
    primary server is called the leader election pattern. There are a few ways to
    do so, for example, by introducing a third-party arbiter, by racing to take exclusive
    ownership of a shared resource, by choosing the instance with the lowest rank,
    or by using algorithms such as bully election or token ring election.
  prefs: []
  type: TYPE_NORMAL
- en: 'Leader election is also an essential part of the next related concept: achieving
    consensus.'
  prefs: []
  type: TYPE_NORMAL
- en: Consensus
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: If you want your system to operate even when network partitions happen or some
    instances of your service experience faults, you need a way for your instances
    to reach consensus. They must agree what values to commit and often in what order.
    A simple approach is by allowing each instance to vote on the correct state. However,
    in some cases this is not enough to reach a consensus correctly or at all. Another
    approach would be to elect a leader and let it propagate its value. Because it's
    not easy to implement such algorithms by hand, we'd recommend using popular industry-proven
    consensus protocols such as Paxos and Raft. The latter is growing in popularity
    as it is simpler and easier to understand.
  prefs: []
  type: TYPE_NORMAL
- en: Let's now discuss another way to prevent your system from faulting.
  prefs: []
  type: TYPE_NORMAL
- en: Replication
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This one is especially popular with databases, and it helps with scaling them,
    too. **Replication** means you will run a few instances of your service in parallel
    with duplicated data, all handling incoming traffic.
  prefs: []
  type: TYPE_NORMAL
- en: Don't confuse replication with sharding. The latter doesn't require any data
    redundancy, but can often bring you great performance at scale. If you're using
    Postgres, we recommend you try out Citus ([https://www.citusdata.com](https://www.citusdata.com)).
  prefs: []
  type: TYPE_NORMAL
- en: In terms of databases, there are two ways you can replicate.
  prefs: []
  type: TYPE_NORMAL
- en: Master-slave replication
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In this scenario, all the servers are able to perform read-only operations,
    but there's only one master server that can also write. The data is replicated
    from the master, through the slaves, either in a one-to-many topology or using
    a tree topology. If the master fails, the system can still operate in read-only
    mode until this fault is remediated.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-master replication
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: You can also have a system with multiple master servers. If there are two servers,
    you have a *master-master replication* scheme. If one of the servers dies, the
    others can still operate normally. However, now you either need to synchronize
    the writes or provide looser consistency guarantees. Also, you need to provide
    a **load balancer**.
  prefs: []
  type: TYPE_NORMAL
- en: Examples of such replication include Microsoft's Active Directory, OpenLDAP,
    Apache's CouchDB, or Postgres-XL.
  prefs: []
  type: TYPE_NORMAL
- en: Let's now discuss two ways to prevent faults caused by too high a load.
  prefs: []
  type: TYPE_NORMAL
- en: Queue-based load leveling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This tactic is aimed at reducing the impact of sudden spikes in your system's
    load. Flooding a service with requests can cause performance issues, reliability
    ones, and even dropping valid requests. Once again, queues are there to save the
    day.
  prefs: []
  type: TYPE_NORMAL
- en: To implement this pattern, we just need to introduce a queue for the incoming
    requests to be added asynchronously. You can use Amazon's SQS, Azure's Service
    Bus, Apache Kafka, ZeroMQ, or other queues to achieve that.
  prefs: []
  type: TYPE_NORMAL
- en: Now, instead of having spikes in incoming requests, the load will get averaged.
    Our service can grab the requests from the said queue and process them without
    even knowing that the load was increased. Simple as that.
  prefs: []
  type: TYPE_NORMAL
- en: If your queue is performant and your tasks can be parallelized, a side benefit
    of this pattern would be better scalability.
  prefs: []
  type: TYPE_NORMAL
- en: Also, if your service isn't available, the requests will still get added into
    the queue for said service to process when it recovers, so this may be a way to
    help with bumping the availability.
  prefs: []
  type: TYPE_NORMAL
- en: If the requests come infrequently, consider implementing your service as a function
    that runs only when there are items in the queue to save costs.
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind that when using this pattern, the overall latency will increase
    by the addition of the queue. Apache Kafka and ZeroMQ should yield low latency,
    but if that's a deal-breaker, there's yet another way to deal with increased load.
  prefs: []
  type: TYPE_NORMAL
- en: Back pressure
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If the load remains high, chances are you'll have more tasks than you're able
    to handle. This can cause cache misses and swapping if the requests will no longer
    fit into memory, as well as dropping requests and other nasty things. If you expect
    a heavy load, applying back pressure might be a great way to deal with it.
  prefs: []
  type: TYPE_NORMAL
- en: In essence, back pressure means that instead of putting more pressure on our
    service with each incoming request, we push back into the caller so it needs to
    handle the situation. There are a few different ways to do so.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, we can block our thread that receives network packets. The caller
    will then see that it is unable to push the request to our service – instead,
    we push the pressure up the stream.
  prefs: []
  type: TYPE_NORMAL
- en: Another way is to recognize greater load and simply return an error code, for
    example, 503\. You can model your architecture so that this is done for you by
    another service. One such service is the Envoy Proxy ([https://envoyproxy.io](https://envoyproxy.io)),
    which can come in handy on many other occasions too.
  prefs: []
  type: TYPE_NORMAL
- en: Envoy can apply back pressure based on predefined quotas, so your service will
    actually never get overloaded. It can also measure the time it takes to process
    requests and apply back pressure only if it goes above a certain threshold. There
    are many other cases for which a variety of error codes will get returned. Hopefully,
    the caller has a plan on what to do if the pressure goes back on them.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know how to prevent faults, let's learn how to detect them once
    they occur.
  prefs: []
  type: TYPE_NORMAL
- en: Detecting faults
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Proper and fast fault detection can save you a lot of trouble, and often money.
    There are many ways to detect faults tailored to different needs. Let's go over
    a selection of them.
  prefs: []
  type: TYPE_NORMAL
- en: Sidecar design pattern
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Since we were discussing Envoy, it might be worth saying that it's an example
    of the **sidecar design pattern**. This pattern is useful in many more cases than
    just error prevention and detection, and Envoy is a great example of this.
  prefs: []
  type: TYPE_NORMAL
- en: In general, sidecars allow you to add a number of capabilities to your services
    without the need to write additional code. Similarly, as a physical sidecar can
    be attached to a motorcycle, a software sidecar can be attached to your service
    – in both cases extending the offered functionality.
  prefs: []
  type: TYPE_NORMAL
- en: How can a sidecar be helpful in detecting faults? First of all, by providing
    health checking capabilities. When it comes to passive health checking, Envoy
    can detect whether any instance in a service cluster has started behaving badly.
    This is called **outlier detection**. Envoy can look for consecutive 5XX error
    codes, gateway failures, and so on. Aside from detecting such faulty instances,
    it can eject them so the overall cluster remains healthy.
  prefs: []
  type: TYPE_NORMAL
- en: Envoy also offers active health checking, meaning it can probe the service itself
    instead of just observing its reactions to incoming traffic.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout this chapter, we'll show a few other usages for the sidecar pattern
    in general, and Envoy in particular. Let's now discuss the next mechanism of fault
    detection.
  prefs: []
  type: TYPE_NORMAL
- en: Heartbeat mechanism
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the most common ways of fault detection is through the **heartbeat mechanism**.
    A **heartbeat** is a signal or a message that is sent on regular intervals (usually
    a few seconds) between two services.
  prefs: []
  type: TYPE_NORMAL
- en: If a few consecutive heartbeats are missing, the receiving service can consider
    the sending service *dead*. In the case of our primary-backup service pair from
    a few sections previously, this can cause a failover to happen.
  prefs: []
  type: TYPE_NORMAL
- en: When implementing a heartbeat mechanism, be sure that it's reliable. False alarms
    can be troublesome, as the services may get confused, for example, about which
    one should be the new master. A good idea might be to provide a separate endpoint
    just for heartbeats, so it won't be as easily affected by the traffic on the regular
    endpoints.
  prefs: []
  type: TYPE_NORMAL
- en: Leaky bucket counter
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another way to detect faults is by adding a so-called **leaky bucket** counter.
    With each error, the counter would get incremented, and after a certain threshold
    is reached (the bucket is full), a fault would get signaled and handled. In regular
    time intervals, the counter would get decreased (hence, leaky bucket). This way,
    the situation would only be considered a fault if many errors occurred in a short
    time period.
  prefs: []
  type: TYPE_NORMAL
- en: This pattern can be useful if in your case it's normal to sometimes have errors,
    for instance, if you're dealing with networking.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know how to detect faults, let's learn what to do once they happen.
  prefs: []
  type: TYPE_NORMAL
- en: Minimizing the impact of faults
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It takes time to detect an ongoing fault, and it takes even more of this precious
    resource to resolve it. This is why you should strive to minimize the impact of
    faults. Here are a few ways that can help.
  prefs: []
  type: TYPE_NORMAL
- en: Retrying the call
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When your application calls another service, sometimes the call will fail. The
    simplest remedy for such a case is to just retry the call. If the fault was transient
    and you don't retry, that fault will likely get propagated through your system,
    making more damage than it should. Implementing an automated way to retry such
    calls can save you a lot of hassle.
  prefs: []
  type: TYPE_NORMAL
- en: Remember our sidecar proxy, Envoy? Turns out it can perform the automatic retries
    on your behalf, saving you from doing any changes to your sources.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, see this example configuration of a retry policy that can be
    added to a route in Envoy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This will make Envoy retry calls if they return errors such as the 503 HTTP
    code or gRPC errors that map to 5XX codes. There will be three retries, each considered
    failed if not finished within 2 seconds.
  prefs: []
  type: TYPE_NORMAL
- en: Avoiding cascading failures
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We mentioned that without retries the error will get propagated, causing a cascade
    of failures throughout the system. Let's now show more ways to prevent this from
    happening.
  prefs: []
  type: TYPE_NORMAL
- en: Circuit breaker
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The **circuit breaker pattern** is a very useful tool for this. It allows us
    to quickly notice that a service is unable to process requests, so the calls to
    it can be short-circuited. This can happen both close to the callee (Envoy provides
    such a capability), or on the caller side (with the additional benefit of shaving
    off time from the calls). In Envoy''s case, it can be as simple as adding the
    following to your config:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: In both cases, the load caused by the calls to the service may drop, which in
    some cases can help the service return to normal operation.
  prefs: []
  type: TYPE_NORMAL
- en: How do we implement a circuit breaker on the caller side? Once you've made a
    few calls, and, say, your leaky bucket overflows, you can just stop making new
    calls for a specified period of time (for example, until the bucket no longer
    overflows). Simple and effective.
  prefs: []
  type: TYPE_NORMAL
- en: Bulkhead
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Another way to limit fault from spreading is taken straight from the stockyards.
    When building ships, you usually don't want the ship to get full of water if a
    hole breaks in the hull. To limit the damage of such holes, you would partition
    the hull into bulkheads, each of which would be easy to isolate. In this case,
    only the damaged bulkhead would get filled with water.
  prefs: []
  type: TYPE_NORMAL
- en: The same principle applies to limiting the fault impact in software architecture.
    You can partition your instances into groups, and you can assign the resources
    they use into groups as well. Setting quotas can also be considered an example
    of this pattern.
  prefs: []
  type: TYPE_NORMAL
- en: Separate bulkheads can be created for different groups of users, which can be
    useful if you need to prioritize them or provide a different level of service
    to your critical consumers.
  prefs: []
  type: TYPE_NORMAL
- en: Geodes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The last way we'll show is called **Geodes**. The name comes from geographical
    nodes. It can be used when your service is deployed in multiple regions.
  prefs: []
  type: TYPE_NORMAL
- en: If a fault occurs in one region, you can just redirect the traffic to other,
    unaffected regions. This will of course make the latency much higher than if you'd
    made calls to other nodes in the same data center, but usually redirecting less
    critical users to remote regions is a much better choice than just failing their
    calls entirely.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you know how to provide availability and fault tolerance through your
    system's architecture, let's discuss how to integrate its components together.
  prefs: []
  type: TYPE_NORMAL
- en: Integrating your system
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A distributed system is not just isolated instances of your applications running
    unaware of the existing world. They constantly communicate with each other and
    have to be properly integrated together to provide the most value.
  prefs: []
  type: TYPE_NORMAL
- en: Much was already said on the topic of integration, so in this section, we'll
    try to showcase just a handful of patterns for effective integration of both entirely
    new systems, as well as new parts of the system that needs to coexist with other
    existing parts, often legacy ones.
  prefs: []
  type: TYPE_NORMAL
- en: To not make this chapter be a whole book on its own, let's start this section
    with a recommendation of an existing one. If you're interested in integration
    patterns, especially focused on messaging, then Gregor Hohpe and Bobby Woolf's
    *Enterprise Integration Patterns* book is a must-read for you.
  prefs: []
  type: TYPE_NORMAL
- en: Let's take a brief look at two patterns covered by this book.
  prefs: []
  type: TYPE_NORMAL
- en: Pipes and filters pattern
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first integration pattern that we'll discuss is called **pipes and filters**.
    Its purpose is to decompose a big processing task into a series of smaller, independent
    ones (called **filters**), which you can then connect together (using pipes, such
    as message queues). This approach gives you scalability, performance, and reusability.
  prefs: []
  type: TYPE_NORMAL
- en: Assume you need to receive and process an incoming order. You can do it in one
    big module, so you don't need extra communication, but the different functions
    of such a module would be hard to test and it would be harder to scale them well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead, you can split the order processing into separate steps, each handled
    by a distinct component: one for decoding, one for validating, another one for
    the actual processing of the order, and then yet another one for storing it somewhere.
    With this approach, you can now independently perform each of those steps, easily
    replace or disable them if needed, and reuse them for processing different types
    of input messages.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to process multiple orders at the same time, you can also pipeline
    your processing: while one thread validates a message, another thread decodes
    the next one, and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: The downside is that you need to use synchronized queues as your pipes, which
    introduces some overhead.
  prefs: []
  type: TYPE_NORMAL
- en: To scale one step of your processing, you might want to use this pattern along
    with the next one on our list.
  prefs: []
  type: TYPE_NORMAL
- en: Competing consumers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The idea of competing consumers is simple: you have an input queue (or a messaging
    channel) and a few instances of consumers that fetch and process items from the
    queue concurrently. Each of the consumers can process the message, so they compete
    with each other to be the receiver.'
  prefs: []
  type: TYPE_NORMAL
- en: This way, you get scalability, free load balancing, and resilience. With the
    addition of the queue, you now also have the **queue-based load leveling pattern**
    in place.
  prefs: []
  type: TYPE_NORMAL
- en: This pattern integrates effortlessly with priority queues if you need to shave
    latency from a request or just want a specific task submitted to your queue to
    be performed in a more urgent manner.
  prefs: []
  type: TYPE_NORMAL
- en: This pattern can get tricky to use if the ordering is important. The order in
    which your consumers receive and finish to process messages may vary, so make
    sure that either this doesn't impact your system, or you find a way to reorder
    the results later on. If you need to process messages in sequence, you might not
    be able to use this pattern.
  prefs: []
  type: TYPE_NORMAL
- en: Let's now see a few more patterns, this time to help us integrate with existing
    systems.
  prefs: []
  type: TYPE_NORMAL
- en: Transitioning from legacy systems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Developing a system from scratch can be a blissful experience. Development instead
    of maintenance and a possibility to use a bleeding-edge technology stack – what's
    not to like? Unfortunately, that bliss often ends when integrating with an existing,
    legacy system starts. Fortunately, though, there are some ways to ease that pain.
  prefs: []
  type: TYPE_NORMAL
- en: Anti-corruption layer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Introducing an **anti-corruption layer** can help your solution in painless
    integration with a legacy system that has different semantics. This additional
    layer is responsible for communication between those two sides.
  prefs: []
  type: TYPE_NORMAL
- en: Such a component allows your solution to be designed with more flexibility –
    without the need to compromise your technology stack nor architectural decisions.
    To achieve that requires only a minimal set of changes in the legacy system (or
    none, if the legacy system doesn't need to make calls to the new system).
  prefs: []
  type: TYPE_NORMAL
- en: For instance, if your solution is based on microservices, the legacy system
    can just communicate with the anti-corruption layer instead of locating and reaching
    each microservice directly. Any translations (for example, due to outdated protocol
    versions) are also done in the additional layer.
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind that adding such a layer can introduce latency and has to satisfy
    quality attributes for your solution, for example, scalability.
  prefs: []
  type: TYPE_NORMAL
- en: Strangler pattern
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The **strangler pattern** allows the gradual migration from a legacy system
    to a new one. While the anti-corruption layer we just looked at is useful for
    communication between the two systems, the strangler pattern is meant for providing
    services from both to the outside world.
  prefs: []
  type: TYPE_NORMAL
- en: 'Early in the migration process, the strangler facade will route most of the
    requests into the legacy system. During the migration, more and more calls can
    be forwarded into the new one, while *strangling* the legacy system more and more,
    limiting the functionality it offers. As the final step of the migration, the
    strangler, along with the legacy system, can be retired – the new system will
    now provide all the functionality:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7b429547-5775-483d-9f3d-c8d57ae2828d.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.1 – The strangling of a monolith. After the migration, the strangler
    can still be used as an entry point, or adapter, for legacy requests
  prefs: []
  type: TYPE_NORMAL
- en: This pattern can be overkill for small systems and can get tricky if the datastore
    should be shared or is for event-sourced systems. When adding it to your solution,
    be sure to plan for achieving the proper performance and scalability.
  prefs: []
  type: TYPE_NORMAL
- en: Speaking of those two attributes, let's now discuss a few things that help achieve
    them.
  prefs: []
  type: TYPE_NORMAL
- en: Achieving performance at scale
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When designing C++ applications, performance is usually a key factor. While
    using the language can go a long way in the scope of a single application, the
    proper high-level design is also essential to achieving optimal latency and throughput.
    Let's discuss a few crucial patterns and aspects.
  prefs: []
  type: TYPE_NORMAL
- en: CQRS and event sourcing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are many ways to scale compute but scaling data access can be tricky.
    However, it's often necessary when your userbase grows. **Command-query responsibility
    segregation** (**CQRS**) is a pattern that can help here.
  prefs: []
  type: TYPE_NORMAL
- en: Command-query responsibility segregation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In traditional CRUD systems, both reads and writes are performed using the same
    data model and the data flows the same way. The titular segregation basically
    means to treat queries (reads) and commands (writes) in two separate ways.
  prefs: []
  type: TYPE_NORMAL
- en: 'Many applications have a strongly biased ratio of reads to writes – there''s
    usually a lot more reading from the database than updating it in a typical app.
    This means making the reads as fast as possible can yield better performance:
    reads and writes can now be optimized and scaled separately. Other than that,
    introducing CQRS can help if many writes are competing with each other, or if
    a track of all the writes needs to be maintained, or if a set of your API users
    should have read-only access.'
  prefs: []
  type: TYPE_NORMAL
- en: Having separate models for reads and writes can allow having different teams
    to work on both sides. The developers working on the read side of things don't
    need to have a deep understanding of the domain, which is required to perform
    updates properly. When they make a request, they get a **data transfer object**
    (**DTO**) from a thin read layer in just one simple call instead of going through
    the domain model.
  prefs: []
  type: TYPE_NORMAL
- en: If you're not aware of what a DTO is, think of returning item data from the
    database. If the caller asks for a list of items, you could provide them with
    an `ItemOverview` object containing just the names and thumbnails of items. On
    the other hand, if they want items for a specific store, you could also provide
    a `StoreItem` object containing a name, more pictures, a description, and a price.
    Both `ItemOverview` and `StoreItem` are DTOs, grabbing data from the same `Item`
    objects in the database.
  prefs: []
  type: TYPE_NORMAL
- en: 'The read layer can reside either on top of the data storage used for writes,
    or it can be a different data storage that gets updated via events as you can
    see in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d9046486-c262-4786-a378-2ca8c579546e.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.2 – CQRS with event sourcing
  prefs: []
  type: TYPE_NORMAL
- en: Using the approach pictured here, you can create as many different commands
    as you like, each having its own handler. Usually, the commands are asynchronous
    and don't return any values to the caller. Each handler uses domain objects and
    persists the changes done. After doing that, events are published, which event
    handlers can use to update the storage used by read operations. Continuing our
    last example, item data queries would grab information from a database updated
    by events such as `ItemAdded` or `ItemPriceChanged`, which could be triggered
    by commands such as `AddItem` or `ModifyItem`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using CQRS allows us to have different data models for read and write operations.
    For instance, you can create stored procedures and materialized views to speed
    up reads. Using different types of storage (SQL and NoSQL) for the read and domain
    stores can also be beneficial: one efficient way to persist data is to use an
    Apache Cassandra cluster while using Elasticsearch is a great way to search through
    the stored data quickly.'
  prefs: []
  type: TYPE_NORMAL
- en: Aside from the preceding pros, CQRS also has its cons. Due to the complexity
    it introduces, it's usually not a good fit for small or less requiring architectures.
    It's often useful to apply it only to the parts of your system where it would
    bring the biggest benefits. You should also notice that updating the read store
    after the domain store means that now we have eventual consistency instead of
    strong consistency.
  prefs: []
  type: TYPE_NORMAL
- en: Command-query separation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: CQRS is actually based on a simpler idea introduced long ago in the Eiffel programming
    language (the same one that introduced contracts). **Command-query separation**
    (**CQS**) is a principle that devises to separate API calls into, well, commands
    and queries – just like in CQRS, but regardless of the scale. It plays really
    well with objective programming and imperative programming in general.
  prefs: []
  type: TYPE_NORMAL
- en: 'If your function''s name starts with a *has*, *is*, *can,* or a similar word,
    it should be just a query and not modify the underlying state or have any side
    effects. This brings two great benefits:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Much easier reasoning about the code**: It''s clear that such functions are
    semantically just *reads*, never *writes*. This can make looking for a change
    of state much easier when debugging.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reduce heisenbugs**: If you have ever had to debug an error that manifested
    in a release build, but not in the debug one (or the other way around), you have
    dealt with a heisenbug. It''s rarely something pleasurable. Many such errors can
    be caused by assert calls that modify the state. Following CQS eliminates such
    bugs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Similarly to asserts, if you want to have contracts (pre- and post-conditions),
    it's super important to only use queries in them. Otherwise disabling some contract
    checks could also lead to heisenbugs, not to mention how counterintuitive it would
    be.
  prefs: []
  type: TYPE_NORMAL
- en: Let's now say a few more words about event sourcing.
  prefs: []
  type: TYPE_NORMAL
- en: Event sourcing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As introduced in [Chapter 2](6fbde08c-f8a2-475d-9984-2a9882a761b4.xhtml), *Architectural
    Styles*, event sourcing means that instead of always storing the whole state of
    your application, possibly dealing with conflicts during updates, you can just
    store the changes that happened to your application's state. Using event sourcing
    can boost your app's performance by eliminating concurrent updates and allowing
    all interested parties to perform gradual changes to their state. Saving the history
    of the operations done (for example, market transactions) can allow easier debugging
    (by replaying them later) and auditing. This also brings more flexibility and
    extensibility to the table. Some domain models can get much simpler when event
    sourcing is introduced.
  prefs: []
  type: TYPE_NORMAL
- en: One cost of event sourcing is being eventually consistent. Another one is slowing
    down the startup of your application – unless you make periodic snapshots of the
    state or can use the read-only store as in CQRS, discussed in the previous section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Okay, enough of CQRS and related patterns. Let''s now move on to another hot
    topic when it comes to performance (no pun intended): caching.'
  prefs: []
  type: TYPE_NORMAL
- en: Caching
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Proper usage of caches can yield better performance, lower latency, reduce the
    server load (and thus, costs of running in the cloud), and help with scalability
    concerns (fewer servers required) – what's not to like?
  prefs: []
  type: TYPE_NORMAL
- en: If you're here for tips on CPU caches, you can find them in [Chapter 11](9d4b9eb1-c0cc-4fdb-b0e2-db0a401405ac.xhtml),
    *Performance*.
  prefs: []
  type: TYPE_NORMAL
- en: Caching is a big topic, so we'll only cover a few aspects of it here.
  prefs: []
  type: TYPE_NORMAL
- en: 'Caching works by simply storing the data that is read most often in non-persistent
    storage with fast access times. There are many different types of caches:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Client-side caches**: For storing data specifically for a given customer,
    often placed on the client''s machine or browser.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Web server caches**: For speeding up reading from web pages, for instance,
    through HTTP accelerators such as Varnish that can cache the web server responses.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Database caches**: Many database engines have built-in, tunable caching.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Application caches**: For speeding up your application, which can now read
    data from a cache instead of reaching out to its database.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**CDNs can be treated as caches too**: For serving content from a location
    close to the user in order to reduce latency.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Some types of caches can be replicated or deployed in clusters to provide performance
    at scale. An alternative can also be to shard them: similarly to as you would
    shard databases, you can use different instances of your caches for distinct parts
    of your data.'
  prefs: []
  type: TYPE_NORMAL
- en: Let's now go through the different approaches to updating the data in the cache.
    After all, no one likes to be served stale data.
  prefs: []
  type: TYPE_NORMAL
- en: Updating caches
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are a few approaches to keeping the cached data fresh. Whether it's you
    who decided how to update cached items or another company, it's worth knowing
    them. In this section, we'll discuss their pros and cons.
  prefs: []
  type: TYPE_NORMAL
- en: Write-through approach
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'If you require strong consistency, synchronously updating both the database
    and the cache is the valid approach for you. This approach protects you from data
    loss: if data became visible to a user, it means it is already written to the
    database. A downside of write-through caches is that the latency to perform the
    update is bigger than in other approaches.'
  prefs: []
  type: TYPE_NORMAL
- en: Write-behind approach
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: An alternative approach, also known as **write-back**, is to provide the user
    with just access to the cache. When the user performs an update, the cache will
    then queue the incoming update, which will then be asynchronously executed, thus
    updating the database. The obvious downside here is that if something goes wrong,
    the data can never be written. It's also not as easy to implement as the other
    approaches. The upside, however, is the lowest latency as seen by the user.
  prefs: []
  type: TYPE_NORMAL
- en: Cache-aside
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'This last approach, also called **lazy loading**, is about filling the cache
    on-demand. In this case, data access looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: A call to the cache is made to check whether the value is already there. If
    so, just return it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reach the main data store or service that provides the value.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Store the value in the cache and return it to the user.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This type of caching is often done using Memcached or Redis. It can be really
    fast and efficient – the cache only contains data that was requested.
  prefs: []
  type: TYPE_NORMAL
- en: However, if data that is not in the cache is often requested, the preceding
    three calls can increase the latency noticeably. To mitigate this for cache restarts,
    the cache can be primed (initialized) with selected data from the persistent store.
  prefs: []
  type: TYPE_NORMAL
- en: The items in the cache can also become stale, so it's best to set a time-to-live
    for each entry. If the data is to be updated, it can happen in a write-through
    manner by removing the record from the cache and updating it in the database.
    Take care when using multi-level caches with just a time-based update policy (for
    instance, as in DNS caches). This may lead to using stale data for long periods
    of time.
  prefs: []
  type: TYPE_NORMAL
- en: We've discussed the types of caches and strategies to update them, so that's
    enough about caches for now. Let's move on to a different aspect of providing
    scalable architectures.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying your system
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Even though deploying services sounds easy, there's a lot of things to think
    about if you take a closer look. This section will describe how to perform efficient
    deployments, configure your services after installing them, check that they stay
    healthy after being deployed, and how to do it all while minimizing downtime.
  prefs: []
  type: TYPE_NORMAL
- en: The sidecar pattern
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Remember Envoy from earlier in this chapter? It's a very useful tool for efficient
    application development. Instead of embedding infrastructure services such as
    logging, monitoring, or networking into your application, you can deploy the Envoy
    proxy along with your app, just like a sidecar would be *deployed* next to a motorbike.
    Together, they can do much more than the app without the sidekick (another name
    for this pattern).
  prefs: []
  type: TYPE_NORMAL
- en: Using a sidecar can speed up development, as many of the functionality it brings
    would need to be developed independently by each of your microservices. Because
    it's separate from your application, a sidecar can be developed using any programming
    language you find best for the job. The sidecar, along with all the functionality
    it provides, can be maintained by an independent team of developers and updated
    independently from your main service.
  prefs: []
  type: TYPE_NORMAL
- en: Because sidecars reside right next to the app they enhance, they can use local
    means of inter-process communication. Usually, it's fast enough and much faster
    than communicating from another host, but remember that it can sometimes be too
    big a burden.
  prefs: []
  type: TYPE_NORMAL
- en: 'Even if you deploy a third-party service, deploying your selected sidecar next
    to it can still provide value: you can monitor the resource usage and the condition
    of both the host and the service, as well as tracing requests throughout your
    distributed system. Sometimes it''s also possible to reconfigure the service dynamically
    based on its condition, via editing the config file or a web interface.'
  prefs: []
  type: TYPE_NORMAL
- en: Deploying a service with tracing and a reverse proxy using Envoy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let''s now use Envoy as a front proxy for our deployment. Start by creating
    Envoy''s configuration file, in our case named `envoy-front_proxy.yaml`, with
    the address of our proxy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ve specified that Envoy is going to listen for incoming traffic on port
    `8080`. Later in the config, we''ll route it to our service. Now, let''s specify
    that we''d like to handle HTTP requests using our set of service instances and
    adding some tracing capabilities on top. First, let''s add an HTTP endpoint:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s specify that requests should have IDs assigned and be traced by
    a distributed tracing system, Jaeger:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: We'll create IDs for requests and use the OpenTracing standard (`DynamicOtConfig`)
    with the native Jaeger plugin. The plugin will report to a Jaeger instance running
    under the specified address and add the specified headers.
  prefs: []
  type: TYPE_NORMAL
- en: 'We also need to specify that all traffic (see the `match` section) from all
    domains shall be routed into our service cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll define our `example_service` cluster in a second. Note that each request
    coming to the cluster will be marked by a predefined operation decorator. We also
    need to specify what router address to use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we know how to handle and trace the requests, so what''s left is to define
    the clusters we used. Let''s start with our service''s cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Each cluster can have multiple instances (endpoints) of our service. Here, if
    we decide to add more endpoints, the incoming requests will be load-balanced using
    the round-robin strategy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s also add an admin interface:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s now place the config inside a container that will run Envoy using a
    Dockerfile, which we named `Dockerfile-front_proxy`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: We also downloaded the Jaeger native plugin that we used in our Envoy config.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s specify how to run our code in several containers using Docker Compose.
    Create a `docker-compose.yaml` file, starting with the front proxy service definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We use our Dockerfile here, a simple network, and we expose two ports from
    the container on the host: our service and the admin interface. Let''s now add
    the service our proxy will direct to:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: In our case, the service will just display a predefined string in a simple web
    server.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s run Jaeger in another container, exposing its port to the outside
    world:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The last step will be to define our network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: And we're done. You can now run the service using `docker-compose up --build`
    and point your browser to the endpoints we specified.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using a sidecar proxy has one more benefit: even if your service will die,
    the sidecar is usually still alive and can respond to external requests while
    the main service is down. The same applies when your service is redeployed, for
    example, because of an update. Speaking of which, let''s learn how to minimize
    the related downtime.'
  prefs: []
  type: TYPE_NORMAL
- en: Zero-downtime deployments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are two common ways to minimize the risk of downtime during deployments:
    **blue-green deployments** and **canary releases**. You can use the Envoy sidecar
    when introducing any of those two.'
  prefs: []
  type: TYPE_NORMAL
- en: Blue-green deployments
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Blue-green deployments** can help you minimize both the downtime and the
    risk related to deploying your app. To do so, you''ll need two identical production
    environments, called *blue* and *green*. While green serves the customers, you
    can perform the update in the blue one. Once the update was made, the services
    were tested, and all looks stable, you can switch the traffic so it now flows
    to the updated (blue) environment.'
  prefs: []
  type: TYPE_NORMAL
- en: If any issues are spotted in the blue environment after the switch, the green
    one is still there – you can just switch them back. The users probably won't even
    notice any changes, and because both the environments are up and running, no downtime
    should be visible during the switch. Just make sure you won't lose any data during
    the switch (for example, transactions made in the new environment).
  prefs: []
  type: TYPE_NORMAL
- en: Canary releases
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The simplest way to not have all your service instances fail after an update
    is often, well, not updating all of them at once. That's the key idea behind the
    incremental variant of blue-green deployments, also called a **canary release**.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Envoy, you could put the following in the `routes` section of your config:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'You should also remember to define the two clusters from the preceding snippet,
    the first one with the old version of your service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The second cluster will run the new version:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'When an update gets deployed, the new version of a service will only be seen
    and used by a small fraction (here: 5%) of your users. If the updated instances
    remain stable and no checks and verifications fail, you can gradually update more
    and more hosts in several steps, until all of them are switched to a new version.
    You can do it either by updating the config files by hand or by using the admin
    endpoint. Voila!'
  prefs: []
  type: TYPE_NORMAL
- en: Let's now move on to the last deployment pattern that we'll cover here.
  prefs: []
  type: TYPE_NORMAL
- en: External configuration store
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you're deploying a simple application, it can be okay to just deploy its
    configuration along with it. However, when you want to have a more complex deployment
    with many application instances, it can quickly become a burden to redeploy a
    new version of the app just to reconfigure it. At the same time, manual configuration
    changes are a no-go if you want to treat your services like cattle, not pets.
    Introducing an external configuration store can be an elegant way to overcome
    such hurdles.
  prefs: []
  type: TYPE_NORMAL
- en: In essence, your apps can grab their configuration from said store instead of
    just relying on their local config files. This allows you to provide common settings
    for multiple instances and tune parameters for some of them, while having an easy
    and centralized way to monitor all your configs. If you want an arbiter to decide
    which nodes will be master nodes and which will serve as backup ones, an external
    config store can provide the instances with such information. It's also useful
    to implement a configuration update procedure so that your instances can be easily
    reconfigured during operation. You can use ready solutions such as Firebase Remote
    Config, leverage the Java-based Netflix Archaius, or write a configuration store
    on your own leveraging cloud storage and change notifications.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we''ve learned some useful deployment patterns, let''s move to another
    important topic when it comes to high-level design: APIs.'
  prefs: []
  type: TYPE_NORMAL
- en: Managing your APIs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Proper APIs are essential for the success of your development team and product.
    We can divide this topic into two smaller ones: system-level APIs and component-level
    APIs. In this section, we''ll discuss handling APIs on the first of those levels,
    while the next chapter will present you with tips on the second.'
  prefs: []
  type: TYPE_NORMAL
- en: Aside from managing objects, you'll also want to manage your whole API. If you
    want to introduce policies regarding API usage, control access to said API, gather
    performance metrics and other analytical data, or just charge your customers based
    on their use of your interfaces, **API management** (**APIM**) is the solution
    you're looking for.
  prefs: []
  type: TYPE_NORMAL
- en: 'Typically a set of APIM tools consists of these components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**An API gateway**: A single entry point for all users of an API. More on this
    in the next section.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reporting and analytics**: To monitor the performance and latency of your
    APIs, resources consumed, or data sent. Such tools can be leveraged to detect
    trends in usage, know which parts of the API and which components behind them
    are performance bottlenecks, or what SLAs are reasonable to offer and how to improve
    them.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**A portal for developers**: To help them get up to speed with your API quickly,
    and to subscribe to your APIs at all.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**A portal for administrators**: To manage policies, users, and package APIs
    into sellable products.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Monetization**: To charge your customers based on how they use your APIs
    and to aid related business processes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: APIM tools are provided both by cloud providers and independent parties, for
    example, NGINX's Controller or Tyk.
  prefs: []
  type: TYPE_NORMAL
- en: When designing APIs for a given cloud, get to know the good practices the cloud
    provider usually documents. For instance, you can find common design patterns
    for Google Cloud Platform in the *Further reading* section. In their case, lots
    of the practices revolve around using Protobufs.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the right way to consume APIs can take you a long way. The most simple
    way to file requests to your servers is by connecting to the services directly.
    While easy to set up and okay for small apps, it can lead to performance issues
    down the road. An API consumer will likely need to call a few different services,
    leading to high latency. Proper scalability is also impossible to achieve using
    this approach.
  prefs: []
  type: TYPE_NORMAL
- en: A better approach is to use an API gateway. Such gateways are often an essential
    part of an APIM solution, but can also be used on their own.
  prefs: []
  type: TYPE_NORMAL
- en: API gateways
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An API gateway is an entry point for clients who want to use your API. It can
    then route the incoming requests into a specific instance or cluster of services.
    This can simplify your client code, as it no longer needs to know all the backend
    nodes, or how they cooperate with each other. All a client needs to know is the
    address of an API gateway — the gateway will handle the rest. Thanks to hiding
    the backend architecture from the client, it can be easily remodeled without even
    touching the client's code.
  prefs: []
  type: TYPE_NORMAL
- en: The gateway can aggregate multiple parts of your system's API into one, and
    then use **layer-7 routing** (for example, based on the URL) to a proper part
    of your system. Layer-7 routing is offered by both cloud providers themselves,
    as well as tools such as Envoy.
  prefs: []
  type: TYPE_NORMAL
- en: As with many patterns described in this chapter, always consider whether it's
    worth it to add more complexity by introducing another pattern to your architecture.
    Think about how adding it will affect your availability, fault tolerance, and
    performance if they matter to you. After all, a gateway usually is just a single
    node, so try to not make it a bottleneck or a single point of failure.
  prefs: []
  type: TYPE_NORMAL
- en: The Backends for Frontends pattern we mentioned a few chapters earlier can be
    thought of as a variant of the API gateway pattern. In the Backends for Frontends
    case, each frontend connects to its own gateway.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you know how system design relates to API design, let's summarize what
    we've discussed in the last sections.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we've learned quite a lot of stuff. You now know when to apply
    which service model and how to avoid the common pitfalls of designing distributed
    systems. You've learned about the CAP theorem and what practical outcomes it has
    for distributed architectures. You can now run transactions in such systems successfully,
    reduce their downtime, prevent issues, and gracefully recover from errors. Dealing
    with unusually high load is no longer black magic. Integrating parts of your system,
    even legacy ones, with your newly designed parts is also something you're able
    to perform. You now also have some tricks up your sleeve to increase the performance
    and scalability of your system. Deploying and load balancing your system are also
    demystified, so you can now perform them efficiently. Last but not least, discovering
    services and designing and managing their APIs are all things that you have now
    learned to perform. Nice!
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we'll learn how you can use specific C++ features to travel
    on the road to excellent architecture in a more pleasant and efficient way.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What is event sourcing?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the practical consequences of the CAP theorem?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What can you use Netflix's Chaos Monkey for?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Where can caching be applied?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do you prevent your app from going down when a whole data center does?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why use an API Gateway?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How can Envoy help you to achieve various architectural goals?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Microsoft Azure cloud design patterns: [https://docs.microsoft.com/en-us/azure/architecture/patterns/](https://docs.microsoft.com/en-us/azure/architecture/patterns/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Common design patterns for cloud APIs by Google: [https://cloud.google.com/apis/design/design_patterns](https://cloud.google.com/apis/design/design_patterns)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Microsoft REST API guidelines: [https://github.com/microsoft/api-guidelines/blob/vNext/Guidelines.md](https://github.com/microsoft/api-guidelines/blob/vNext/Guidelines.md)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Envoy Proxy''s *Getting Started* page: [https://www.envoyproxy.io/docs/envoy/latest/start/start](https://www.envoyproxy.io/docs/envoy/latest/start/start)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Active-active application architectures with MongoDB: [https://developer.mongodb.com/article/active-active-application-architectures](https://developer.mongodb.com/article/active-active-application-architectures)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
