- en: Chapter 8.  Adapting Your Machine Learning Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter covers advanced **machine learning** (**ML**) techniques in order
    to be able to make algorithms adaptable to new data. Readers will also see how
    machine learning algorithms learn incrementally over the data, that is to say
    that the models are updated each time they see a new training instance. Learning
    in dynamic environments by conceding different constraints will also be discussed.
    In summary, the following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Adapting machine learning models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generalization of ML models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adapting through incremental algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adapting through reusing ML models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Machine learning in dynamic environments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adapting machine learning models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we discussed earlier, as a part of the ML training process, a model is trained
    using a set of data (that is, training, test, and validation set). Machine learning
    models that can adapt to their environments and learn from their experience have
    attracted consumers and researchers from diverse areas, including computer science,
    engineering, mathematics, physics, neuroscience, and cognitive science. In this
    section, we will provide a technical overview of how to adopt machine learning
    models for the new data and requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Technical overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Technically, the same models might need to be retrained at a later stage if
    required for the betterment. This is really dependent on several factors, for
    example, when the new data becomes available, or when the consumer of the API
    has their own data to train the model or when the data needs to be filtered and
    the model trained with the subset of data. In these scenarios, ML algorithms should
    provide enough APIs to provide a convenient way to allow its consumers to produce
    a client that can be used on a one-time or regular basis, so that they can retrain
    the model using their own data.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a result, the client will be able to evaluate the results of retraining
    and updating the web service API accordingly. Alternatively, they will be able
    to use the newly trained model. In this regard, there are several contexts of
    domain adaptation. However, they differ in the information considered for application
    type and requirements:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Unsupervised domain adaptation**: The learning sample contains a set of labeled
    source examples, a set of unlabeled source examples, and an unlabeled set of target
    examples'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Semi-supervised domain adaptation**: In this situation, we also consider
    a small set of labeled target examples'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Supervised domain adaptation**: All the examples considered are supposed
    to be labeled:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Technical overview](img/00017.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: The retraining process overview (dashed line presents the retrain
    steps)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Technically, there should be three alternatives for making the ML models adaptable:'
  prefs: []
  type: TYPE_NORMAL
- en: The most widely used machine learning techniques and algorithms include decision
    trees, decision rules, neural networks, statistical classifiers, and probabilistic
    graphical models, and they all need to be developed so that they can be adaptable
    for the new requirements
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Secondly, previous mentioned algorithms or techniques should be generalized
    so that they can be used with minimum effort
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Moreover, more robust theoretical frameworks and algorithms such as Bayesian
    learning theory, classical statistical theory, minimum description length theory,
    and statistical mechanics approaches need to be developed in order to understand
    computational learning theory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The benefits from these three adaptation properties and techniques will provide
    insight into experimental results that will also guide machine learning communities
    to contribute towards the different learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: The generalization of ML models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 5](part0043_split_000.html#190862-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 5.  Supervised and Unsupervised Learning by Examples"), *Supervised and
    Unsupervised Learning by Example*, we discussed how and why to generalize the
    learning algorithm to fit semi-supervised learning, active learning, structured
    prediction, and reinforcement learning. In this section, we will discuss how to
    generalize the linear regression algorithm on the **Optical Character Recognition**
    (**OCR**) dataset to show an example of generalization of the linear regression
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Generalized linear regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As discussed in [Chapter 5](part0043_split_000.html#190862-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 5.  Supervised and Unsupervised Learning by Examples"), *Supervised and
    Unsupervised Learning by Example*, the linear regression and logistic regression
    techniques assume that the output follows a Gaussian distribution. The **generalized
    linear models** (**GLMs**) on the other hand are specifications of the linear
    models where the response variable, that is, Yi, follows the linear distribution
    from an exponential family of distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Spark's `GeneralizedLinearRegression` API allows us a flexible specification
    of the GLMs. The current implementation of the generalized linear regression in
    Spark can be used for numerous types of prediction problems, for example, linear
    regression, Poisson regression, logistic regression, and others.
  prefs: []
  type: TYPE_NORMAL
- en: However, the limitation is that only a subset of the exponential family distributions
    is supported in the current implementation of the Spark-based GLM algorithm. In
    addition, there is another scalability issue that only 4096 features are supported
    through its `GeneralizedLinearRegression` API. Consequently, if the number of
    features are beyond 4096, the algorithm will throw an exception.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, your models with an increased number of features can be trained
    using the LinearRegression and LogisticRegression estimators, as shown in several
    examples in [Chapter 6](part0049_split_000.html#1ENBI2-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 6.  Building Scalable Machine Learning Pipelines"), *Building Scalable
    Machine Learning Pipelines*.
  prefs: []
  type: TYPE_NORMAL
- en: Generalized linear regression with Spark
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this sub-section, we discuss a step-by-step example that shows how to apply
    the generalized linear regression on the `libsvm` version of the **Optical Character
    Recognition** (**OCR**) data that we discussed in [Chapter 7](part0059_split_000.html#1O8H62-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 7. Tuning Machine Learning Models"), *Tuning Machine Learning Models*.
    Since the same dataset will be re-used here, we've decided not to describe them
    further.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1: Load the necessary API and packages**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the code to load the necessary API and packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 2: Create the Spark session**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code shows how to create the Spark session:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 3: Load and create the Dataset**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Load and create the dataset from the OCR dataset. Here we have specified the
    dataset format as `libsvm`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 4: Prepare the training and test sets**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code illustrates how to prepare the training and test sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 5: Create a generalized linear regression estimator**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create the generalized linear regression estimator by specifying the family,
    link, and max iteration and regression parameters. Here we have selected the family
    as `"gaussian"` and link as `"identity"`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that according to the API documentation at [http://spark.apache.org/docs/latest/ml-classification-regression.html#generalized-linear-regression](http://spark.apache.org/docs/latest/ml-classification-regression.html#generalized-linear-regression),
    the following options are supported with this algorithm implementation with Spark:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Generalized linear regression with Spark](img/00153.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Available supported families with the current implementation of Generalized
    Linear Regression'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 6: Fit the model**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the code to fit the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 7: Check the coefficients and intercept**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Print the coefficients and intercept for the linear regression model that we
    created in Step 6:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Output for these two parameters will be similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: It is to be noted that the `System.out.println` method will not work in cluster
    mode. This will work only in standalone or Pseudo mode. It is only for the verification
    of the result.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 8: Summarize the model**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Summarize the model over the training set and print out some metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 9: Verify some generalized metrics**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s print some generalized metrics such as **Coefficient Standard Errors**
    (**CSE**), T values, P values, Dispersions, Null deviance, the Residual degree
    of freedom null, AIC and Deviance residuals. Owing to the page limitations, we
    have not shown how these values are significant or the calculations procedure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s see the values for the training set we created previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 10: Show the deviance residuals**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code is used to show the deviance residuals:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '![Generalized linear regression with Spark](img/00002.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Summary of the deviance residuals for the OCR dataset'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Interested readers should refer to the following web page to get more information
    on and insight into this algorithm and its implementation details: [http://spark.apache.org/docs/latest/ml-classification-regression.html](http://spark.apache.org/docs/latest/ml-classification-regression.html)'
  prefs: []
  type: TYPE_NORMAL
- en: Adapting through incremental algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'According to Robi Polikar et al., (*Learn++: An Incremental Learning Algorithm
    for Supervised Neural Networks, IEEE Transactions on Systems, Man, And Cybernetics,
    V-21, No-4, November 2001*), various algorithms have been suggested for incremental
    learning. The incremental learning is therefore implied for solving different
    problems. In some literature, the term incremental learning has been used to refer
    to either the growing of or pruning of a classifier. Alternatively, it may refer
    to the selection of most informative training samples for solving a problem in
    an incremental way.'
  prefs: []
  type: TYPE_NORMAL
- en: In other cases, making a regular ML algorithm incremental means performing some
    form of controlled modification of weights in the classifier, by retraining with
    misclassified signals. Some algorithms are capable of learning new information;
    however, they do not synchronously satisfy all of the previously mentioned criteria.
    Moreover, they either require access to the old data or need to forget the prior
    knowledge along the way, and as they are unable to accommodate new classes they
    are not adaptable for new datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Considering the previously mentioned issues, in this section, we will discuss
    how to adopt ML models using an incremental version of the original algorithms.
    Incremental SVM, Bayesian Network, and Neural networks will be discussed in brief.
    Moreover, when applicable, we will provide regular Spark implementation of these
    algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Incremental support vector machine
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It's pretty difficult to make a regular ML algorithm incremental. In short,
    it's possible but not altogether easy. If you want to do it you have to change
    the underlying source codes in the Spark library you are using or implement the
    training algorithm yourself.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, Spark does not have an incremental version of SVM implemented.
    However, before making the linear SVM incremental, you need to first understand
    the linear SVM itself. Therefore, we provide some concepts of linear SVMs in the
    next sub-section using Spark for the new dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'According to our knowledge, we have found only two possible solutions called
    SVMHeavy ([http://people.eng.unimelb.edu.au/shiltona/svm/](http://people.eng.unimelb.edu.au/shiltona/svm/))
    and LaSVM ([http://leon.bottou.org/projects/lasvm](http://leon.bottou.org/projects/lasvm)),
    which support incremental training. But we haven''t used either. Interested readers
    should follow these two papers on incremental SVMs to get some insight. These
    two papers are straightforward and show good research if you''re just getting
    started:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://cbcl.mit.edu/cbcl/publications/ps/cauwenberghs-nips00.pdf](http://cbcl.mit.edu/cbcl/publications/ps/cauwenberghs-nips00.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://www.jmlr.org/papers/volume7/laskov06a/laskov06a.pdf](http://www.jmlr.org/papers/volume7/laskov06a/laskov06a.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: Adapting SVMs for new data with Spark
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we will first discuss how to perform binary classification
    using linear SVMs of Spark implementation. Then we will show how to adopt the
    same algorithm for the new data type.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1: Data collection and exploration**'
  prefs: []
  type: TYPE_NORMAL
- en: 'We have collected a colon cancer dataset from [https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html).
    Originally, the dataset was labeled as -1.0 and 1.0 as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Adapting SVMs for new data with Spark](img/00096.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Original colon cancer data snapshot'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The dataset was used in the following publication: *U. Alon, N. Barkai, D.
    A. Notterman, K. Gish, S.Ybarra, D.Mack, and A. J. Levine. Broad patterns of gene
    expression revealed by clustering analysis of tumour and normal colon tissues
    probed by oligonucleotide arrays*. *Cell Biology, 96:6745-6750, 1999*. Interested
    readers should refer to the publication to get more insights into the dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: 'After that, instance-wise normalization is carried out to mean zero and variance
    one. Then feature wise normalization is carried out to get zero and variance one
    as a pre-processing step. However, for simplicity, we have considered -1.0 as
    0.1, since SVM does not recognize symbols (that is, + or -). Therefore, the dataset
    now contains two labels 1 and 0 (that is, to say it''s a binary classification
    problem). After pre-processing and scaling, there are two classes and 2000 features.
    Here is a sample of the dataset in *Figure 5*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Adapting SVMs for new data with Spark](img/00051.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Pre-processed colon cancer data'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 2: Load the necessary packages and APIs**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the code to load the necessary packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 3: Configure the Spark session**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code helps us to create the Spark session:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 4: Create a Dataset out of the data**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the code to create a Dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 5: Prepare the training and test sets**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the code to prepare the training and test sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 6: Build and train the SVM model**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code illustrates how to build and train the SVM model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 7: Compute the raw prediction score on the test set**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the code to compute the raw prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 8: Evaluate the model**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the code to evaluate the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: However, the value of ROC is between 0.5 and 1.0\. Where the value is more than
    0.8 this indicates a good classifier and if the value of ROC is less than 0.8,
    this signals a bad classifier. The `SVMWithSGD.train()` method by default performs
    Level Two (L2) regularization with the regularization parameter set to 1.0.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to configure this algorithm, you should customize the `SVMWithSGD`
    further by creating a new object directly. After that, you can further the setter
    methods to set the value of the object.
  prefs: []
  type: TYPE_NORMAL
- en: Interestingly, all the other Spark MLlib algorithms can be customized this way.
    However, after the customization has been completed, you need to build the source
    code to make changes up to API level. Interested readers can add themselves to
    the Apache Spark mailing list if they want to contribute to the open source.
  prefs: []
  type: TYPE_NORMAL
- en: Note the source code of Spark is available on GitHub at the URL [https://github.com/apache/spark](https://github.com/apache/spark)
    as an open source and it sends pull requests to enrich Spark. More technical discussion
    can be found at the Spark website at [http://spark.apache.org/](http://spark.apache.org/).
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, the following code produces a level one (`L1`) regularized variant
    of SVMs with the regularization parameter set to 0.1, and runs the training algorithm
    for 500 iterations as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Your model is now trained. Now if you perform *step 7* and *step 8*, the following
    metrics will be generated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: If you compare this result with the result produced in *step 8*, it's much better
    now, isn't it? However, depending on the data preparation, you might experience
    different results.
  prefs: []
  type: TYPE_NORMAL
- en: It indicates a better classification (please see also at [https://www.researchgate.net/post/What_is_the_value_of_the_area_under_the_roc_curve_AUC_to_conclude_that_a_classifier_is_excellent](https://www.researchgate.net/post/What_is_the_value_of_the_area_under_the_roc_curve_AUC_to_conclude_that_a_classifier_is_excellent)).
    In this way the SVM can be optimized or adaptive for the new data type.
  prefs: []
  type: TYPE_NORMAL
- en: However, the parameters (that is, number of iterations, regression params, and
    updater) should be set accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: Incremental neural networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The incremental version of the neural network in R or Mat lab provides adaptability
    using the adapt function. Does this update instead of overwriting iteratively?
    To verify this statement, readers can try using the R or Mat lab version of the
    incremental neural network-based classifier that may need to select a subset of
    your first data chunk as the second chunk in training. If it is overwriting, when
    you use the trained net with the subset to test your first data chunk, it will
    likely poorly predict the data that does not belong to the subset.
  prefs: []
  type: TYPE_NORMAL
- en: Multilayer perceptron classification with Spark
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To date, there is no implementation of the incremental version of the neural
    network in Spark yet. According to the API documentation provided at [https://spark.apache.org/docs/latest/ml-classification-regression.html#multilayer-perceptron-classifier](https://spark.apache.org/docs/latest/ml-classification-regression.html#multilayer-perceptron-classifier),
    Spark's **Multilayer Perceptron Classifier** (**MLPC**) is a classifier based
    on the **Feedforward Artificial Neural Network** (**FANN**). The MLPC consists
    of multiple layers of nodes including hidden layers. Each layer is fully connected
    to the next layer and so on in a network. A node in the input layer represents
    the input data. All other nodes map inputs to outputs by a linear combination
    of the inputs with the node's weights *w* and bias *b* and by applying the activation
    function. The number of nodes *N* in the output layer corresponds to the number
    of classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'MLPC also performs backpropagation for learning the model. Spark uses the logistic
    loss function for optimization and **Limited-memory Broyden-Fletcher-Goldfarb-Shanno**
    (**L-BFGS**) as an optimization routine. Note that the L-BFGS is an optimization
    algorithm in the family of **Quasi-Newton Method** (**QNM**) that approximates
    the Broyden-Fletcher-Goldfarb-Shanno algorithm using a limited main memory. To
    train the multilayer perceptron classifier, the following parameters need to be
    set:'
  prefs: []
  type: TYPE_NORMAL
- en: Layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tolerance of iteration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The block size of the learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Seed size
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Max iteration number
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note the layers consist of the input, hidden, and output layers. Moreover, a
    smaller value of convergence tolerance will lead to higher accuracy with the cost
    of more iterations. The default block size parameter is 128 and the maximum number
    of iteration is set to be 100 as a default value. We suggest you set these values
    accordingly and carefully.
  prefs: []
  type: TYPE_NORMAL
- en: In this sub-section, we will show how Spark has implemented the neural network
    learning algorithms through the multilayer perception classifier on the Iris dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1: Dataset collection, processing, and exploration**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The original Iris plant dataset was collected from the UCI machine learning
    repositories ([http://www.ics.uci.edu/~mlearn/MLRepository.html](http://www.ics.uci.edu/~mlearn/MLRepository.html))
    and then pre-processed, scaled to libsvm format by Chang et al., and placed as
    the libsvm a comprehensive library for support vector machine at ([https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass.html](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass.html))
    for the binary, multi-class, and multi-label classification task. The Iris dataset
    contains three classes and four features, where the sepal and petal lengths are
    scaled according to the libsvm format. More specifically, here is the attribute
    information:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Class: Iris Setosa, Iris Versicolour, Iris Virginica (column 1)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sepal length in cm (column 2)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sepal width in cm (column 3)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Petal length in cm (column 4)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Petal width in cm (column 5)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A snapshot of the dataset is shown in *Figure 6*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Multilayer perceptron classification with Spark](img/00044.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Irish dataset snapshot'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 2: Load the required packages and APIs**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the code to load the required packages and APIs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 3: Create a Spark session**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code helps us to create the Spark session:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Note, the `mySession()` method that creates and returns a Spark session object
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 4: Parse and prepare the dataset**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Load the input data as `libsvm` format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 5: Prepare the training and test set**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Prepare the training and test set: training = 70%, test = 30%, and seed = 12345L:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 6: Specify the layers for the neural network**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Specify the layers for the neural network. Here, input layer size 4 (features),
    two intermediate layers (that is, hidden layers) of size 4 and 3, and output size
    3 (classes):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 7: Create the multilayer perceptron estimator**'
  prefs: []
  type: TYPE_NORMAL
- en: Create the `MultilayerPerceptronClassifier` trainer and set its parameters.
    Here, set the value of `param [[layers]]` using the `setLayers()` method from
    *Step 6*. Set the convergence tolerance of iterations using the `setTol()` method,
    since, a smaller value will lead to higher accuracy with the cost of more iterations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note the default is `1E-4`. Set the value of Param `[[blockSize]]` using the
    `setBlockSize()` method, where the default is 128KB. Set the seed for weight initialization
    if the weights using the `setInitialWeights()` are not set. Finally, set the maximum
    number of iterations using the `setMaxIter()` method, where the default is 100:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 8: Train the model**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Train the `MultilayerPerceptronClassificationModel` using the preceding estimator
    from *step 7*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 9: Compute the accuracy on the test set**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the code to compute the accuracy on the test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 10: Evaluate the model**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Evaluate the model, calculate the metrics`, and print the accuracy, weighted
    precision and weighted recall:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The output should appear as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 11: Stop the Spark session**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code is used to stop the Spark session:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: From the preceding prediction metrics, it is clear that the classification task
    is quite impressive. Now it's your turn to make your model adaptable. Now try
    training and testing with the new dataset and make your ML model adaptable.
  prefs: []
  type: TYPE_NORMAL
- en: Incremental Bayesian networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we discussed earlier, Naive Bayes is a simple multiclass classification algorithm
    with the assumption of independence between each pair of features. The Naive Bayes
    based model can be trained very efficiently. The model can compute the conditional
    probability distribution of each feature, given the label, since a pass to the
    training data. After that, it applies the Bayes theorem to compute the conditional
    probability distribution of the labels for making the prediction.
  prefs: []
  type: TYPE_NORMAL
- en: However, there is still no implementation of the incremental version of the
    Bayesian network into Spark yet. According to the API documentation provided at
    [http://spark.apache.org/docs/latest/mllib-naive-bayes.html](http://spark.apache.org/docs/latest/mllib-naive-bayes.html),
    each observation is a document and each feature represents a term. The value of
    an observation is the frequency of the term or a zero or one. This value indicates
    if the term has been found in the document for the multinomial Naive Bayes and
    Bernoulli Naive Bayes respectively for the document classification.
  prefs: []
  type: TYPE_NORMAL
- en: Note that as with linear SVM-based learning, here the feature values must be
    non-negative too. The type of the model is selected with an optional parameter,
    multinomial or Bernoulli. The default model type is multinomial. Furthermore,
    additive smoothing (that is, lambda) can be used by setting the parameter Î».
    Note the default of lambda is 1.0.
  prefs: []
  type: TYPE_NORMAL
- en: 'More technical details on the big data approach of Bayesian network based learning
    can be found in the paper: *A Scalable Data Science Workflow Approach for Big
    Data Bayesian Network Learning b* *y Jianwu W., et al.*, ([http://users.sdsc.edu/~jianwu/JianwuWang_files/A_Scalable_Data_Science_Workflow_Approach_for_Big_Data_Bayesian_Network_Learning.pdf](http://users.sdsc.edu/~jianwu/JianwuWang_files/A_Scalable_Data_Science_Workflow_Approach_for_Big_Data_Bayesian_Network_Learning.pdf)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Interested readers also should refer to the following publications for more
    insight into the incremental Bayesian networks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://www.jmlr.org/papers/volume11/henderson10a/henderson10a.pdf](http://www.jmlr.org/papers/volume11/henderson10a/henderson10a.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://www.machinelearning.org/proceedings/icml2007/papers/351.pdf](http://www.machinelearning.org/proceedings/icml2007/papers/351.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://tel.archives-ouvertes.fr/tel-01284332/document](https://tel.archives-ouvertes.fr/tel-01284332/document)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classification using Naive Bayes with Spark
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The current implementation in Spark MLlib supports both the multinomial Naive
    Bayes and Bernoulli Naive Bayes. However, the incremental version has not been
    implemented yet. Therefore, in this section, we will show you how to perform the
    classification using the Spark MLlib version of NaÃ¯ve Bayes on the Vehicle Scale
    dataset to provide you with some concepts of the NaÃ¯ve Bayes based learning.
  prefs: []
  type: TYPE_NORMAL
- en: Note, due to the low accuracy and precision using Spark ML, we did not provide
    the Pipeline version but implemented the same using only Spark MLlib. Moreover,
    if you have suitable and better data, you can try to implement the Spark ML version
    with ease.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1: Data collection, pre-processing, and exploration**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset was downloaded from [https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass.html#aloi](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass.html#aloi)
    and provided by David D. Lewis, Yiming Yang, Tony G. Rose, and Fan Li. RCV1: A
    new benchmark collection for text categorization research. *Journal of Machine
    Learning Research*, 5:361-397, 2004.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pre-processing:** For the pre-processing, two steps were considered as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The label hierarchy is reorganized by mapping the data set to the second level
    of RCV1 (that is, revision) topic hierarchy. The documents, having the third or
    fourth level, are mapped to their parent category of the second level only. Consequently,
    documents having the first level are not considered for creating the mapping.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multi-labeled instances were removed since the current implementation of multi-level
    classifier in Spark is not robust enough.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After performing these two steps, there are finally 53 classes and 47,236 features
    collected. Here is a snapshot of the dataset shown in *Figure 7:*
  prefs: []
  type: TYPE_NORMAL
- en: '![Classification using Naive Bayes with Spark](img/00035.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: RCV1 topic hierarchy dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 2: Load the required library and packages**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the code to load the library and packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 3: Initiate a Spark session**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code helps us to create the Spark session:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 4: Prepare LabeledPoint RDDs**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Parse the dataset in the libsvm format and prepare `LabeledPoint` RDDs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: For document classification, the input feature vectors are usually sparse, and
    sparse vectors should be supplied as input to take advantage of sparsity. Since
    the training data is only used once, it is not necessary to cache it.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 5: Prepare the training and test set**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the code to prepare the training and test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 6: Train the Naive Bayes model**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Train a Naive Bayes model by specifying the model type as multinomial and lambda
    = 1.0, which is the default and suitable for the multiclass classification of
    any features. However, note that Bernoulli naive Bayes requires 0 or 1 feature
    values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 7: Calculate the prediction on the test dataset**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the code to calculate the prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 8: Calculate the prediction accuracy**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the code to calculate the prediction accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 9: Print the accuracy**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the code to print the accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'This provides the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: This is pretty low, right? This is as we discussed when we tuned the ML models
    in [Chapter 7](part0059_split_000.html#1O8H62-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 7. Tuning Machine Learning Models"), *Tuning Machine Learning Models*.
    There are further opportunities to improve the prediction accuracy by selecting
    appropriate algorithms (that is, classifier or regressor) via cross-validation
    and train split.
  prefs: []
  type: TYPE_NORMAL
- en: Adapting through reusing ML models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will describe how to make a machine learning model adaptable
    for new datasets. An example will be shown for the prediction of heart disease.
    At first we will describe the problem statement, and then we will explore the
    heart diseases dataset. Following the dataset exploration, we will train and save
    the model to local storage. After that the model will be evaluated to see how
    it performs. Finally, we will reuse/reload the same model trained to work for
    the new data type.
  prefs: []
  type: TYPE_NORMAL
- en: More specifically, we will show how to predict the possibility of future heart
    disease by using the Spark machine learning APIs including Spark MLlib, Spark
    ML, and Spark SQL.
  prefs: []
  type: TYPE_NORMAL
- en: Problem statements and objectives
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Machine learning and big data together are a radical combination that has created
    some great impacts in the field of research to academia and the industry as well
    as in the biomedical sector. In the area of biomedical data analytics, this carries
    a better impact on a real dataset for diagnosis and prognosis for better healthcare.
    Moreover, life science research is also entering into big data since datasets
    are being generated and produced in an unprecedented way. This imposes great challenges
    to machine learning and bioinformatics tools and algorithms to find the VALUE
    from big data criteria such as volume, velocity, variety, veracity, visibility,
    and value.
  prefs: []
  type: TYPE_NORMAL
- en: Data exploration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In recent times, biomedical research has advanced enormously and more and more
    life sciences datasets are being generated making many of them open source. However,
    for simplicity and ease, we have decided to use the Cleveland database. To date,
    most of the researchers who have applied the machine learning technique to biomedical
    data analytics have used this dataset. According to the dataset description at
    [https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/heart-disease.names](https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/heart-disease.names),
    this heart disease dataset is one of the most used and well-studied datasets by
    researchers from biomedical data analytics and machine learning fields, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: The dataset is freely available at the UCI machine learning dataset repository
    at [https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/](https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/).
    This data contains a total of 76 attributes, however, most of the published research
    papers refer to using a subset of only 14 features in the field. The "goal" field
    is used to refer to if the heart diseases are present or absent. It has five possible
    values ranging from 0 to 4\. The value 0 signifies no presence of heart diseases.
    The values 1 and 2 signify that the disease is present but in the primary stage.
    The values 3 and 4, on the other hand, indicate the strong possibility of heart
    disease. Biomedical laboratory experiments with the Cleveland dataset have simply
    attempted to distinguish presence (values 1, 2, 3, 4) from absence (value 0).
    In short, the higher the value the more the disease is possible and the more evidence
    of the presence there is. Another thing is that privacy is an important concern
    in the area of biomedical data analytics as well as all kinds of diagnosis and
    prognosis. Therefore, the names and social security numbers of the patients were
    recently removed from the dataset to avoid the privacy issue. Consequently, those
    values have been replaced with dummy values instead.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is to be noted that three files have been processed, containing the Cleveland,
    Hungarian, and Switzerland datasets altogether. All four unprocessed files also
    exist in this directory. To demonstrate the example, we will use the Cleveland
    dataset for training and evaluating the models. However, the Hungarian dataset
    will be used to re-use the saved model. As we have said already, although the
    number of attributes is 76 (including the predicted attribute), like other ML/Biomedical
    researchers, we will also use only 14 attributes with the following attribute
    information:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **No.** | **Attribute name** | **Explanation** |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | age | Age in years |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | sex | Either male or female: sex (1 = male; 0 = female) |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | cp | Chest pain type:— Value 1: typical angina— Value 2: atypical angina—
    Value 3: non-angina pain— Value 4: asymptomatic |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | trestbps | Resting blood pressure (in mm Hg on admission to the hospital)
    |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | chol | Serum cholesterol in mg/dl |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | fbs | Fasting blood sugar. If > 120 mg/dl)(1 = true; 0 = false) |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | restecg | Resting electrocardiographic results:— Value 0: normal— Value
    1: having ST-T wave abnormality— Value 2: showing probable or definite left ventricular
    hypertrophy by Estes'' criteria. |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | thalach | Maximum heart rate achieved |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | exang | Exercise induced angina (1 = yes; 0 = no) |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | oldpeak | ST depression induced by exercise relative to rest |'
  prefs: []
  type: TYPE_TB
- en: '| 11 | slope | The slope of the peak exercise ST segment— Value 1: upsloping—
    Value 2: flat— Value 3: down-sloping |'
  prefs: []
  type: TYPE_TB
- en: '| 12 | ca | Number of major vessels (0-3) colored by fluoroscopy |'
  prefs: []
  type: TYPE_TB
- en: '| 13 | thal | Heart rate:—Value 3 = normal;—Value 6 = fixed defect—Value 7
    = reversible defect |'
  prefs: []
  type: TYPE_TB
- en: '| 14 | num | Diagnosis of heart disease (angiographic disease status)— Value
    0: < 50% diameter narrowing— Value 1: > 50% diameter narrowing |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Dataset characteristics'
  prefs: []
  type: TYPE_NORMAL
- en: 'A sample snapshot of the dataset is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Data exploration](img/00091.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: A sample snapshot of the heart diseases dataset'
  prefs: []
  type: TYPE_NORMAL
- en: Developing a heart diseases predictive model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Step 1: Loading the required packages and APIs**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following packages and APIs need to be imported for our purpose. We believe
    the packages are self-explanatory if you have the minimum working experience with
    Spark 2.0.0:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 2: Create an active Spark session**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code helps us to create the Spark session:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the `UtilityForSparkSession` class that creates and returns an active
    Spark session:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Note that here in the Windows 7 platform, we have set the Spark SQL warehouse
    as `E:/Exp/`, but set your path accordingly, based on your operating system.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 3: Data parsing and RDD of Labelpoint creation**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Take the input as a simple text file, parse them as a text file, and create
    an RDD of the label point that will be used for the classification and regression
    analysis. Also specify the input source and number of partition. Adjust the number
    of partition based on your dataset size. Here the number of partition has been
    set to 2:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Since `JavaRDD` cannot be created directly from the text files, we have created
    a simple RDDs so that we can convert them to `JavaRDD` when necessary. Now let''s
    create the `JavaRDD` with a Label Point. However, we first need to convert the
    RDD to `JavaRDD` to serve our purpose as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Using the `replaceAll()` method, we have handled the invalid values such as
    the missing values that are specified in the original file using the *?* character.
    To get rid of the missing or invalid values we have replaced them with a very
    large value that has no side effect to the original classification or predictive
    results. The reason for this is that missing or sparse data can lead you to highly
    misleading results.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 4: Splitting the RDD of the label point into training and test sets**'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous step, we created RDD label point data that can be used for
    the regression or classification task. Now we need to split the data into training
    and test sets as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: If you look at the preceding code segments, you will find that we have split
    the RDD label point as 70% for the training and 30% for the test set. The `randomSplit()`
    method performs this split. Note that we have set this RDD's storage level to
    persist its values across operations after the first time it is computed. This
    can only be used to assign a new storage level if the RDD does not have a storage
    level set yet. The split seed value is a long integer that signifies that the
    split would be random, but the result would not be a change in each run or iteration
    during the model building or training.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 5: Train the model**'
  prefs: []
  type: TYPE_NORMAL
- en: 'First we will train the linear regression model, which is the simplest regression
    classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the preceding code trains a linear regression model with no
    regularization using the Stochastic Gradient Descent. This solves the least squares
    regression formulation f *(weights) = 1/n ||A weights-y||^2^*, which is the mean
    squared error. Here the data matrix has *n* rows, and the input RDD holds the
    set of rows of A, each with its corresponding right-hand side label y. Also, to
    train the model, it takes the training set, number of iterations, and the step
    size. We provide some random values for the last two parameters here.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 6: Model saving for future use**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s save the model that we just created for future use. It''s pretty
    simple - just use the following code by specifying the storage location as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the model is saved in your desired location, you will see the following
    output in your Eclipse console:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Developing a heart diseases predictive model](img/00047.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: The log after the model is saved to the storage'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 7: Evaluate the model with a test set**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s calculate the prediction score on the test dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Predict the accuracy of the prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'The output appears as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 8: Predictive analytics using a different classifier**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Unfortunately, there is no prediction accuracy at all, right? There might be
    several reasons for that, including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The dataset characteristic
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model selection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parameters selection - also called hyperparameter tuning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For simplicity, we assume the dataset is okay since, as we have already said,
    it is a widely used dataset used for machine learning research used by many researchers
    around the globe. Now, what next? Let''s consider another classifier algorithm,
    for example, a Random forest or decision tree classifier. What about the Random
    forest? Let''s go for the random forest classifier at second place. Just use the
    following code to train the model using the training set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Now use the `HashMap` to restrict the delicacy in the tree construction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Now declare the other parameters needed to train the Random Forest classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'We believe the parameters used by the `trainClassifier()` method are self-explanatory
    and so we''ll leave it to the readers to get to know the significance of each
    parameter. Fantastic! We have trained the model using the Random forest classifier
    and managed the cloud to save the model for future use. Now if you reuse the same
    code that we described in the *Evaluate the model with test set* step, you should
    have the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: Now the predictive accuracy should be much better. If you are still not satisfied,
    you can try with another classifier model such as the Naive Bayes classifier and
    carry out the hyperparameter tuning discussed in [Chapter 7](part0059_split_000.html#1O8H62-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 7. Tuning Machine Learning Models"), *Tuning Machine Learning Models*.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 9: Making the model adaptable for a new dataset**'
  prefs: []
  type: TYPE_NORMAL
- en: We already mentioned that we have saved the model for future use, now we should
    take the opportunity to use the same model for new datasets. The reason, if you
    recall the steps, is that we have trained the model using the training set and
    evaluated it using the test set. Now, if you have more data or new data available
    to be used, what will you do? Will you go for re-training the model? Of course
    not, since you will have to iterate several steps and you will have to sacrifice
    valuable time and cost too.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, it would be wise to use the already trained model and predict the
    performance on a new dataset. Well, now let''s reuse the stored model. Note that
    you will have to reuse the same model that is to be trained for the same model.
    For example, if you have done the model training using the Random forest classifier
    and saved the model while reusing it, you will have to use the same classifier
    model to load the saved model. Therefore, we will use the Random forest to load
    the model while using the new dataset. Use the following code to do that. Now
    create an RDD label point from the new dataset (that is, the Hungarian database
    with the same 14 attributes):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s load the saved model using the Random forest model algorithm as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s calculate the prediction on the test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Now calculate the accuracy of the prediction as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'We should have the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: Now train the NaÃ¯ve Bayesian classifier and see the predictive performance.
    Just download the source code for the Naive Bayesian classifier and run the code
    as a Maven-friendly project using the `pom.xml` file that includes all the dependencies
    of the required JARs and APIs.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table shows a comparison of the predictive accuracies among three
    classifiers (that is, Linear Regression, Random Forest, and the Naive Bayesian
    classifier). Note that, depending upon the training, the model you get might have
    different output since we randomly split the dataset into training and testing:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Classifier** | **Model building time** | **Model saving time** | **Accuracy**
    |'
  prefs: []
  type: TYPE_TB
- en: '| Linear regression | 1199 ms | 2563 ms | 0.0% |'
  prefs: []
  type: TYPE_TB
- en: '| NaÃ¯ve Bayes | 873 ms | 2514 ms | 45% |'
  prefs: []
  type: TYPE_TB
- en: '| Random forest | 2120 ms | 2538 ms | 91% |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Comparison between three classifiers'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We avail the preceding output in a machine with a Windows 7(64-bit), Core i7
    (2.90GHz) processor, and 32GB of main memory. Therefore, depending upon your OS
    type and hardware configuration, you might receive different results.
  prefs: []
  type: TYPE_NORMAL
- en: This way the ML model can be made adaptable for the new data type. However,
    make sure that you use the same classifier or regressor to train and reuse the
    model to make the ML application adaptable.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning in dynamic environments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Making a prediction in dynamic environments does not always succeed in producing
    desired outcomes, particularly in complex and unstructured data.
  prefs: []
  type: TYPE_NORMAL
- en: There are several reasons for that. For example, how do you infer a realistic
    outcome from a bit of data or deal with unstructured and high dimensional data
    that has been found too tedious? Moreover, model revision with efficient strategies
    to control the realistic environments is also costly.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, sometimes the dimensionality of the input dataset is high. Consequently,
    data might be too dense or very sparse. In that case, how you deal with very large
    settings and how to apply the static models in emerging application areas such
    as robotics, image processing, deep learning, computer vision, or web mining is
    challenging. On the other hand, ensemble methods are becoming more popular for
    selecting and combining models from existing models to make the ML model more
    adaptable. A hierarchical and dynamic environment-based learning is shown in *Figure
    10:*
  prefs: []
  type: TYPE_NORMAL
- en: '![Machine learning in dynamic environments](img/00021.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: The hierarchy of machine learning in a dynamic environment'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, ML techniques such as neural networks and statistically-based
    learning are also becoming popular for their success with numerous applications
    in industry and research such as biological systems. In particular, classical
    learning algorithms such as neural networks, decision trees, or vector quantizes
    are often restricted to purely feedforward settings, and simple vectorial data,
    instead of dynamic environments. The feature of vectorization often provides a
    better prediction because of the rich structure. In summary, there are three challenges
    in developing ML applications in a dynamic environment:'
  prefs: []
  type: TYPE_NORMAL
- en: How does the data structure emerge to shape in an autonomous environment?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do we deal with input data that is statistically sparse and high dimensional?
    More specifically, what about making predictive analysis using online algorithms
    for large-scale datasets, applying the dimensionality reduction and so on?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With only limited reinforcement signals, ill-posed domains, or partially underspecified
    settings, how do we develop controlled and effective strategies in dynamic environments?
    Considering these issues and promising advancement in the research, in this section,
    we will provide some insights into online learning techniques through a statistical
    and adversarial model. Since learning in a dynamic environment such as streaming
    will be discussed in [Chapter 9](part0073_split_000.html#25JP22-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 9.  Advanced Machine Learning with Streaming and Graph Data"), *Advanced
    Machine Learning with Streaming and Graph Data*, we will not discuss streaming-based
    learning in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Online learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Batch learning techniques generate the best predictor by learning on the entire
    training dataset at once and are often called static learning. Static learning
    algorithms take batches of training data to train a model, then a prediction is
    made using the test sample and the found relationship, whereas online learning
    algorithms take an initial guess model and then pick up a one-one observation
    from the training population and recalibrate the weights on each input parameter.
    Data usually becomes available in a sequential order as batches. The sequential
    data is used to update the best predictor of the outcome at each step as outlined
    in *Figure 11*. There are three use cases of online-based learning:'
  prefs: []
  type: TYPE_NORMAL
- en: Firstly, where it is computationally infeasible to train an ML model over the
    entire dataset, an online learning is commonly used
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Secondly, it is also used in a situation where it is necessary for the algorithm
    to dynamically adapt to new patterns in the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thirdly, it is used when the data itself is generated as a function of time,
    for example, the stock price prediction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Online learning, therefore, requires out-of-core algorithms, that is, algorithms
    that can perform considering the constraints of networks. There are two general
    modeling strategies that exist for online learning models:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Statistical learning models**: For example, stochastic gradient descent and
    perceptron'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Adversarial models**: For example, spam filtering falls into this category,
    as the adversary will dynamically generate new spam based on the current behavior
    of the spam detector'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Although online and incremental learning techniques are similar, they also differ
    slightly. In online, it's generally a single pass (epoch=1) or a number of epochs
    that could be configured, whereas, incremental would mean that you already have
    a model. No matter how it is built, the model can be mutable by new examples.
    Also, a combination of online and incremental is often what is required.
  prefs: []
  type: TYPE_NORMAL
- en: Data is being generated in an unprecedented way everywhere, every day. This
    huge data imposes an enormous challenge to building ML tools that can handle data
    with high volume, velocity, and veracity. In short, data generated online is also
    big data. Therefore, we need to know the technique by which to learn about the
    online learning algorithms that are meant to handle data with such high volume
    and velocity with limited performance machines.
  prefs: []
  type: TYPE_NORMAL
- en: '![Online learning](img/00016.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: Batch (static) versus online learning, an overview'
  prefs: []
  type: TYPE_NORMAL
- en: Statistical learning model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As already outlined, in statistically-based learning models such as **stochastic
    gradient descents** **(SGD)** and artificial neural networks or perceptron, data
    samples are assumed to be independent of each other. In addition to this, it is
    also assumed that the dataset is identically distributed as random variables.
    In other words, they don't adapt with time. Therefore, an ML algorithm has a limited
    access to the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the field of the statistical learning model there are two interpretations
    that are considered significant:'
  prefs: []
  type: TYPE_NORMAL
- en: '**First interpretation**: This considers the stochastic gradient descent method
    as applied to the problem of minimizing the expected risks. In an infinite stream
    of data, the predictive analytics is assumed to be drawn from the normal distribution.
    Therefore only the stochastic gradient descent method is used to bind the deviation.
    This interpretation is also valid for the finite training set.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Second interpretation:** This applies to the case of a finite training set
    and considers the SGD algorithm as an instance of incremental gradient descent
    method. In this case, one instead looks at the empirical risk: Since the gradients
    of in the incremental gradient descent, iterations are also stochastic estimates
    of the gradient of, this interpretation but applied to minimize the empirical
    risk as opposed to the expected risk. For why multiple passes through the data
    are readily allowed and actually lead to tighter bounds on the deviations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adversarial model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Classical machine learning, which is especially taught in classes, emphasizes
    a static environment where usually unchanging data is used to make predictions.
    It is, therefore, formally easier compared to a statistical or causal inference
    or dynamic environment. On the other hand, finding and solving the learning problem
    as a game between two players, for example, learner versus data generator in a
    dynamic environment, is an example of an adversarial model. This kind of modeling
    and making predictive analytics is critically tedious since the world does not
    know that you are trying to model it formally.
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, your model does not have any positive or negative effect on the
    world. Therefore, the ultimate goal of this kind of model is to minimize losses
    prevailing from circumstances generated by the move made and played by the other
    player. The opponent can adapt the data generated based on the output of the learning
    algorithm in run-time or dynamically. Since no distributional assumptions are
    made about the data, performing well for the entire sequence that could be viewed
    ahead of time becomes the ultimate goal. Additionally, regret is to be minimized
    on the hypothesis at the last pace. According to Cathy O. et al (*Weapons of Math
    Destruction*, Cathy O''Neil, and Crown, September 6, 2016) t adversarial-based
    machine learning can be defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Adversarial machine learning is the formal name for studying what happens when
    conceding even a slightly more realistic alternative to assumptions of these types
    (harmlessly called **relaxing assumptions**).
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Up to the Spark 2.0.0 release, there was no formal algorithm implemented in
    the release. Therefore, we were unable to provide any concrete examples that could
    be further explained elaborately. Interested readers should check the latest Spark
    release to understand the updates.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we tried to cover some advanced machine learning techniques
    to make machine learning models and applications adaptable for new problem and
    data types.
  prefs: []
  type: TYPE_NORMAL
- en: We have shown several examples of machine learning algorithms that learn from
    batch or static-based learning over the data of models that are updated each time
    they see a new training instance.
  prefs: []
  type: TYPE_NORMAL
- en: We have also discussed how to make the models adaptable through generalization,
    through incremental learning, through model reusing, and in dynamic environments.
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 9](part0073_split_000.html#25JP22-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 9.  Advanced Machine Learning with Streaming and Graph Data"), *Advanced
    Machine Learning with Streaming and Graph Data*, we will guide you on how to apply
    machine learning techniques with the help of Spark MLlib and Spark ML on streaming
    and graph data, for example, topic modeling.
  prefs: []
  type: TYPE_NORMAL
