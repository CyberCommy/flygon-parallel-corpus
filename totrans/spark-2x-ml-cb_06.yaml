- en: Practical Machine Learning with Regression and Classification in Spark 2.0 -
    Part II
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression with SGD optimization in Spark 2.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logistic regression with SGD optimization in Spark 2.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ridge regression with SGD optimization in Spark 2.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lasso regression with SGD optimization in Spark 2.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logistic regression with L-BFGS optimization in Spark 2.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Support Vector Machine (SVM) with Spark 2.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Naïve Bayes machine learning with Spark 2.0 MLlib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring ML pipelines and DataFrames using logistic regression in Spark 2.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, the second half of regression and classification in Spark 2.0,
    we highlight RDD-based regression, which is currently in practice in a lot of
    existing Spark ML implementations. Any intermediate to advanced practitioner is
    expected to be able to work with these techniques due to the existing code base.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you will learn how to implement a small application using various
    regressions (linear, logistic, ridge, and lasso) with **Stochastic Gradient Descent**
    (**SGD**) and L-BFGS optimization with linear yet powerful classifiers such as
    **Support Vector Machines** (**SVM**) and **Naive Bayes** **classifiers** using
    the Apache Spark API. We augment each recipe with sample fit measurement when
    appropriate (for example, MSE, RMSE, ROC, and binary and multi-class metrics)
    to demonstrate the power and completeness of Spark MLlib. We introduce RDD-based
    linear, logistic, ridge, and lasso regression, and then discuss SVM and Naïve
    Bayes to demonstrate more sophisticated classifiers.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram depicts the regression and classification coverage in
    this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00129.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: There have been reports of issues with regression using SGD in the field, but
    the issues are most likely due to poor tuning of SGD or a failure to understand
    the pros and cons of this technique in large parametric systems.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter and going forward, we start moving toward more complete (plug
    and play) regression and classification systems that can be leveraged while building
    ML applications. While each recipe is a program by itself, a more complicated
    system can be assembled using Spark's ML pipeline to create an end-to-end ML system
    (for example, classifying cancer clusters via Naive Bayes then performing parameter
    selection on each segment using lasso). You will see a good example of ML pipelines
    in the last recipe in this chapter. While the two regression and classification
    chapters give you a good sample of what is available in Spark 2.0 classification,
    we reserve the more complicated methods for later chapters.
  prefs: []
  type: TYPE_NORMAL
- en: It is preferable to use the latest methods in data science, but it is important
    to master the fundamentals first, starting with GLM, LRM, ridge, lasso, and SVM
    - make sure you understand when to use each model before graduating to more complex
    models.
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression with SGD optimization in Spark 2.0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we use Spark RDD-based regression API to demonstrate how to
    use an iterative optimization technique to minimize the cost function and arrive
    at a solution for a linear regression.
  prefs: []
  type: TYPE_NORMAL
- en: We examine how Spark uses an iterative method to converge on a solution to the
    regression problem using a well-known method called **Gradient Descent**. Spark
    provides a more practical implementation known as SGD, which is used to compute
    the intercept (in this case set to 0) and the weights for the parameters.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We use a housing dataset from the UCI machine library depository. You can download
    the entire dataset from the following URL:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://archive.ics.uci.edu/ml/machine-learning-databases/housing/](https://archive.ics.uci.edu/ml/machine-learning-databases/housing/)'
  prefs: []
  type: TYPE_NORMAL
- en: The dataset comprises 14 columns with the first 13 columns being the independent
    variables (features) that try to explain the median price (last column) of an
    owner-occupied house in Boston, USA.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have chosen and cleaned the first eight columns as features. We use the
    first 200 rows to train and predict the median price:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00130.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the necessary packages for Spark session to gain access to the cluster
    and `Log4j.Logger` to reduce the amount of output produced by Spark:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize a SparkSession specifying configurations with the builder pattern,
    thus making an entry point available for the Spark cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Set the output level to ERROR to reduce Spark''s output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We ingest and parallelize the dataset (first 200 rows only):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We take the parallelized RDD (that is, the data variable) and split the columns
    using a `map()` function. We then proceed to walk through the columns and store
    them in a structure required by Spark (LabeledPoint). LabeledPoint is a data structure
    with the first column being the dependent variable (that is, label) followed by
    a DenseVector (that is, `Vectors.Dense`). We must present the data in this format
    for Spark''s `LinearRegressionWithSGD()` algorithm:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We now examine the regression data via the output to get ourselves familiar
    with the LabeledPoint data structure:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We set model parameters, which are the number of iterations and SGD steps.
    Since this is a gradient descent approach, one must experiment with various values
    to find the optimal values that result in a good fit and avoid wasting resources.
    We usually use values ranging from 100 to 20000 (rare cases) for iterations and
    values ranging from .01 to .00001 for SGD steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We make a call to build the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'In this step, we use the dataset to predict values using the model that was
    built in the previous step. We then place the predicted and labelled values in
    the `predictedLabelValue` data structure. To clarify, the previous step was to
    build a model (that is, decide on the fit for the data), while this step uses
    the model to predict:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'In this step, we examine the intercept (by default, no intercept selected)
    and the weights for the eight columns (columns 0 to 7):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'To get a feel for the predicted value, we randomly select twenty values without
    replacement using the `takesample()` function. In this case, we show the values
    for only seven of the twenty values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'We use Root Mean Squared Error (one of the many) to quantify the fit. The fit
    can be improved drastically (more data, stepsSGD, the number of iterations, and
    most importantly, experimentation with feature engineering), but we leave that
    to a statistics book to explore. Here is the formula for RMSD:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00131.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We used selected columns from a housing data (independent variables) file to
    predict housing prices (dependent variable). We used the RDD-based regression
    method with an SGD optimizer to iterate toward a solution. We then proceeded to
    output the intercept and each parameter's weight. In the last step, we predicted
    with sample data and showed the output. The last step was to output the MSE and
    RMSE values for the model. Please note that this was for demonstration purposes
    only and you should use the evaluation metrics demonstrated in [Chapter 4](part0200.html#5UNGG0-4d291c9fed174a6992fd24938c2f9c77),
    *Common Recipes for Implementing a Robust Machine Learning System*, for model
    evaluation and the final selection process.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Signatures for this method constructor are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Defaults for parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`stepSize= 1.0`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`numIterations= 100`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '``miniBatchFraction= 1.0``'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`miniBatchFraction` is an important parameter that can have a significant impact
    on performance. This is what is referred to as batch gradient versus gradient
    in academic literature.'
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can also change the default intercept behavior using the constructor to
    create a new regression model and then using `setIntercept(true)` accordingly:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The sample code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'If the weight for the model is computed as NaN, you must change the model parameters
    (SGD steps or number of iterations) until you get convergence. An example is model
    weights that are not computed correctly (there is a convergence problem with SGD
    in general) due to poor parameter selection. The first move should be to use a
    more fine-grain step parameter for SGD:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We cover both Gradient Descent and SGD in detail in [Chapter 9](part0437.html#D0O5Q0-4d291c9fed174a6992fd24938c2f9c77),
    *Optimization - Going Down the Hill with Gradient Descent*. In this chapter, the
    reader should abstract the SGD as an optimization technique that minimizes the
    loss function for fitting a line to a series of points. There are parameters that
    will affect the behavior of the SGD and we encourage the reader to change these
    parameters to either extreme to observe poor performance and non-convergence (that
    is, the result will appear as NaN).
  prefs: []
  type: TYPE_NORMAL
- en: 'The documentation for the `LinearRegressionWithSGD()` constructor is available
    at the following URL:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.package](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.package)'
  prefs: []
  type: TYPE_NORMAL
- en: Logistic regression with SGD optimization in Spark 2.0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we use admission data from the UCI Machine Library Repository
    to build and then train a model to predict student admissions based on a given
    set of features (GRE, GPA, and Rank) used during the admission process using the
    RDD-based `LogisticRegressionWithSGD()` Apache Spark API set.
  prefs: []
  type: TYPE_NORMAL
- en: This recipe demonstrates both optimization (SGD) and regularization (penalizing
    the model for complexity or over-fitting). We emphasize that they are two different
    things and often cause confusion to beginners. In the upcoming chapter, we demonstrate
    both concepts in more detail since understanding both is fundamental to a successful
    study of ML.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We use the admission dataset from the UCLA **Institute for Digital Research**
    **and** **Education** (**IDRE**). You can download the entire dataset from the
    following URLs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For home page, you can refer to [http://www.ats.ucla.edu/stat/](http://www.ats.ucla.edu/stat/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For data file, you can refer to [https://stats.idre.ucla.edu/stat/data/binary.csv](https://stats.idre.ucla.edu/stat/data/binary.csv)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The dataset comprises four columns, with the first column being the dependent
    variable (label - whether the student was admitted or not) and the next three
    columns being the explanatory variables, that is, the features that will explain
    the admission of a student.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have chosen and cleaned the first three columns as features. We use the
    first 200 rows to train and predict the median price:'
  prefs: []
  type: TYPE_NORMAL
- en: Admission - 0, 1 indicating whether the student was admitted or not
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: GRE - The score from Graduate Record Examination
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GPA - Grade Point Average score
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RANK - The ranking
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here''s sample data from the first 10 rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00132.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the necessary packages for the Spark session to gain access to the cluster,
    and `Log4j.Logger` to reduce the amount of output produced by Spark:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize a `SparkSession` specifying configurations with the builder pattern,
    thus making an entry point available for the Spark cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Set the output level to `ERROR` to reduce Spark''s output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Load the data file and turn it into RDDs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Ingest the data by splitting it and then converting to double while building
    a LabeledPoint (a data structure required by Spark) dataset. Column 1 (position
    0) is the dependent variable in the regression, while columns 2 through 4 (GRE,
    GPA, Rank) are features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'We examine the dataset after loading, which is always recommended, and also
    demonstrate the label point internals, which is a single value (for example, label
    or dependent variable), followed by a DenseVector of features we are trying to
    use to explain the dependent variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: We set up the model parameter for `LogisticRegressionWithSGD()`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'These parameters ultimately control the fit, and hence, some level of experimentation
    is required to achieve a good fit. We saw the first two parameters in the previous
    recipe. The third parameter''s value will affect the selection of the weights.
    You must experiment and use model selection techniques to decide the final value.
    In the *There''s more...* section of this recipe, we show the weight selection
    of the features (that is, which weights are set to 0) based on two extreme values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Create and train the logistic regression model with a call to `LogisticRegressionWithSGD()`
    using the LabeledPoint and the preceding parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Predict the values using our model and the dataset (similar to all Spark regression
    methods):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: We print our model intercept and weights. If you compare values with linear
    or ridge regression, you will see the selection effect. The effect will be more
    dramatic at extreme values or when choosing a dataset with more collinearity.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In this example, lasso eliminated three parameters by setting the weights to
    0.0 using a regularization parameter (for example, 4.13):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: The principles of model parameter selection from statistics still applies whether
    we use Spark MLlib or not. For example, a parameter weight of -8.625514722380274E-6
    might be too small to include in the model. We need to look at `t-statistic` and
    `p value` for each parameter and decide on the final model.
  prefs: []
  type: TYPE_NORMAL
- en: 'We randomly choose 20 predicted values and visually inspect the predicted result
    (only the first five values are shown here):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'We calculate the RMSE and show the result:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We used the admission data and tried to use the logistic regression with some
    of the features in order to predict whether a student with a given feature set
    (vector) will be admitted or not (the label). We fitted the regression, set SGD
    parameters (you should experiment), and ran the API. We then output intercept
    and model weights for the regression coefficients. Using the model, we predicted
    and output some predicted values for visual inspection. The last step was to output
    MSE and RMSE values for the model. Please note that this was for demonstration
    purposes only and you should use evaluation metrics demonstrated in the previous
    chapter for model evaluation and the final selection process. Looking at SME and
    RMSE, we probably need a different model, parameter settings, parameters, or more
    data points to do a better job.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Signatures for this method constructor are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Defaults for Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`stepSize`= 1.0'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`numIterations`= 100'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`regParm`= 0.01'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`miniBatchFraction`= 1.0'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While non-logistic regression attempts to discover a linear or non-linear relationship
    that relates the explanatory factors (features) to a numeric variable on the left-hand
    side of the equation, logistic regression attempts to classify a feature set to
    a set of discrete classes (for example, pass/fail, good/bad, or multi-class).
  prefs: []
  type: TYPE_NORMAL
- en: The best way to understand logistic regression is to think of the domain on
    the left-side as a discrete set of outcomes (that is, the classification class)
    that is used to label a new prediction. Using a discrete label (for example, 0
    or 1) we are able to predict whether a set of features belong to a specific class
    (for example, the presence or absence of disease).
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00133.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In short, the main difference between regular regression and logistic regression
    is the type of variable that can be used on the left-hand side. In regular regression,
    the predicted outcome (that is, the label) will be a numeric value, while in logistic
    regression, prediction is a selection from a discrete class of possible outcomes
    (that is, labels).
  prefs: []
  type: TYPE_NORMAL
- en: In the interest of time and space, we do not cover breaking a dataset to train
    and test in every recipe since we have demonstrated this in the previous recipes.
    We also do not use any caching, but emphasize that a real-life application must
    cache the data due to the way lazy instantiation, staging, and optimization is
    used in Spark. See [Chapter 4](part0200.html#5UNGG0-4d291c9fed174a6992fd24938c2f9c77),
    *Common Recipes for Implementing a Robust Machine Learning System*, to refer to
    recipes regarding caching and training/test data split during ML development.
  prefs: []
  type: TYPE_NORMAL
- en: If the weight for the model is computed as NaN, you must change the model parameters
    (that is, SGD steps or number of iterations) until you get convergence.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here''s the documentation for the constructor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.classification.LogisticRegressionWithSGD](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.classification.LogisticRegressionWithSGD)'
  prefs: []
  type: TYPE_NORMAL
- en: Ridge regression with SGD optimization in Spark 2.0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we use admission data from the UCI Machine Library Repository
    to build and then train a model to predict student admission using the RDD-based
    `LogisticRegressionWithSGD()` Apache Spark API set. We use a given set of features
    (GRE, GPA, and Rank) used during the admission to predict model weights using
    ridge regression. We demonstrate the input feature standardization in a different
    recipe, but it should be noted that parameter standardization has an important
    effect on the results, especially in a ridge regression setting.
  prefs: []
  type: TYPE_NORMAL
- en: Spark's ridge regression API (`LogisticRegressionWithSGD`) is meant to deal
    with multicollinearity (the explanatory variable or features are correlated and
    the assumption of intendent and randomly distributed feature variables are somewhat
    flawed). Ridge is about shrinking (penalizing via L2 regularization or a quadratic
    function) some of the parameters, therefore reducing their effect and in turn
    reducing complexity. It is critical to remember that `LogisticRegressionWithSGD()`
    does not have a lasso effect in which some of the parameters are actually reduced
    to zero (that is, eliminated). Ridge regression only shrinks the parameter and
    does not set them to zero (a small effect will still remain after shrinkage).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We use a housing dataset from UCI Machine Library Repository. You can download
    the entire dataset from the following URL:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://archive.ics.uci.edu/ml/machine-learning-databases/housing/](https://archive.ics.uci.edu/ml/machine-learning-databases/housing/)'
  prefs: []
  type: TYPE_NORMAL
- en: The dataset comprises 14 columns, with the first 13 columns being the independent
    variables (that is, features) that try to explain the median price (the last column)
    of an owner-occupied house in Boston, USA.
  prefs: []
  type: TYPE_NORMAL
- en: We have chosen and cleaned the first eight columns as features. We use the first
    200 rows to train and predict the median price.
  prefs: []
  type: TYPE_NORMAL
- en: '| 1 | CRIM | Per capita crime rate by town |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | ZN | Proportion of residential land zoned for lots over 25,000 sq. ft.
    |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | INDUS | Proportion of non-retail business acres per town |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | CHAS | Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)
    |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | NOX | Nitric oxides concentration (parts per 10 million) |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | RM | Average number of rooms per dwelling |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | AGE | Proportion of owner-occupied units built prior to 1940 |'
  prefs: []
  type: TYPE_TB
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the necessary packages for SparkSession to gain access to the cluster
    and `Log4j.Logger` to reduce the amount of output produced by Spark:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize a SparkSession specifying configurations with the builder pattern,
    thus making an entry point available for the Spark cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'To effectively demonstrate the shrinkage of the model parameter (it will shrink
    to a small value, but never get eliminated) in ridge regression, we use the same
    housing data file, and clean and use the first eight columns to predict the value
    of last column (median housing price):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Ingest the data by splitting it and then converting to double while building
    a LabeledPoint (a data structure required by Spark) dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Examine the dataset after loading, which is always recommended, and also demonstrate
    the LabeledPoint internals, which is a single value (label/ dependent variable),
    followed by a DenseVector of the features we are trying to use to explain the
    dependent variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: We set up the model parameter for `RidgeRegressionWithSGD()`.
  prefs: []
  type: TYPE_NORMAL
- en: In the *There's more...* section of this recipe, we show the shrinkage effect
    based on two extreme values.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Create and train the ridge regression model with a call to `RidgeRegressionWithSGD()`
    and LabeledPoint using the preceding parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Predict values using our model and the dataset (similar to all Spark regression
    methods):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Print the model intercept and weights. If you compare values with linear regression,
    you will see the shrinkage effect. The effect will be more dramatic at extreme
    values or when choosing a dataset with more collinearity:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Randomly choose 20 predicted values and visually inspect the predicted result
    (only the first five values are shown):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Calculate the RMSE and show the result:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To be able to compare with other regression methods and see the shrinkage effect,
    we used the housing data again and trained a model using `RidgeRegressionWithSGD.train`*.*
    After fitting the model, we output intercept and parameter weights for the model
    that we just trained. We then proceeded to predict the values using the `*.predict()*`
    API. We printed the predicted values and visually inspected the first 20 numbers
    before outputting the MSE and RMSE.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Signature for this method constructor is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: These parameters ultimately control the fit, and hence, some level of experimentation
    is required to achieve a good fit. We saw the first two parameters in the previous
    recipe. The third parameter will affect the shrinkage of the weights based on
    the value selected. You must experiment and use model selection techniques to
    decide the final value.
  prefs: []
  type: TYPE_NORMAL
- en: 'Defaults for Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`stepSize`= 1.0'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`numIterations`= 100'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`regParm`= 0.01'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`miniBatchFraction`= 1.0'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We cover optimization and L1 (absolute value) versus L2 (quadratic) in detail
    in [Chapter 9](part0437.html#D0O5Q0-4d291c9fed174a6992fd24938c2f9c77), *Optimization
    - Going Down the Hill with Gradient Descent*, but for the purposes of this recipe
    the reader should understand that ridge regression uses L2 to penalize (that is,
    shrink some of the parameters) while the upcoming recipe, *Lasso regression with
    SGD optimization in Spark 2.0*, uses L1 to penalize (that is, eliminate some of
    the parameters based on the threshold used). We encourage the user to compare
    the weights of this recipe with the linear and lasso regression recipes to see
    the effect first-hand. We use the same housing dataset to demonstrate the effect.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows ridge regression with a regularization function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00134.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In short, it is a remedy to deal with feature dependency by adding a small bias
    factor (ridge regression) that reduces the variables using a regularization penalty.
    Ridge regression shrinks the explanatory variable but never sets it to 0, unlike
    lasso regression, which will eliminate variables.
  prefs: []
  type: TYPE_NORMAL
- en: The scope of this recipe is limited to a demonstration of the API call for ridge
    regression in Spark. The math and a deep explanation for ridge regression is a
    topic of multiple chapters in statistical books. For better understanding, we
    strongly recommend that the reader familiarizes himself/herself with the concept
    while taking into account the topics of L1, L2, ... L4 regularization along with
    the relationship between ridge and linear PCA.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The amount of shrinkage for parameters varies by parameter selection, but the
    presence of collinearity is required for the weights to shrink. You can demonstrate
    this to yourself by using truly random (IID explanatory variables generated by
    a random generator) versus features that are highly dependent on one another (for
    example, waist line and weight).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are two examples of extreme regularization values and their effect on
    model weights and shrinkage:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: If the weight for the model is computed as NaN, you must change the model parameters
    (SGD steps or number of iterations) until you get convergence.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of model weights that are not computed correctly due to
    poor parameter selection. The first move should be to use a more fine-grained
    step parameter for SGD:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here''s the documentation for the constructor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.regression.RidgeRegressionWithSGD](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.regression.RidgeRegressionWithSGD)'
  prefs: []
  type: TYPE_NORMAL
- en: Lasso regression with SGD optimization in Spark 2.0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will use the housing dataset from the previous recipes to
    demonstrate shrinkage with Spark's RDD-based lasso regression `LassoWithSGD()`,
    which can select a subset of parameters by setting the other weights to zero (hence
    eliminating some parameters based on the threshold) while reducing the effect
    of others (regularization). We emphasize again that ridge regression reduces the
    parameter weight, but never sets it to zero.
  prefs: []
  type: TYPE_NORMAL
- en: '`LassoWithSGD()`, which is Spark''s RDD-based lasso (Least Absolute Shrinkage
    and Selection Operator) API, a regression method that performs both variable selection
    and regularization at the same time in order to eliminate non-contributing explanatory
    variables (that is, features), therefore enhancing the prediction''s accuracy.
    Lasso, which is based on **Ordinary Least Squares** (**OLS**), can be easily extended
    to other methods, such as **General Liner Methods** (**GLM**).'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the necessary packages for SparkSession to gain access to the cluster
    and `Log4j.Logger` to reduce the amount of output produced by Spark:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize a SparkSession specifying configurations with the builder pattern,
    thus making an entry point available for the Spark cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'To effectively demonstrate the shrinkage of the model parameter (it will shrink
    to a small value but never get eliminated) in ridge regression, we use the same
    housing data file and clean and use the first eight columns to predict the value
    of the last column (median housing price):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'We ingest the data by splitting it and then converting it to double while building
    a LabeledPoint (a data structure required by Spark) dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'We examine the dataset after loading, which is always recommended, and also
    demonstrate the Label point internals, which is a single value (that is, the label/dependent
    variable), followed by a DenseVector of the features we are trying to use to explain
    the dependent variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'We set up the model parameter for `lassoWithSGD()`. These parameters ultimately
    control the fit, so some level of experimentation is required to achieve a good
    fit. We saw the first two parameters in the previous recipe. The third parameter''s
    value will affect the selection of the weights. You must experiment and use model
    selection techniques to decide the final value. In the *There''s more...* section
    of this recipe, we show the weight selection of the features (that is, which weights
    are set to 0) based on two extreme values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'We create and train the ridge regression model with a call to `RidgeRegressionWithSGD()`
    and our LabeledPoint using the preceding parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'We predict values using our model and the dataset (similar to all Spark regression
    methods):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: We print our model intercept and weights. If you compare values with linear
    or ridge regression, you will see the selection effect. The effect will be more
    dramatic at extreme values or when choosing a dataset with more collinearity.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In this example, lasso eliminated three parameters by setting the weights to
    0.0 using a regularization parameter (for example, 4.13):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'We randomly choose 20 predicted values and visually inspect the predicted result
    (only the first five values shown here):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'We calculate the RMSE and show the result:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Again, we used the housing data so we can compare this method with ridge regression
    and show how lasso not only shrinks the parameters, like ridge regression, but
    it goes all the way and sets the parameters that are not significantly contributing
    to zero.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Signatures for this method constructor are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'Defaults for Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`stepSize`= 1.0'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`numIterations`= 100'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`regParm`= 0.01'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`miniBatchFraction`= 1.0'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As a reminder, ridge regression reduces the parameter's weight but does not
    eliminate them. When dealing with a huge number of parameters without a deep learning
    system in data mining/machine learning, lasso is usually preferred to reduce the
    number of inputs in the early stages of a ML pipeline, at least in the exploration
    phase.
  prefs: []
  type: TYPE_NORMAL
- en: Lasso plays a substantial role in advanced data mining and machine learning
    due to its ability to select a subset of the weights (that is, parameters) based
    on the threshold. In short, lasso regression decides which parameters to include
    or exclude (that is, set weight to 0) based on the threshold.
  prefs: []
  type: TYPE_NORMAL
- en: While ridge regression can substantially reduce a parameter's contribution to
    the overall result, it will never reduce the weight to zero. Lasso regression
    differs from ridge regression by being able to reduce the weight of a feature's
    contribution to zero (hence selecting a subset of features that contribute the
    most).
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00135.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The parameter selection (that is, setting some weights to zero) varies by regularization
    parameter value.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s two examples of extreme regularization values and their effect on model
    weights and shrinkage:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, we eliminated one of the parameters using lasso:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, we eliminated four of the parameters using lasso:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: If the weight for the model is computed as NaN, you must change the model parameters
    (that is, SGD steps or number of iterations) until you get convergence.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of model weights that are not computed correctly (that is,
    convergence problem with SGD in general) due to poor parameter selection. The
    first move should be to use a more fine-grained step parameter for SGD:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here is the documentation for the constructor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.regression.LassoWithSGD](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.regression.LassoWithSGD)'
  prefs: []
  type: TYPE_NORMAL
- en: Logistic regression with L-BFGS optimization in Spark 2.0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will use the UCI admission dataset again so we can demonstrate
    Spark's RDD-based logistic regression solution, `LogisticRegressionWithLBFGS()`,
    for an extremely large number of parameters that are present in certain types
    of ML problem.
  prefs: []
  type: TYPE_NORMAL
- en: We recommend L-BFGS for very large variable space since the Hessian matrix of
    second derivatives can be approximated using updates. If you have an ML problem
    with millions or billions of parameters, we recommend deep learning techniques.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We use the admission dataset from UCLA IDRE. You can download the entire dataset
    from the following URLs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For home page go through the [http://www.ats.ucla.edu/stat/](http://www.ats.ucla.edu/stat/)
    link.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the data file go through the [https://stats.idre.ucla.edu/stat/data/binary.csv](https://stats.idre.ucla.edu/stat/data/binary.csv)
    link.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The dataset comprises four columns, with the first column being the dependent
    variable (that is, the label - whether the student was admitted or not) and the
    next three columns being the explanatory variables (features that will explain
    the admission of a student).
  prefs: []
  type: TYPE_NORMAL
- en: We have chosen and cleaned the first eight columns as features. We use the first
    200 rows to train and predict the median price.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00136.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here is some sample data from the first three rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00137.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the necessary packages for SparkSession to gain access to the cluster
    and `Log4j.Logger` to reduce the amount of output produced by Spark:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize a SparkSession specifying configurations with the builder pattern,
    thus making an entry point available for the Spark cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'Load the data file and turn it into RDDs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'Ingest the data by splitting it and then converting to double while building
    a LabeledPoint (that is, data structure required by Spark) dataset. Column 1 (that
    is, position 0) is the dependent variable in the regression, while columns 2 through
    4 (that is, GRE, GPA, Rank) are the features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'Examine the dataset after loading, which is always recommended, and also demonstrate
    the Label point internals, which is a single value (that is, the label/dependent
    variable), followed by a DenseVector of the features we are trying to use to explain
    the dependent variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a LBFGS regression object with new operator and set the intercept to
    false so we can compare the result equally with the `logisticregressionWithSGD()`
    recipe:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the `run()` method to create the trained model using the dataset (that
    is, structured as LabeledPoint):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'The model is trained. Use the `predict()` method to have it predict and classify
    the groups accordingly. In the next lines, simply use a dense vector to define
    two students'' data (the GRE, GPA, and Rank features) and have it predict whether
    the student will be admitted or not (0 means denied admission and 1 means the
    student will be admitted):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: 'To show a slightly complicated process, define a SEQ data structure for five
    students and attempt to use `map()` and `predict()` in the next step to predict
    in bulk. It should be obvious that any data file can be read at this point and
    converted so we can predict in larger chunks:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'Now use `map()` and `predict()` to run through the SEQ data structure and produce
    predictions in bulk with the trained model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: 'Look at the output and the resulting predictions for the students. The presence
    of 0 or 1 indicates denial or acceptance of the student based on the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We used the UCI admission data with `LogisticRegressionWithLBFGS()` to predict
    whether a student will be admitted or not. The intercept was set to false and
    the `.run()` and `.predict()` API is used to predict with the fitted model. The
    point here was that L-BFGS is suitable for an extremely large number of parameters,
    particularly when there is a lot of sparsity. Regardless of what optimization
    technique was used, we emphasized again that ridge regression reduces the parameter
    weight, but never sets it to zero.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Signature for this method constructor is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: L-BFGS optimization in Spark, `L-BFGS()`, is based on Newton's optimization
    algorithm (uses curvature in addition to 2^(nd) derivative of the curve at point),
    which can be thought of as a maximizing likelihood function that seeks a stationary
    point on a differentiable function. The convergence for this algorithm should
    be given special attention (that is, require optimal or gradient of zero).
  prefs: []
  type: TYPE_NORMAL
- en: Please note that this recipe is for demonstration purposes only and you should
    use the evaluation metrics demonstrated in [Chapter 4](part0200.html#5UNGG0-4d291c9fed174a6992fd24938c2f9c77),
    *Common Recipes for Implementing a Robust Machine Learning System*,for model evaluation
    and the final selection process.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `LogisticRegressionWithLBFGS()` object has a method called `setNumClasses()`
    that allows it to deal with multinomials (that is, more than two groups). By default,
    it is set to two, which is a binary logistic regression.
  prefs: []
  type: TYPE_NORMAL
- en: L-BFGS is a limited memory adaptation of the original BFGS (Broyden-Fletcher-Goldfarb-Shanno)
    method. L-BFGS is well suited for regression models that deal with a large number
    of variables. It is a form of BFGS approximation with limited memory in which
    it tries to estimate the Hessian matrix while searching through the large search
    space.
  prefs: []
  type: TYPE_NORMAL
- en: We encourage the reader to step back and look at the problem as regression plus
    an optimization technique (regression with SGD versus regression with L-BFGS).
    In this recipe, we used logistic regression, which itself is a form of linear
    regression except with discrete labels, plus an optimization algorithm (that is,
    we choose L-BFGS rather than SGD) to solve the problem.
  prefs: []
  type: TYPE_NORMAL
- en: In order to appreciate the details of L-BFGS, one must understand the Hessian
    matrix and its role, along with the concomitant difficulties with large numbers
    of parameters (Hessian or Jacobian techniques), especially when using a sparse
    matrix configuration in optimization.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here is the documentation for the constructor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.classification.LogisticRegressionWithLBFGS](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.classification.LogisticRegressionWithLBFGS)'
  prefs: []
  type: TYPE_NORMAL
- en: Support Vector Machine (SVM) with Spark 2.0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we use Spark's RDD-based SVM API `SVMWithSGD` with SGD to classify
    the population into two binary classes, and then use count and `BinaryClassificationMetrics`
    to look at model performance.
  prefs: []
  type: TYPE_NORMAL
- en: In the interest of time and space, we use the sample `LIBSVM` format already
    supplied with Spark, but provide links to additional data files offered by National
    Taiwan University so the reader can experiment on their own. **Support Vector
    Machine** (**SVM**) as a concept is fundamentally very simple, unless you want
    to get into the details of its implementation in Spark or any other package.
  prefs: []
  type: TYPE_NORMAL
- en: While the mathematics behind SVM is beyond the scope of this book, readers are
    encouraged to read the following tutorials and the original SVM paper for a deeper
    understanding.
  prefs: []
  type: TYPE_NORMAL
- en: 'The original papers are by *Vapnik* and *Chervonenkis* (1974, 1979 - in Russian)
    and there''s also *Vapnik''s* 1982 translation of his 1979 book:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.amazon.com/Statistical-Learning-Theory-Vladimir-Vapnik/dp/0471030031](https://www.amazon.com/Statistical-Learning-Theory-Vladimir-Vapnik/dp/0471030031)'
  prefs: []
  type: TYPE_NORMAL
- en: 'For a more modern write up, we recommend the following three books from our
    library:'
  prefs: []
  type: TYPE_NORMAL
- en: '*The Nature of Statistical Learning Theory* by V. Vapnik:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.amazon.com/Statistical-Learning-Information-Science-Statistics/dp/0387987800](https://www.amazon.com/Statistical-Learning-Information-Science-Statistics/dp/0387987800)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Learning with Kernels: Support Vector Machines, Regularization, Optimization,
    and Beyond* by B. Scholkopf and A. Smola:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://mitpress.mit.edu/books/learning-kernels](https://mitpress.mit.edu/books/learning-kernels)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Machine Learning: A Probabilistic Perspective* by K. Murphy:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://mitpress.mit.edu/books/machine-learning-0](https://mitpress.mit.edu/books/machine-learning-0)'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the necessary packages for SparkSession to gain access to the cluster
    and `Log4j.Logger` to reduce the amount of output produced by Spark:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize a SparkSession specifying configurations with the builder pattern
    thus making an entry point available for the Spark cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: 'Spark provides the MLUtils package, which enables us to read any file that
    is properly formatted as `libsvm`. We use `LoadLibSVMFile()` to load one of the
    short sample files (100 rows) that is included with Spark for easy experimentation.
    The `sample_libsvm_data` file can be found in the `.../data/mlib/` directory of
    Spark home. We simply copy the file into our own directory on a Windows machine
    as is:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: 'Print and examine the content of the sample file via output. A short version
    of the output is included for reference:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: 'Check to make sure all the data is loaded and there are no duplicates of the
    file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: 'In this step, split the data into two sets (80/20) and get ready to train the
    model accordingly. The `allDataSVM` variable will have two randomly selected sections
    based on the split ratios. The sections can be referenced by an index, 0 and 1
    to refer to the training and test datasets respectively. You can also use a second
    parameter in `randomSplit()` to define the initial seed for random split:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: 'Set the number of iterations to 100\. The next two parameters are SGD steps
    and the regularization parameter - we use the defaults here, but you must experiment
    to make sure the algorithms converge:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: 'After training the model in the previous step, now use the `map()` and `predict()`
    functions to predict the outcome for the test data (that is, index 1 of the split
    data):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: 'Visually examine the predictions via output (shortened for convenience). The
    next steps attempt to quantify how well we did with our predictions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: 'First, use a quick count/ratio method to get a feel for the accuracy. Since
    we did not set the seed, the numbers will vary from run to run (but remain stable):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: Now use a more formal way to quantify the ROC (that is, the area under the curve).
    This is one of the most basic standard measures of accuracy. Readers can find
    many tutorials on this subject. We use a combination of standard and proprietary
    methods (that is, hand-coded) to quantify the measurement.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Spark comes with a binary classification quantification measurement out of
    the box. Use this to collect the measurement:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: 'Access the `areaUnderROC()` method to obtain the ROC measurement:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We used the sample data provided with Spark, which is in `LIBSVM` format, to
    run the SVM classification recipe. After reading the file, we used `SVMWithSGD.train`
    to train the model and then proceeded to predict the data into two sets of labeled
    output, 0 and 1\. We used the `BinaryClassificationMetrics` metric to measure
    the performance. We focused on a popular metric, the area under the ROC curve,
    using `metrics.areaUnderROC()` to measure performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Signature for this method constructor is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: 'Defaults for Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`stepSize`= 1.0'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`numIterations`= 100'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`regParm`= 0.01'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`miniBatchFraction`= 1.0'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is suggested that the readers should experiment with various parameters in
    order to get the best settings.
  prefs: []
  type: TYPE_NORMAL
- en: What makes SVM great is the fact that it is OK for some of the points to fall
    on the wrong side, but the model penalizes the models to pick the best fit.
  prefs: []
  type: TYPE_NORMAL
- en: 'The SVM implementation in Spark uses SGD optimization to classify the labels
    for the feature sets. When we use SVM in Spark, we need to prepare the data into
    a format called `libsvm`. The user can use the following links to understand the
    format and also obtain ready-to-use datasets in `libsvm` format from National
    Taiwan University:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.csie.ntu.edu.tw/~cjlin/libsvm/](https://www.csie.ntu.edu.tw/~cjlin/libsvm/)'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/)'
  prefs: []
  type: TYPE_NORMAL
- en: 'In short, the `libsvm` format is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: You can simply create pipelines with Python or Scala programs to transform text
    files into `libsvm` format as needed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Spark has a good number of sample files for various algorithms in the `/data/mlib`
    directory. We encourage the readers to use these files while getting familiar
    with the Spark MLlib algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: '**Receiver Operating Characteristic** (**ROC**) is a graphical plot that illustrates
    the diagnostic ability of a binary classifier system, as its discrimination threshold
    is varied.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A tutorial for ROC can be found at the following link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://en.wikipedia.org/wiki/Receiver_operating_characteristic](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)'
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can either use publically available data sources in `libsvm` format or
    a Spark API call, `SVMDataGenerator()`, which generates sample data for SVM (that
    is, Gaussian distribution):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: 'The idea behind SVM can be summarized as follows: rather than using a linear
    discriminant (for example, selecting a line among many lines) and an objective
    function (for example, least square minimization) to separate and label the left-hand
    variable, use the largest separating margin (as shown in the following graph)
    first and then draw the solid line in between the largest margin. Another way
    to think about it is how you can use two lines (the dashed lines in the following
    graph) to separate the classes the most (that is, the best and most discriminate
    separators). In short, the wider we can separate the classes, the better the discrimination,
    and hence, more accuracy in labeling the classes.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps to find out more about SVM:'
  prefs: []
  type: TYPE_NORMAL
- en: Pick the widest margin that can best separate the two groups.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Second, draw a line that divides the widest margin. This will serve as a linear
    discriminant.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The objective function: maximize the two separating lines.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00138.gif)'
  prefs: []
  type: TYPE_IMG
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here is the documentation for the constructor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.classification.SVMWithSGD](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.classification.SVMWithSGD)'
  prefs: []
  type: TYPE_NORMAL
- en: Naive Bayes machine learning with Spark 2.0 MLlib
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we use the famous Iris dataset and use Apache Spark API `NaiveBayes()`
    to classify/predict which of the three classes of flower a given set of observations
    belongs to. This is an example of a multi-class classifier and requires multi-class
    metrics for measurements of fit. The previous recipe used a binary classification
    and metric to measure the fit.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For the Naive Bayes exercise, we use a famous dataset called `iris.data`, which
    can be obtained from UCI. The dataset was originally introduced in the 1930s by
    R. Fisher. The set is a multivariate dataset with flower attribute measurements
    classified into three groups.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In short, by measuring four columns, we attempt to classify a species into one
    of the three classes of Iris flower (that is, Iris Setosa, Iris Versicolor, Iris
    Virginica).
  prefs: []
  type: TYPE_NORMAL
- en: 'We can download the data from here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://archive.ics.uci.edu/ml/datasets/Iris/](https://archive.ics.uci.edu/ml/datasets/Iris/)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The column definition is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Sepal length in cm
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: Sepal width in cm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Petal length in cm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Petal width in cm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Class:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: -- Iris Setosa => Replace it with 0
  prefs:
  - PREF_UL
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: -- Iris Versicolour => Replace it with 1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: -- Iris Virginica => Replace it with 2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The steps/actions we need to perform on the data are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Download and then replace column five (that is, the label or classification
    classes) with a numerical value, thus producing the iris.data.prepared data file.
    The Naïve Bayes call requires numerical labels and not text, which is very common
    with most tools.
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: Remove the extra lines at the end of the file.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Remove duplicates within the program by using the `distinct()` call.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the necessary packages for SparkSession to gain access to the cluster
    and `Log4j.Logger` to reduce the amount of output produced by Spark:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize a SparkSession specifying configurations with the builder pattern,
    thus making an entry point available for the Spark cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE
- en: 'Load the `iris.data` file and turn the data file into RDDs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs: []
  type: TYPE_PRE
- en: 'Parse the data using `map()` and then build a LabeledPoint data structure.
    In this case, the last column is the Label and the first four columns are the
    features. Again, we replace the text in the last column (that is, the class of
    Iris) with numeric values (that is, 0, 1, 2) accordingly:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs: []
  type: TYPE_PRE
- en: 'Then make sure that the file does not contain any redundant rows. In this case,
    it has three redundant rows. We will use the distinct dataset going forward:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs: []
  type: TYPE_PRE
- en: 'We inspect the data by examining the output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs: []
  type: TYPE_PRE
- en: 'Split the data into training and test sets using a 30% and 70% ratio. The 13L
    in this case is simply a seeding number (L stands for long data type) to make
    sure the result does not change from run to run when using a `randomSplit()` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs: []
  type: TYPE_PRE
- en: 'Print the count for each set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE109]'
  prefs: []
  type: TYPE_PRE
- en: 'Build the model using `train()` and the training dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE110]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the training dataset plus the `map()` and `predict()` methods to classify
    the flowers based on their features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE111]'
  prefs: []
  type: TYPE_PRE
- en: 'Examine the predictions via the output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE112]'
  prefs: []
  type: TYPE_PRE
- en: 'Use `MulticlassMetrics()` to create metrics for the multi-class classifier.
    As a reminder, this is different from the previous recipe, in which we used `BinaryClassificationMetrics()`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE113]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the commonly used confusion matrix to evaluate the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE114]'
  prefs: []
  type: TYPE_PRE
- en: 'We examine other properties to evaluate the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE115]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We used the IRIS dataset for this recipe, but we prepared the data ahead of
    time and then selected the distinct number of rows by using the `NaiveBayesDataSet.distinct()`
    API. We then proceeded to train the model using the `NaiveBayes.train()` API.
    In the last step, we predicted using `.predict()` and then evaluated the model
    performance via `MulticlassMetrics()` by outputting the confusion matrix, precision,
    and F-Measure metrics.
  prefs: []
  type: TYPE_NORMAL
- en: The idea here was to classify the observations based on a selected feature set
    (that is, feature engineering) into classes that correspond to the left-hand label.
    The difference here was that we are applying joint probability given conditional
    probability to the classification. This concept is known as **Bayes' theorem**,
    which was originally proposed by Thomas Bayes in the 18th century. There is a
    strong assumption of independence that must hold true for the underlying features
    to make Bayes' classifier work properly.
  prefs: []
  type: TYPE_NORMAL
- en: 'At a high level, the way we achieved this method of classification was to simply
    apply Bayes'' rule to our dataset. As a refresher from basic statistics, Bayes''
    rule can be written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00139.gif)'
  prefs: []
  type: TYPE_IMG
- en: The formula states that the probability of A given B is true is equal to probability
    of B given A is true times probability of A being true divided by probability
    of B being true. It is a complicated sentence, but if we step back and think about
    it, it will make sense.
  prefs: []
  type: TYPE_NORMAL
- en: The Bayes' classifier is a simple yet powerful one that allows the user to take
    the entire probability feature space into consideration. To appreciate its simplicity,
    one must remember that probability and frequency are two sides of the same coin.
    The Bayes' classifier belongs to the incremental learner class in which it updates
    itself upon encountering a new sample. This allows the model to update itself
    on-the-fly as the new observation arrives rather than only operating in batch
    mode.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We evaluated a model with different metrics. Since this is a multi-class classifier,
    we have to use `MulticlassMetrics()` to examine model accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'For more information, see the following link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.evaluation.MulticlassMetrics](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.evaluation.MulticlassMetrics)'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here is the documentation for the constructor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.classification.NaiveBayes](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.classification.NaiveBayes)'
  prefs: []
  type: TYPE_NORMAL
- en: Exploring ML pipelines and DataFrames using logistic regression in Spark 2.0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have gone out of our way to present the code in detail and as simply as possible
    so you can get started without the additional syntactic sugar that Scala uses.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we combine the ML pipelines and logistic regression to demonstrate
    how you can combine various steps in a single pipeline that operates on DataFrames
    as they get transformed and travel through the pipe. We skip some of the steps,
    such as splitting the data and model evaluation, and reserve them for later chapters
    to make the program shorter, but provide a full treatment of pipeline, DataFrame,
    estimators, and transformers in a single recipe.
  prefs: []
  type: TYPE_NORMAL
- en: This recipe explores the details of the pipeline and DataFrames as they travel
    through the pipeline and get operated on.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE116]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the `LogisticRegression` package required to build and train the model.
    There are others forms of `LogisticRegression` in Spark MLlib, but for now we
    just concentrate on the basic logistic regression method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE117]'
  prefs: []
  type: TYPE_PRE
- en: 'Import SparkSession so we can gain access to the cluster, and Spark SQL, and
    hence the DataFrame and Dataset abstractions as needed via SparkSession:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE118]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the Vector Package from `ml.linlang`. This will allow us to import and
    use vectors, both dense and sparse, from the Spark ecosystem:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE119]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the necessary packages from `log4j` so we can set the output level to
    ERROR and make the output less verbose for the program:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE120]'
  prefs: []
  type: TYPE_PRE
- en: Use the imported SparkSession to set various parameters needed to successfully
    initiate and gain a handle to the Spark cluster. The style for instantiating and
    getting access to Spark has changed in Spark 2.0\. See the *There's more...* section
    in this recipe for more detail.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set the parameters as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE121]'
  prefs: []
  type: TYPE_PRE
- en: Set the type of Spark cluster that you need and define the additional parameters
    needed to get access to Spark.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Here, set it to a local cluster and let it grab as many threads/cores as are
    available. You can use a number rather than `*` to tell Spark exactly how many
    cores/threads to
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE122]'
  prefs: []
  type: TYPE_PRE
- en: 'You have the option of specifying the exact number of cores you want to allocate
    by using a number rather than `*`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE123]'
  prefs: []
  type: TYPE_PRE
- en: This will allocate two cores. This might come handy on smaller laptops with
    limited resources.
  prefs: []
  type: TYPE_NORMAL
- en: 'Set the application name so it is easy to trace if more than one app is running
    on the cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE124]'
  prefs: []
  type: TYPE_PRE
- en: 'Configure the working directory relative to Spark home:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE125]'
  prefs: []
  type: TYPE_PRE
- en: 'We now progress toward building the data structure needed to house the first
    20 rows of the downloaded student admission data (see the previous recipe):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE126]'
  prefs: []
  type: TYPE_PRE
- en: 'The best way to understand a given row is to look at it as two parts:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The label - 0.0, meaning the student was not admitted.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The feature vector - `Vectors.dense(380.0, 3.61, 3.0)`, which shows the student's
    GRE, GPA, and RANK. We will cover the details of a dense vector in upcoming chapters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'SEQ is a Scala collection with special properties. Sequences can be thought
    of as iterable data structures with defined orders. For more information on SEQ
    see the following URL:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[http://www.scala-lang.org/api/current/index.html#scala.collection.Seq](http://www.scala-lang.org/api/current/index.html#scala.collection.Seq)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step takes the SEQ structure and converts it into a DataFrame. We
    highly recommend that you use DataFrame and Dataset rather than the low-level
    RDDs for any new programming, in order to stay aligned with the new Spark programming
    paradigm:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE127]'
  prefs: []
  type: TYPE_PRE
- en: '`label` and `feature` will be the column headings in the DataFrame.'
  prefs: []
  type: TYPE_NORMAL
- en: 'An Estimator is an API abstraction that accepts its data as a DataFrame and
    produces an actual model by calling the `Fit` function. Here, we create an Estimator
    from the `LogisticRegression` class in Spark MLlib and then set the maximum iteration
    to 80; it is 100 by default. We set the regularization parameter to 0.01 and tell
    the model we want to also fit an intercept:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE128]'
  prefs: []
  type: TYPE_PRE
- en: 'To get a better feeling for what the program does, see the following output
    and examine the parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE129]'
  prefs: []
  type: TYPE_PRE
- en: 'Output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE130]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is how to interpret and understand the `Admission_lr_Model` parameters
    listed in the previous step:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`elasticNetParam`: The ElasticNet mixing parameter, in the range [0, 1]. For
    alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty (default:
    0.0).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`featuresCol`: Features column name (default: features).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fitIntercept`: Whether to fit an intercept term (default: true, current: true).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`labelCol`: Label column name (default: label).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`maxIter`: Maximum number of iterations (>= 0) (default: 100, current: 80).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`predictionCol`: Prediction column name (default: prediction).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`probabilityCol`: Column name for predicted class conditional probabilities.
    Note that not all models output well-calibrated probability estimates! These probabilities
    should be treated as confidences, not precise probabilities (default: probability).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rawPredictionCol`: Raw prediction, otherwise known as confidence, column name
    (default: rawPrediction).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`regParam`: Regularization parameter (>= 0) (default: 0.0, current: 0.01).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`standardization`: Whether to standardize the training features before fitting
    the model (default: true).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`threshold`: Threshold in binary classification prediction, in range [0, 1]
    (default: 0.5).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`thresholds`: Thresholds in multi-class classification to adjust the probability
    of predicting each class. The array must have a length equal to the number of
    classes, with values >= 0\. The class with the largest value p/t is predicted,
    where p is the original probability of that class and t is the class'' threshold
    (undefined).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tol`: The convergence tolerance for iterative algorithms (default: 1.0E-6).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now make the call to fit the prepared DataFrame and produce our logistic regression
    model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE131]'
  prefs: []
  type: TYPE_PRE
- en: 'Now explore the model summary to understand what we get after fitting. We need
    to understand the components so we know what to extract for next steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE132]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE133]'
  prefs: []
  type: TYPE_PRE
- en: 'Now build the actual and final model from our training DataFrame. Take the
    Estimator we created and have it run the model by executing the transform function.
    We will now have a new DataFrame with all the parts populated (for example, predictions).
    Print the schema for our DataFrame to get a feel for the newly populated DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE134]'
  prefs: []
  type: TYPE_PRE
- en: This is the actual transformation step.
  prefs: []
  type: TYPE_NORMAL
- en: 'Print the schema to understand the newly populated DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE135]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE136]'
  prefs: []
  type: TYPE_PRE
- en: The first two columns are our label and features vector as we set in our API
    call when converting to DataFrames. The `rawPredictions` column is referred to
    as confidence. The probability column will contain our probability pair. The last
    column, prediction, will be the outcome predicted by our model. This shows us
    the structure for the fitted model and what information is available for each
    parameter.
  prefs: []
  type: TYPE_NORMAL
- en: 'We now proceed with extracting the model parameter for the regression. To make
    the code clear and simple, we extract each parameter''s property separately into
    a collection:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE137]'
  prefs: []
  type: TYPE_PRE
- en: 'For information only purposes, we print the number of the original training
    set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE138]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE139]'
  prefs: []
  type: TYPE_PRE
- en: 'We now proceed to extract the model predictions (outcome, confidence, and probability)
    for each row:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE140]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE141]'
  prefs: []
  type: TYPE_PRE
- en: Looking at the output from the previous step, we can examine how the model performed
    and what it predicted versus the actual via the output. In the subsequent chapters,
    we will use models to predict outcomes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here''s some examples from a couple of rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Row 10**: Model predicted correctly'
  prefs:
  - PREF_OL
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Row 13**: Model predicted incorrectly'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the last step, we stop the cluster and signal resource de-allocation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE142]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We started by defining a `Seq` data structure to house a series of vectors,
    each being a label and a feature vector. We then proceeded to convert the data
    structure to a DataFrame and ran it through `Estimator.fit()`to produce a model
    that fits the data. We examined the model's parameters and DataFrame schemas to
    understand the resulting model. We then proceeded to combine `.select()` and `.predict()`
    to decompose the DataFrame before looping to display the predictions and result.
  prefs: []
  type: TYPE_NORMAL
- en: While we don't have to use pipelines (a workflow concept in Spark borrowed from
    scikit-learn, [http://scikit-learn.org/stable/index.html](http://scikit-learn.org/stable/index.html))
    to run a regression, we decided to expose you to the power of Spark ML pipelines
    and logistic regression algorithms in an all-in-one recipe.
  prefs: []
  type: TYPE_NORMAL
- en: In our experience, all the production ML code uses a form of pipeline to combine
    multiple steps (for example, wrangle data, cluster, and regress). The upcoming
    chapters show you how to use these algorithms without a pipeline to reduce coding
    during the development process.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since we have just seen how to code pipeline concepts in Scala and Spark, let's
    revisit and define some of the concepts at a high level for a solid conceptual
    understanding.
  prefs: []
  type: TYPE_NORMAL
- en: PipeLine
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spark makes it easy to combine steps in the machine learning pipelines (MLlib)
    by standardizing APIs that can be combined into a workflow (that is, referred
    to as pipeline in Spark). While a regression can be invoked without these pipelines,
    the reality of a working system (that is, end-to-end) requires us to take a multi-step
    pipeline approach.
  prefs: []
  type: TYPE_NORMAL
- en: 'The pipeline concept comes from another popular library called **scikit-learn**:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Transformer**: A Transformer is a method that can transform one DataFrame
    into another DataFrame.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Estimator**: An Estimator operates on a DataFrame to produce a Transformer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vectors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A base class of vectors supports both dense and sparse vectors. The fundamental
    difference is the efficiency of presentation for data structures with sparsity.
    The dense vector is the choice here, since the training data is all meaningful
    per row and very little sparsity is present. In the cases where we deal with sparse
    vectors, matrices, and so on, the sparse tuple will contain the index and corresponding
    values at the same time.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While using the Spark documentation and Scala references is optional and perhaps
    too early for this chapter, they are included for completeness:'
  prefs: []
  type: TYPE_NORMAL
- en: SEQ documentation in Scala is available at [http://www.scala-lang.org/api/current/index.html#scala.collection.Seq](http://www.scala-lang.org/api/current/index.html#scala.collection.Seq)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark DataFrame documentation is available at [http://spark.apache.org/docs/latest/sql-programming-guide.html](http://spark.apache.org/docs/latest/sql-programming-guide.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark vectors documentation is available at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.linalg.Vectors$](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.linalg.Vectors%24)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Spark pipeline documentation is available at the following URLs:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.Pipeline](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.Pipeline)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.PipelineModel](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.PipelineModel)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.PipelineStage](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.PipelineStage)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You should also familiarize yourself with the basic linear algebra package in
    Spark, you can do this by referring to [http://spark.apache.org/docs/latest/mllib-statistics.html](http://spark.apache.org/docs/latest/mllib-statistics.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Familiarity with basic data types, especially vectors, is highly recommended,
    for that, you can refer to [http://spark.apache.org/docs/latest/mllib-data-types.html](http://spark.apache.org/docs/latest/mllib-data-types.html)
    link
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
