- en: Statistical Analysis of Time Series Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In financial portfolios, the returns on their constituent assets depend on a
    number of factors, such as macroeconomic and microeconomical conditions, and various
    financial variables. As the number of factors increases, so does the complexity
    involved in modeling portfolio behavior. Given that computing resources are finite,
    coupled with time constraints, performing an extra computation for a new factor
    only increases the bottleneck on portfolio modeling calculations. A linear technique
    for dimensionality reduction is **Principal Component Analysis** (**PCA**). As
    its name suggests, PCA breaks down the movement of portfolio asset prices into
    its principal components, or common factors, for further statistical analysis.
    Common factors that don't explain much of the movement of the portfolio assets
    receive less weighting in their factors and are usually ignored. By keeping the
    most useful factors, portfolio analysis can be greatly simplified without compromising
    on computational time and space costs.
  prefs: []
  type: TYPE_NORMAL
- en: In statistical analysis of time series data, it is important for the data to
    be stationary in order to avoid spurious regression. Non-stationary data may be
    generated by an underlying process that is affected by a trend, a seasonal effect,
    presence of a unit root, or a combination of all three. The statistical properties
    of non-stationary data, such as mean and variance, changes over time. Non-stationary
    data needs to be transformed into stationary data for statistical analysis to
    produce consistent and reliable results. This can be achieved by removing the
    trend and seasonality components. Stationary data can thereafter be used for prediction
    or forecasting.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Performing PCA on the Dow and its 30 components
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reconstructing the Dow index
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the difference between stationary and non-stationary data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Checking data for stationarity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Types of stationary and non-stationary processes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the Augmented Dickey-Fuller Test to test the presence of a unit root
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Making stationary data by detrending, differencing, and seasonal decomposing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using an Autoregressive Integrated Moving Average for time series prediction
    and forecasting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Dow Jones industrial average and its 30 components
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **Dow Jones Industrial Average** (**DJIA**) is a stock market index that
    comprises the 30 largest US companies. Commonly known as the **Dow**, it is owned
    by S&P Dow Jones Indices LLC and computed on a price-weighted basis (see [https://us.spindices.com/index-family/us-equity/dow-jones-averages](https://us.spindices.com/index-family/us-equity/dow-jones-averages)
    for more information on the Dow).
  prefs: []
  type: TYPE_NORMAL
- en: This section involves downloading the datasets of Dow and its components into
    `pandas` DataFrame objects for use in later sections of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Downloading Dow component datasets from Quandl
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following code retrieves the Dow component datasets from Quandl. The data
    provider that we will be using is WIKI Prices, a community formed by members of
    the public and that provides datasets free of charge back to the public. Such
    data isn''t free from errors, so please use them with caution. At the time of
    writing, this data feed is no longer supported actively by the Quandl community,
    though past datasets are still available for use. We will download historical
    daily closing prices for 2017:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The `wiki_symbols` variable contains a list of Quandl codes that we use for
    downloading. Notice that in the parameter arguments of `quandl.get()`, we specified
    `column_index=11`. This tells Quandl to download only the 11th column of each
    dataset, which coincides with the adjusted daily closing prices. The datasets
    are downloaded into our `df_components` variable as a single `pandas` DataFrame
    object.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s normalize our dataset before using it for analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: If you inspect every value in this data feed, you will notice `NaN` values,
    or missing data. Since we are using data that is error-prone, and for quick studies
    of PCA, we can temporarily fill in these unknown variables by propagating previous
    observed values. The `fillna(method='ffill')` method helps to do this and stores
    the result in the `filled_df_components` variable.
  prefs: []
  type: TYPE_NORMAL
- en: An additional step in normalizing is to resample the time series at regular
    intervals and match it up exactly with our Dow time series dataset, which we will
    be downloading later. The `daily_df_components` variable stores the result from
    resampling the time series on a daily basis, and any missing values during resampling
    are propagated using the forward fill method. And finally, to account for incomplete
    starting data, we will simply perform a backfill of values with `fillna(method='bfill')`.
  prefs: []
  type: TYPE_NORMAL
- en: For the purpose of PCA demonstration, we have to make do with free, low-quality
    datasets. If you require high quality datasets, consider subscribing to a data
    publisher.
  prefs: []
  type: TYPE_NORMAL
- en: Quandl doesn't provide free datasets on the DJIA. In the next section, we will
    explore another data provider named Alpha Vantage as an alternative method of
    downloading datasets.
  prefs: []
  type: TYPE_NORMAL
- en: About Alpha Vantage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Alpha Vantage ([https://www.alphavantage.co](https://www.alphavantage.co)) is
    a data provider that provides real-time and historical data on equities, foreign
    exchange, and cryptocurrencies. Similar to Quandl, you can obtain a Python wrapper
    for the Alpha Vantage REST API interface and download free datasets directly into
    a `pandas` DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: Obtaining an Alpha Vantage API key
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'From your web browser, visit [https://www.alphavantage.co](https://www.alphavantage.co),
    and click **Get your free API Key today** from the home page. You will be brought
    to a registration page. Fill in basic information about yourself and submit the
    form. Your API key will be shown in the same page. Copy this API key for use in
    the next section:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/2c9fad0e-8f40-4aeb-b4ce-4bfba2f743a4.png)'
  prefs: []
  type: TYPE_IMG
- en: Installing the Alpha Vantage Python wrapper
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'From your terminal window, type the following command to install the Python
    module for Alpha Vantage:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Downloading the DJIA dataset from Alpha Vantage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following code connects to Alpha Vantage and downloads the Dow dataset,
    with the ticker code `^DJI`. Replace the value of the constant variable, `ALPHA_VANTAGE_API_KEY`,
    with your own API key:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The `TimeSeries` class of the `alpha_vantage.timeseries` module is instantiated
    with the API key and specifies that datasets are automatically downloaded as `pandas`
    DataFrame objects. The `get_daily_adjusted()` method with the `outputsize='full'`
    parameter downloads the entire available daily adjusted prices for the given ticker
    symbol in the `df` variable as a `DataFrame` object.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s inspect this DataFrame with the `info()` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The Dow dataset that we downloaded from Alpha Vantage gives us the full time
    series data from the most recent available trading date, all the way back to the
    year 2000\. It contains several columns that give us additional information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s also inspect the indexes of this DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The outputs suggests that the index values are made up of an object of string
    type. Let''s convert this DataFrame into something suitable for our analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Here, we are taking the adjusted closing prices of Dow Jones for the year of
    2017, resampled on a daily basis. The resulting DataFrame object is stored in
    `djia_2017`, which we can use for applying PCA.
  prefs: []
  type: TYPE_NORMAL
- en: Applying a kernel PCA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will perform kernel PCA to find eigenvectors and eigenvalues
    so that we can reconstruct the Dow index.
  prefs: []
  type: TYPE_NORMAL
- en: Finding eigenvectors and eigenvalues
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can perform a kernel PCA using the `KernelPCA` class of the `sklearn.decomposition`
    module in Python. The default kernel method is linear. The dataset that''s used
    in PCA is required to be normalized, which we can perform with z-scoring. The
    following code do this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The `fn_z_score` variable is an inline function to perform z-scoring on a `pandas`
    DataFrame, which is applied with the `apply()` method. These normalized datasets
    can be fitted into a kernel PCA with the `fit()` method. The fitted results of
    the daily Dow component prices are stored in the `fitted_pca` variable, which
    is of the same `KernelPCA` object.
  prefs: []
  type: TYPE_NORMAL
- en: Two main outputs of PCA are eigenvectors and eigenvalues. **Eigenvectors** are
    vectors containing the direction of the principal component line, which doesn't
    change when a linear transformation is applied. **Eigenvalues** are scalar values
    indicating the amount of variance of the data in a direction with respect to a
    particular eigenvector. In fact, the eigenvector with the highest eigenvalue forms
    the principal component.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `alphas_` and `lambdas_` attributes of the `KernelPCA` object return the
    eigenvectors and eigenvalues of the centered kernel matrix dataset, respectively.
    When we plot the eigenvalues, we get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We should then get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/1db86b3d-e5bc-44d2-91f2-8557034eac96.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can see that the first few eigenvalues explain much of the variances in
    the data, and become more negligent further down the components. Taking the first
    five eigenvalues, let''s see how much explanation each of these eigenvalues gives
    us by obtaining their weighted average values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see that the first component explains 65% of the variance of the data,
    the second component explains 14%, and so on. Taking the sum of these values,
    we get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The first five eigenvalues would explain 92% of the variance in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Reconstructing the Dow index with PCA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'By default, the `KernelPCA` is instantiated with the `n_components=None` parameter,
    which constructs a kernel PCA with non-zero components. We can also create a PCA
    index with five components:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: With the `fit()` method, we fitted the normalized dataset using the linear kernel
    PCA function with five components. The `transform()` method transforms the original
    dataset with the kernel PCA. These values are normalized using the weights indicated
    by the eigenvectors, computed with dot matrix multiplication. We then create a
    copy of the Dow time series `pandas` DataFrame with the `copy()` method, and combine
    it with the reconstructed values in the `df_combined` DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: 'The new DataFrame is normalized by z-scoring, and plotted out to see how well
    the reconstructed PCA index tracks the original Dow movements. This gives us the
    following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/eac0f8e9-5056-4623-a021-d4291c231a56.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding graph shows the original Dow index against the reconstructed Dow
    index with five principal components for the year 2017.
  prefs: []
  type: TYPE_NORMAL
- en: Stationary and non-stationary time series
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is important that time series data that's used for statistical analysis is
    stationary in order to perform statistical modeling correctly, as such usages
    may be for prediction and forecasting. This section introduces the concepts of
    stationarity and non-stationarity in time series data.
  prefs: []
  type: TYPE_NORMAL
- en: Stationarity and non-stationarity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In empirical time series studies, price movements are observed to drift toward
    some long-term mean, either upwards or downwards. A stationary time series is
    one whose statistical properties, such as mean, variance, and autocorrelation,
    are constant over time. Conversely, observations on non-stationary time series
    data have their statistical properties change over time, mostly likely due to
    trends, seasonality, presence of a unit root, or a combination of all three.
  prefs: []
  type: TYPE_NORMAL
- en: In time series analysis, it is assumed that the data of the underlying process
    is stationary. Otherwise, modeling from non-stationary data may produce unpredictable
    results. This would lead to a condition known as spurious regression. **Spurious
    regression** is a regression that produces misleading statistical evidence of
    relationships between independent non-stationary variables. In order to receive
    consistent and reliable results, non-stationary data needs to be transformed into
    stationary data.
  prefs: []
  type: TYPE_NORMAL
- en: Checking for stationarity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are a number of ways to check whether time series data is stationary
    or non-stationary:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Through visualizations**: You can review a time series graph for obvious
    indication of trends or seasonality.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Through statistical summaries**: You can review the statistical summaries
    of your data significant differences. For example, you can partition your time
    series data and compare the mean and variance of each group.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Through statistical tests**: You can use statistical tests such as the Augmented
    Dickey-Fuller Test to check if stationarity expectations have been met or violated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Types of non-stationary processes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following points help to identify non-stationary behavior in time series
    data for consideration in transforming stationary data:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pure random walk**: A process with a unit root or a stochastic trend. It
    is a non-mean reverting process with a variance that evolves over time and goes
    to infinity.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Random walk with drift**: A process with a random walk and a constant drift.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deterministic trend**: A process with a mean that grows around a fixed trend,
    which is constant and independent of time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Random walk with drift and deterministic trend**: A process combining a random
    walk with a drift component, and a deterministic trend.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Types of stationary processes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'These are a number of definitions of stationarity that you may come across
    in time series studies:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Stationary process**: A process that generates a stationary series of observations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Trend stationary**: A process that does not exhibit a trend.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Seasonal stationary**: A process that does not exhibit seasonality.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Strictly stationary**: Also known as **strongly stationary**. A process whose
    unconditional joint probability distribution of random variables does not change
    when shifted in time (or along the *x *axis).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Weakly stationary**: Also known as **covariance-stationary**, or **second-order
    stationary**. A process whose mean, variance, and correlation of random variables
    doesn''t change when shifted in time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Augmented Dickey-Fuller Test
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An **Augmented Dickey-Fuller Test** (**ADF**) is a type of statistical test
    that determines whether a unit root is present in time series data. Unit roots
    can cause unpredictable results in time series analysis. A null hypothesis is
    formed on the unit root test to determine how strongly time series data is affected
    by a trend. By accepting the null hypothesis, we accept the evidence that the
    time series data is non-stationary. By rejecting the null hypothesis, or accepting
    the alternative hypothesis, we accept the evidence that the time series data is
    generated by a stationary process. This process is also known as **trend-stationary.**
    Values of the ADF test statistic are negative. Lower values of ADF indicates stronger
    rejection of the null hypothesis.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some basic autoregression models for use in ADF testing:'
  prefs: []
  type: TYPE_NORMAL
- en: 'No constant and no trend:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](Images/2177ca63-55d4-40b3-93ae-80a298b53df9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'A constant without a trend:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](Images/77f7073a-b720-45a8-b4ca-ab166a427590.png)'
  prefs: []
  type: TYPE_IMG
- en: 'With a constant and trend:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](Images/c05c5f00-9d72-463e-ba26-7018a93c8faa.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *α* is the drift constant, *β* is the coefficient on a time trend, *γ*
    is the coefficient of our hypothesis, *p* is the lag order of the first-differences
    autoregressive process, and *ϵ[t]* is an independent and identically distributed
    residual term. When *α=0* and *β=0*, the model is a random walk process. When
    *β=0*, the model is a random walk with a drift process. The length of the lag
    *p* is to be chosen so that the residuals are not serially correlated. Some approaches
    for examining the information criteria for choosing lags are by minimizing the
    **Akaike information criterion** (**AIC**), the **Bayesian information criterion**
    (**BIC**), and the **Hannan-Quinn information criterion**.
  prefs: []
  type: TYPE_NORMAL
- en: 'The hypothesis can then be formulated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Null hypothesis, *H[0]*: If failed to be rejected, it suggests that the time
    series contains a unit root and is non-stationary'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Alternate hypothesis, *H[1]*: If *H[0]* is rejected, it suggests that the time
    series does not contain a unit root and is stationary'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To accept or reject the null hypothesis, we use the p-value. We reject the
    null hypothesis if the p-value falls below a threshold value such as 5% or even
    1%. We can fail to reject the null hypothesis if the p-value is above this threshold
    value and consider the time series as non-stationary. In other words, if our threshold
    value is 5%, or 0.05, note the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'p-value > 0.05: We fail to reject the null hypothesis *H[0]* and conclude that
    the data has a unit root and is non-stationary'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'p-value ≤ 0.05: We reject the null hypothesis *H[0]* and conclude that the
    data has a unit root and is non-stationary'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `statsmodels` library provides the `adfuller()` function that implements
    this test.
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing a time series with trends
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s examine a time series dataset. Take, for example, the prices of gold
    futures traded on the CME. On Quandl, the gold futures continuous contract is
    available for download with the following code: `CHRIS/CME_GC1`. This data is
    curated by the Wiki Continuous Futures community group, taking into account the
    front month contracts only. The sixth column of the dataset contains the settlement
    prices. The following code downloads the dataset from the year 2000 onward:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Examine the head of the dataset using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Settle** | **Date** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **2000-01-31** | 283.2 |'
  prefs: []
  type: TYPE_TB
- en: '| **2000-02-29** | 294.2 |'
  prefs: []
  type: TYPE_TB
- en: '| **2000-03-31** | 278.4 |'
  prefs: []
  type: TYPE_TB
- en: '| **2000-04-30** | 274.7 |'
  prefs: []
  type: TYPE_TB
- en: '| **2000-05-31** | 271.7 |'
  prefs: []
  type: TYPE_TB
- en: 'Compute the rolling mean and standard deviation into the `df_mean` and `df_std`
    variables, respectively, with a window period of one year:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The `resample()` method helps to ensure that the data is smoothed out on a monthly
    basis, and the `ffill()` method forward fills any missing values.
  prefs: []
  type: TYPE_NORMAL
- en: A list of useful common time series frequencies for specifying the `resample()`
    method can be found at [http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases](http://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html)[.](http://pandas.pydata.org/pandas-docs/stable/timeseries.offset-aliases)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s visualize the plot of the rolling mean against the original time series:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We obtain the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/a4bbffb4-80d2-45a3-9f5a-aa1ddb4c9b54.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Visualizing the rolling standard deviation separately, we get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We obtain the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/0d2dd772-94e7-47dd-a254-4e0613a566e6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Using the `statsmodels` module, perform an ADF unit root test on our dataset
    with the `adfuller()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The `adfuller()` method returns a tuple of seven values. Particularly, we are
    interested in the first, second, and fifth values, which give us the test statistic,
    `p-value`, and a dictionary of critical values, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Observe from the plots that the mean and standard deviations swing over time,
    with the mean exhibiting an overall upward trend. The ADF test statistic value
    is more than the critical values (especially at 5%), and the `p-value` is more
    than 0.05\. With these, we cannot reject the null hypothesis that there is a unit
    root and consider that our data is non-stationary.
  prefs: []
  type: TYPE_NORMAL
- en: Making a time series stationary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A non-stationary time series data is likely to be affected by a trend or seasonality.
    Trending time series data has a mean that is not constant over time. Data that
    is affected by seasonality have variations at specific intervals in time. In making
    a time series data stationary, the trend and seasonality effects have to be removed.
    Detrending, differencing, and decomposition are such methods. The resulting stationary
    data is then suitable for statistical forecasting.
  prefs: []
  type: TYPE_NORMAL
- en: Let's look at all three methods in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Detrending
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The process of removing a trend line from a non-stationary data is known as
    **detrending**. This involves a transformation step that normalizes large values
    into smaller ones. Examples could be a logarithmic function, a square root function,
    or even a cube root. A further step is to subtract the transformation from the
    moving average.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s perform detrending on the same dataset, `df_settle`, with logarithmic
    transformation and subtracting from the moving average of two periods, as given
    in the following Python code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The `df_log` variable is our transformed `pandas` DataFrame by logarithmic function
    using the `numpy` module, and the `df_detrend` variable contains the detrended
    data. We plot this detrended data to visualize its mean and standard deviation
    over a rolling one-year period.
  prefs: []
  type: TYPE_NORMAL
- en: 'We get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/10d6a205-d53e-4b5f-8c88-92be573fdd6f.png)'
  prefs: []
  type: TYPE_IMG
- en: Observe that the mean and standard deviation do not exhibit a long-term trend.
  prefs: []
  type: TYPE_NORMAL
- en: 'Looking at the ADF test statistic for the detrended data, we get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The `p-value` for this detrended data is less than 0.05\. Our ADF test statistic
    is lower than all the critical values. We can reject the null hypothesis and say
    that this data is stationary.
  prefs: []
  type: TYPE_NORMAL
- en: Removing trend by differencing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Differencing involves the difference of time series values with a time lag.
    The first-order difference of the time series is given by the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/99483d9c-05ff-42c5-9824-a96edcbd8c2b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can reuse the `df_log` variable in the previous section as our logarithmic
    transformed time series, and utilize the `diff()` and `shift()` methods of NumPy
    modules in our differencing, with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The parameter of `diff()` given as `periods=3` indicates that the dataset is
    shifted by three periods in calculating the differences.
  prefs: []
  type: TYPE_NORMAL
- en: 'This provides the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/145641ad-aa7f-495f-aace-58872e076217.png)'
  prefs: []
  type: TYPE_IMG
- en: Observe from the plots that the rolling mean and standard deviation tend to
    change very little over time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Looking at our ADF test statistic, we get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: From the ADF test, the `p-value` for this data is less than 0.05\. Our ADF test
    statistic is lower than the 5% critical value, indicating a 95% confidence level
    that this data is stationary. We can reject the null hypothesis and say that this
    data is stationary.
  prefs: []
  type: TYPE_NORMAL
- en: Seasonal decomposing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Decomposing involves modeling both the trend and seasonality, and then removing
    them. We can use the `statsmodel.tsa.seasonal` module to model a non-stationary
    time series dataset using moving averages and remove its trend and seasonal components.
  prefs: []
  type: TYPE_NORMAL
- en: 'By reusing our `df_log` variable containing the logarithm of our dataset from
    the previous section, we get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The `seasonal_decompose()` method of `statsmodels.tsa.seasonal` requires a parameter,
    `freq`, which is an integer value specifying the number of periods per seasonal
    cycle. Since we are using monthly data, we expect 12 periods in a seasonal year.
    The method returns an object with three attributes, mainly the trend and seasonal
    components, as well as the final `pandas` series data with its trend and seasonal
    components removed.
  prefs: []
  type: TYPE_NORMAL
- en: More information on the `seasonal_decompose()` method of the `statsmodels.tsa.seasonal`
    module can be found at [https://www.statsmodels.org/dev/generated/statsmodels.tsa.seasonal.seasonal_decompose.html](https://www.statsmodels.org/dev/generated/statsmodels.tsa.seasonal.seasonal_decompose.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s visualize the different plots by running the following Python code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following graphs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/77071b95-4137-43ff-8101-d2b84ffb6c09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, we can see the individual trend and seasonality components being removed
    from the dataset and plotted, and the residuals plotted at the bottom. Let''s
    visualize the statistical properties of our residuals:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/9ff7a79e-3d33-4822-9285-692d0f87b6a2.png)'
  prefs: []
  type: TYPE_IMG
- en: Observe from the plots that the rolling mean and standard deviation tend to
    change very little over time.
  prefs: []
  type: TYPE_NORMAL
- en: 'By checking our residual data for stationarity, we get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: From the ADF test, the `p-value` for this data is less than 0.05\. Our ADF test
    statistic is lower than all the critical values. We can reject the null hypothesis
    and say that this data is stationary.
  prefs: []
  type: TYPE_NORMAL
- en: Drawbacks of ADF testing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here are some considerations when using ADF tests for reliable checking of
    non-stationary data:'
  prefs: []
  type: TYPE_NORMAL
- en: The ADF test do not truly tell apart between pure and non-unit root generating
    processes. In long-term moving average processes, the ADF tests becomes biased
    in rejecting the null hypothesis. Other stationarity testing methods such as the
    **Kwiatkowski–Phillips–Schmidt–Shin** (**KPSS**) tests and the **Phillips-Perron**
    test take a different approach in treating the presence of unit roots.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is no fixed methodology in determining the lag length *p*. If *p* is too
    small, the remaining serial correlation in the errors may affect the size of the
    test. If *p* is too large, the power of the test will deteriorate. Additional
    consideration is to be given for this lag order.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As deterministic terms are added to the test regressions, the power of unit
    root tests diminishes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Forecasting and predicting a time series
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we identified non-stationarity in time series data
    and discussed techniques for making time series data stationary. With stationary
    data, we can proceed to perform statistical modeling such as prediction and forecasting.
    Prediction involves generating best estimates of in-sample data. Forecasting involves
    generating best estimates of out-of-sample data. Predicting future values is based
    on previously observed values. One such commonly used method is the Autoregressive
    Integrated Moving Average.
  prefs: []
  type: TYPE_NORMAL
- en: About the Autoregressive Integrated Moving Average
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The **Autoregressive Integrated Moving Average** (**ARIMA**) is a forecasting
    model for stationary time series based on linear regression. As its name suggests,
    it is based on three components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Autoregression** (**AR**): A model that uses the dependency between an observation
    and its lagged values'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Integrated** (**I**): The use of differencing an observation with an observation
    from a previous time stamp in making the time series stationary'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Moving average** (**MA**): A model that uses the dependency between an observed
    error term and a combination of previous error terms, *e*[*t*]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ARIMA models are referenced by the notation *ARIMA(p, d, q)*, which corresponds
    to the parameters of the three components. Non-seasonal ARIMA models can be specified
    by changing the values of *p*, *d*, and *q*, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**ARIMA**(***p*,0,0**): First-order autoregressive model, notated by *AR(p)*.
    *p* is the lag order, indicating the number of lagged observations in the model.
    For example, *ARIMA(2,0,0)* is *AR(2)* and represented as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](Images/9517941c-16d9-4dfa-9022-9112151baa9c.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *ϕ[1]* and *ϕ[2]* are parameters for the model.
  prefs: []
  type: TYPE_NORMAL
- en: '**ARIMA**(**0,*d*,0**): First degree of differencing in the integrated component,
    also known as random walk, notated by *I(d)*. *d* is the degree of differencing,
    indicating the number of times the data have had past values subtracted. For example,
    *ARIMA(0,1,0)* is *I(1)* and represented as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](Images/2f918895-6293-475c-bbae-ede1814afb27.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *μ* is the mean of the seasonal difference.
  prefs: []
  type: TYPE_NORMAL
- en: '**ARIMA(0,0,*q*)**: Moving average component, notated by *MA(q)*. The order
    *q* determines the number of terms to be included in the model:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](Images/9624d155-da2e-4709-99bc-0b0798ca96ac.png)'
  prefs: []
  type: TYPE_IMG
- en: Finding model parameters by grid search
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A grid search, also known as the hyperparameter optimization method, can be
    used to iteratively explore different combinations of parameters for fitting our
    ARIMA model. We can fit a seasonal ARIMA model with the `SARIMAX()` function of
    the `statsmodels` module in each iteration, returning an object of the `MLEResults`
    class. The `MLEResults` object holds an `aic` attribute for returning the AIC value.
    The model with the lowest AIC value gives us the best-fitting model that determines
    our parameters of *p*, *d*, and *q*. More information on SARIMAX can be found
    at [https://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.sarimax.SARIMAX.html](https://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.sarimax.SARIMAX.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'We define the grid search procedure as the `arima_grid_search()` function,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Our variable, `df_settle`, holds the monthly prices of the futures data that
    we downloaded in the previous section. In the **SARIMAX (seasonal autoregressive
    integrated moving average with exogenous regressors model)** function, we provided
    the `seasonal_order` parameter, which is the *ARIMA(p,d,q,s)* seasonal component,
    where *s* is the number of periods in a season of the dataset. Since we are using
    monthly data, we use 12 periods to define a seasonal pattern. The `enforce_stationarity=False`
    parameter doesn't transform the AR parameters to enforce stationarity in the AR
    component of the model. The `enforce_invertibility=False` parameter doesn't transform
    MA parameters to enforce invertibility in the MA component of the model. The `disp=False`
    parameter suppresses output information when fitting our models.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the grid function defined, we can now call this with our monthly data
    and print out the model parameters with the lowest AIC value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: An `ARIMA(0,1,1,12)` seasonal component model would give us the lowest AIC value
    at 2149.636\. We shall use these parameters to fit our SARIMAX model in the next
    section.
  prefs: []
  type: TYPE_NORMAL
- en: Fitting the SARIMAX model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Having obtained the optimal model parameters, inspect the model properties
    using the `summary()` method on the fitted results to view detailed statistical
    information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives us the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'It is important to run model diagnostics to investigate that model assumptions
    haven''t been violated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/f01811a8-00c5-4d0a-989c-50686fddabf5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The top-right plot shows the **kernel density estimate** (**KDE**) of the standardized
    residuals, which suggests the errors are Gaussian with a mean close to zero. Let''s
    see a more accurate statistic of the residuals:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: From the description of the residuals, the non-zero mean suggests that the prediction
    may be biased positively.
  prefs: []
  type: TYPE_NORMAL
- en: Predicting and forecasting the SARIMAX model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `model_results` variable is a `SARIMAXResults` object of the `statsmodel`
    module, representing the output of the SARIMAX model. It contains a `get_prediction()`
    method for performing in-sample prediction and out-of-sample forecasting. It also
    contains a `conf_int()` method, which returns the confidence intervals of the
    predictions, both lower- and upper-bounded, of the fitted parameters, which is
    at a 95% confidence interval by default. Let''s apply these methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: The `start` parameter in the `get_prediction()` method indicates we are performing
    an in-sample prediction of the most recent five years' prices. At the same time,
    with the `end` parameter, we are performing an out-of-sample forecast of the next
    five months.
  prefs: []
  type: TYPE_NORMAL
- en: 'By inspecting the top three forecasted confidence interval values, we get the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s plot the predicted and forecasted prices against our original dataset,
    from the year 2008 onwards:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives us the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/25f3ccd6-0eb0-42c6-b52f-d37861e4be7c.png)'
  prefs: []
  type: TYPE_IMG
- en: The solid line plot shows the observed values, while the dotted lines plot the
    five-year rolling predictions trailing closely and bounded by the confidence intervals
    in the shaded area. Observe that as the next five-month forecast goes into the
    future, and the confidence interval widens to reflect the loss of certainty in
    the outlook.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we were introduced to PCA as a dimension reduction technique
    in portfolio modeling. By breaking down the movement of asset prices of a portfolio
    into its principal components, or common factors, the most useful factors can
    be kept, and portfolio analysis can be greatly simplified without compromising
    on computational time and space complexity. In applying PCA to the Dow and its
    thirty components using the `KernelPCA` function of the `sklearn.decomposition`
    module, we obtained eigenvectors and eigenvalues, which we used to reconstruct
    the Dow with five components.
  prefs: []
  type: TYPE_NORMAL
- en: In the statistical analysis of time series data, the data is considered as either
    stationary or non-stationary. Stationary time series data is data whose statistical
    properties are constant over time. Non-stationary time series data has its statistical
    properties change over time, most likely due to trends, seasonality, presence
    of a unit root, or a combination of all three. Modeling from non-stationary data
    may produce spurious regression. In order to receive consistent and reliable results,
    non-stationary data needs to be transformed into stationary data.
  prefs: []
  type: TYPE_NORMAL
- en: We used statistical tests such as the ADF to check whether stationary expectations
    are met or violated. The `adfuller` method of the `statsmodels.tsa.stattools`
    module provides the test statistic, p-value, and critical values, from which we
    can fail to reject the null hypothesis that the data has a unit root and is non-stationary.
  prefs: []
  type: TYPE_NORMAL
- en: We transformed non-stationary data into stationary data by detrending, differencing,
    and seasonal decomposition. By using ARIMA, we fitted models using the `SARIMAX`
    function of the `statsmodels.tsa.statespace.sarimax` module to find suitable model
    parameters that give the lowest AIC value through an iterative grid search procedure.
    The fitted results are used for prediction and forecasting.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will perform interactive financial analytics with the
    VIX.
  prefs: []
  type: TYPE_NORMAL
