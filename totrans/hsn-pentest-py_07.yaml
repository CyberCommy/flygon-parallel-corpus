- en: Machine Learning and Cybersecurity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: These days, **Machine Learning** (**ML**) is a term we come across quite often.
    In this chapter, we are going to look at an overview of what exactly ML is, what
    kinds of problems it solves, and finally what kinds of applications it can have
    in the cyber security ecosystem. We are also going to look at the various different
    kinds of ML models, and which models we can use in which circumstances. It should
    be noted that the scope of this book is not to cover ML in detail, but instead
    to provide a solid understanding of ML and its applications in the cyber security
    domain.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter in detail:'
  prefs: []
  type: TYPE_NORMAL
- en: Machine Learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regression-based Machine Learning models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classification models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Natural language processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Machine Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's start with a basic question: *what is machine learning, and why should
    we use it?*
  prefs: []
  type: TYPE_NORMAL
- en: We can define ML as a branch of data science that can efficiently solve prediction
    problems. Let's assume that we have data on the customers of an e-commerce website
    over the last three months, and that data contains the purchase history of a particular
    product (`c_id`, `p_id`**,** `age`, `gender`, `nationality`, `purchased[yes/no]`).
  prefs: []
  type: TYPE_NORMAL
- en: Our objective is to use the dataset to identify a customer who would be likely
    to purchase the product, based on their purchase history. We might think that
    a good idea would be to take the purchase column into account and to assume that
    those who have purchased the product previously would be most likely to purchase
    it again. However, a better business solution would take all parameters into account,
    including the region from which the most purchases happen, the age group of the
    customer, and their gender as well. Based upon the permutation of all of these
    fields, a business owner can get a better idea of the kind of customer who is
    most influenced by the product and the marketing team can therefore design more
    specific, targeted campaigns.
  prefs: []
  type: TYPE_NORMAL
- en: We can do this in two different ways. The first solution would be to write software
    in the programming language of our choice and to write logic that gives a specific
    weight to each of the parameters discussed. The logic would then be able to tell
    us who all the potential buyers are. The downside of this approach, however, is
    that a significant amount of time would be required to draft the logic and if
    new parameters are added (such as the profession of the customer), the logic would
    need to change. Furthermore, the logic written would solve only one specific business
    problem. This is the traditional approach adopted before machine learning was
    developed, and is still used by various businesses.
  prefs: []
  type: TYPE_NORMAL
- en: The second solution would be to use ML. Based on the customer dataset, we can
    train an ML model and make it predict whether a customer is a potential buyer
    or not. Training the model involves feeding all the training data to an ML library
    that would take all the parameters into account and learn which are the common
    attributes of customers who purchased the product, and which are the attributes
    of the customers who didn't purchase the product. Whatever is learned by the model
    is persisted in the memory and the obtained model is said to be trained. If the
    model is presented with the data of a new customer, it would use its training
    and make a prediction based upon the learned attributes that usually lead to a
    purchase. The same business problem that used to have to be solved with a computer
    program and hardcoded logic is now solved with a mathematical ML model. This is
    one of the many cases in which we can use ML.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to remember that if the problem at hand is a prediction problem,
    ML can be applied to obtain a good prediction. However, if the objective of the
    problem is to automate a manual task, ML would not be helpful; we would need to
    use a traditional programming approach. ML solves prediction problems by using
    mathematical models.
  prefs: []
  type: TYPE_NORMAL
- en: '**Artificial intelligence** (**AI**) is another word that we are likely to
    come across very often. Let''s now try to answer another question: What is artificial
    intelligence and how is it different than machine learning?'
  prefs: []
  type: TYPE_NORMAL
- en: Setting up a Machine Learning environment in Kali Linux
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'All ML libraries come packaged within a package called `anaconda`. This will
    install Python 3.5 or the latest Python version available. To run ML code, we
    require Python 3 or higher:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Download anaconda from the following URL: [https://conda.io/miniconda.html](https://conda.io/miniconda.html).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Install all the packages by running `bash Anaconda-latest-Linux-x86_64.sh.>`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For more details, refer to the following URL: [https://conda.io/docs/user-guide/install/linux.html](https://conda.io/docs/user-guide/install/linux.html).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Regression-based machine learning models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We make use of regression models when we have to predict a continuous value
    rather than a discrete one. For example, let's say that a dataset contains the
    number of years of experience of an employee and the employee's salary. Based
    upon these two values, this model is trained and expected to make a prediction
    on the employee's salary based on their *years of experience*. Since the salary
    is a continuous number, we can make use of regression-based machine learning models
    to solve this kind of problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'The various regression models we will discuss are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Simple linear regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multiple linear regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Polynomial regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Support vector regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision tree regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random forest regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simple linear regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Simple linear regression** (**SLR**) takes linear data and applies feature
    scaling to it if required. **Feature scaling** is a method used to balance the
    effects of various attributes. All machine learning models are mathematical in
    nature, so before training the model with the data, we need to apply a few steps
    to make sure the predictions made are not biased.'
  prefs: []
  type: TYPE_NORMAL
- en: For example, if the dataset contains three attributes (`age`, `salary`, and `item_purchased[0/1]`),
    we as humans know that the age group that is likely to visit shops is between
    10 and 70, and the salary can range between 10,000 and 100,000 or higher. When
    making the prediction, we want to take both parameters into consideration, to
    know which age group with what salary is most likely to purchase the product.
    However, if we train the model without scaling the age and the salary to the same
    level, the value of the salary will overshadow the effect of age due to the large
    numeric difference between them. To make sure this does not happen, we apply feature
    scaling to the dataset to balance them out.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another step required is data encoding, using a **one-hot encoder**. For example,
    if the dataset has a country attribute, this a categorical value, which, let''s
    say, has three categories: Russia, US, and UK. These words do not make sense to
    a mathematical model. Using a one-hot encoder, we transform the dataset so it
    reads (`id`, `age`, `salary`, `Russia`, `UK`, `USA`, `item_purchased`). Now, all
    the customers who have purchased the product and are from Russia would have the
    number 1 under the column named Russia, and the number 0 under the USA and UK
    columns.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example, let''s say the data initially looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **ID** | **Country** | **Age** | **Salary** | **Purchased** |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | USA | 32 | 70 K | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | Russia | 26 | 40 K | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | UK | 32 | 80 K | 0 |'
  prefs: []
  type: TYPE_TB
- en: 'After performing the data transformations, we would get the following dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **ID** | **Russia** | **USA** | **UK** | **Age** | **Salary** | **Purchased**
    |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0 | 1 | - | 0.5 | 0.7 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 1 | 0 | 0 | 0.4 | 0.4 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 0 | 0 | 1 | 0.5 | 0.8 | 0 |'
  prefs: []
  type: TYPE_TB
- en: It can be seen that the dataset obtained is purely mathematical and so we can
    now give it to our regression model to learn from and then make predictions.
  prefs: []
  type: TYPE_NORMAL
- en: It should be noted that the input variables that help to make the prediction
    are called independent variables. In the preceding example, `country`, `age`,
    and `salary` are the independent variables. The output variable that defines the
    prediction is called the dependent variable, which is the `Purchased` column in
    our case.
  prefs: []
  type: TYPE_NORMAL
- en: How does the regression model work?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our objective is to train a machine learning model on the dataset and then ask
    the model to make predictions in order to establish the salary that should be
    given to an employee based on their years of experience.
  prefs: []
  type: TYPE_NORMAL
- en: 'The example that we are considering is based on an Excel sheet. Basically,
    we have data from a company where we have a salary structure based on years of
    experience. We want our machine learning model to derive the correlation between
    the years of experience and the salary given. From the derived correlation, we
    want the model to provide future predictions and specify the modeled salary. The
    machine does this through simple linear regression. In simple linear regression,
    various lines are drawn through the given scattered data (trend lines). The idea
    of the trend line is it should best-fit (cut across) all the scattered data. After
    that, the best trend line is chosen by computing the modeled differences. This
    can be further explained as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4065a88b-4f56-47b7-8862-add0c9322521.png)'
  prefs: []
  type: TYPE_IMG
- en: Continuing with the same example, let's take the case of an employee "e" who
    is earning a salary of 100,000 after 10 years of experience in their actual job.
    According to the model, however, the employee should be earning a little less
    than what he is actually earning, as shown by the green  `+` and the line beneath
    the green `+` is actually less than the line followed by the organization (the
    modeled salary). The green dotted line represents the difference between the actual
    salary and the modeled salary (`~=80K`). It is given by *yi -yi^*, where *yi*
    is actual salary and *yi^* is the mode.
  prefs: []
  type: TYPE_NORMAL
- en: SLR draws all possible trend lines through your data, then computes the sum
    *(y-y^)**²* for the whole line. It then finds the minimum of the computed squares.
    The line with the minimum sum of the squares is considered to be the one that
    would best fit the data. This method is called the **least squares method** or
    the **Euclidean distance method**. The least squares method is a form of mathematical
    regression analysis that finds the line of best fit for a dataset, providing a
    visual demonstration of the relationship between the data points.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot represents the various prediction lines drawn by a
    regression model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c2866745-289f-447b-9c1a-6605e57a305b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Based on the sum of squares method, the best fitting line is chosen, as shown
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/59bca6d8-6d99-404d-8429-e775a15af39e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Basically, the data points plotted are not in a line, but the the actual dots
    are plotted symmetrically either side of the straight line, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f05273ca-db2c-4379-a092-95e7ed7a331f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following section represents the code to implement SLR:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/625a4c93-18bf-4808-89c7-8de1604e1995.png)'
  prefs: []
  type: TYPE_IMG
- en: Multiple linear regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: SLR works on datasets that have one independent and one dependent variable.
    It plots both in *XY* dimensional space, draws trend lines based on the dataset,
    and finally makes a prediction by choosing the best fitting line. However, we
    now need to think about what would happen if the number of dependent variables
    is more than *one*. This is where multiple linear regression comes into the picture.
    **Multiple linear regression** (**MLR**) takes multiple independent variables
    and plots them over n-dimensions in order to make a prediction.
  prefs: []
  type: TYPE_NORMAL
- en: We will now be working on a different dataset that contains information relating
    to 50 startup companies. The data essentially consists of expenditure made on
    various verticals of the company such as R&D, administration, and marketing. It
    also indicates the state in which the company is located and the net profit made
    by each verticals . Clearly, profit is the dependent variable and the other factors
    are the independent variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we will be acting from the perspective of an investor who wants to analyze
    various parameters and predict which verticals more revenue should be spent on,
    and in which state, in order to maximize profit. For example, there may be states
    in which spending more on R&D provides better results, or others in which spending
    more on marketing is more profitable. The model should be able to predict which
    verticals to invest in, shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f49826c0-9324-4325-a2e5-5da36f4187d5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Given that we have multiple independent variables, as shown here, it is also
    important for us to identify those that are actually useful and those that aren''t:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/82204bed-9eeb-48f0-b68f-04c601b37613.png)'
  prefs: []
  type: TYPE_IMG
- en: 'While some independent variables may have an impact on the final dependent
    variable, others might not. To improve the model''s accuracy, we must eliminate
    all the variables that have a minimal impact on the dependent variable. There
    are five ways to eliminate such variables, shown in the following figure, but
    the most reliable one is **backward elimination**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/38446cdc-c2a3-44bf-9cbd-ea35b7243dae.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The working principles of backward elimination are shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/35ac9100-ec27-4f8e-a22f-9adfd8722ff8.png)'
  prefs: []
  type: TYPE_IMG
- en: What we mean by significance level in the previous method is the minimum threshold
    value that would signify that the variable under examination is crucial to the
    dependent variable or final prediction.
  prefs: []
  type: TYPE_NORMAL
- en: The **P value** is the probability that determines whether the relation between
    dependent and independent variables is random. For any given variable, if the
    computed P value is equal to 0.9, this would suggest that the relation between
    that independent variable and final dependent variable is 90% random, so any change
    to the independent variable may not have a direct impact on the dependent one.
    On the other hand, if the P value for a different variable is 0.1, this means
    that the relation between this variable and the dependent one is not random in
    nature, and a change to this variable would have a direct impact on the output.
  prefs: []
  type: TYPE_NORMAL
- en: 'We should start by analyzing the dataset to figure out the independent variables
    that are significant for the prediction. We must train our data model only on
    those variables. The following code snippet represents the implementation of backward
    elimination, which will give us an idea about which variables to take out and
    which to leave in:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/035d5bc5-1d47-4097-bfc1-2c07b2c491d1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following is the explanation for the main functions used in the preceding
    code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '`X[:,[0,1,2,3,4,5]]` indicates that we pass all rows and columns from 90 to
    5 to the backward elimination function'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sm.OLS` is an internal Python library that helps in P value computation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`regressor_OLS.summary()` will display a summary on the console that will help
    us decide which data variables to keep and which to leave out'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the following example, we are training the model over all the variables.
    It is recommended, however, to use `X_Modeled`, as obtained before, instead of
    `X`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8d93203b-2aed-462a-a4cc-539647e36472.png)'
  prefs: []
  type: TYPE_IMG
- en: 'It should be noted that in MLR, the prediction is also made based upon the
    best fitting line, but in this case the best fitting line is plotted over multiple
    dimensions. The following screenshot gives an idea of how the dataset will be
    plotted in n-dimensional space:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/83debebc-309c-4c0a-9bad-1bf7706ccca2.png)'
  prefs: []
  type: TYPE_IMG
- en: There are various other regression models that work for other types of datasets,
    but to cover them all is beyond the scope of this book. However, the two models
    mentioned should have given us an idea about how regression models work. In the
    next section, we are going to discuss **classification models**. We will look
    at one classification model in greater detail and see how we can use it in natural
    language processing to apply ML in the penetration testing ecosystem.
  prefs: []
  type: TYPE_NORMAL
- en: Classification models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Unlike regression models, where the model predicts a continuous number, classification
    models are used to predict a category among a given list of categories. The business
    problem discussed previously, where we have data related to customers of an e-commerce
    website over the last three months containing the purchase history of a particular
    product as (`c_id`, `p_id`, `age`, `gender`, `nationality`, `salary`, `purchased[yes/no]`).
    Our objective, as before, is to identify a customer who would be likely to purchase
    the product based upon their purchase history. Based on the permutation of all
    independent variables (`age`, `gender`, `nationality`, `salary`), a classification
    model can make a prediction in terms of 1 and 0, 1 being the prediction that a
    given customer will purchase the product, and 0 being that they won''t. In this
    particular case, there are two categories (0 and 1). However, depending upon the
    business problem, the number of output categories may vary. The different classification
    models that are commonly used are shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: Naive Bayes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logistic regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: K-nearest neighbors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Support vector machines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kernel SVM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision tree classifier
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random forest classifier
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Naive Bayes classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's try to understand how classification models work with the help of a Naive
    Bayes classifier. In order to understand Naive Bayes classifiers, we need to understand
    the Bayes theorem. The **Bayes theorem** is the theorem we studied in probability,
    and can be explained with the help of an example.
  prefs: []
  type: TYPE_NORMAL
- en: Let's say that we have two machines, both of which produce spanners. The spanners
    are marked with which machine has produced them. M1 is the label for machine 1
    and M2 is the label for machine 2.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s say that one spanner is defective and we want to find the probability
    that the defective spanner was produced by machine 2\. The probability of event
    A happening provided B has already occurred is determined by the Naive Bayes theorem. We
    therefore make use of the Bayes theorem as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/df7a805a-b276-42e2-993c-e7896f01041a.png)'
  prefs: []
  type: TYPE_IMG
- en: P(A) represents the probability of an event happening.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: p(B/A) represents the probability of B given A (the probability of B happening
    assuming that A has already happened).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: P(B) represents the probability of B happening.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: p(A/B) represents the probability of A given B (the probability of A happening,
    assuming that B has already happened).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If we put the data in terms of probability, we get the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| ![](img/b12e6835-1d4d-4e64-b138-5c5c9ac71843.png) | ![](img/5cdc2f99-b110-44d9-81cd-a87f6fc44aae.png)
    |'
  prefs: []
  type: TYPE_TB
- en: 'Let''s say we have a dataset of people, of whom some walk to work and some
    drive to work, depending upon the age category they fall into:'
  prefs: []
  type: TYPE_NORMAL
- en: '| ![](img/f0956274-0f90-4867-a8f2-4dd7a3523508.png) |                     ![](img/7afadd61-6a01-4735-8e3f-114e18432885.png)
    |'
  prefs: []
  type: TYPE_TB
- en: If a new data point is added, we should be able to say whether that person drives
    to work or walks to work. This is supervised learning; we are training the machine
    on a dataset and deriving a learned model from that. We will apply Bayes theorem
    to determine the probability of the new data point belonging to the walking category
    and the driving category.
  prefs: []
  type: TYPE_NORMAL
- en: 'To calculate the probability of the new data point belonging to the walking
    category, we calculate *P(Walk/X).* Here, *X* represents the features of the given
    person, including their age and their salary:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/21e1f515-fb99-4bc9-b9fd-5b76d37e936d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To calculate the probability of the new data point belonging to the driving
    category, we calculate *P(Drives/X)* as shown in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f96b5bbf-4908-41d9-b8b5-7200a42d8b36.png)'
  prefs: []
  type: TYPE_IMG
- en: Finally, we will compare *P(Walks/X)* and *P(Drives/X). *Based on this comparison,
    we will establish where to put the new data point (in the category in which the
    probability is higher). The initial plotting happens over n-dimensional space,
    depending upon the values of independent variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we compute the marginal likelihood, as shown in the following figure,
    which is P(X):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/68ba163a-89e4-432b-8094-7c2ca7eb70f2.png)'
  prefs: []
  type: TYPE_IMG
- en: '*P(X)* actually refers to the probability of adding the new data point to a
    place that has data points with similar features. The algorithm divides or makes
    a circle around the data points that it finds are similar in features to the one
    it is about to add. Then, the probability of the features is computed as *P(X)
    =number of similar observations/Total observations*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The radius of the circle is an important parameter in this case. This radius
    is given as an input parameter to the algorithm:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/dad55bff-e896-4df6-a2a8-2d5cb5f3efdf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In this example, all the points inside the circle are assumed to have similar
    features to the data point that is to be added. Let''s say that the data point
    that we are adding relates to someone who is 35 years old and has a salary of
    $40,000\. In this case, everybody within the bracket $25-40K would be selected
    in the circle:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/91023a2b-3bab-4adc-b518-c30bdbe5fe93.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, we need to compute the likelihood, which means the probability that someone
    chosen randomly who walks contains the features of X. The following will determine
    *P(X/walks)*:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/f67595a3-a52a-4c9d-84b3-a03673837041.png)'
  prefs: []
  type: TYPE_IMG
- en: We will be doing the same to derive the probability of the data point belonging
    to the driving section given that it has features identical to people who walk
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this case, P(X) is equal to the number of similar observations that fall
    in the circle shown before, divided by the total number of observations . P(X)
    =4/30 = 0.133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: P(drives)= P(# who drive) /(#total) =20/30 = 0.666
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: P(X|Drivers) = P (similar observations that are drivers) /total drivers = 1/20
    =0.05
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying the values we get P(Drivers|X) =0.05 *0.666 /0.133 =0.25 =&gt;25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the given problem, we will assume that the data point will belong to the
    set of walkers.
  prefs: []
  type: TYPE_NORMAL
- en: Summarizing the Naive Bayes classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following bullet points put all of the concepts discussed so far into perspective,
    to summarize what we have learned about the Naive Bayes classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: It should be noted that the Naive Bayes classifier does not have a computed
    model that is obtained after training. In fact, at the time of prediction, all
    the data points are simply labeled according to which class they belong to.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At the time of prediction, based on the values of the independent variables,
    a data point would be computed and plotted at a particular place in the n-dimensional
    space. The aim is to predict which class a data point belongs to among N classes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Based on the independent variables, the data point will be plotted in vector
    space in close proximity to data points of similar features. However, this still
    does not determine which class the data point belongs to.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Based on the initially chosen optimal value of the radius, a circle would be
    drawn around that data point, encapsulating a few other points within the proximity
    of the radius of the circle.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's say we have two classes, A and B, and we need to determine the class for
    the new data point X. Bayes theorem will be used to determine the probability
    of X belonging to class A and the probability of X belonging to class B. The one
    that has the higher probability is the class in which the data point is predicted
    to belong.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementation code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s assume that we have a car company X that holds some data on people,
    which contains their age, their salary, and other information. It also has details
    about whether the person has purchased an SUV that the company has launched at
    a very expensive price. This data is used to help them understand who buys their
    cars:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/124e9018-1f28-4010-9421-bda54f403750.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We will use the same data to train our model so that it can predict whether
    a person will buy a car, given their `age`, `salary`, and `gender`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/db67f62a-5f20-48e8-b463-bf9599ded9c7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following screenshot shows the difference between `y_pred` and `y_test`
    for the first 12 data points:'
  prefs: []
  type: TYPE_NORMAL
- en: '| ![](img/e31d4c8a-3e8d-4908-96f0-2995a35ed172.png) |                                                  
    ![](img/fcc87af1-e5dd-423e-986a-75ad337a02e6.png)The previous screenshot represents
    the output of the confusion matrix.'
  prefs: []
  type: TYPE_NORMAL
- en: The cell [0,0] represents the total cases where the output was 0 and was predicted
    as 0.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The cell [0,1] represents the total cases where the output was 0 but was predicted
    as 1.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The cell [1,0] represents the total cases where the output was 1 but was predicted
    as 0.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The cell [1,1] represents the total cases where the output was 1 and was predicted
    as 1.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we take the statistics from the previous dataset, we can see that out of
    the 100 predictions, 90 were correct and 10 were wrong, giving us an accuracy
    of 90%.                      |
  prefs: []
  type: TYPE_NORMAL
- en: Natural language processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Natural language processing** (**NLP**) is about analyzing text, articles
    and involves carrying out predictive analysis on textual data. The algorithm we
    make will address a simple problem, but the same concept is applicable to any
    text. We can also predict the genre of a book with NLP.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following Tab Separated Values (TSV), which is a tab-delimited
    dataset for us to apply NLP to and see how it works:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3dfcd7e2-6f90-4a5b-8d50-b606750de3fe.png)'
  prefs: []
  type: TYPE_IMG
- en: This is a small portion of the data we will be working on. In this case, the
    data represents customer reviews about a restaurant. The reviews are given as
    text, and they have a rating, which is 0 or 1 to indicate whether the customer
    liked the restaurant or not. 1 would mean the review is positive and 0 would indicate
    that it's not positive.
  prefs: []
  type: TYPE_NORMAL
- en: Usually, we would use a CSV file. Here, however, we are using a TSV file where
    the delimiter is a tab because we are working on text-based data, so we may have
    commas that don't indicate a separator. If we take the 14^(th) record, for example,
    we can see a comma in the text. Had this been a CSV file, Python would have taken
    the first half of the sentence as the review and the second half as a rating,
    while the `1` would have been taken as a new review. This would mess up the whole
    model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset has got around 1,000 reviews and has been labeled manually. Since
    we are importing a TSV file, some parameters of `pandas.read_csv` will need to
    change. First of all, we specify that the delimiters are tab separated, using /t.
    We should also ignore double quotes, which can be done by specifying parameter
    quoting=3:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/df6b9cce-a6df-4b7b-a18b-b20e9854ad23.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The imported dataset is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d1cc2af3-48f2-4eb9-b875-e35a85069898.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see that the 1,000 reviews have been imported successfully. All the reviews
    are in the review column and all the ratings are in the **Liked** column. In NLP,
    we have to clean text-based data before we use it. This is because NLP algorithms
    work using the bag of words concept, which means that only the words that lead
    to a prediction are maintained. The bag of words actually contains only the relevant
    words that impact the prediction. Words such as `a`, `the`, `on`, and so on are
    considered to be irrelevant in this context. We also get rid of dots and numbers
    unless numbers are needed, and apply stemming on the words. An example of stemming
    would be taking the word `love` in place of `loved`. The reason why we apply stemming
    is because we don't want to have too many words in the end, and also to regroup
    words such as `loving` and `loved` to one word, `love`. We also remove the capital
    letters and have everything in lowercase. To apply our bag-of-words model, we
    need to apply tokenization. After we do this, we will have different words, because
    the pre-processing will have got rid of those that are irrelevant.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we take all the words of the different reviews and make one column for
    each word. There are likely to be many columns as there may be many different
    words in the reviews. Then, for each review, each column would contain a number
    that indicates the number of times that word has occurred in that specific review.
    This kind of matrix is called a sparse matrix, as there is likely to be lots of
    zeros in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `dataset[''Review''][0]` command will give us the first review:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d3908299-bf7b-4646-935b-bab54b2ad262.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We use a sub module of regular expressions, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b6819ab8-f6ae-4d4f-b8a4-4bf010baa0fd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The sub module we are using is called a subtract function. This subtracts specified
    characters from our input string. It can also club words together and replace
    the specified characters with a character of your choice. The characters to be
    replaced can either be input as a string or in regular expression format. In regular
    expression format shown in the previous example, the ^ sign means not and [a-zA-Z]
    means everything other then a-z and A-Z should be replaced by a single space `''
    ''`. In the given string, the dots will be removed and replaced by spaces, producing
    this output: `Wow Loved this place`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We now remove all non-significant words such as `the`, `a`, `this`, and so
    on. To do this, we will use the `nltk` library (natural language toolkit). This
    has a sub module called stopwords, which contains all the words (generic words
    ) that are mostly irrelevant with regard to fetching the meaning of the sentence.
    To download stopwords, we use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/49314ee0-a4a3-4823-948d-d286336a9418.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This downloads stop words to the current path from where they can be used directly. First,
    we break the reviews into a list of words and then we move through the different
    words and compare them with the downloaded stopwords, removing those that are
    unnecessary:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/15bff5a3-c53b-4677-a0bd-7eac69eff2da.png)'
  prefs: []
  type: TYPE_IMG
- en: In the previous code snippet, we are using a for loop. Declaring the `[]` sign
    in front of review signifies that the list will contain the words that will be
    returned from the for loop, which are the stopwords in this case.
  prefs: []
  type: TYPE_NORMAL
- en: The code preceding the `for` loop indicates that we should assign the string
    word, and update the list with new words every time that word is present in the
    review list and not present in the `stopwords.words('English')` list. Note that
    we are making use of the `set()` function to actually convert the given stop word
    list to a set, because in Python the search operation over sets is much faster
    than over lists. Finally, the review will hold the string with our irrelevant
    words. In this case, for the first review, it will hold [`wov`, `loved`, `place`].
  prefs: []
  type: TYPE_NORMAL
- en: The next step is to perform stemming. The reason why we apply stemming is to
    avoid sparsity, which occurs when we have lots and lots of zeros in our matrix
    (known as a sparse matrix). To reduce sparsity, we need to reduce the proportion
    of zeros in the matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the portstemmer library to apply stemming to each word:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ec31b7c4-d334-4a2f-9abb-efd9a0340a7b.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, the review will hold [`wov`, `love`, `place`].
  prefs: []
  type: TYPE_NORMAL
- en: In this step, we will join the transformed string review from the list back
    to a string by calling `join`. We will put a space as the `delimiter` `' '.join(review)` to
    join all the words in the review list together and then we use `' '` as a separator
    to separate the words.
  prefs: []
  type: TYPE_NORMAL
- en: 'The review is now a string of relevant words all in lowercase:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/30639f06-fc93-482a-a023-5c550e3188dc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'After executing the code, if we compare the original dataset and the obtained
    corpus list, we will obtain the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b5544e88-4137-4d4a-b4bc-ee9ac777d7e7.png)'
  prefs: []
  type: TYPE_IMG
- en: Since the stopword list also had the word `Not`, the string at index 1, `Crust
    is not good` (which had a `Liked` rating of 0), became `crust good`. We need to
    make sure that this does not happen. Likewise, `would not go back` became `would
    go back`. One of the ways to handle it would be to use a stop word list as `set(stopwords.words('english'))]`.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will create a bag of words model. Here, the different words from the
    obtained corpus (list of sentences) would be taken, and a column would be made
    for each distinct word. None of the words will be repeated.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, words such as `wov love place`, `crust good`, `tasti textur nasti`, and
    so on will be taken and a column will be made for each. Each column will correspond
    to a different word. We will also have the review comment and an entry number,
    specifying how many times the word has existed in that specific review.
  prefs: []
  type: TYPE_NORMAL
- en: With this kind of setup, there would be many zeros in our table because there
    may be words that do not appear frequently. The objective should always be to
    keep sparsity to a minimum, such that only the relevant words point to a prediction.
    This will yield a better model. The sparse matrix we have just created will be
    our bag of words model, and it works just like our classification model. We have
    some independent variables that take some values (in this case, the independent
    variables are the review words) and, based on the values of the independent variables,
    we will predict the dependent variables, which is if the review is positive or
    not. To create our bag of words model, we will apply a classification model to
    predict whether each new review is positive or negative. We will create a bag
    of words model with the help of tokenization and a tool called **CountVectoriser**.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the following code to use this library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will create an instance of this class. The parameters take stop words
    as one of the arguments, but since we have already applied stop words to our dataset,
    we do not need to do that again. This class also allows us to control the case
    and the token pattern. We could have chosen to perform all the steps before with
    this class as well, but doing it separately gives better granular control:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cb89493e-113d-4c2b-8dd6-233afab1ec52.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that the line `cv.fit_transform` will actually fit the sparse matrix to
    cv and return  a matrix of features that has all the words of the corpus.
  prefs: []
  type: TYPE_NORMAL
- en: Up until now, we have made our bag of words, or sparse matrix, a matrix of independent
    variables. The next step is to use a classification model and train the model
    over a part of the bag of words, -X, and the dependent variable over the same
    indexes, -Y. The dependent variable in this case is the `Liked` column.
  prefs: []
  type: TYPE_NORMAL
- en: Executing the preceding code will create a matrix of features with around 1,565
    features (different columns). If the number of distinct features come out to be
    very large, we can limit the max features and specify a maximum threshold number.
    Let's say that if we specify the threshold number to be 1,500, then only 1,500
    features or distinct words will be taken in the sparse matrix and those that are
    less frequent as compared to the first 1,500 would get removed. This would make
    a better correlation between the independent and dependent variables, further
    reducing sparsity.
  prefs: []
  type: TYPE_NORMAL
- en: 'We now need to train our classification model on the bag of model words and
    the dependent variables:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Extract the dependent variable as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5db0ad4c-bc11-414b-9d54-43e3d95031dd.png)'
  prefs: []
  type: TYPE_IMG
- en: '`X` and `Y` would look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9d2394b1-f70c-4465-9d54-a54adc8e28f1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Note that in the previous case, each index (0-1499) corresponds to a word in
    the original corpus list. We now have exactly what we had in the classification
    model: a metric of independent variables and a result, 0 for a negative review
    and 1 for a positive review. However, we have still got a significant amount of
    sparsity.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step for us is to make use of a classification model for training. There
    are two ways to use classifications models. One way is to test all the classification
    models against our dataset and determine false positives and false negatives,
    and the other method is based on experience and past experiments. The most common
    models used alongside NLP are Naive Bayes and decision trees or random forest
    classification. In this tutorial, we will be using a Naive Bayes model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5cd1b943-ad4e-49b2-822a-965ab3b6baae.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The whole code is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f1d38bb1-4220-4957-8226-b5f71e06b262.png)'
  prefs: []
  type: TYPE_IMG
- en: 'From the preceding code, we can see that we are splitting the train and test
    sets as 80% and 20%. We will give 800 observations to the training set and 200
    observations to the test set, and see how our model will behave. The value of
    the confusion metric after the execution is given as following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/13c1413a-f21b-41c4-97cb-8b840c48a641.png)'
  prefs: []
  type: TYPE_IMG
- en: There are 55 correct predictions for negative reviews and 91 correct predictions
    for positive reviews. There are 42 incorrect predictions for negative reviews
    and 12 incorrect predictions for positive reviews. Therefore, out of 200 predictions,
    there are 146 total correct predictions, which is equal to 73%.
  prefs: []
  type: TYPE_NORMAL
- en: Using natural language processing with penetration testing reports
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the applications of ML in the cyber security space that I have experimented
    with is automating the task of report analysis to find vulnerabilities. We now
    know how the vulnerability scanner that we built in the last chapter works, but
    the amount of data produced by all the integrated scripts and tools is enormous
    and we need to deal with it or analyze it manually. What happens in typical scanners
    such as Nessus or Qualys is that the plugins are actually scripts. Since they
    are developed in-house by Nessus and Qualys, the scripts are designed to find
    flaws and report them in a manner that can be easily understood. However, in our
    case, we are integrating many open source scripts and tool sets, and the output
    produced is not integrated. In order to automate this task and get an overview
    of the vulnerabilities, we need to figure out the output the script or tool produces,
    in a scenario where it flags a vulnerability, and also in a scenario where the
    results returned are safe. Based on our understanding and the expected output
    patterns of each script, we have to draft our Python code logic to discover which
    plugin produced unsafe check results and which returned safe checks. This requires
    a huge amount of effort. Any time we increase the number of integrated scripts,
    the logic of our code also needs to be updated, so it is up to you whether you
    want to follow this path.
  prefs: []
  type: TYPE_NORMAL
- en: The other method we have at hand is to make use of machine learning and NLP.
    Since there is a huge pool of historic pentesting data that is available to us,
    why not feed it to a machine learning model and train it to understand what is
    unsafe and what is safe? Thanks to the historic penetration testing reports that
    we have performed with our vulnerability scanner, we have an awful lot of data
    in our database tables. We can try to reuse this data to automate manual report
    analysis using machine learning and NLP. We are talking about supervised learning,
    which requires a one-time effort to tag the data appropriately. Let's say that
    we take the  historic data of the last 10 penetration tests we conducted, with
    an average of three IPs to be tested in each. Let's also assume that on average
    we executed 100 scripts per IP (depending on the number of open ports). This means
    that we have the data of 3,000 scripts.
  prefs: []
  type: TYPE_NORMAL
- en: We would need to tag the results manually. Alternatively, if the tester is presented
    with the data in a user interface, while testing, the tester can select **vulnerable**/**not
    vulnerable** with the help of a checkbox, which will act as a tag to the data
    presented. Let's say that we are able to tag all the result data with 1 where
    the test case or check resulted as safe, and 0 where the test case resulted as
    unsafe. We would then have tagged data that will be pre-processed and given to
    our NLP model, which will receive training on it. Once the model is trained, we
    persist the model. Finally, during live scanning, we pass the results of the test
    case to our trained model, making it carry out predictions for test cases that
    result as vulnerable against the ones that don't. The tester then only needs to focus
    on the vulnerable test cases and prepare their exploitation steps.
  prefs: []
  type: TYPE_NORMAL
- en: For us to demonstrate a POC for this concept, let's take the results from one
    project, and consider only the scripts that ran for `ssl` and `http`. Let's see
    the code in action.
  prefs: []
  type: TYPE_NORMAL
- en: Step 1– tagging the raw data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following is the output of the `ssl` and `http` checks we did on one of
    the projects we scanned with our vulnerability scanner. The data is obtained from
    the backend IPexploits table and is tagged with 0 where the check was not vulnerable
    and 1 where the test was unsafe. We can see this in the following screenshot.
    This is a TSV file with the schema (`command_id`, `recored_id`, `service_result`,
    `vul[0/1]`):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/96902653-c1ab-4eba-86c5-fa3b73031522.png)'
  prefs: []
  type: TYPE_IMG
- en: Now that we have tagged the data, let's process and clean it. After that, we
    will train our NLP model with it. We will be using a Naive Bayes classifier with
    NLP. I have had decent success with this model for the current dataset. It would
    be a good exercise to test various other models and see whether we can achieve
    a better prediction success rate.
  prefs: []
  type: TYPE_NORMAL
- en: Step 2–writing the code to train and test our model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following code is identical to what we discussed in the section on NLP,
    with a few additions where we are using `pickle.dump` to save the trained model
    in a file. We also use `pickle.load` to load the saved model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ee1568fe-3ca7-4213-937c-b9d793dee0b1.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/29912ad7-0f20-4edd-9828-08b7e55b8c10.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/ae164868-e89d-4e97-864b-f07ef3f48571.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following screenshot shows the results, in the form of a confusion matrix
    given by our trained model for the dataset. We trained the model on 80% of our
    dataset, specified by 0.8, and tested it on 20%, specified by 0.2\. The result
    set obtained suggests that we have a 92% accuracy rate with the model prediction.
    It should be noted that the accuracy may vary for a larger dataset. The idea here
    was to give you an understanding of how NLP can be used with penetration testing
    reports. We can improve the processing to give cleaner data and change the choice
    of model to arrive at better results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9f2f6f9c-797b-4166-b014-6d0004572b07.png)'
  prefs: []
  type: TYPE_IMG
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed using ML with Python and how we can apply it to
    the cyber security domain. There are many other wonderful applications of data
    science and ML in the cyber security space related to log analysis, traffic monitoring,
    anomaly detection, data exfiltration, URL analysis, spam detection, and so on.
    Modern SIEM solutions are mostly built on top of machine learning, and a big data
    engine is used to reduce human analysis in monitoring. Refer to the further reading
    section to see the various other use cases of machine learning with cyber security.
    It must also be noted that it is important for pen testers to have an understanding
    of machine learning, in order to find vulnerabilities.  In the next chapter, the
    user is going to understand how they can use Python to automate various web application
    attack categories, which include SQLI, XSS, CSRF, and clickjacking.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What are the various vulnerabilities associated with machine learning?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is big data and what is an example of a big data product with known vulnerabilities?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the difference between machine learning and artificial intelligence?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Which pentesting tools use machine learning and why?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Detecting phishing websites with machine learning: [https://github.com/abhishekdid/detecting-phishing-websites](https://github.com/abhishekdid/detecting-phishing-websites)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Using machine learning for log analysis: [https://github.com/logpai](https://github.com/logpai)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'NLP for cyber security: [https://www.recordedfuture.com/machine-learning-](https://www.recordedfuture.com/machine-learning-cybersecurity-applications/)[cybersecurity-applications/](https://www.recordedfuture.com/machine-learning-cybersecurity-applications/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Spam detection using machine learning: [https://github.com/Meenapintu/Spam-Detection](https://github.com/Meenapintu/Spam-Detection)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Deep learning with Python: [https://www.manning.com/books/deep-learning-with-python](https://www.manning.com/books/deep-learning-with-python)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
