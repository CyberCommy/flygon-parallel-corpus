- en: Immutable Design
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will look at the immutable design of Apache Spark. We will
    delve into the Spark RDD's parent/child chain and use RDD in an immutable way.
    We will then use DataFrame operations for transformations to discuss immutability
    in a highly concurrent environment. By the end of this chapter, we will use the
    Dataset API in an immutable way.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Delving into the Spark RDD's parent/child chain
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using RDD in an immutable way
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using DataFrame operations to transform
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Immutability in the highly concurrent environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the Dataset API in an immutable way
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Delving into the Spark RDD's parent/child chain
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will try to implement our own RDD that inherits the parent
    properties of RDD.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will go through the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Extending an RDD
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chaining a new RDD with the parent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing our custom RDD
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extending an RDD
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This is a simple test that has a lot of hidden complexity. Let''s start by
    creating a list of the record, as shown in the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The `Record` is just a case class that has an `amount` and `description`, so
    the `amount` is `1` and `d1` is the description.
  prefs: []
  type: TYPE_NORMAL
- en: 'We then created `MultipledRDD` and passed `rdd` to it, and then set the multiplier
    equal to `10`, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: We are passing the parent RDD because it has data that was loaded in another
    RDD. In this way, we build the inheritance chain of two RDD's.
  prefs: []
  type: TYPE_NORMAL
- en: Chaining a new RDD with the parent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We first created a multiple RDD class. In the `MultipliedRDD` class, we have
    two things that pass the parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: A brief RDD of the record, that is, `RDD[Record]`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A multiplier, that is, `Double`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In our case, there could be a chain of multiple RDD's, which means that there
    could be multiple RDD's inside our RDD. So, this is not always the parent of all
    the directed acyclic graphs. We are just extending the RDD of the type record
    and so we need to pass the RDD that is extended.
  prefs: []
  type: TYPE_NORMAL
- en: RDD has a lot of methods and we can override any method we want. However, this
    time, we are going with the `compute` method, where we will override the compute
    method to calculate the multiplier. Here, we get a `Partition` split and `TaskContext`.
    These are passed by this part execution engine to our method, so we don't need
    to worry about this. However, we need to return the iterator of the exact same
    type as the type that we pass through the RDD class in the inheritance chain.
    This will be an iterator of the record.
  prefs: []
  type: TYPE_NORMAL
- en: We then execute the first parent logic, where the first parent is just taking
    that first RDD in our chain. The type here is `Record`, and we are taking an `iterator`
    of `split` and `context`, where the `split` is just a partition that will be executed.
    We know that the Spark RDD is partitioned by the partitioner, but, here, we are
    just getting the specific partition that we need to split. So, the iterator is
    taking the partition and task context, and so it knows which values should be
    returned from that iterative method. For every record in that iterator, which
    is a `salesRecord`, like `amount` and `description`, we are multiplying the `amount`
    by the `multiplier` that was passed to the constructor to get our `Double`.
  prefs: []
  type: TYPE_NORMAL
- en: By doing this, we have multiplied our amount by the multiplier, and we can then
    return the new record which has the new amount. So, we now have an amount of the
    old record multiplied by our `multiplier` and the description of the `salesRecord`.
    For the second filter, what we need to `override` is `getPartitions`, as we want
    to keep the partitioning of the parent RDD. If the previous RDD has 100 partitions,
    for example, we also want our `MultipledRDD` to have 100 partitions. So, we want
    to retain that information about partitions rather than losing it. For the same
    reason, we are just proxying it to the `firstParent`. The `firstParent` of the
    RDD will then just take the previous partitions from that specific RDD.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this way, we have created a new `multipliedRDD`, which passes the parent
    and multiplier. For our `extendedRDD`, we need to `collect` it and call `toList`,
    and our list should have `10` and `d1`, as shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Compute was executed automatically when we created the new RDD, and so it is
    always executed without the explicit method call.
  prefs: []
  type: TYPE_NORMAL
- en: Testing our custom RDD
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s start this test to check if this has created our RDD. By doing this,
    we can extend our parent RDD and add behavior to our RDD. This is shown in the
    following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: In the next section, we'll be using RDD in an immutable way.
  prefs: []
  type: TYPE_NORMAL
- en: Using RDD in an immutable way
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we know how to create a chain of execution using RDD inheritance, let's
    learn how to use RDD in an immutable way.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will go through the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understating DAG immutability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating two leaves from the one root RDD
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Examining results from both leaves
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's first understand directed acyclic graph immutability and what it gives
    us. We will then be creating two leaves from one node RDD, and checking if both
    leaves are behaving totally independently if we create a transformation on one
    of the leaf RDD's. We will then examine results from both leaves of our current
    RDD and check if any transformation on any leaf does not change or impact the
    root RDD. It is imperative to work like this because we have found that we will
    not be able to create yet another leaf from the root RDD, because the root RDD
    will be changed, which means it will be mutable. To overcome this, the Spark designers
    created an immutable RDD for us.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is a simple test to show that the RDD should be immutable. First, we
    will create an RDD from `0 to 5`, which is added to a sequence from the Scala
    branch. `to` is taking the `Int`, and the first parameter is an implicit one,
    which is from the Scala package, as shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have our RDD data, we can create the first leaf. The first leaf is
    a result (`res`) and we are just mapping every element multiplied by `2`. Let''s
    create a second leaf, but this time it will be marked by `4`, as shown in the
    following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'So, we have our root RDD and two leaves. First, we will collect the first leaf
    and see that the elements in it are `0, 2, 4, 6, 8, 10`, so everything here is
    multiplied by `2`, as shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'However, even though we have that notification on the `res`, the data is still
    exactly the same as it was in the beginning, which is `0, 1, 2, 3, 4, 5`, as shown
    in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'So, everything is immutable, and executing the transformation of `* 2` didn''t
    change our data. If we create a test for `leaf2`, we will `collect` it and call
    `toList`. We will see that it should `contain` elements like `0, 4, 8, 12, 16,
    20`, as shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'When we run the test, we will see that every path in our execution, the root,
    that is, data, or the first leaf and second leaf, behave independently from each
    other, as shown in the following code output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Every mutation is different; we can see that the test passed, which shows us
    that our RDD is immutable.
  prefs: []
  type: TYPE_NORMAL
- en: Using DataFrame operations to transform
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The data from the API has an RDD underneath it, and so there is no way that
    the DataFrame could be mutable. In DataFrame, the immutability is even better
    because we can add and subtract columns from it dynamically, without changing
    the source dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding DataFrame immutability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating two leaves from the one root DataFrame
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding a new column by issuing transformation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will start by using data from operations to transform our DataFrame. First,
    we need to understand DataFrame immutability and then we will create two leaves,
    but this time from the one root DataFrame. We will then issue a transformation
    that is a bit different than the RDD. This will add a new column to our resulting
    DataFrame because we are manipulating it this way in a DataFrame. If we want to
    map data, then we need to take data from the first column, transform it, and save
    it to another column, and then we'll have two columns. If we are no longer interested,
    we can drop the first column, but the result will be yet another DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: So, we'll have the first DataFrame with one column, the second one with result
    and source, and the third one with only one result. Let's look at the code for
    this section.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will be creating a DataFrame, so we need to call the `toDF()` method. We
    are creating the `UserData` with `"a"` as `"1"`, `"b"` as `"2"`, and `"d"` as `"200"`. The
    `UserData` has `userID` and `data`, two fields that are both `String`, as shown
    in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'It is important to create an RDD using a case class in tests because, when
    we are called to the DataFrame, this part will infer the schema and name columns
    accordingly. The following code follows an example of this, where we are filtering
    only a `userID` column from the `userData` that is in `"a"`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Our result should have only one record and so we are dropping two columns,
    but still, the `userData` source that we created will have `3` rows. So, modifying
    it by filtering created yet another DataFrame that we call the `res` without modifying
    the input `userData`, as shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'So, let''s start this test and see how immutable data from API behaves, as
    shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see, our test passes, and, from the result (`res`), we know that
    our parent was not modified. So, for example, if we want to map something on `res.map()`,
    we can map the `userData` column, as shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Another leaf will have an additional column without changing the `userId` source
    code, so that was the immutability of DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: Immutability in the highly concurrent environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We saw how immutability affects the creation and design of programs, so now
    we will understand how it is useful.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: The cons of mutable collections
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating two threads that simultaneously modify a mutable collection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reasoning about a concurrent program
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s first understand the cause of mutable collections. To do this, we will
    be creating two threads that simultaneously modify the mutable collection. We
    will be using this code for our test. First, we will create a `ListBuffer` that
    is a mutable list. Then, we can add and delete links without creating another
    list for any modification. We can then create an `Executors` service with two
    threads. We need two threads to start simultaneously to modify the state. Later,
    we will use a `CountDownLatch` construct from `Java.util.concurrent:`. This is
    shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The `CountDownLatch` is a construct that helps us to stop threads from processing
    until we request them to. We need to wait with our logic until both threads start
    executing. We then submit a `Runnable` to the `executors` and our `run()` method
    does the `countDown()` by signaling when it is ready for action and appends `"A"`
    to `listMutable`, as shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, another thread starts, and also uses `countDown` by signaling that it
    is ready to start. But first, it checks whether the list contains `"A"` and, if
    not, it appends that `"A"`, as shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'We then use `await()` to wait until `countDown` is issued and, when it is issued,
    we are able to progress with the verification of our program, as shown in the
    following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '`listMutable` contains `"A"` or it can have `"A","A"`. `listMutable` checks
    if the list contains `"A"` and, if not, it will not add that element, as shown
    in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: But there is a race condition here. There could be a possibility that, after
    the check `if(!listMutable.contains("A"))`, the `run()` thread will add the `"A"`
    element to the list. But we are inside `if`, so we will add another `"A"` by/using
    `listMutable += "A"`. Because of the mutability of the state and the fact that
    it was modified via another thread, we can have `"A"` or `"A","A"`.
  prefs: []
  type: TYPE_NORMAL
- en: We need to be careful while using mutable state since we cannot have such a
    corrupted state. To alleviate this problem, we can use `java.util` collections
    and synchronized lists on it.
  prefs: []
  type: TYPE_NORMAL
- en: 'But if we have the synchronized block, then our program will be very slow because
    we will need to coordinate access to that exclusively. We can also employ `lock`
    from the `java.util.concurrent.locks` package. We can use an implementation, like `ReadLock`
    or `WriteLock`. In the following example, we will use `WriteLock`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'We also need to `lock` our `lock()` and only then proceed, as shown in the
    following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Later, we can use `unlock()`. However, we should also do this in the second
    thread so that our list only has one element, as shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b19108c-26d0-4115-92a9-8191a1900bdb.png)'
  prefs: []
  type: TYPE_IMG
- en: Locking is a very hard and expensive operation, and so immutability is key to
    performance programs.
  prefs: []
  type: TYPE_NORMAL
- en: Using the Dataset API in an immutable way
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will use the Dataset API in an immutable way. We will cover
    the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Dataset immutability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating two leaves from the one root dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding a new column by issuing transformation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The test case for the dataset is quite similar, but we need to do a `toDS()` for
    our data to be type safe. The type of dataset is `userData`, as shown in the following
    example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will issue a filter of `userData` and specify `isin`, as shown in the
    following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'It will return the result (`res`), which is a leaf with our `1` element. `userData`
    will still have `3` elements because of this apparent root. Let''s execute this
    program, as shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see our test passed, which means that the dataset is also an immutable
    abstraction on top of the DataFrame, and employs the same characteristics. `userData`
    has a something very useful known as a typeset, and, if you use the `show()` method,
    it will infer the schema and know that the `"a"` field is a string or another
    type, as shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding output, we have both `userID` and `data` fields.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we delved into the Spark RDD parent-child chain and created
    a multiplier RDD that was able to calculate everything based on the parent RDD,
    and also based on the partitioning scheme on the parent. We used RDD in an immutable
    way. We saw that the modification of the leaf that was created from the parent didn't
    modify the part. We also learned a better abstraction, that is, a DataFrame, so
    we learned that we can employ transformation there. However, every transformation
    is just adding to another column—it is not modifying anything in place. Next,
    we just set immutability in a highly concurrent environment. We saw how the mutable
    state is bad when accessing multiple threads. Finally, we saw that the Dataset
    API is also created in an immutable type of way and that we can leverage those
    things here.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we'll look at how to avoid shuffle and reduce personal
    expense.
  prefs: []
  type: TYPE_NORMAL
