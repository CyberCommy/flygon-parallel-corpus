- en: Using Spark SQL for Processing Structured and Semistructured Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will familiarize you with using Spark SQL with different
    types of data sources and data storage formats. Spark provides easy and standard
    structures (that is, RDDs and DataFrames/Datasets) to work with both structured
    and semistructured data. We include some of the data sources that are most commonly
    used in big data applications, such as, relational data, NoSQL databases, and
    files (CSV, JSON, Parquet, and Avro). Spark also allows you to define and use
    custom data sources. A series of hands-on exercises in this chapter will enable
    you to use Spark with different types of data sources and data formats.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, you shall learn the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding data sources in Spark applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using JDBC to work with relational databases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Spark with MongoDB (NoSQL database)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with JSON data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Spark with Avro and Parquet Datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding data sources in Spark applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spark can connect to many different data sources, including files, and SQL and
    NoSQL databases. Some of the more popular data sources include files (CSV, JSON,
    Parquet, AVRO), MySQL, MongoDB, HBase, and Cassandra.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00028.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In addition, it can also connect to special purpose engines and data sources,
    such as ElasticSearch, Apache Kafka, and Redis. These engines enable specific
    functionality in Spark applications such as search, streaming, caching, and so
    on. For example, Redis enables deployment of cached machine learning models in
    high performance applications. We discuss more on Redis-based application deployment
    in [Chapter 12](part0216.html#6DVPG0-e9cbc07f866e437b8aa14e841622275c), *Spark
    SQL in Large-Scale Application Architectures*. Kafka is extremely popular in Spark
    streaming applications, and we will cover more details on Kafka-based streaming
    applications in [Chapter 5](part0085.html#2H1VQ0-e9cbc07f866e437b8aa14e841622275c),
    *Using Spark SQL in Streaming Applications,* and [Chapter 12](part0216.html#6DVPG0-e9cbc07f866e437b8aa14e841622275c), *Spark
    SQL in Large-Scale Application Architectures*. The DataSource API enables Spark
    connectivity to a wide variety of data sources including custom data sources.
  prefs: []
  type: TYPE_NORMAL
- en: Refer to the Spark packages website [https://spark-packages.org/](https://spark-packages.org/) to
    work with various data sources, algorithms, and specialized Datasets.
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 1](part0022.html#KVCC0-e9cbc07f866e437b8aa14e841622275c), *Getting
    Started with Spark SQL,*  we used CSV and JSON files on our filesystem as input
    data sources and used SQL to query them. However, using Spark SQL to query data
    residing in files is not a replacement for using databases. Initially, some people
    used HDFS as a data source because of the simplicity and the ease of using Spark
    SQL for querying such data. However, the execution performance can vary significantly
    based on the queries being executed and the nature of the workloads. Architects
    and developers need to understand which data stores to use in order to best meet
    their processing requirements. We discuss some high-level considerations for selecting
    Spark data sources below.
  prefs: []
  type: TYPE_NORMAL
- en: Selecting Spark data sources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Filesystems are a great place to dump large volumes of data and for supporting
    general purpose processing of large Datasets. Some of the benefits you will get
    by using files are inexpensive storage, flexible processing, and scale. The decision
    to store large-scale data in files is usually driven by the prohibitive costs
    of storing the same on commercial databases. Additionally, file storage is also
    preferred when the nature of the data does not benefit from typical database optimizations,
    for example, unstructured data. Additionally, workloads, such as machine learning
    applications, with iterative in-memory processing requirements and distributed
    algorithms may be better suited to run on distributed file systems.
  prefs: []
  type: TYPE_NORMAL
- en: The types of data you would typically store on filesystems are archival data,
    unstructured data, massive social media and other web-scale Datasets, and backup
    copies of primary data stores. The types of workloads best supported on files
    are batch workloads, exploratory data analysis, multistage processing pipelines,
    and iterative workloads. Popular use cases for using files include ETL pipelines,
    splicing data across varied data sources, such as log files, CSV, Parquet, zipped
    file formats, and so on. In addition, you can choose to store the same data in
    multiple formats optimized for your specific processing requirements.
  prefs: []
  type: TYPE_NORMAL
- en: What's not so great about Spark connected to a filesystem are use cases involving
    frequent random accesses, frequent inserts, frequent/incremental updates, and
    reporting or search operations under heavy load conditions across many users.
    These use cases are discussed in more detail as we move on.
  prefs: []
  type: TYPE_NORMAL
- en: Queries selecting a small subset of records from your distributed storage are
    supported in Spark but are not very efficient, because it would typically require
    Spark to go through all your files to find your result row(s). This may be acceptable
    for data exploration tasks but not for sustained processing loads from several
    concurrent users. If you need to frequently and randomly access your data, using
    a database can be a more effective solution. Making the data available to your
    users using a traditional SQL database and creating indexes on the key columns
    can better support this use case. Alternatively, key-value NoSQL stores can also
    retrieve the value of a key a lot more efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: As each insert creates a new file, the inserts are reasonably fast however querying
    becomes low as the Spark jobs will need to open all these files and read from
    them to support queries. Again, a database used to support frequent inserts may
    be a much better solution. Alternatively, you can also routinely compact your
    Spark SQL table files to reduce the overall number of files. Use the `Select *`
    and `coalesce` DataFrame commands to write the data out from a DataFrame created
    from multiple input files to a single / combined output file.
  prefs: []
  type: TYPE_NORMAL
- en: Other operations and use cases, such as frequent/incremental updates, reporting,
    and searching are better handled using databases or specialized engines. Files
    are not optimized for updating random rows. However, databases are ideal for executing
    efficient update operations. You can connect Spark to HDFS and use BI tools, such
    as Tableau, but it is better to dump the data to a database for serving concurrent
    users under load. Typically, it is better to use Spark to read the data, perform
    aggregations, and so on, and then write the results out to a database that serves
    end users. In the search use case, Spark will need to go through each row to find
    and return the search results, thereby impacting performance. In this case, using
    the specialized engines such as ElasticSearch and Apache Solr may be a better
    solution than using Spark.
  prefs: []
  type: TYPE_NORMAL
- en: In cases where the data is heavily skewed, or for executing faster joins on
    a cluster, we can use cluster by or bucketing techniques to improve performance.
  prefs: []
  type: TYPE_NORMAL
- en: Using Spark with relational databases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There is a huge debate on whether relational databases fit into big data processing
    scenarios. However, it's undeniable that vast quantities of structured data in
    enterprises live in such databases, and organizations rely heavily on the existing
    RDBMSs for their critical business transactions.
  prefs: []
  type: TYPE_NORMAL
- en: A vast majority of developers are most comfortable working with relational databases
    and the rich set of tools available from leading vendors. Increasingly, cloud
    service providers, such as Amazon AWS, have made administration, replication,
    and scaling simple enough for many organizations to transition their large relational
    databases to the cloud.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some good big data use cases for relational databases include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Complex OLTP transactions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applications or features that need ACID compliance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Support for standard SQL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Real-time ad hoc query functionality
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Systems implementing many complex relationships
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For an excellent coverage of NoSQL and relational use cases, refer to the blog
    titled What the heck are you actually using NoSQL for? at [http://highscalability.com/blog/2010/12/6/what-the-heck-are-you-actually-using-nosql-for.html](http://highscalability.com/blog/2010/12/6/what-the-heck-are-you-actually-using-nosql-for.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'In Spark, it is easy to work with relational data and combine it with other
    data sources in different forms and formats:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00029.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: As an example that illustrates using Spark with a MySQL database, we will implement
    a use-case in which we split the data between HDFS and MySQL. The MySQL database
    will be targeted to support interactive queries from concurrent users, while the
    data on HDFS will be targeted for batch processing, running machine learning applications,
    and for making the data available to BI tools. In this example, we assume that
    the interactive queries are against the current month's data only. Hence, we will
    retain only the current month's data in MySQL and write out the rest of data to
    HDFS (in JSON format).
  prefs: []
  type: TYPE_NORMAL
- en: 'The implementation steps, we will follow are:'
  prefs: []
  type: TYPE_NORMAL
- en: Create MySQL database.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define a table.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a user ID and grant privileges.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Start Spark shell with MySQL JDBC driver.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a RDD from input data file, separate the header, define a schema, and
    create a DataFrame.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a new column for timestamps.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Separate the data into two DataFrames based on the timestamp value (data for
    the current month and rest of data previous months).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Drop the original invoiceDate column and then rename the timestamp column to
    invoiceDate.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write out the DataFrame containing current month data to the MySQL table.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write out the DataFrame containing data (other than current month data) to HDFS
    (in JSON format).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you do not have MySQL already installed and available, you can download it
    from [https://www.mysql.com/downloads/](https://www.mysql.com/downloads/). Follow
    the installation instructions for your specific OS to install the database. Also,
    download the JDBC connector available on the same website.
  prefs: []
  type: TYPE_NORMAL
- en: 'After you have your MySQL database server up and running, fire up the MySQL
    shell. In the following steps, we will create a new database and define a transactions
    table. We use a transnational dataset that contains all the transactions occurring
    between 01/12/2010 and 09/12/2011 for a UK-based and registered nonstore online
    retail. The dataset has been contributed by Dr Daqing Chen, Director: Public Analytics
    group, School of Engineering, London South Bank University and is available at
    [https://archive.ics.uci.edu/ml/datasets/Online+Retail](https://archive.ics.uci.edu/ml/datasets/Online+Retail).'
  prefs: []
  type: TYPE_NORMAL
- en: 'You should see a screen similar to the following when you start the MySQL shell:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00030.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Create a new database called `retailDB` to store our customer transactions
    data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we define a transactions table with `transactionID` as the primary key.
    In a production scenario, you would also create indexes on other fields, such
    as `CustomerID`, to support queries more efficiently:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we verify the transactions table schema using the `describe` command
    to ensure that it is exactly how we want it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00031.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Create a user ID `retaildbuser` and grant all privileges to it. We will use
    this user from our Spark shell for connecting and executing our queries.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Start the Spark shell with the classpath containing the path to the MySQL JDBC
    driver, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a `RDD` containing all the rows from our downloaded Dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Separate the header from the rest of the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the fields and define a schema for our data records, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Create an `RDD` of Row objects, create a DataFrame using the previously created
    schema:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Add a column called `ts` (a timestamp column) to the DataFrame, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00032.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Create a table object and execute appropriate SQLs to separate the table data
    into two DataFrames based on the timestamps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Drop the `invoiceDate` column in our new DataFrame.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Rename the `ts` column to `invoiceDate`, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00033.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Create a variable to point to the database URL. Additionally, create a `Properties` object
    to hold the user ID and password required for connecting to `retailDB.` Next,
    connect to the MySQL database and insert the records from the "current month"
    into the transactions table:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Select the columns of interest from the DataFrame (containing data other than
    for the current month), and write them out to the HDFS filesystem in JSON format:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Using Spark with MongoDB (NoSQL database)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will use Spark with one of the most popular NoSQL databases
    - MongoDB. MongoDB is a distributed document database that stores data in JSON-like
    format. Unlike the rigid schemas in relational databases, the data structure in
    MongoDB is a lot more flexible and the stored documents can have arbitrary fields.
    This flexibility combined with high availability and scalability features make
    it a good choice for storing data in many applications. It is also free and open-source
    software.
  prefs: []
  type: TYPE_NORMAL
- en: If you do not have MongoDB already installed and available, then you can download
    it from [https://www.mongodb.org/downloads](https://www.mongodb.com/download-center#community).
    Follow the installation instructions for your specific OS to install the database.
  prefs: []
  type: TYPE_NORMAL
- en: The New York City schools directory dataset for this example has been taken
    from the New York City Open Data website and can be downloaded from [https://nycplatform.socrata.com/data?browseSearch=&scope=&agency=&cat=education&type=datasets](https://nycplatform.socrata.com/data?browseSearch=&scope=&agency=&cat=education&type=datasets).
  prefs: []
  type: TYPE_NORMAL
- en: After you have your MongoDB database server up and running, fire up the MongoDB
    shell. In the following steps, we will create a new database, define a collection,
    and insert New York City school's data using the MongoDB import utility from the
    command line.
  prefs: []
  type: TYPE_NORMAL
- en: 'You should see a screen similar to the following when you start the MongoDB
    shell:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00034.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Next, execute the use `<DATABASE>` command to select an existing database or
    create a new one, if it does not exist.
  prefs: []
  type: TYPE_NORMAL
- en: If you make a mistake while creating a new collection, you can use the `db.dropDatabase()`
    and/or `db.collection.drop()` commands to delete the dababase and/or the collection,
    respectively, and then recreate it with the required changes.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The `mongoimport` utility needs to be executed from the command prompt (and
    not in the `mongodb` shell):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'You can list the imported collection and print a record to validate the import operation,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00035.jpeg)![](img/00036.jpeg)![](img/00037.jpeg)![](img/00038.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: You can download the `mongo-spark-connector jar` for Spark 2.2 (`mongo-spark-connector_2.11-2.2.0-assembly.jar`)
    from [http://repo1.maven.org/maven2/org/mongodb/spark/mongo-spark-connector_2.11/2.2.0/](http://repo1.maven.org/maven2/org/mongodb/spark/mongo-spark-connector_2.11/2.2.0/).
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, start the Spark shell with the `mongo-spark-connector_2.11-2.2.0-assembly.jar` file
    specified on the command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we define the URIs for `read` and `write` operations from Spark:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a `case` class for the school record, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00039.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Next, you can create a DataFrame from our collection and display a record from
    our newly created DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00040.jpeg)![](img/00041.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Note: The following sections will be updated with the latest versions of the
    connector packages later.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next several sections, we describe using Spark with several popular big
    data file formats.
  prefs: []
  type: TYPE_NORMAL
- en: Using Spark with JSON data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: JSON is a simple, flexible, and compact format used extensively as a data-interchange
    format in web services. Spark's support for JSON is great. There is no need for
    defining the schema for the JSON data, as the schema is automatically inferred.
    In addition, Spark greatly simplifies the query syntax required to access fields
    in complex JSON data structures. We will present detailed examples of JSON data
    in [Chapter 12](part0216.html#6DVPG0-e9cbc07f866e437b8aa14e841622275c), *Spark
    SQL in Large-Scale Application Architectures*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset for this example contains approximately 1.69 million Amazon reviews
    for the electronics category, and can be downloaded from: [http://jmcauley.ucsd.edu/data/amazon/](http://jmcauley.ucsd.edu/data/amazon/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can directly read a JSON dataset to create Spark SQL DataFrame. We will
    read in a sample set of order records from a JSON file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: You can print the schema of the newly created DataFrame to verify the fields
    and their characteristics using the `printSchema` method.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00042.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Once, the JSON Dataset is converted to a Spark SQL DataFrame, you can work
    with it extensively in a standard way. Next, we will execute an SQL statement
    to select certain columns from our orders that are received from customers in
    a specific age bracket:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Display the results of the SQL execution (stored in another DataFrame) using
    the `show` method, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00043.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can access the array elements of the `helpful` column in the `reviewDF`
    DataFrame (using DSL) as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00044.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: An example of writing out a DataFrame as a JSON file was presented in an earlier
    section where we selected the columns of interest from the DataFrame (containing
    data other than for the current month), and wrote them out to the HDFS filesystem
    in JSON format.
  prefs: []
  type: TYPE_NORMAL
- en: Using Spark with Avro files
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Avro is a very popular data serialization system that provides a compact and
    fast binary data format. Avro files are self-describing because the schema is
    stored along with the data.
  prefs: []
  type: TYPE_NORMAL
- en: You can download `spark-avro connector` JAR from [https://mvnrepository.com/artifact/com.databricks/spark-avro_2.11/3.2.0](https://mvnrepository.com/artifact/com.databricks/spark-avro_2.11/3.2.0).
  prefs: []
  type: TYPE_NORMAL
- en: We will switch to Spark 2.1 for this section. At the time of writing this book
    due to a documented bug in the `spark-avro connector` library, we are getting
    exceptions while writing Avro files (using `spark-avro connector 3.2`) with Spark
    2.2.
  prefs: []
  type: TYPE_NORMAL
- en: 'Start Spark shell with the spark-avro JAR included in the session:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'We will use the JSON file from the previous section containing the Amazon reviews
    data to create the `Avro` file. Create a DataFrame from the input JSON file and
    display the number of records:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we filter all the reviews with an overall rating of less than `3`, `coalesce`
    the output to a single file, and write out the resulting DataFrame to an `Avro`
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we show how to read an `Avro` file by creating a DataFrame from the `Avro`
    file created in the previous step and display the number of records in it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we select a few columns and display five records from the results DataFrame
    by specifying `show(5)`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00045.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, we specify compression options for `Avro` files by setting the Spark
    session configuration values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, when we write the DataFrame, the `Avro` file is stored in a compressed
    format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also write out the DataFrame partitioned by a specific column. Here,
    we partition based on the `overall` column (containing `values < 3` in each row):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: The screenshot of the Avro files from this session are shown here. Notice the
    sizes of the compressed version (67 MB) versus the original file (97.4 MB) . Additionally,
    notice the two separate directories created for the partitioned (by `overall`
    values) `Avro` files.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00046.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'For more details on `spark-avro`, refer: [https://github.com/databricks/spark-avro](https://github.com/databricks/spark-avro)'
  prefs: []
  type: TYPE_NORMAL
- en: Using Spark with Parquet files
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apache Parquet is a popular columnar storage format. It is used in many big
    data applications in the Hadoop ecosystem. Parquet supports very efficient compression
    and encoding schemes that can give a significant boost to the performance of such
    applications. In this section, we show you the simplicity with which you can directly
    read Parquet files into a standard Spark SQL DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we use the reviewsDF created previously from the Amazon reviews contained
    in a JSON formatted file and write it out in the Parquet format to create the
    Parquet file. We use `coalesce(1)` to create a single output file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'In the next step, we create a DataFrame from the Parquet file using just one
    statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'After the DataFrame is created, you can operate on it as you normally would
    with the DataFrames created from any other data source. Here, we register the
    DataFrame as a temp view and query it using SQL:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Here, we specify two parameters to display the records in the resulting DataFrame.
    The first parameter specifies the number of records to display and a value of
    false for the second parameter shows the full values in the columns (with no truncation).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00047.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Defining and using custom data sources in Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can define your own data sources and combine the data from such sources
    with data from other more standard data sources (for example, relational databases,
    Parquet files, and so on). In [Chapter 5](part0085.html#2H1VQ0-e9cbc07f866e437b8aa14e841622275c), *Using
    Spark SQL in Streaming Applications*, we define a custom data source for streaming
    data from public APIs available from **Transport for London** (**TfL**) site.
  prefs: []
  type: TYPE_NORMAL
- en: Refer to the video *Spark DataFrames Simple and Fast Analysis of Structured
    Data - Michael Armbrust (Databricks)* at [https://www.youtube.com/watch?v=xWkJCUcD55w](https://www.youtube.com/watch?v=xWkJCUcD55w)
    for a good example of defining a data source for Jira and creating a Spark SQL
    DataFrame from it.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we demonstrated using Spark with various data sources and data
    formats. We used Spark to work with a relational database (MySQL), NoSQL database
    (MongoDB), semistructured data (JSON), and data storage formats commonly used
    in the Hadoop ecosystem (Avro and Parquet). This sets you up very nicely for the
    more advanced Spark application-oriented chapters to follow.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will shift our focus from the mechanics of working with
    Spark to how Spark SQL can be used to explore data, perform data quality checks,
    and visualize data.
  prefs: []
  type: TYPE_NORMAL
