- en: Concurrent Execution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '"What do we want? Now! When do we want it? Fewer race conditions!"– Anna Melzer'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, I'm going to up the game a little bit, both in terms of the
    concepts I'll present, and in the complexity of the code snippets I'll show you.
    If you don't feel up to the task, or as you are reading through you realize it
    is getting too difficult, feel free to skip it. You can always come back to it
    when you feel ready.
  prefs: []
  type: TYPE_NORMAL
- en: The plan is to take a detour from the familiar single-threaded execution paradigm,
    and deep dive into what can be described as concurrent execution. I will only
    be able to scratch the surface of this complex topic, so I won't expect you to
    be a master of concurrency by the time you're done reading, but I will, as usual,
    try to give you enough information so that you can then proceed by *walking the
    path*, so to speak.
  prefs: []
  type: TYPE_NORMAL
- en: We will learn about all the important concepts that apply to this area of programming,
    and I will try to show you examples coded in different styles, to give you a solid
    understanding of the basics of these topics. To dig deep into this challenging
    and interesting branch of programming, you will have to refer to the *Concurrent
    Execution* section in the Python documentation ([https://docs.python.org/3.7/library/concurrency.html](https://docs.python.org/3.7/library/concurrency.html)),
    and maybe supplement your knowledge by studying books on the subject.
  prefs: []
  type: TYPE_NORMAL
- en: 'In particular, we are going to explore the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The theory behind threads and processes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Writing multithreaded code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Writing multiprocessing code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using executors to spawn threads and processes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A brief example of programming with `asyncio`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's start by getting the theory out of the way.
  prefs: []
  type: TYPE_NORMAL
- en: Concurrency versus parallelism
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Concurrency and parallelism are often mistaken for the same thing, but there
    is a distinction between them. **Concurrency** is the ability to run multiple
    things at the same time, not necessarily in parallel. **Parallelism** is the ability
    to do a number of things at the same time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Imagine you take your other half to the theater. There are two lines: that
    is, for VIP and regular tickets. There is only one functionary checking tickets
    and so, in order to avoid blocking either of the two queues, they check one ticket
    from the VIP line, then one from the regular line. Over time, both queues are
    processed. This is an example of concurrency.'
  prefs: []
  type: TYPE_NORMAL
- en: Now imagine that another functionary joins, so now we have one functionary per
    queue. This way, both queues will be processed each by its own functionary. This
    is an example of parallelism.
  prefs: []
  type: TYPE_NORMAL
- en: Modern laptop processors feature multiple cores (normally two to four). A **core**
    is an independent processing unit that belongs to a processor. Having more than
    one core means that the CPU in question has the physical ability to actually execute
    tasks in parallel. Within each core, normally there is a constant alternation
    of streams of work, which is concurrent execution.
  prefs: []
  type: TYPE_NORMAL
- en: Bear in mind that I'm keeping the discussion generic on purpose here. According
    to which system you are using, there will be differences in how execution is handled,
    so I will concentrate on the concepts that are common to all, or at least most,
    systems.
  prefs: []
  type: TYPE_NORMAL
- en: Threads and processes – an overview
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A **thread** can be defined as a sequence of instructions that can be run by
    a scheduler, which is that part of the operating system that decides which chunk
    of work will receive the necessary resources to be carried out. Typically, a thread
    lives within a process. A process can be defined as an instance of a computer
    program that is being executed.
  prefs: []
  type: TYPE_NORMAL
- en: In previous chapters, we have run our own modules and scripts with commands
    similar to `$ python my_script.py`. What happens when a command like that is run,
    is that a Python process is created. Within it, a main thread of execution is
    spawned. The instructions in the script are what will be run within that thread.
  prefs: []
  type: TYPE_NORMAL
- en: This is just one way of working though, and Python can actually use more than
    one thread within the same process, and can even spawn multiple processes. Unsurprisingly,
    these branches of computer science are called **multithreading** and **multiprocessing**.
  prefs: []
  type: TYPE_NORMAL
- en: In order to understand the difference, let's take a moment to explore threads
    and processes in slightly more depth.
  prefs: []
  type: TYPE_NORMAL
- en: Quick anatomy of a thread
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Generally speaking, there are two different types of threads:'
  prefs: []
  type: TYPE_NORMAL
- en: '**User-level threads**: Threads that we can create and manage in order to perform
    a task'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kernel-level threads**: Low-level threads that run in kernel mode and act
    on behalf of the operating system'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Given that Python works at the user level, we're not going to deep dive into
    kernel threads at this time. Instead, we will explore several examples of user-level
    threads in this chapter's examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'A thread can be in any of the following states:'
  prefs: []
  type: TYPE_NORMAL
- en: '**New thread**: A thread that hasn''t started yet, and hasn''t been allocated
    any resources.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Runnable**: The thread is waiting to run. It has all the resources needed
    to run, and as soon as the scheduler gives it the green light, it will be run.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Running**: A thread whose stream of instructions is being executed. From
    this state, it can go back to a non-running state, or die.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Not-running**: A thread that has been paused. This could be due to another
    thread taking precedence over it, or simply because the thread is waiting for
    a long-running IO operation to finish.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dead**: A thread that has died because it has reached the natural end of
    its stream of execution, or it has been killed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transitions between states are provoked either by our actions or by the scheduler.
    There is one thing to bear in mind, though; it is best not to interfere with the
    death of a thread.
  prefs: []
  type: TYPE_NORMAL
- en: Killing threads
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Killing threads is not considered to be good practice. Python doesn't provide
    the ability to kill a thread by calling a method or function, and this should
    be a hint that killing threads isn't something you want to be doing.
  prefs: []
  type: TYPE_NORMAL
- en: One reason is that a thread might have children—threads spawned from within
    the thread itself—which would be orphaned when their parent dies. Another reason
    could be that if the thread you're killing is holding a resource that needs to
    be closed properly, you might prevent that from happening and that could potentially
    lead to problems.
  prefs: []
  type: TYPE_NORMAL
- en: Later, we will see an example of how we can work around these issues.
  prefs: []
  type: TYPE_NORMAL
- en: Context-switching
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have said that the scheduler can decide when a thread can run, or is paused,
    and so on. Any time a running thread needs to be suspended so that another can
    be run, the scheduler saves the state of the running thread in a way that it will
    be possible, at a later time, to resume execution exactly where it was paused.
  prefs: []
  type: TYPE_NORMAL
- en: This act is called **context-switching**. People do that all the time too. We
    are doing some paperwork, and we hear *bing!* on our phone. We stop the paperwork
    and check our phone. When we're done dealing with what was probably the umpteenth
    picture of a funny cat, we go back to our paperwork. We don't start the paperwork
    from the beginning, though; we simply continue where we had left off.
  prefs: []
  type: TYPE_NORMAL
- en: Context-switching is a marvelous ability of modern computers, but it can become
    troublesome if you generate too many threads. The scheduler then will try to give
    each of them a chance to run for a little time, and there will be a lot of time
    spent saving and recovering the state of the threads that are respectively paused
    and restarted.
  prefs: []
  type: TYPE_NORMAL
- en: In order to avoid this problem, it is quite common to limit the amount of threads
    (the same consideration applies to processes) that can be run at any given point
    in time. This is achieved by using a structure called a pool, the size of which
    can be decided by the programmer. In a nutshell, we create a pool and then assign
    tasks to its threads. When all the threads of the pool are busy, the program won't
    be able to spawn a new thread until one of them terminates (and goes back to the
    pool). Pools are also great for saving resources, in that they provide recycling
    features to the thread ecosystem.
  prefs: []
  type: TYPE_NORMAL
- en: When you write multithreaded code, it is useful to have information about the
    machine our software is going to run on. That information, coupled with some profiling
    (we'll learn about it in [Chapter 11](part0288.html#8IL200-2ddb708647cc4530a187c2c6c0e9acfe),
    *Debugging and Troubleshootin*g), should enable us to calibrate the size of our
    pools correctly.
  prefs: []
  type: TYPE_NORMAL
- en: The Global Interpreter Lock
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In July 2015, I attended the EuroPython conference in Bilbao, where I gave a
    talk about test-driven development. The camera operator unfortunately lost the
    first half of it, but I've since been able to give that talk another couple of
    times, so you can find a complete version of it on the web. At the conference,
    I had the great pleasure of meeting Guido van Rossum and talking to him, and I
    also attended his keynote speech.
  prefs: []
  type: TYPE_NORMAL
- en: One of the topics he addressed was the infamous **Global Interpreter Lock**
    (**GIL**). The GIL is a mutex that protects access to Python objects, preventing
    multiple threads from executing Python bytecodes at once. This means that even
    though you can write multithreaded code in Python, there is only one thread running
    at any point in time (per process, of course).
  prefs: []
  type: TYPE_NORMAL
- en: In computer programming, a mutual exclusion object (mutex) is a program object
    that allows multiple program threads to share the same resource, such as file
    access, but not simultaneously.
  prefs: []
  type: TYPE_NORMAL
- en: This is normally seen as an undesired limitation of the language, and many developers
    take pride in cursing this great villain. The truth lies somewhere else though,
    as was beautifully explained by Raymond Hettinger in his Keynote on Concurrency,
    at PyBay 2017 ([https://bit.ly/2KcijOB](https://bit.ly/2KcijOB)). About 10 minutes
    in, Raymond explains that it is actually quite simple to remove the GIL from Python.
    It takes about a day of work. The price you pay for this *GIL-ectomy* though,
    is that you then have to apply locks yourself wherever they are needed in your
    code. This leads to a more expensive footprint, as multitudes of individual locks
    take more time to be acquired and released, and most importantly, it introduces
    the risk of bugs, as writing robust multithreaded code is not easy and you might
    end up having to write dozens or hundreds of locks.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to understand what a lock is, and why you might want to use it, we
    first need to talk about one of the perils of multithreaded programming: race
    conditions.'
  prefs: []
  type: TYPE_NORMAL
- en: Race conditions and deadlocks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When it comes to writing multithreaded code, you need to be aware of the dangers
    that come when your code is no longer executed linearly. By that, I mean that
    multithreaded code is exposed to the risk of being paused at any point in time
    by the scheduler, because it has decided to give some CPU time to another stream
    of instructions.
  prefs: []
  type: TYPE_NORMAL
- en: This behavior exposes you to different types of risks, the two most famous being
    race conditions and deadlocks. Let's talk about them briefly.
  prefs: []
  type: TYPE_NORMAL
- en: Race conditions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A **race condition** is a behavior of a system where the output of a procedure
    depends on the sequence or timing of other uncontrollable events. When these events
    don't unfold in the order intended by the programmer, a race condition becomes
    a bug.
  prefs: []
  type: TYPE_NORMAL
- en: It's much easier to explain this with an example.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine you have two threads running. Both are performing the same task, which
    consists of reading a value from a location, performing an action with that value,
    incrementing the value by *1* unit, and saving it back. Say that the action is
    to post that value to an API.
  prefs: []
  type: TYPE_NORMAL
- en: Scenario A – race condition not happening
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Thread *A* reads the value (*1*), posts *1* to the API, then increments it to
    *2*, and saves it back. Right after this, the scheduler pauses Thread *A*, and
    runs Thread *B*. Thread *B* reads the value (now *2*), posts *2* to the API, increments
    it to *3*, and saves it back.
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, after the operation has happened twice, the value stored is
    correct: *1 + 2 = 3*. Moreover, the API has been called with both *1* and *2*,
    correctly.'
  prefs: []
  type: TYPE_NORMAL
- en: Scenario B – race condition happening
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Thread *A* reads the value (*1*), posts it to the API, increments it to *2*,
    but before it can save it back, the scheduler decides to pause thread *A* in favor
    of Thread *B*.
  prefs: []
  type: TYPE_NORMAL
- en: Thread *B* reads the value (still *1*!), posts it to the API, increments it
    to *2*, and saves it back. The scheduler then switches over to Thread *A* again.
    Thread *A* resumes its stream of work by simply saving the value it was holding
    after incrementing, which is *2*.
  prefs: []
  type: TYPE_NORMAL
- en: After this scenario, even though the operation has happened twice as in Scenario
    *A*, the value saved is *2*, and the API has been called twice with *1*.
  prefs: []
  type: TYPE_NORMAL
- en: In a real-life situation, with multiple threads and real code performing several
    operations, the overall behavior of the program explodes into a myriad of possibilities.
    We'll see an example of this later on, and we'll fix it using locks.
  prefs: []
  type: TYPE_NORMAL
- en: The main problem with race conditions is that they make our code non-deterministic,
    which is bad. There are areas in computer science where non-determinism is used
    to achieve things, and that's fine, but in general you want to be able to predict
    how your code will behave, and race conditions make it impossible to do so.
  prefs: []
  type: TYPE_NORMAL
- en: Locks to the rescue
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Locks come to the rescue when dealing with race conditions. For example, in
    order to fix the preceding example, all you need is a lock around the procedure.
    A lock is like a guardian that will allow only one thread to take hold of it (we
    say *to acquire* a lock), and until that thread releases the lock, no other thread
    can acquire it. They will have to sit and wait until the lock is available again.
  prefs: []
  type: TYPE_NORMAL
- en: Scenario C – using a lock
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Thread *A* acquires the lock, reads the value (*1*), posts to the API, increases
    to *2*, and the scheduler suspends it. Thread *B* is given some CPU time, so it
    tries to acquire the lock. But the lock hasn't been released yet by Thread *A*,
    so Thread *B* sits and waits. The scheduler might notice this, and quickly decide
    to switch back to Thread *A*.
  prefs: []
  type: TYPE_NORMAL
- en: Thread *A* saves 2, and releases the lock, making it available to all other
    threads.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, whether the lock is acquired again by Thread *A*, or by Thread
    *B* (because the scheduler might have decided to switch again), is not important.
    The procedure will always be carried out correctly, since the lock makes sure
    that when a thread reads a value, it has to complete the procedure (ping API,
    increment, and save) before any other thread can read the value as well.
  prefs: []
  type: TYPE_NORMAL
- en: There are a multitude of different locks available in the standard library.
    I definitely encourage you to read up on them to understand all the perils you
    might encounter when coding multithreaded code, and how to solve them.
  prefs: []
  type: TYPE_NORMAL
- en: Let's now talk about deadlocks.
  prefs: []
  type: TYPE_NORMAL
- en: Deadlocks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A **deadlock** is a state in which each member of a group is waiting for some
    other member to take action, such as sending a message or, more commonly, releasing
    a lock, or a resource.
  prefs: []
  type: TYPE_NORMAL
- en: A simple example will help you get the picture. Imagine two little kids playing
    together. Find a toy that is made of two parts, and give each of them one part.
    Naturally, neither of them will want to give the other one their part, and they
    will want the other one to release the part they have. So neither of them will
    be able to play with the toy, as they each hold half of it, and will indefinitely
    wait for the other kid to release the other half.
  prefs: []
  type: TYPE_NORMAL
- en: Don't worry, no kids were harmed during the making of this example. It all happened
    in my mind.
  prefs: []
  type: TYPE_NORMAL
- en: Another example could be having two threads execute the same procedure again.
    The procedure requires acquiring two resources, *A* and *B*, both guarded by a
    separate lock. Thread *1* acquires *A*, and Thread *2* acquires *B*, and then
    they will wait indefinitely until the other one releases the resource it has.
    But that won't happen, as they both are instructed to wait and acquire the second
    resource in order to complete the procedure. Threads can be much more stubborn
    than kids.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can solve this problem in several ways. The easiest one might be simply
    to apply an order to the resources acquisition, which means that the thread that
    gets *A*, will also get all the rest: *B*, *C*, and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: Another way is to put a lock around the whole resources acquisition procedure,
    so that even if it might happen out of order, it will still be within the context
    of a lock, which means only one thread at a time can actually gather all the resources.
  prefs: []
  type: TYPE_NORMAL
- en: Let's now pause our talk on threads for a moment, and explore processes.
  prefs: []
  type: TYPE_NORMAL
- en: Quick anatomy of a process
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Processes are normally more complex than threads. In general, they contain a
    main thread, but can also be multithreaded if you choose. They are capable of
    spawning multiple sub-threads, each of which contains its own set of registers
    and a stack. Each process provides all the resources that the computer needs in
    order to execute the program.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly to using multiple threads, we can design our code to take advantage
    of a multiprocessing design. Multiple processes are likely to run over multiple
    cores, therefore with multiprocessing, you can truly parallelize computation.
    Their memory footprints, though, are slightly heavier than those of threads, and
    another drawback to using multiple processes is that **inter-process communication**
    (**IPC**) tends to be more expensive than communication between threads.
  prefs: []
  type: TYPE_NORMAL
- en: Properties of a process
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A UNIX process is created by the operating system. It typically contains the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: A process ID, process group ID, user ID, or group ID
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An environment and working directory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Program instructions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Registers, a stack, and a heap
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: File descriptors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Signal actions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shared libraries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inter-process communication tools (pipes, message queues, semaphores, or shared
    memory)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If you are curious about processes, open up a shell and type `$ top`. This
    command displays and updates sorted information about the processes that are running
    in your system. When I run it on my machine, the first line tells me the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This gives you an idea about how much work our computers are doing without us
    being really aware of it.
  prefs: []
  type: TYPE_NORMAL
- en: Multithreading or multiprocessing?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Given all this information, deciding which approach is the best means having
    an understanding of the type of work that needs to be carried out, and knowledge
    about the system that will be dedicated to doing that work.
  prefs: []
  type: TYPE_NORMAL
- en: There are advantages to both approaches, so let's try to clarify the main differences.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some advantages of using multithreading:'
  prefs: []
  type: TYPE_NORMAL
- en: Threads are all born within the same process. They share resources and can communicate
    with one another very easily. Communication between processes requires more complex
    structures and techniques.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The overhead of spawning a thread is smaller than that of a process. Moreover,
    their memory footprint is also smaller.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Threads can be very effective at blocking IO-bound applications. For example,
    while one thread is blocked waiting for a network connection to give back some
    data, work can be easily and effectively switched to another thread.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Because there aren't any shared resources between processes, we need to use
    IPC techniques, and they require more memory than communication between threads.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here are some advantages of using multiprocessing:'
  prefs: []
  type: TYPE_NORMAL
- en: We can avoid the limitations of the GIL by using processes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sub-processes that fail won't kill the main application.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Threads suffer from issues such as race conditions and deadlocks; while using
    processes the likelihood of having to deal with them is greatly reduced.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Context-switching of threads can become quite expensive when their amount is
    above a certain threshold.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Processes can make better use of multicore processors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Processes are better than multiple threads at handling CPU-intensive tasks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, I'll show you both approaches for multiple examples, so hopefully
    you'll gain a good understanding of the various different techniques. Let's get
    to the code then!
  prefs: []
  type: TYPE_NORMAL
- en: Concurrent execution in Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's start by exploring the basics of Python multithreading and multiprocessing
    with some simple examples.
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind that several of the following examples will produce an output that
    depends on a particular run. When dealing with threads, things can get non-deterministic,
    as I mentioned earlier. So, if you experience different results, it is absolutely
    fine. You will probably notice that some of your results will vary from run to
    run too.
  prefs: []
  type: TYPE_NORMAL
- en: Starting a thread
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First things first, let''s start a thread:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: After importing `threading`, we define a function: `sum_and_product`. This function
    calculates the sum and the product of two numbers, and prints the results. The
    interesting bit is after the function. We instantiate `t` from `threading.Thread`.
    This is our thread. We passed the name of the function that will be run as the
    thread body, we gave it a name, and passed the arguments `3` and `7`, which will
    be fed into the function as `a` and `b`, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: After having created the thread, we start it with the homonymous method.
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, Python will start executing the function in a new thread, and
    when that operation is done, the whole program will be done as well, and exit.
    Let''s run it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Starting a thread is therefore quite simple. Let''s see a more interesting
    example where we display more information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: In this example, the thread logic is exactly the same as in the previous one,
    so you don't need to sweat on it and can concentrate on the (insane!) amount of
    logging information I added. We use two functions to display information: `status`
    and `print_current`. The first one takes a thread in input and displays its name
    and whether or not it's alive by calling its `is_alive` method. The second one
    prints the current thread, and then enumerates all the threads in the process.
    This information comes from `threading.current_thread` and `threading.enumerate`.
  prefs: []
  type: TYPE_NORMAL
- en: There is a reason why I put `.2` seconds of sleeping time within the function.
    When the thread starts, its first instruction is to sleep for a moment. The sneaky
    scheduler will catch that, and switch execution back to the main thread. You can
    verify this by the fact that in the output, you will see the result of `status(t)`
    before that of `print_current` from within the thread. This means that that call
    happens while the thread is sleeping.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, notice I called `t.join()` at the end. That instructs Python to block
    until the thread has completed. The reason for that is because I want the last
    call to `status(t)` to tell us that the thread is gone. Let''s peek at the output
    (slightly rearranged for readability):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, at first the current thread is the main thread. The enumeration
    shows only one thread. Then we create and start `SumProd`. We print its status
    and we learn it is alive. Then, and this time from within `SumProd`, we display
    information about the current thread again. Of course, now the current thread
    is `SumProd`, and we can see that enumerating all threads returns both of them.
    After the result is printed, we verify, with one last call to `status`, that the
    thread has terminated, as predicted. Should you get different results (apart from
    the IDs of the threads, of course), try increasing the sleeping time and see whether
    anything changes.
  prefs: []
  type: TYPE_NORMAL
- en: Starting a process
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s now see an equivalent example, but instead of using a thread, we''ll
    use a process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The code is exactly the same as for the first example, but instead of using
    a `Thread`, we actually instantiate `multiprocessing.Process`. The `sum_and_product`
    function is the same as before. The output is also the same, except the numbers
    are different.
  prefs: []
  type: TYPE_NORMAL
- en: Stopping threads and processes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As mentioned before, in general, stopping a thread is a bad idea, and the same
    goes for a process. Being sure you''ve taken care to dispose and close everything
    that is open can be quite difficult. However, there are situations in which you
    might want to be able to stop a thread, so let me show you how to do it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: For this example, we use a Fibonacci generator. We've seen it before so I won't
    explain it. The important bit to focus on is the `_running` attribute. First of
    all, notice the class inherits from `Thread`. By overriding the `__init__` method,
    we can set the `_running` flag to `True`. When you write a thread this way, instead
    of giving it a target function, you simply override the `run` method in the class.
    Our `run` method calculates a new Fibonacci number, and then sleeps for about
    `0.07` seconds.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the last block of code, we create and start an instance of our class. Then
    we sleep for one second, which should give the thread time to produce about 14
    Fibonacci numbers. When we call `fibo.stop()`, we aren''t actually stopping the
    thread. We simply set our flag to `False`, and this allows the code within `run`
    to reach its natural end. This means that the thread will die organically. We
    call `join` to make sure the thread is actually done before we print `All done.`
    on the console. Let''s check the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Check how many numbers were printed: 14, as predicted.'
  prefs: []
  type: TYPE_NORMAL
- en: This is basically a workaround technique that allows you to stop a thread. If
    you design your code correctly according to multithreading paradigms, you shouldn't
    have to kill threads all the time, so let that need become your alarm bell that
    something could be designed better.
  prefs: []
  type: TYPE_NORMAL
- en: Stopping a process
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When it comes to stopping a process, things are different, and fuss-free. You
    can use either the `terminate` or `kill` method, but please make sure you know
    what you're doing, as all the preceding considerations about open resources left
    hanging are still true.
  prefs: []
  type: TYPE_NORMAL
- en: Spawning multiple threads
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Just for fun, let''s play with two threads now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The `run` function simply prints the current thread, and then enters a loop
    of `n` cycles, in which it prints a greeting message, and sleeps for a random
    amount of time, between `0` and `0.2` seconds (`random()` returns a float between
    `0` and `1`).
  prefs: []
  type: TYPE_NORMAL
- en: 'The purpose of this example is to show you how a scheduler might jump between
    threads, so it helps to make them sleep a little. Let''s see the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the output alternates randomly between the two. Every time that
    happens, you know a context switch has been performed by the scheduler.
  prefs: []
  type: TYPE_NORMAL
- en: Dealing with race conditions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have the tools to start threads and run them, let''s simulate a
    race condition such as the one we discussed earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we define the `incr` function, which gets a number `n` in input,
    and loops over `n`. In each cycle, it reads the value of the counter, sleeps for
    a random amount of time (between `0` and `0.1` seconds) by calling `randsleep`,
    a tiny Lambda function I wrote to improve readability, then increases the value of
    the `counter` by `1`.
  prefs: []
  type: TYPE_NORMAL
- en: I chose to use `global` in order to have read/write access to `counter`, but
    it could be anything really, so feel free to experiment with that yourself.
  prefs: []
  type: TYPE_NORMAL
- en: The whole script basically starts two threads, each of which runs the same function,
    and gets `n = 5`. Notice how we need to join on both threads at the end to make
    sure that when we print the final value of the counter (last line), both threads
    are done doing their work.
  prefs: []
  type: TYPE_NORMAL
- en: When we print the final value, we would expect the counter to be 10, right?
    Two threads, five loops each, that makes 10\. However, we almost never get 10
    if we run this script. I ran it myself many times, and it seems to always hit
    somewhere between 5 and 7\. The reason this happens is that there is a race condition
    in this code, and those random sleeps I added are there to exacerbate it. If you
    removed them, there would still be a race condition, because the counter is increased
    in a non-atomic way (which means an operation that can be broken down in multiple
    steps, and therefore paused in between). However, the likelihood of that race
    condition showing is really low, so adding the random sleep helps.
  prefs: []
  type: TYPE_NORMAL
- en: Let's analyze the code. `t1` gets the current value of the counter, say, `3`.
    `t1` then sleeps for a moment. If the scheduler switches context in that moment,
    pausing `t1` and starting `t2`, `t2` will read the same value, `3`. Whatever happens
    afterward, we know that both threads will update the counter to be `4`, which
    will be incorrect as after two readings it should have gone up to `5`. Adding
    the second random sleep call, after the update, helps the scheduler switch more
    frequently, and makes it easier to show the race condition. Try commenting out
    one of them, and see how the result changes (it will do so, dramatically).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have identified the issue, let''s fix it by using a lock. The code
    is basically the same, so I''ll show you only what changes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: This time we have created a lock, from the `threading.Lock` class. We could
    call its `acquire` and `release` methods manually, or we can be Pythonic and use
    it within a context manager, which looks much nicer, and does the whole acquire/release
    business for us. Notice I left the random sleeps in the code. However, every time
    you run it, it will now return `10`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The difference is this: when the first thread acquires that lock, it doesn''t
    matter that when it''s sleeping, a moment later, the scheduler switches the context.
    The second thread will try to acquire the lock, and Python will answer with a
    resounding *no*. So, the second thread will just sit and wait until that lock
    is released. As soon as the scheduler switches back to the first thread, and the
    lock is released, then the other thread will have a chance (if it gets there first,
    which is not necessarily guaranteed), to acquire the lock and update the counter.
    Try adding some prints into that logic to see whether the threads alternate perfectly
    or not. My guess is that they won''t, at least not every time. Remember the `threading.current_thread`
    function, to be able to see which thread is actually printing the information.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Python offers several data structures in the `threading` module: Lock, RLock,
    Condition, Semaphore, Event, Timer, and Barrier. I won''t be able to show you
    all of them, because unfortunately I don''t have the room to explain all the use
    cases, but reading the documentation of the `threading` module ([https://docs.python.org/3.7/library/threading.html](https://docs.python.org/3.7/library/threading.html)) will
    be a good place to start understanding them.'
  prefs: []
  type: TYPE_NORMAL
- en: Let's now see an example about thread's local data.
  prefs: []
  type: TYPE_NORMAL
- en: A thread's local data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `threading` module offers a way to implement local data for threads. Local
    data is an object that holds thread-specific data. Let me show you an example,
    and allow me to sneak in a `Barrier` too, so I can tell you how it works:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: We start by defining `local`. That is the special object that holds thread-specific
    data. We run three threads. Each of them will assign a random value to `local.my_value`,
    and print it. Then the thread reaches a `Barrier` object, which is programmed
    to hold three threads in total. When the barrier is hit by the third thread, they
    all can pass. It's basically a nice way to make sure that *N* amount of threads
    reach a certain point and they all wait until every single one of them has arrived.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, if `local` was a normal, dummy object, the second thread would override
    the value of `local.my_value`, and the third would do the same. This means that
    we would see them printing different values in the first set of prints, but they
    would show the same value (the last one) in the second round of prints. But that
    doesn''t happen, thanks to `local`. The output shows the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Notice the wrong order, due to the scheduler switching context, but the values
    are all correct.
  prefs: []
  type: TYPE_NORMAL
- en: Thread and process communication
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have seen quite a lot of examples so far. So, let's explore how to make threads
    and processes talk to one another by employing a queue. Let's start with threads.
  prefs: []
  type: TYPE_NORMAL
- en: Thread communication
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For this example, we will be using a normal `Queue`, from the `queue` module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The logic is very basic. We have a `producer` function that generates Fibonacci
    numbers and puts them in a queue. When the next number is greater than a given `n`,
    the producer exits the `while` loop, and puts one last thing in the queue: a `SENTINEL`.
    A `SENTINEL` is any object that is used to signal something, and in our case,
    it signals to the consumer that the producer is done.'
  prefs: []
  type: TYPE_NORMAL
- en: The interesting bit of logic is in the `consumer` function. It loops indefinitely,
    reading values out of the queue and printing them out. There are a couple of things
    to notice here. First, see how we are calling `q.task_done()`? That is to acknowledge
    that the element in the queue has been processed. The purpose of this is to allow
    the final instruction in the code, `q.join()`, to unblock when all elements have
    been acknowledged, so that the execution can end.
  prefs: []
  type: TYPE_NORMAL
- en: Second, notice how we use the `is` operator to compare against the items in
    order to find the sentinel. We'll see shortly that when using a `multiprocessing.Queue`
    this won't be possible any more. Before we get there, would you be able to guess
    why?
  prefs: []
  type: TYPE_NORMAL
- en: Running this example produces a series of lines, such as `Got number 0`, `Got
    number 1`, and so on, until `34`, since the limit we put is `35`, and the next
    Fibonacci number would be `55`.
  prefs: []
  type: TYPE_NORMAL
- en: Sending events
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Another way to make threads communicate is to fire events. Let me quickly show
    you an example of that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Here we have two threads that run `fire` and `listen`, respectively firing
    and listening for an event. To fire an event, call the `set` method on it. The `t2` thread,
    which is started first, is already listening to the event, and will sit there
    until the event is fired. The output from the previous example is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Events are great in some situations. Think about having threads that are waiting
    on a connection object to be ready, before they can actually start using it. They
    could be waiting on an event, and one thread could be checking that connection,
    and firing the event when it's ready. Events are fun to play with, so make sure
    you experiment and think about use cases for them.
  prefs: []
  type: TYPE_NORMAL
- en: Inter-process communication with queues
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s now see how to communicate between processes using a queue. This example
    is very very similar to the one for threads:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, in this case, we have to use a queue that is an instance of
    `multiprocessing.Queue`, which doesn't expose a `task_done` method. However, because
    of the way this queue is designed, it automatically joins the main thread, therefore
    we only need to start the two processes and all will work. The output of this
    example is the same as the one before.
  prefs: []
  type: TYPE_NORMAL
- en: When it comes to IPC, be careful. Objects are pickled when they enter the queue,
    so IDs get lost, and there are a few other subtle things to take care of. This
    is why in this example I can no longer use an object as a sentinel, and compare
    using `is`, like I did in the multi-threaded version. That sentinel object would
    be pickled in the queue (because this time the `Queue` comes from `multiprocessing`
    and not from `queue` like before), and would assume a new ID after unpickling,
    failing to compare correctly. The string `"STOP"` in this case does the trick,
    and it will be up to you to find a suitable value for a sentinel, which needs
    to be something that could never clash with any of the items that could be in
    the same queue. I leave it up to you to refer to the documentation, and learn
    as much as you can on this topic.
  prefs: []
  type: TYPE_NORMAL
- en: Queues aren't the only way to communicate between processes. You can also use
    pipes (`multiprocessing.Pipe`), which provide a connection (as in, a pipe, clearly)
    from one process to another, and vice versa. You can find plenty of examples in
    the documentation; they aren't that different from what we've seen here.
  prefs: []
  type: TYPE_NORMAL
- en: Thread and process pools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned before, pools are structures designed to hold *N* objects (threads,
    processes, and so on). When the usage reaches capacity, no work is assigned to
    a thread (or process) until one of those currently working becomes available again.
    Pools, therefore, are a great way to limit the number of threads (or processes)
    that can be alive at the same time, preventing the system from starving due to
    resource exhaustion, or the computation time from being affected by too much context
    switching.
  prefs: []
  type: TYPE_NORMAL
- en: In the following examples, I will be tapping into the `concurrent.futures` module
    to use the `ThreadPoolExecutor` and `ProcessPoolExecutor` executors. These two
    classes, use a pool of threads (and processes, respectively), to execute calls
    asynchronously. They both accept a parameter, `max_workers`, which sets the upper
    limit to how many threads (or processes) can be used at the same time by the executor.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start from the multithreaded example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: After importing the necessary bits, we define the `run` function. It gets a
    random value, prints it, and returns it, along with the `name` argument it was
    called with. The interesting bit comes right after the function.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, we're using a context manager to call `ThreadPoolExecutor`,
    to which we pass `max_workers=3`, which means the pool size is `3`. This means
    only three threads at any time will be alive.
  prefs: []
  type: TYPE_NORMAL
- en: We define a list of future objects by making a list comprehension, in which
    we call `submit` on our executor object. We instruct the executor to run the `run`
    function, with a name that will go from `T0` to `T4`. A `future` is an object
    that encapsulates the asynchronous execution of a callable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then we loop over the `future` objects, as they are are done. To do this, we
    use `as_completed` to get an iterator of the `future` instances that returns them
    as soon as they complete (finish or were cancelled). We grab the result of each
    `future` by calling the homonymous method, and simply print it. Given that `run`
    returns a tuple `name`*,* `value`, we expect the result to be a two-tuple containing
    `name` and `value`. If we print the output of a `run` (bear in mind each `run`
    can potentially be slightly different), we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Before reading on, can you tell why the output looks like this? Could you explain
    what happened? Spend a moment thinking about it.
  prefs: []
  type: TYPE_NORMAL
- en: So, what goes on is that three threads start running, so we get three `Hi, I
    am...` messages printed out. Once all three of them are running, the pool is at
    capacity, so we need to wait for at least one thread to complete before anything
    else can happen. In the example run, `T0` and `T2` complete (which is signaled
    by the printing of what they returned), so they return to the pool and can be
    used again. They get run with names `T3` and `T4`, and finally all three, `T1`, `T3`,
    and `T4` complete. You can see from the output how the threads are actually reused,
    and how the first two are reassigned to `T3` and `T4` after they complete.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now see the same example, but with the multiprocess design:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The difference is truly minimal. We use `ProcessPoolExecutor` this time, and
    the `run` function is exactly the same, with one small addition: we sleep for
    50 milliseconds at the beginning of each `run`. This is to exacerbate the behavior
    and have the output clearly show the size of the pool, which is still three. If
    we run the example, we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: This output clearly shows the pool size being three. It is very interesting
    to notice that if we remove that call to `sleep`, most of the time the output
    will have five prints of `Hi, I am...`, followed by five prints of `Process Px
    returned...`. How can we explain that? Well it's simple. By the time the first
    three processes are done, and returned by `as_completed`, all three are asked
    for their result, and whatever is returned, is printed. While this happens, the
    executor can already start recycling two processes to run the final two tasks,
    and they happen to print their `Hi, I am...` messages, before the prints in the
    `for` loop are allowed to take place.
  prefs: []
  type: TYPE_NORMAL
- en: This basically means `ProcessPoolExecutor` is quite fast and aggressive (in
    terms of getting the scheduler's attention), and it's worth noting that this behavior
    doesn't happen with the thread counterpart, in which, if you recall, we didn't
    need to use any artificial sleeping.
  prefs: []
  type: TYPE_NORMAL
- en: The important thing to keep in mind though, is being able to appreciate that
    even simple examples such as these can already be slightly tricky to understand
    or explain. Let this be a lesson to you, so that you raise your attention to 110%
    when you code for multithreaded or multiprocess designs.
  prefs: []
  type: TYPE_NORMAL
- en: Let's now move on to a more interesting example.
  prefs: []
  type: TYPE_NORMAL
- en: Using a process to add a timeout to a function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Most, if not all, libraries that expose functions to make HTTP requests, provide
    the ability to specify a timeout when performing the request. This means that
    if after *X* seconds (*X* being the timeout), the request hasn''t completed, the
    whole operation is aborted and execution resumes from the next instruction. Not
    all functions expose this feature though, so, when a function doesn''t provide
    the ability to being interrupted, we can use a process to simulate that behavior. In
    this example, we''ll be trying to translate a hostname into an IPv4 address. The `gethostbyname` function,
    from the `socket` module, doesn''t allow us to put a timeout on the operation
    though, so we use a process to do that artificially. The code that follows might
    not be so straightforward, so I encourage you to spend some time going through
    it before you read on for the explanation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Let's start from `resolve`. It simply takes a `hostname` and a `timeout`, and
    calls `resolve_host` with them. If the exit code is `0` (which means the process
    terminated correctly), it returns the IPv4 that corresponds to that host. Otherwise,
    it returns the `hostname` itself, as a fallback mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let's talk about `gethostbyname`. It takes a `hostname` and a `queue`,
    and calls `socket.gethostbyname` to resolve the `hostname`. When the result is
    available, it is put into the `queue`. Now, this is where the issue lies. If the
    call to `socket.gethostbyname` takes longer than the timeout we want to assign,
    we need to kill it.
  prefs: []
  type: TYPE_NORMAL
- en: The `resolve_host` function does exactly this. It receives the `hostname` and
    the `timeout`, and, at first, it simply creates a `queue`. Then it spawns a new
    process that takes `gethostbyname` as the `target`, and passes the appropriate
    arguments. Then the process is started and joined on, but with a `timeout`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, the successful scenario is this: the call to `socket.gethostbyname` succeeds
    quickly, the IP is in the queue, the process terminates well before its timeout
    time, and when we get to the `if` part, the queue will not be empty. We fetch
    the IP from it, and return it, alongside the process exit code.'
  prefs: []
  type: TYPE_NORMAL
- en: In the unsuccessful scenario, the call to `socket.gethostbyname` takes too long,
    and the process is killed after its `timeout` has expired. Because the call failed,
    no IP has been inserted in the `queue`, and therefore it will be empty. In the `if`
    logic, we therefore set the IP to `None`, and return as before. The `resolve`
    function will find that the exit code is not `0` (as the process didn't terminate
    happily, but was killed instead), and will correctly return the hostname instead
    of the IP, which we couldn't get anyway.
  prefs: []
  type: TYPE_NORMAL
- en: In the source code of the book, in the `hostres` folder of this chapter, I have
    added some tests to make sure this behavior is actually correct. You can find
    instructions on how to run them in the `README.md` file in the folder. Make sure
    you check the test code too, it should be quite interesting.
  prefs: []
  type: TYPE_NORMAL
- en: Case examples
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this final part of the chapter, I am going to show you three case examples
    in which we'll see how to do the same thing by employing different approaches
    (single-thread, multithread, and multiprocess). Finally, I'll dedicate a few words
    to `asyncio`, a module that introduces yet another way of doing asynchronous programming
    in Python.
  prefs: []
  type: TYPE_NORMAL
- en: Example one – concurrent mergesort
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first example will revolve around the mergesort algorithm. This sorting
    algorithm is based on the *divide et impera* (divide and conquer) design paradigm.
    The way it works is very simple. You have a list of numbers you want to sort.
    The first step is to divide the list into two parts, sort them, and merge the
    results back into one sorted list. Let me give you a simple example with six numbers.
    Imagine we have a list, `v=[8, 5, 3, 9, 0, 2]`. The first step would be to divide
    the list, `v`, into two sublists of three numbers: `v1=[8, 5, 3]` and `v2=[9,
    0, 2]`. Then we sort `v1` and `v2` by recursively calling mergesort on them. The
    result would be `v1=[3, 5, 8]` and `v2=[0, 2, 9]`. In order to combine `v1` and `v2`
    back into a sorted `v`, we simply consider the first item in both lists, and pick
    the minimum of those. The first iteration would compare `3` and `0`. We pick `0`,
    leaving `v2=[2, 9]`. Then we rinse and repeat: we compare `3` and `2`, we pick
    `2`, so now `v2=[9]`. Then we compare `3` and `9`. This time we pick `3`, leaving `v1=[5,
    8]`, and so on and so forth. Next we would pick `5` (`5` versus `9`), then `8`
    (`8` versus `9`), and finally `9`. This would give us a new, sorted version of `v`: `v=[0,
    2, 3, 5, 8, 9]`.'
  prefs: []
  type: TYPE_NORMAL
- en: The reason why I chose this algorithm as an example is twofold. First, it is
    easy to parallelize. You split the list in two, have two processes work on them,
    and then collect the results. Second, it is possible to amend the algorithm so
    that it splits the initial list into any *N ≥ 2*, and assigns those parts to *N*
    processes. Recombination is as simple as dealing with just two parts. This characteristic
    makes it a good candidate for a concurrent implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Single-thread mergesort
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see how all this translates into code, starting by learning how to code
    our own homemade `mergesort`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Let's start from the `sort` function. First we encounter the base of the recursion,
    which says that if the list has `0` or `1` elements, we don't need to sort it,
    we can simply return it as it is. If that is not the case, then we calculate the
    midpoint (`mid`), and recursively call sort on `v[:mid]` and `v[mid:]`. I hope
    you are by now very familiar with the slicing syntax, but just in case you need
    a refresher, the first one is all elements in `v` up to the `mid` index (excluded),
    and the second one is all elements from `mid` to the end. The results of sorting
    them are assigned respectively to `v1` and `v2`. Finally, we call `merge`, passing `v1`
    and `v2`.
  prefs: []
  type: TYPE_NORMAL
- en: The logic of `merge` uses two pointers, `h` and `k`, to keep track of which
    elements in `v1` and `v2` we have already compared. If we find that the minimum
    is in `v1`, we append it to `v`, and increase `h`. On the other hand, if the minimum
    is in `v2`, we append it to `v` but increase `k` this time. The procedure is running
    in a `while` loop whose condition, combined with the inner `if`, makes sure we
    don't get errors due to indexes out of bounds. It's a pretty standard algorithm
    that you can find in many different variations on the web.
  prefs: []
  type: TYPE_NORMAL
- en: In order to make sure this code is solid, I have written a test suite that resides
    in the `ch10/ms` folder. I encourage you to check it out.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have the building blocks, let's see how we modify this to make it
    so that it works with an arbitrary number of parts.
  prefs: []
  type: TYPE_NORMAL
- en: Single-thread multipart mergesort
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The code for the multipart version of the algorithm is quite simple. We can
    reuse the `merge` function, but we''ll have to rewrite the `sort` one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: We saw `reduce` in [Chapter 4](part0108.html#36VSO0-2ddb708647cc4530a187c2c6c0e9acfe),
    *Functions, the Building Blocks of Code*, when we coded our own factorial function.
    The way it works within `multi_merge` is to merge the first two lists in `v`.
    Then the result is merged with the third one, after which the result is merged
    with the fourth one, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Take a look at the new version of `sort`. It takes the `v` list, and the number
    of parts we want to split it into. The first thing we do is check that we passed
    a correct number for `parts`, which needs to be at least two. Then, like before,
    we have the base of the recursion. And finally we get into the main logic of the
    function, which is simply a multipart version of the one we saw in the previous
    example. We calculate the length of each `chunk` using the `max` function, just
    in case there are fewer elements in the list than parts. And then we write a generator
    expression that calls `sort` recursively on each `chunk`. Finally, we merge all
    the results by calling `multi_merge`.
  prefs: []
  type: TYPE_NORMAL
- en: I am aware that in explaining this code, I haven't been as exhaustive as I usually
    am, and I'm afraid it is on purpose. The example that comes after the mergesort
    will be much more complex, so I would like to encourage you to really try to understand
    the previous two snippets as thoroughly as you can.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s take this example to the next step: multithreading.'
  prefs: []
  type: TYPE_NORMAL
- en: Multithreaded mergesort
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this example, we amend the `sort` function once again, so that, after the
    initial division into chunks, it spawns a thread per part. Each thread uses the
    single-threaded version of the algorithm to sort its part, and then at the end
    we use the multi-merge technique to calculate the final result. Translating into
    Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: We import all the required tools, including executors, the `ceiling` function,
    and `sort` and `merge` from the single-threaded version of the algorithm. Notice
    how I changed the name of the single-threaded `sort` into `_sort` upon importing
    it.
  prefs: []
  type: TYPE_NORMAL
- en: In this version of `sort`, we check whether `v` is empty first, and if not we
    proceed. We calculate the dimension of each `chunk` using the `ceil` function.
    It's basically doing what we were doing with `max` in the previous snippet, but
    I wanted to show you another way to solve the issue.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we have the dimension, we calculate the `chunks` and prepare a nice generator
    expression to serve them to the executor. The rest is straightforward: we define
    a list of future objects, each of which is the result of calling `submit` on the
    executor. Each future object runs the single-threaded `_sort` algorithm on the
    `chunk` it has been assigned to.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally as they are returned by the `as_completed` function, the results are
    merged using the same technique we saw in the earlier multipart example.
  prefs: []
  type: TYPE_NORMAL
- en: Multiprocess mergesort
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To perform the final step, we need to amend only two lines in the previous
    code. If you have paid attention in the introductory examples, you will know which
    of the two lines I am referring to. In order to save some space, I''ll just give
    you the diff of the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: That's it! Basically all you have to do is use `ProcessPoolExecutor` instead
    of `ThreadPoolExecutor`, and instead of spawning threads, you are spawning processes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Do you recall when I was saying that processes can actually run on different
    cores, while threads run within the same process so they are not actually running
    in parallel? This is a good example to show you a consequence of choosing one
    approach or the other. Because the code is CPU-intensive, and there is no IO going
    on, splitting the list and having threads working the chunks doesn''t add any
    advantage. On the other hand, using processes does. I have run some performance
    tests (run the `ch10/ms/performance.py` module by yourself and you will see how
    your machine performs) and the results prove my expectations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The two tests are run on two lists of 100,000 and 500,000 items, respectively.
    And I am using four workers for the multithreaded and multiprocessing versions.
    Using different sizes is quite useful when looking for patterns. As you can see,
    the time elapsed is basically the same for the first two versions (single-threaded,
    and multithreaded), but they are reduced by about 50% for the multiprocessing
    version. It's slightly more than 50% because having to spawn processes, and handle
    them, comes at a price. But still, you can definitely appreciate that I have a
    processor with two cores on my machine.
  prefs: []
  type: TYPE_NORMAL
- en: This also tells you that even though I used four workers in the multiprocessing
    version, I can still only parallelize proportionately to the amount of cores my
    processor has. Therefore, two or more workers makes very little difference.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you are all warmed up, let's move on to the next example.
  prefs: []
  type: TYPE_NORMAL
- en: Example two – batch sudoku-solver
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this example, we are going to explore a sudoku-solver. We are not going to
    go into much detail with it, as the point is not that of understanding how to
    solve sudoku, but rather to show you how to use multi-processing to solve a batch
    of sudoku puzzles.
  prefs: []
  type: TYPE_NORMAL
- en: What is interesting in this example, is that instead of making the comparison
    between single and multithreaded versions again, we're going to skip that and
    compare the single-threaded version with two different multiprocess versions.
    One will assign one puzzle per worker, so if we solve 1,000 puzzles, we'll use
    1,000 workers (well, we will use a pool of *N* workers, each of which is constantly
    recycled). The other version will instead divide the initial batch of puzzles
    by the pool size, and batch-solve each chunk within one process. This means, assuming
    a pool size of four, dividing those 1,000 puzzles into chunks of 250 puzzles each,
    and giving each chunk to one worker, for a total of four of them.
  prefs: []
  type: TYPE_NORMAL
- en: The code I will present to you for the sudoku-solver (without the multiprocessing
    part), comes from a solution designed by Peter Norvig, which has been distributed
    under the MIT license. His solution is so efficient that, after trying to re-implement
    my own for a few days, and getting to the same result, I simply gave up and decided
    to go with his design. I did do a lot of refactoring though, because I wasn't
    happy with his choice of function and variable names, so I made those more *book
    friendly*, so to speak. You can find the original code, a link to the original
    page from which I got it, and the original MIT license, in the `ch10/sudoku/norvig`
    folder. If you follow the link, you'll find a very thorough explanation of the
    sudoku-solver by Norvig himself.
  prefs: []
  type: TYPE_NORMAL
- en: What is Sudoku?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First things first. What is a sudoku puzzle? Sudoku is a number-placement puzzle
    based on logic that originated in Japan. The objective is to fill a *9x9* grid
    with digits so that each row, column, and box (*3x3* subgrids that compose the
    grid) contains all of the digits from *1* to *9*. You start from a partially populated
    grid, and add number after number using logic considerations.
  prefs: []
  type: TYPE_NORMAL
- en: Sudoku can be interpreted, from a computer science perspective, as a problem
    that fits in the *exact cover* category. Donald Knuth, the author of *The Art
    of Computer Programming* (and many other wonderful books), has devised an algorithm,
    called **Algorithm X**, to solve problems in this category. A beautiful and efficient
    implementation of Algorithm X, called **Dancing Links**, which harnesses the power
    of circular doubly-linked lists, can be used to solve sudoku. The beauty of this
    approach is that all it requires is a mapping between the structure of the sudoku,
    and the Dancing Links algorithm, and without having to do any of the logic deductions
    normally needed to solve the puzzle, it gets to the solution at the speed of light.
  prefs: []
  type: TYPE_NORMAL
- en: Many years ago, when my free time was a number greater than zero, I wrote a
    Dancing Links sudoku-solver in C#, which I still have archived somewhere, which
    was great fun to design and code. I definitely encourage you to check out the
    literature and code your own solver, it's a great exercise, if you can spare the
    time.
  prefs: []
  type: TYPE_NORMAL
- en: In this example's solution though, we're going to use a **search** algorithm
    used in conjunction with a process that, in artificial intelligence, is known
    as **constraint propagation**. The two are quite commonly used together to make
    a problem simpler to solve. We'll see that in our example, they are enough for
    us to be able to solve a difficult sudoku in a matter of milliseconds.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a sudoku-solver in Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s now explore my refactored implementation of the solver. I''m going to
    present the code to you in steps, as it is quite involved (also, I won''t repeat
    the source name at the top of each snippet, until I move to another module):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'We start with some imports, and then we define a couple of useful functions: `cross_product`
    and `chunk`. They do exactly what the names hint at. The first one returns the
    cross-product between two iterables, while the second one returns a list of chunks
    from `iterable`, each of which has `n` elements, and the last of which might be
    padded with a given `fillvalue`, should the length of `iterable` not be a multiple
    of `n`. Then we proceed to define a few structures, which will be used by the
    solver:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Without going too much into detail, let's hover over these objects. `squares`
    is a list of all squares in the grid. Squares are represented by a string such
    as *A3* or *C7*. Rows are numbered with letters, and columns with numbers, so *A3*
    will indicate the square in the first row, and third column.
  prefs: []
  type: TYPE_NORMAL
- en: '`all_units` is a list of all possible rows, columns, and blocks. Each of those
    elements is represented as a list of the squares that belong to the row/column/block. `units`
    is a more complex structure. It is a dictionary with 81 keys. Each key represents
    a square, and the corresponding value is a list with three elements in it: a row,
    a column, and a block. Of course, those are the row, column, and block that the
    square belongs to.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, `peers` is a dictionary very similar to `units`, but the value of each
    key (which still represents a square), is a set containing all peers for that
    square. Peers are defined as all the squares belonging to the row, column, and
    block the square in the key belongs to. These structures will be used in the calculation
    of the solution, when attempting to solve a puzzle.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we take a look at the function that parses the input lines, let me give
    you an example of what an input puzzle looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The first nine characters represent the first row, then another nine for the
    second row, and so on. Empty squares are represented by dots:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: This simple `parse_puzzle` function is used to parse an input puzzle. We do
    a little bit of sanity checking at the beginning, asserting that the input puzzle
    has to shrink into a set that is a subset of the set of all numbers plus a dot.
    Then we make sure we have `81` input characters, and finally we define `grid`,
    which initially is simply a dictionary with `81` keys, each of which is a square,
    all with the same value, which is a string of all possible digits. This is because
    a square in a completely empty grid has the potential to become any number from
    1 to 9.
  prefs: []
  type: TYPE_NORMAL
- en: The `for` loop is definitely the most interesting part. We parse each of the
    81 characters in the input puzzle, coupling them with the corresponding square
    in the grid, and we try to *"place"* them. I put that in double quotes because,
    as we'll see in a moment, the `place` function does much more than simply setting
    a given number in a given square. If we find that we cannot place a digit from
    the input puzzle, it means the input is invalid, and we return `False`. Otherwise,
    we're good to go and we return the `grid`.
  prefs: []
  type: TYPE_NORMAL
- en: '`parse_puzzle` is used in the `solve` function, which simply parses the input
    puzzle, and unleashes `search` on it. What follows is therefore the heart of the
    algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: This simple function first checks whether the grid is actually non-empty. Then
    it tries to see whether the grid is solved. A solved grid will have one value
    per square. If that is not the case, it loops through each square and finds the
    square with the minimum amount of candidates. If a square has a string value of
    only one digit, it means a number has been placed in that square. But if the value
    is more than one digit, then those are possible candidates, so we need to find
    the square with the minimum amount of candidates, and try them. Trying a square
    with `23` candidates is much better than trying one with `23589`. In the first
    case, we have a 50% chance of getting the right value, while in the second one,
    we only have 20%. Choosing the square with the minimum amount of candidates therefore
    maximizes the chances for us to place good numbers in the grid.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the candidates have been found, we try them in order and if any of them
    results in being successful, we have solved the grid and we return. You might
    have noticed the use of the `place` function in the search too. So let''s explore
    its code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'This function takes a work-in-progress grid, and tries to place a given digit
    in a given square. As I mentioned before, *"placing"* is not that straightforward.
    In fact, when we place a number, we have to propagate the consequences of that
    action throughout the grid. We do that by calling the `eliminate` function, which
    applies two strategies of the sudoku game:'
  prefs: []
  type: TYPE_NORMAL
- en: If a square has only one possible value, eliminate that value from the square's
    peers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If a unit has only one place for a value, place the value there
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let me briefly offer an example of both points. For the first one, if you place,
    say, number 7 in a square, then you can eliminate 7 from the list of candidates
    for all the squares that belong to the row, column, and block that square belongs
    to.
  prefs: []
  type: TYPE_NORMAL
- en: For the second point, say you're examining the fourth row and, of all the squares
    that belong to it, only one of them has number 7 in its candidates. This means
    that number 7 can only go in that precise square, so you should go ahead and place
    it there.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following function, `eliminate`, applies these two rules. Its code is quite
    involved, so instead of going line by line and offering an excruciating explanation,
    I have added some comments, and will leave you with the task of understanding
    it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: The rest of the functions in the module aren't important for the rest of this
    example, so I will skip them. You can run this module by itself; it will first
    perform a series of checks on its data structures, and then it will solve all
    the sudoku puzzles I have placed in the `sudoku/puzzles` folder. But that is not
    what we're interested in, right? We want to see how to solve sudoku using multiprocessing
    techniques, so let's get to it.
  prefs: []
  type: TYPE_NORMAL
- en: Solving sudoku with multiprocessing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this module, we''re going to implement three functions. The first one simply
    solves a batch of sudoku puzzles, with no multiprocessing involved. We will use
    the results for benchmarking. The second and the third ones will use multiprocessing,
    with and without batch-solving, so we can appreciate the differences. Let''s start:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'After a long list of imports, we define a context manager that we''re going
    to use as a timer device. It takes a reference to the current time (`t`), and
    then it yields. After having yielded, that''s when the body of the managed context
    is executed. Finally, on exiting the managed context, we calculate `tot`, which
    is the total amount of time elapsed, and print it. It''s a simple and elegant
    context manager written with the decoration technique, and it''s super fun. Let''s
    now see the three functions I mentioned earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'This one is a single-threaded simple batch solver, which will give us a time
    to compare against. It simply returns a list of all solved grids. Boring. Now,
    check out the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'This one is much better. It uses `ProcessPoolExecutor` to use a pool of `workers`,
    each of which is used to solve roughly one-fourth of the puzzles. This is because
    we are spawning one future object per puzzle. The logic is extremely similar to
    any multiprocessing example we have already seen in the chapter. Let''s see the
    third function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: This last function is slightly different. Instead of spawning one `future` object
    per puzzle, it splits the total list of puzzles into `workers` chunks, and then
    creates one `future` object per chunk. This means that if `workers` is eight,
    we're going to spawn eight `future` objects. Notice that instead of passing `solve`
    to `executor.submit`, we're passing `batch_solve`, which does the trick. The reason
    why I coded the last two functions so differently is because I was curious to
    see the severity of the impact of the overhead we incur into when we recycle processes
    from a pool a non-negligible amount of times.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have the functions defined, let''s use them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: We use a set of 234 very hard sudoku puzzles for this benchmarking session.
    As you can see, we simply run the three functions, `batch_solve`, `parallel_single_solver`,
    and `parallel_batch_solver`, all within a timed context. We collect the results,
    and, just to make sure, we verify that all the runs have produced the same results.
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, in the second and third runs, we have used multiprocessing, so we
    cannot guarantee that the order in the results will be the same as that of the
    single-threaded `batch_solve`. This minor issue is brilliantly solved with the
    aid of `assertCountEqual`, one of the worst-named methods in the Python standard
    library. We find it in the `TestCase` class, which we can instantiate just to
    take a reference to the method we need. We''re not actually running unit tests,
    but this is a cool trick, and I wanted to show it to you. Let''s see the output
    of running this module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Wow. That is quite interesting. First of all, you can once again see that my
    machine has a two-core processor, as the time elapsed for the multiprocessing
    runs is about half the time taken by the single-threaded solver. However, what
    is actually much more interesting is the fact that there is basically no difference
    in the time taken by the two multiprocessing functions. Multiple runs sometimes
    end in favor of one approach, and sometimes in favor of the other. Understanding
    why requires a deep understanding of all the components that are taking part in
    the game, not just the processes, and therefore is not something we can discuss
    here. It is fairly safe to say though, that the two approaches are comparable
    in terms of performance.
  prefs: []
  type: TYPE_NORMAL
- en: In the source code for the book, you can find tests in the `sudoku` folder,
    with instructions on how to run them. Take the time to check them out!
  prefs: []
  type: TYPE_NORMAL
- en: And now, let's get to the final example.
  prefs: []
  type: TYPE_NORMAL
- en: Example three – downloading random pictures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This example has been fun to code. We are going to download random pictures
    from a website. I''ll show you three versions: a serial one, a multiprocessing
    one, and finally a solution coded using `asyncio`. In these examples, we are going
    to use a website called [http://lorempixel.com](http://lorempixel.com/), which
    provides you with an API that you can call to get random images. If you find that
    the website is down or slow, you can use an excellent alternative to it: [https://lorempizza.com/](https://lorempizza.com/).'
  prefs: []
  type: TYPE_NORMAL
- en: It may be something of a *cliché* for a book written by an Italian, but the
    pictures are gorgeous. You can search for another alternative on the web, if you
    want to have some fun. Whatever website you choose, please be sensible and try
    not to hammer it by making a million requests to it. The multiprocessing and `asyncio`
    versions of this code can be quite aggressive!
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start by exploring the single-threaded version of the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'This code should be straightforward to you by now. We define a `download` function,
    which makes a request to the given `URL`, saves the result by calling `save_image`,
    and feeds it the body of the response from the website. Saving the image is very
    simple: we create a random filename with `token_hex`, just because it''s fun,
    then we calculate the full path of the file, create it in binary mode, and write
    into it the content of the response. We return the `filename` to be able to print
    it on screen. Finally `batch_download` simply runs the `n` requests we want to
    run and returns the filenames as a result.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can leapfrog the `if __name__ ...` line for now, it will be explained in
    [Chapter 12](part0305.html#92RRI0-2ddb708647cc4530a187c2c6c0e9acfe),* GUIs and
    Scripts* and it''s not important here. All we do is call `batch_download` with
    the URL and we tell it to download `10` images. If you have an editor, open the `pics`
    folder, and you can see it getting populated in a few seconds (also notice: the
    script assumes the `pics` folder exists).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s spice things up a bit. Let''s introduce multiprocessing (the code is
    vastly similar, so I will not repeat it):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: The technique should be familiar to you by now. We simply submit jobs to the
    executor, and collect the results as they become available. Because this is IO
    bound code, the processes work quite fast and there is heavy context-switching
    while the processes are waiting for the API response. If you have a view over
    the `pics` folder, you will notice that it's not getting populated in a linear
    fashion any more, but rather, in batches.
  prefs: []
  type: TYPE_NORMAL
- en: Let's now look at the `asyncio` version of this example.
  prefs: []
  type: TYPE_NORMAL
- en: Downloading random pictures with asyncio
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The code is probably the most challenging of the whole chapter, so don't feel
    bad if it is too much for you at this moment in time. I have added this example
    just as a mouthwatering device, to encourage you to dig deeper into the heart
    of Python asynchronous programming. Another thing worth knowing is that there
    are probably several other ways to write this same logic, so please bear in mind
    that this is just one of the possible examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `asyncio` module provides infrastructure for writing single-threaded, concurrent
    code using coroutines, multiplexing IO access over sockets and other resources,
    running network clients and servers, and other related primitives. It was added
    to Python in version 3.4, and some claim it will become the *de facto* standard
    for writing Python code in the future. I don''t know whether that''s true, but
    I know it is definitely worth seeing an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'First of all, we cannot use `requests` any more, as it is not suitable for
    `asyncio`. We have to use `aiohttp`, so please make sure you have installed it
    (it''s in the requirements for the book):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: The previous code does not look too friendly, but it's not so bad, once you
    know the concepts behind it. We define the async coroutine `download_image`, which
    takes a URL as parameter.
  prefs: []
  type: TYPE_NORMAL
- en: In case you don't know, a coroutine is a computer program component that generalizes
    subroutines for non-preemptive multitasking, by allowing multiple entry points
    for suspending and resuming execution at certain locations. A subroutine is a
    sequence of program instructions that performs a specific task, packaged as a
    unit.
  prefs: []
  type: TYPE_NORMAL
- en: Inside `download_image`, we create a session object using the `ClientSession`
    context manager, and then we get the response by using another context manager,
    this time from `session.get`. The fact that these managers are defined as asynchronous
    simply means that they are able to suspend execution in their enter and exit methods.
    We return the content of the response by using the `await` keyword, which allows
    suspension. Notice that creating a session for each request is not optimal, but
    I felt that for the purpose of this example I would keep the code as straightforward
    as possible, so I leave its optimization to you, as an exercise.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s proceed with the next snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Another coroutine, `download`, gets a `URL` and a `semaphore`. All it does is
    fetch the content of the image, by calling `download_image`, saving it, and returning
    the `filename`. The interesting bit here is the use of that `semaphore`. We use
    it as an asynchronous context manager, so that we can suspend this coroutine as
    well, and allow a switch to something else, but more than *how*, it is important
    to understand *why* we want to use a `semaphore`. The reason is simple, this `semaphore`
    is kind of the equivalent of a pool of threads. We use it to allow at most *N*
    coroutines to be active at the same time. We instantiate it in the next function,
    and we pass 10 as the initial value. Every time a coroutine acquires the `semaphore`,
    its internal counter is decreased by `1`, therefore when 10 coroutines have acquired
    it, the next one will sit and wait, until the semaphore is released by a coroutine
    that has completed. This is a nice way to try to limit how aggressively we are
    fetching images from the website API.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `save_image` function is not a coroutine, and its logic has already been
    discussed in the previous examples. Let''s now get to the part of the code where
    execution takes place:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'We define the `batch_download` function, which takes a number, `images`, and
    the URL of where to fetch them. The first thing it does is create an event loop,
    which is necessary to run any asynchronous code. The event loop is the central
    execution device provided by `asyncio`. It provides multiple facilities, including:'
  prefs: []
  type: TYPE_NORMAL
- en: Registering, executing, and cancelling delayed calls (timeouts)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating client and server transports for various kinds of communication
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Launching subprocesses and the associated transports for communication with
    an external program
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Delegating costly function calls to a pool of threads
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After the event loop is created, we instantiate the semaphore, and then we proceed
    to create a list of futures, `cors`. By calling `loop.run_until_complete`, we
    make sure the event loop will run until the whole task has been completed. We
    feed it the result of a call to `asyncio.wait`, which waits for the futures to
    complete.
  prefs: []
  type: TYPE_NORMAL
- en: When done, we close the event loop, and return a list of the results yielded
    by each future object (the filenames of the saved images). Notice how we capture
    the results of the call to `loop.run_until_complete`. We don't really care for
    the errors, so we assign `_` to the second item in the tuple. This is a common
    Python idiom used when we want to signal that we're not interested in that object.
  prefs: []
  type: TYPE_NORMAL
- en: At the end of the module, we call `batch_download` and we get 20 images saved.
    They come in batches, and the whole process is limited by a semaphore with only
    10 available spots.
  prefs: []
  type: TYPE_NORMAL
- en: And that's it! To learn more about `asyncio`, please refer to the documentation
    page ([https://docs.python.org/3.7/library/asyncio.html](https://docs.python.org/3.7/library/asyncio.html))
    for the `asyncio` module on the standard library. This example was fun to code,
    and hopefully it will motivate you to study hard and understand the intricacies
    of this wonderful side of Python.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we learned about concurrency and parallelism. We saw how threads
    and processes help in achieving one and the other. We explored the nature of threads
    and the issues that they expose us to: race conditions and deadlocks.'
  prefs: []
  type: TYPE_NORMAL
- en: We learned how to solve those issues by using locks and careful resource management.
    We also learned how to make threads communicate and share data, and we talked
    about the scheduler, which is that part of the operating system that decides which
    thread will run at any given time. We then moved to processes, and explored a
    bunch of their properties and characteristics.
  prefs: []
  type: TYPE_NORMAL
- en: Following the initial theoretical part, we learned how to implement threads
    and processes in Python. We dealt with multiple threads and processes, fixed race
    conditions, and learned workarounds to stop threads without leaving any resource
    open by mistake. We also explored IPC, and used queues to exchange messages between
    processes and threads. We also played with events and barriers, which are some
    of the tools provided by the standard library to control the flow of execution
    in a non-deterministic environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'After all these introductory examples, we deep dived into three case examples,
    which showed how to solve the same problem using different approaches: single-thread,
    multithread, multiprocess, and `asyncio`.'
  prefs: []
  type: TYPE_NORMAL
- en: We learned about mergesort and how, in general, *divide and conquer* algorithms
    are easy to parallelize.
  prefs: []
  type: TYPE_NORMAL
- en: We learned about sudoku, and explored a nice solution that uses a little bit
    of artificial intelligence to run an efficient algorithm, which we then ran in
    different serial and parallel modes.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we saw how to download random pictures from a website, using serial,
    multiprocess, and `asyncio` code. The latter was by far the hardest piece of code
    in the whole book, and its presence in the chapter serves as a reminder, or some
    sort of milestone that will encourage the reader to learn Python well, and deeply.
  prefs: []
  type: TYPE_NORMAL
- en: Now we'll move on to much simpler, and mostly project-oriented chapters, where
    we get a taste of different real-world applications in different contexts.
  prefs: []
  type: TYPE_NORMAL
