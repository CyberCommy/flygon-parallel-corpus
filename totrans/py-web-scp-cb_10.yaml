- en: Creating Scraper Microservices with Docker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover:'
  prefs: []
  type: TYPE_NORMAL
- en: Installing Docker
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing a RabbitMQ container from Docker Hub
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running a Docker container (RabbitMQ)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stopping and removing a container and image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating an API container
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a generic microservice with Nameko
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a scraping microservice
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a scraper container
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a backend (ElasticCache) container
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Composing and running the scraper containers with Docker Compose
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will learn to containerize our scraper, getting it ready
    for the real world by starting to package it for real, modern, cloud-enabled operations.
    This will involve packaging the different elements of the scraper (API, scraper,
    backend storage) as Docker containers that can be run locally or in the cloud.
    We will also examine implementing the scraper as a microservice that can be independently
    scaled.
  prefs: []
  type: TYPE_NORMAL
- en: Much of the focus will be upon using Docker to create our containerized scraper.
    Docker provides us a convenient and easy means of packaging the various components
    of the scraper as a service (the API, the scraper itself, and other backends such
    as Elasticsearch and RabbitMQ). By containerizing these components using Docker,
    we can easily run the containers locally, orchestrate the different containers
    making up the services, and also conveniently publish to Docker Hub. We can then
    deploy them easily to cloud providers to create our scraper in the cloud.
  prefs: []
  type: TYPE_NORMAL
- en: One of the great things about Docker (and containers in general) is that we
    can both easily install pre-packaged containers without all the fuss of having
    to get an installer for an application and deal with all of the configuration
    hassle. We can then also package our own software that we wrote into a container,
    and run that container without having to deal with all those details. Additionally,
    we can also publish to a private or public repository to share our software.
  prefs: []
  type: TYPE_NORMAL
- en: What is really great about Docker is that the containers are, for the most part,
    platform-independent. Any Linux-based container can be run on any operating system,
    including Windows (which uses VirtualBox underneath to virtualize Linux and is
    mostly transparent to the Windows user). So one benefit is that any Linux-based
    Docker container can be run on any Docker supported operating system. No more
    need to create multiple OS versions of your application!
  prefs: []
  type: TYPE_NORMAL
- en: So let's go and learn a little Docker and put our scraper components into containers.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Docker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we look at how to install Docker and verify that it is running.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Docker is supported on Linux, macOS, and Windows, so it has the major platforms
    covered. The installation process for Docker is different depending on the operating
    system that you are using, and even differs among the different Linux distributions.
  prefs: []
  type: TYPE_NORMAL
- en: The Docker website has good documentation on the installation processes, so
    this recipe will quickly walk through the important points of the installation
    on macOS. Once the install is complete, the user experience for Docker, at least
    from the CLI, is identical.
  prefs: []
  type: TYPE_NORMAL
- en: For reference, the main page for installation instructions for Docker is found
    at: [https://docs.docker.com/engine/installation/](https://docs.docker.com/engine/installation/)
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will be proceeding with the recipe as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: We will be using a variant of Docker known as Docker Community Edition, and
    walk through the installation on macOS. On the download page for macOS you will
    see the following section. Click on the download for the Stable channel, unless
    you are feeling brave and want to use the Edge channel.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/3fe42443-f9f8-4dc5-8679-221293a203fa.png)The docker download page'
  prefs: []
  type: TYPE_NORMAL
- en: 'This will download a `Docker.dmg` file. Open the DMG and you will be presented
    with the following window:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/b2a7044a-5f06-4874-96c6-112496770357.png)The Docker for Mac installer
    window'
  prefs: []
  type: TYPE_NORMAL
- en: 'Drag *Moby* the whale into your applications folder. Then open `Docker.app`.
    You will be asked to authenticate the installation, so enter your password and
    the installation will complete. When that is done, you will see Moby in your status
    bar:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/db37ccda-6c94-44dd-b327-f943fe3afbdb.png)The Moby toolbar icon'
  prefs: []
  type: TYPE_NORMAL
- en: There are number of configuration settings, statuses, and pieces of information
    available by clicking on Moby. We will mostly use the command-line tools. To verify
    things are working from the command line, open a terminal and enter the command,
    docker info. Docker will give you some information on its configuration and status.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Installing a RabbitMQ container from Docker Hub
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Pre-built containers can be obtained from a number of container repositories.
    Docker is preconfigured with connectivity to Docker Hub, where many software vendors,
    and also enthusiasts, publish containers with one or more configurations.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will install RabbitMQ, which will be used by another tool
    we use in another recipe, Nameko, to function as the messaging bus for our scraping
    microservice.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Normally, the installation of RabbitMQ is a fairly simple process, but it does
    require several installers: one for Erlang, and then one for RabbitMQ itself.
    If management tools, such as the web-based administrative GUI are desired, that
    is yet one more step (albeit a fairly small one). By using Docker, we can simply
    get the container with all of this preconfigured. Let''s go do that.'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We proceed with the recipe as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Containers can be obtained using the `docker pull` command.  This command will
    check and see if a container is installed locally, and if not, go and fetch it
    for us. Try the command from the command line, including the `--help flag. `You
    will get the following, informing you that you need at least one more parameter:
    the name and possibly tag for the container:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: We are going to pull the `rabbitmq:3-management` container. The portion before
    the colon is the container name, and the second part is a tag. Tags often represent
    a version of the container, or a specific configuration. In this case, we will
    want to get the RabbitMQ container with tag 3-management. This tag means we want
    the container version with version 3 of RabbitMQ and with the management tools
    installed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Before we do this, you might be thinking where this comes from. It comes from
    Docker Hub (`hub.docker.com`), from the RabbitMQ repository. The page for this
    repository is at [https://hub.docker.com/_/rabbitmq/](https://hub.docker.com/_/rabbitmq/),
    and will look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/2210e62e-0f9d-4a21-b71c-41c37d0dd8c6.png)Page for RabbitMQ repositoryNote
    the section showing tags, and that it has the 3-management tag. If you scroll
    down, you will also see a lot more information about the container and tags, and
    what they comprise.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s pull this container. Issue the following command from the terminal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Docker will go out to Docker Hub and start the download. You''ll see this in
    action with output similar to the following, which will likely run for a few minutes
    depending on your download speed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Congratulations! If this is your first time using Docker, you have downloaded
    your first container image. You can verify it is downloaded and installed using
    the docker images command.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Running a Docker container (RabbitMQ)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe we learn how to run a docker image, thereby making a container.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will start the RabbitMQ container image that we downloaded in the previous
    recipe. This process is representative of how many containers are run, so it makes
    a good example.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We proceed with the recipe as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: What we have downloaded so far is an image that can be run to create an actual
    container. A container is an actual instantiation of an image with specific parameters
    needed to configure the software in the container. We run the container by running
    an image using docker run and passing the image name/tag, and any other parameters
    required to run the image (these are specific to the image and normally can be
    found on the Docker Hub page for the image).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The specific command we need to run RabbitMQ using this image is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '`docker run` tells Docker to run an image in a container. The image we want
    to run is at the end of the statement: `rabbitmq:3-management`. The `-d` option
    tells Docker to run the container detached, meaning the output of the container
    is not routed to the terminal. This allows us to retain control of the terminal.
    The `-p` option maps a host port to a container port. RabbitMQ uses port 5672
    for actual commands, and port 15672 for the web UI. This maps an identical port
    on your actual operating system to the ports used by the software running in the
    container.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The big hexidecimal value output is an identifier of the container. The first
    portion, 094a13838376, is the container ID created by Docker (this will be different
    for every container that is started).
  prefs: []
  type: TYPE_NORMAL
- en: 'We can check which containers are running using docker ps, which gives us the
    process status of each container:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We can see the container ID and other information such as which image it is
    based on, how long it has been up, which ports the container exposes, the port
    mappings we defined, and a friendly name made up by Docker for us to refer to
    the container.
  prefs: []
  type: TYPE_NORMAL
- en: 'The real way to check whether this is running is to open the browser and navigate
    to `localhost:15672`, the RabbitMQ management UI URL:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/d8715fbc-5686-48c9-9f38-16e8c54aa325.png)The RabbitMQ Admin UI login
    page'
  prefs: []
  type: TYPE_NORMAL
- en: 'The default username and password for this container is guest:guest. Enter
    those values and you will see the management UI:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/20179fcc-8e5c-467f-8421-49f1dd7b3e9c.png)The management UI'
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is actually as far as we will progress with RabbitMQ. In a later recipe,
    we will use the Nameko Python microservice framework, which will transparently
    use RabbitMQ without our knowledge. We first needed to make sure it was installed
    and running.
  prefs: []
  type: TYPE_NORMAL
- en: Creating and running an Elasticsearch container
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While we are looking at pulling container images and starting containers, let's
    go and run an Elasticsearch container.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Like most things Docker, there are a lot of different versions of Elasticsearch
    containers available. We will use the official Elasticsearch image available in
    Elastic''s own Docker repository:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To install the image, enter the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Note that we are using another way of specifying the image to pull. Since this
    is on Elastic's Docker repository, we include the qualified name that includes
    the URL to the container image instead of just the image name. The :6.1.1 is the
    tag and specifies a specific version of that image.
  prefs: []
  type: TYPE_NORMAL
- en: 'You will see some output while this is processing, showing the download process.
    When it is complete, you will have a few lines letting you know it is done:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s check that the images are available for Docker:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can run Elasticsearch with the following Docker command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The environment variable, `ELASTIC_PASSWORD` passes in a password, and the two
    ports map the host ports to the Elasticsearch ports exposed in the container.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, check that the container is running in Docker:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'And finally, perform the following curl. If Elasticsearch is running you will
    get the `You Know, for Search` message:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Stopping/restarting a container and removing the image
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's look at how to stop and remove a container, and then also its image.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We proceed with the recipe as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First query Docker for running containers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Let's  stop the Elasticsearch container. To stop a container, we use `docker
    stop <container-id>`. Elasticsearch has a container ID of 308a02f0e1a5\. The following
    stops the container
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: To acknowledge the container is stopped, Docker will echo the container ID you
    told it to stop
  prefs: []
  type: TYPE_NORMAL
- en: Note that I didn't have to enter the full container ID and only entered 30\.
    You only have to enter the first digits of the container ID until what you have
    entered is unique among all containers. It's a nice shortcut!.
  prefs: []
  type: TYPE_NORMAL
- en: 'Checking the running container status, Docker only reports the other container:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The container is not running, but it is also not deleted. Let''s look at using
    `docker ps -a` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: This lists all containers currently on the system. I actually truncated my listing
    by quite a bit as I have a lot of these!
  prefs: []
  type: TYPE_NORMAL
- en: 'We can restart our Elasticsearch container using `docker restart`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: If you check `docker ps` you will see the container is operational again.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This is important as this container is storing the Elasticsearch data within
    the file system of the container. By stopping and restarting, this data is not
    lost. So, you can stop to reclaim the resources (CPU and memory) used by the container,
    and then restart without loss at a later time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Running or stopped, a container takes up disk space. A container can be removed
    to reclaim disk space. This can be done using `docker container rm <container-id>`,
    however a container can only be removed if it is not running. Let''s try and remove
    the running container:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'We got a warning about the container running. We can force it with a flag,
    but it''s best to stop it first. Stopping ensures the application inside shuts
    down cleanly:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Now if you go back to docker `ps -a`, the Elasticsearch container is no longer
    in the list and disk space for the container is reclaimed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note that we have now lost any data that was stored in that container! It's
    beyond the scope of this book, but most containers can be told to store data on
    the host's file system, and therefore we don't lost data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The disk space for the container has been removed, but the image for the container
    is still on the disk. That''s good if we want to make another container. But if
    you also want to free that space, you can use `docker images rm <image-id>`. 
    Going back to the Docker images result, we can see the image had an ID of `06f0d8328d66`.
    The following deletes that image and we get that space back (in this case 539MB):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: And now the image is gone and we have that space reclaimed also.
  prefs: []
  type: TYPE_NORMAL
- en: Note that if there are any containers that have been run off that image that
    still exist, then this will fail and those containers can be either running or
    stopped. Just doing a `docker ps -a` may not show the offending container, so
    you may have to use `docker ps -a` to find the stopped containers and delete them
    first.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At this point you know enough about Docker to become very dangerous! So let's
    move on to examining how we can create our own containers with our own applications
    installed. First, let's go and look at making the crawler into a microservice
    that can be run in a container.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a generic microservice with Nameko
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the next few recipes, we are going to create a scraper that can be run as
    a microservice within a Docker container. But before jumping right into the fire,
    let's first look at creating a basic microservice using a Python framework known
    as Nameko.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will use a Python framework known as Nameko (pronounced [nah-meh-koh] to
    implement microservices. As with Flask-RESTful, a microservice implemented with
    Nameko is simply a class. We will instruct Nameko how to run the class as a service,
    and Nameko will wire up a messaging bus implementation to allow clients to communicate
    with the actual microservice.
  prefs: []
  type: TYPE_NORMAL
- en: Nameko, by default, uses RabbitMQ as a messaging bus. RabbitMQ is a high-performance
    messaging bus that is great for performing the type of messaging service used
    between microservices. It's a similar model to what we saw earlier with SQS, but
    designed more for services located in the same data center instead of across that
    cloud. That's actually a great use for RabbitMQ, as we tend to cluster/scale microservices
    in the same environment these days, particularly within a containerized cluster
    such as Docker or Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, we will need to have a local instance of RabbitMQ running. Make sure
    that you have a RabbitMQ container running as show in the earlier recipe.
  prefs: []
  type: TYPE_NORMAL
- en: 'Also make sure you have Nameko installed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: How to do it
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We proceed with the recipe as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The sample microservice is implemented in `10/01/hello_microservice.py`.  This
    is a very simple service that can be passed a name for which the microservice
    replies `Hello, <name>!`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To run the microservice, we need to simply execute the following command from
    the terminal (while in the directory for the script):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Nameko opens the Python file matching the specified microservices name and
    starts up the microservice. When starting, we will see a few lines of output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: This states that Nameko has found our microservice, and that it has connected
    to an AMQP server (RabbitMQ) on port 5672 (RabbitMQ's default port). The microservice
    is now up and running and awaiting requests.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you go into the RabbitMQ API and go into the queues tab, you will see that
    Nameko has automatically created a queue for the microservice.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we have to do something to make a request of the microservice. We will
    look at two ways of doing this. First, Nameko comes with an interactive shell
    that lets us interactively make requests to Nameko microservices. You can start
    the shell with the following command in a separate terminal window to the one
    running the microservice:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'You will see an interactive Python session start, with output similar to the
    following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'In this shell, we can simply refer to Nameko as ''n''.  To talk to our service
    we issue the following statement:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'This tells Nameko that we want to make an rpc call to the `hello` method of
    `hello_microservice`. When pressing *Enter* you will get the following result
    back:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'If you check in the terminal window running the service you should see an additional
    line of output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'It is also possible to call the microservice from within Python code. There
    is an implementation of this available in `10/01/say_hi.py`. Executing this script
    with Python has the following output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: So let's go and see how these are implemented.
  prefs: []
  type: TYPE_NORMAL
- en: How it works
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s first look at the implementation of the microservice in `hello_microservice.py`. 
    There really isn''t a lot of code, so here it all is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: There are two thing to point out about this class. The first is the declaration
    of `name = "hello_microservice"`.  This is a declaration of the actual name of
    the microservice. This member variable is used instead of the class name.
  prefs: []
  type: TYPE_NORMAL
- en: The second is the use of the `@rpc` attribute on the `hello` method. This is
    a Nameko attribute that specifies that this method should be exposed as an `rpc`
    style method by the microservice. Hence, the caller is suspended until the reply
    is received from the microservice. There are other implementations, but for our
    purposes this is the only one we will use.
  prefs: []
  type: TYPE_NORMAL
- en: When run with the nameko run command, that module will interrogate the file
    for methods with Nameko attributes and wire them up to the underlying bus.
  prefs: []
  type: TYPE_NORMAL
- en: 'The implementation in `say_hi.py` constructs a dynamic proxy that can call
    the service. The code for that is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: The dynamic proxy is implemented by the `ClusterRpcProxy` class. When creating
    the class, we pass it a configuration object, which in this case specifies the
    address of the AMQP server that the service is located on, and we refer to this
    instance as the variable `rpc`. Nameko then dynamically identifies the next portion,
    `.hello_microservice`, as the name of the microservice (as specified earlier with
    the name field on the microservice class).
  prefs: []
  type: TYPE_NORMAL
- en: The next section, `.hello` then represents the method to call. Combined together,
    Nameko makes a call to the `hello` method of `hello_microservice`, passing it
    the specified string, and since this is an RPC proxy, waits until the reply is
    received.
  prefs: []
  type: TYPE_NORMAL
- en: Remote procedure calls, RPC for short, block until the result comes back from
    the other system. In contrast with a publish model, where the message is sent
    off and the sending app continues along.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There is quite a lot of good stuff in Nameko that we have not even seen. One
    very useful factor is that Nameko runs listeners for multiple instances of your
    microservice. The default at the time of writing is 10\. Under the covers, Nameko
    sends requests from the clients of the microservice to a RabbitMQ queue, of which
    there will be 10 simultaneous request processors listening to that queue.  If
    there are too many requests to be handled at once, RabbitMQ will hold the message
    until Nameko recycles an existing microservice instance to process the queued
    message. To increase the scalability of the microservice, we can simply increase
    the number of workers through the configuration of the microservice, or run a
    separate Nameko microservice container in another Docker container or on another
    computer system.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a scraping microservice
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now let's take our scraper and make it into a Nameko microservice. This scraper
    microservice will be able to be run independently of the implementation of the
    API. This will allow the scraper to be operated, maintained, and scaled independently
    of the API's implementation.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We proceed with the recipe as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for the microservice is straightforward. The code for it is in `10/02/call_scraper_microservice.py`
    and is shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'We have created a class to implement the microservice and given it a single
    method, `get_job_listing_info`.  This method simply wraps the implementation in
    the `sojobs.scraping` module, but gives it an `@rpc` attribute so that Nameko
    exposes that method on the microservice bus. This can be run by opening a terminal
    and running the service with Nameko:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can run the scraper with the code in the `10/02/call_scraper_microservice.py`
    script. The code in the files is the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'It''s basically the same as the code for the client in the previous recipe,
    but changing the microservice and method names, and of course passing the specific
    job listing ID.  When run, you will see the following output (truncated):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: And just like that, we have created a microservice to get job listings from
    StackOverflow!
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This microservice is only callable using the `ClusterRpcProxy` class and is
    not open to being called by anyone on the internet or even locally using REST.
    We'll solve this issue in an upcoming recipe, where we create a REST API in a
    container that will talk to this microservice, which will be running in another
    container.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a scraper container
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now we create a container for our scraper microservice. We will learn about
    Dockerfiles and how to instruct Docker on how to build a container. We will also
    examine giving our Docker container hostnames so that they can find each other
    through Docker's integrated DNS system. Last but not least, we will learn how
    to configure our Nameko microservice to talk to RabbitMQ in another container
    instead of just on localhost.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first thing we want to do is make sure that RabbitMQ is running in a container
    and assigned to a custom Docker network, where various containers connected to
    that network will talk to each other. Among many other features, it also provides
    software defined network (SDN) capabilities to provide various types of integration
    between containers, hosts, and other systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'Docker comes with several predefined networks built. You can see the networks
    currently installed by using the `docker network ls` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: To get our containers to communicate with each other, let's create a new bridge
    network named `scraper-net`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Now when we start a container, we attach it to `scraper-net` using the `--network`
    parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: This container is now connected to both the scraper-net network and to the host
    network. Because it is also connected to host, it is still possible to connect
    to it from the host system.
  prefs: []
  type: TYPE_NORMAL
- en: Note also that we used `--name rabbitmq` as an option. This gives this container
    the name, `rabbitmq` , but Docker will also resolve DNS queries from other containers
    attached to `scraper-net` so that they can find this container!
  prefs: []
  type: TYPE_NORMAL
- en: Now let's go and put the scraper in a container.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We proceed with the recipe as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The way that we create a container is by creating a `dockerfile` and then using
    that to tell Docker to create a container.  I''ve included a Dockerfile in the
    `10/03` folder. The contents are the following (we will examine what this means
    in the *How it works* section):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'To create an image/container from this Dockerfile, from a terminal, and within
    the `10/03` folder, run the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: This tells Docker that we want to *build* a container based upon the instructions
    in the given Dockerfile (specified with -f).  The image that is created is specified
    by
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`-t scraping-microservice`. The `../..` after build specifies the context of
    the build.  When building, we will copy files into the container. This context
    specifies the home  directory that copies are relative to.  When you run this
    command, you will see output similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'This will likely take a while as the build process needs to download all of
    the NLTK files into the container. To check that the image is created we can run
    the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that this container is 4.16GB in size. This image is based on the `Python:3`
    container, which can be seen to be `692MB` in size:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Most of the size of this container is because of the inclusion of the NTLK data
    files.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now run this image as a container using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: The scraper that we put together is now running in this container, and this
    output shows that it has connected to an AMQP server located on a system named
    `rabbitmq`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s test that this is working. In another terminal window run the Nameko
    shell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, enter the following in the prompt to call the microservice:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'You will see quite a bit of output as a result of the scrape (the following
    is truncated):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Congratulations! We now have successfully called our scraper microservice. Now,
    let's discuss how this works, and how the Dockerfile constructed the Docker image
    for the microservice.
  prefs: []
  type: TYPE_NORMAL
- en: How it works
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s first discuss the Dockerfile by walking through what it told Docker
    to do during the build process. The first line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: This informs Docker that we want to build our container image based on the `Python:3`
    image found on Docker Hub. This is a prebuilt Linux image with Python 3 installed.
    The next line informs Docker that we want all of our file operations to be relative
    to the `/usr/src/app` folder.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point in building the image we have a base Python 3 install in place.
    We need to then install the various libraries that our scraper uses, so the following
    tells Docker to run pip to install them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'We also need to install the NLTK data files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: Next, we copy in the implementation of our scraper. The following copies the
    `scraper_microservice.py` file from the previous recipe's folder into the container
    image.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'This also depends on the `sojobs` module, so we copy that also:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'The final line informs Docker of the command to run when the container is started:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: This tells Nameko to run the microservices in `scraper_microservice.py`, and
    to also talk to the RabbitMQ message broker located on a system with the name,
    `rabbitmq`. Since we attached our scraper container to the scraper-net network,
    and also did the same for the RabbitMQ container, Docker connects these two up
    for us!
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we ran the Nameko shell from the Docker host system. When it started,
    it reported that it would communicate with the AMQP server (RabbitMQ) at `pyamqp://guest:guest@localhost`.
    When we executed the command in the shell, the Nameko shell sent that message
    to localhost.
  prefs: []
  type: TYPE_NORMAL
- en: So how does it talk to the RabbitMQ instance in that container? When we started
    the RabbitMQ container, we told it to connect to the `scraper-net` network. It
    is still also connected to the host network, so we can still talk to the RabbitMQ
    broker as long as we mapped the `5672` port when we started it.
  prefs: []
  type: TYPE_NORMAL
- en: Our microservice in the other container is listening for messages in the RabbitMQ
    container, and then responds to that container, which is then picked up by the
    Nameko shell. Isn't that cool?
  prefs: []
  type: TYPE_NORMAL
- en: Creating an API container
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At this point, we can only talk to our microservice using AMQP, or by using
    the Nameko shell or a Nameko `ClusterRPCProxy` class.  So let's put our Flask-RESTful
    API into another container, run that alongside the other containers, and make
    REST calls. This will also require that we run an Elasticsearch container, as
    that API code also communicates with Elasticsearch.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First let''s start up Elasticsearch in a container that is attached to the
    `scraper-net` network. We can kick that off with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: Elasticsearch is now up and running on our `scarper-net` network. It can be
    reached by apps in other containers using the name elastic. Now let's move onto
    creating the container for the API.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We proceed with the recipe as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `10/04` folder is an `api.py` file that implements a modified Flask-RESTful
    API from earlier, but with several  modifications. Let''s examine the code of
    the API:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'The first change is that there is only one method on the API. We''ll focus
    on the `JobListing` method for now. Within that method, we now make the following
    call to create the Elasticsearch object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: The default constructor assumes that the Elasticsearch server is on localhost.
    This change now points to the host with the name elastic on our scraper-net network.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The second change is the removal of the calls to the functions in the sojobs
    module. Instead, we use a `Nameko ClusterRpcProxy` object to make the call to
    the scraper microservice running within our scraper container. This object is
    passed a configuration that points the RPC proxy to the rabbitmq container.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The final change is to the startup of the Flask application:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: The default connects to localhost, or 127.0.0.1\. Within a container, this doesn't
    bind to our `scraper-net` network or even on the host network. Using `0.0.0.0`
    binds the service to all network interfaces, and hence we can communicate with
    it via port mapping on the container. The port has also been moved to `8080`,
    a more common port for REST APIs than 5000.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'With the API modified to run within a container, and to talk to the scraper
    microservice, we can now construct the container. In the `10/04` folder is a Dockerfile
    to configure the container. Its content is the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: This is simpler than the Dockerfile for the previous container. This container
    doesn't have all the weight of NTLK. Finally, the startup simply executes the
    `api.py` files.
  prefs: []
  type: TYPE_NORMAL
- en: 'The container is built using the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'And then we can run the container using the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s now check that all of our containers are running:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, from the terminal on the host we can issue a curl to the REST endpoint
    (output truncated):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: And there we have it. We have containerized the API and the functionality, and
    also run RabbitMQ and Elasticsearch in containers.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This type of containerization is a great boon to the design and deployment of
    operations but still, we needed to create a number of Dockerfiles, containers,
    and a network to connect them, and run them all independently. Fortunately, we
    can simplify this with docker-compose. We'll see this in the next recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Composing and running the scraper locally with docker-compose
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Compose is a tool for defining and running multi-container Docker applications.
    With Compose, you use a YAML file to configure your application’s services. Then,
    with a single command and a simple configuration file, you create and start all
    the services from your configuration.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first thing that needs to be done to use Compose is to make sure it is installed.
    Compose is automatically installed with Docker for macOS. On other platforms,
    it may or not be installed. You can find the instructions at the following URL: [https://docs.docker.com/compose/install/#prerequisites](https://docs.docker.com/compose/install/#prerequisites).
  prefs: []
  type: TYPE_NORMAL
- en: Also, make sure all of the existing containers that we created earlier are not
    running, as we will create new ones.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We proceed with the recipe as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Docker Compose uses a `docker-compose.yml` file that tells Docker how to compose
    containers as `services`.  In the `10/05` folder there is a `docker-compose.yml`
    file to start up all the parts of our scraper as a service.  The following is
    the file''s contents:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: With Docker Compose we move away from thinking in terms of containers and toward
    working with services. In this file, we described four services (api, scraper,
    elastic, and rabbitmq) and how they are created. The image tag for each tells
    Compose which Docker image to use for that service. If we need to map ports, then
    we can use the `ports` tag. The `network` tag specifies a network to connect the
    service to, in this case the network is also declared in the file to be a `bridged`
    network. One last thing to point out is the use of the `depends_on` tag for the
    scraper service. This service requires the `rabbitmq` service to be running beforehand,
    and this tells docker compose to make sure that this happens in the specified
    sequence.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now to bring everything up, open a terminal and from that folder run the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'There will be pause while Compose reads the configuration and figures out what
    to do, and then there will be quite a bit of output as every container''s output
    will be streamed into this one console. At the beginning of the output you will
    see something similar to the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'In another terminal, you can issue a `docker ps` to see the containers that
    have started:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: Note the names of the service containers. They are wrapped with two different
    identifiers. The prefix is simply the folder that the composition is run from,
    in this case 10 (for a '10_' prefix).  You can change this using the -p option
    to docker-compose up to specify something different. The trailing number is the
    instance number of the container for that service. In this scenario, we only started
    one container per service, so these are all _1 at this point. In a little while,
    we will see this change when we do scaling.
  prefs: []
  type: TYPE_NORMAL
- en: 'You might ask then: if my service is named `rabbitmq`, and Docker creates a
    container with the name `10_rabbitmq_1`, how does the microservice, which uses
    `rabbitmq` as a hostname, still connect to the RabbitMQ instance? Docker Compose
    has you covered in this situation, as it knows that `rabbitmq` needs to be translated
    to `10_rabbitmq_1`. Nice!'
  prefs: []
  type: TYPE_NORMAL
- en: 'As part of bringing this environment up, Compose has also created the specified
    network:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: If we didn't specify a network, then Compose would have made a default network
    and wired everything to that. In this case that would work fine. But in more complicated
    scenarios this default may not be correct.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, at this point everything is up and running. Let''s check things are working
    well by making a call to the REST scraping API:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'And let''s also check that Elasticsearch is running by examining the index
    for the job listings now that we have requested one:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also use docker-compose to scale the services. If we want to add more
    microservice containers to increase the amount of requests that can be handled,
    we can tell Compose to increase the number of scraper service containers. The
    following increases the number of scraper containers to 3:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'Compose will go and think about this request for a bit and then emit the following,
    stating that it is starting up two more scraper service containers (and this will
    be followed with a lot of output from those containers initializing):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'A `docker ps` will now show three scraper containers running:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can see that we have three containers named `10_scraper_1`, `10_scraper_2`,
    and `10_scraper_3`. Cool!  And if you go into the RabbitMQ admin UI, you can see
    that there are three connections:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/79eb12d7-59c6-41d7-be3f-5dd118fdf202.png)The Nameko queues in RabbitMQNote
    that each has a different IP address. On a bridged network like the one we have
    created, Compose allocates the IP addresses on the `172.23.0` network, starting
    at `.2`.'
  prefs: []
  type: TYPE_NORMAL
- en: Operationally, all incoming scraping requests from the API will be routed to
    the rabbitmq container, and the actual RabbitMQ service would then spread the
    messages across all of the active connections and hence across all three containers,
    helping us to scale out processing.
  prefs: []
  type: TYPE_NORMAL
- en: Service instances can also be scaled down by issuing a scale value with a smaller
    number of containers, which Compose will respond to by removing containers until
    the value is achieved.
  prefs: []
  type: TYPE_NORMAL
- en: 'And when we are all done, we can tell Docker Compose to bring everything down:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: Executing a docker ps will now show that all of the containers have been removed.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have barely touched many of the capabilities of Docker and Docker Compose,
    and we have not even yet got into looking at using services such as Docker swarm.
    While docker Compose is convenient, it only runs the containers on a single host,
    which ultimately has scalability limitations. Docker swarm will perform similar
    things to Docker Compose, but work that magic across multiple systems within a
    cluster, allowing much greater scalability. But hopefully this has given you a
    feel for the value of Docker and Docker Compose, and how they can be of value
    when creating a flexible scraping service.
  prefs: []
  type: TYPE_NORMAL
