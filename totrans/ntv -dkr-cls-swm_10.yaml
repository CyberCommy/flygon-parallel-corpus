- en: Chapter 10. Swarm and the Cloud
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Throughout this book, we worked with Docker Swarm on a set of different underlying
    technologies without, so far, diving too deep into this implication: We ran Swarm
    on the top of AWS, DigitalOcean, and on our local workstations. For test and staging
    purposes, the platform onto which we run Swarm might be of secondary importance
    (*let''s fire up some AWS instances with Docker Machine and work that way*), but
    for production it''s mandatory to understand the pros and cons, reason, evaluate,
    and follow the trend.'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we're going to review several public and private cloud options
    and technologies and their possible intersections. We'll finally treat the brand
    new buzzwords of **CaaS** (**Container as a Service**) and **IaaC** (**Infrastructure
    as a Code**) in [Chapter 11](ch11.html "Chapter 11. What is next?"), *What it
    Next? *
  prefs: []
  type: TYPE_NORMAL
- en: 'Mainly, we will be looking at:'
  prefs: []
  type: TYPE_NORMAL
- en: Docker for AWS and Azure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Docker Datacenter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Swarm on OpenStack
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Docker for AWS and Azure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As long as with Docker For Mac and Windows, the Docker team started working
    on the *new generation* toolsets for operators: Docker for AWS and Docker for
    Windows. These are intended to provide a taste of automatic for deploying Docker
    infrastructures, especially Swarm-ready ones.'
  prefs: []
  type: TYPE_NORMAL
- en: The goals are to provide with a standard way of doing things, integrating the
    underlying infrastructure with the Docker tools and let people to run, with no
    effort, the latest software versions on the platform they love. The ultimate goal
    is really to let developers to move things from their laptops with Docker for
    Mac/Windows to the cloud, with Docker for AWS/Azure.
  prefs: []
  type: TYPE_NORMAL
- en: Docker for AWS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The user experience is, as always in the Docker ecosystem, great. The requirements
    are:'
  prefs: []
  type: TYPE_NORMAL
- en: An AWS ID
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An SSH key imported into your AWS keyring
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ready security groups
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Basically, Docker for AWS is a clickable template for CloudForms. CloudForms
    is the orchestration system for AWS, which allows to create templates of complex
    systems, for example, you can specify a web infrastructure made of three web servers,
    one database, and one load balancer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of a web or other generic infrastructure, Docker for AWS of course
    comes with the capability of creating Docker Swarm (mode) infrastructures: it
    creates as many masters and workers as you specify, puts a load balancer in front,
    and configures all networking accordingly.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the welcome screen:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Docker for AWS](images/image_10_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Then, you can specify some basic and advanced options:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Docker for AWS](images/image_10_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, you can select the number of managers and workers, as well as
    the flavor of the instances to be launched. So far, up to 1,000 workers are supported.
    After that, you just have to click on Create Stack in the next step, and wait
    for a few minutes for CloudForms to bring the infrastructure up.
  prefs: []
  type: TYPE_NORMAL
- en: 'What the template does is to:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a new Virtual Private Cloud inside your AWS account, networks, and subnets
    included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create two auto-scaling groups, One for managers and one for workers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Start the managers and ensure that they are healthy up with Raft quorum reached.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Start and enroll the workers one by one to the Swarm.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create **Elastic Load Balancers** (**ELBs**) to route traffic
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finish
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once CloudFormation has finished, it will prompt with a green confirmation.
  prefs: []
  type: TYPE_NORMAL
- en: '![Docker for AWS](images/image_10_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, we''re ready to jump into our new Docker Swarm infrastructure. Just pick
    up one of the manager''s public IPs and connect to it using the SSH key specified
    in the first step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![Docker for AWS](images/image_10_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Docker for Azure
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Thanks to an agreement with Microsoft, also automatic Swarm deployment for Azure
    is available as a one-click experience (or almost).
  prefs: []
  type: TYPE_NORMAL
- en: 'The prerequisites for deploying Swarm on Azure are:'
  prefs: []
  type: TYPE_NORMAL
- en: Have a valid Azure account
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Have this account ID associated to Docker for Azure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An Active Directory Principal application ID
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To generate the last, you can conveniently use a docker image, and launch it
    with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: During the process, at some point, you will be required to login through a browser
    to the specified URL. At the end, a pair ID/secret will be available for you to
    input in the Azure wizard form.
  prefs: []
  type: TYPE_NORMAL
- en: '![Docker for Azure](images/image_10_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Once everything is fine, you can just click on **OK** and **Create**.
  prefs: []
  type: TYPE_NORMAL
- en: '![Docker for Azure](images/image_10_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'A set of classical virtual machines will be created to run the specified number
    of managers (here 1) and workers (here 4), as long as with the proper internal
    networks, load balancers, and routers. Just as in Docker for AWS, you can start
    using your deployed Swarm by SSHing to the public IP of one manager:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![Docker for Azure](images/image_10_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: There is one limitation in the Azure template now, it only supports one manager.
    The possibility to add new managers should, however, come very soon.
  prefs: []
  type: TYPE_NORMAL
- en: Docker Datacenter
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Docker Datacenter, formerly Tutum and acquired by Docker, is the single-click
    deploy solution by Docker to use UCP, the Universal Control Panel, Docker's commercial
    and enterprise product.
  prefs: []
  type: TYPE_NORMAL
- en: 'Docker Datacenter includes:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Universal Control Plane** (**UCP**), the UI, refer to [https://docs.docker.com/ucp/overview](https://docs.docker.com/ucp/overview)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Docker Trusted Registry (DTR),** the private registry, refer to [https://docs.docker.com/docker-trusted-registry](https://docs.docker.com/docker-trusted-registry)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At the Dockercon 16, the team released support (currently in Beta) for Docker
    Datacenter running both on AWS and Azure. To try out Docker Datacenter, you need
    to associate a license to your company/project AWS or Azure ID.
  prefs: []
  type: TYPE_NORMAL
- en: 'For Datacenter for AWS, as for Docker for AWS, there is a CloudFormation template
    that makes immediate to start a Docker Datacenter. Requirements are:'
  prefs: []
  type: TYPE_NORMAL
- en: Have at least one Route53 configured, the AWS DNS service, see [http://docs.aws.amazon.com/Route53/latest/DeveloperGuide/Welcome.html](http://docs.aws.amazon.com/Route53/latest/DeveloperGuide/Welcome.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Docker datacenter license
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What you have to do is to follow the link from your license, to enter the Create
    Stack page. From here, you just input the **HostedZone** ID and the Docker Datacenter
    license and start the Stack creation. Internally, Docker Datacenter places some
    VMs on a Private network (nodes), and some, load balanced by an Elastic Load Balancer
    (ELBs, for controllers), on which it installs the commercially supported version
    of the Engine. The current version of Docker Datacenter VMs run internally Swarm
    standalone and a discovery mechanism, to connect to each other. We can expect
    the stable version of Datacenter to be released soon.
  prefs: []
  type: TYPE_NORMAL
- en: The main difference between Docker Datacenter and Docker for AWS is that the
    first one is intended to be all-inclusive enterprise ready. While the latter is
    the fastest way to deploy specifically Swarm clusters, the first is more of a
    complete solution, with a fancy UI, Notary, and optional services from the ecosystem.
  prefs: []
  type: TYPE_NORMAL
- en: Swarm on OpenStack
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Speaking of private cloud, the most popular open source solution for IaaS is
    OpenStack. OpenStack is a great ecosystem of programs (formerly known as projects),
    with the goal of providing a so-called cloud operating system. The core OpenStack
    programs are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Keystone**: The identity and authorization system'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Nova**: The virtual machine abstraction layer. Nova can be plugged with virtualization
    modules, such as Libvirt, VMware'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Neutron**: The network module, which handles tenant networks, instances ports,
    routing, and traffic'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cinder**: The storage module responsible for handling volumes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Glance**: The image storage'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Everything is glued up by additional actors:'
  prefs: []
  type: TYPE_NORMAL
- en: A database system, such as MySQL, keeping the configurations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An AMQP broker, such as Rabbit, to queue and deliver operations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A proxy system, such as HAproxy, to proxy HTTP API requests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In a typical VM creation in OpenStack, the following happens:'
  prefs: []
  type: TYPE_NORMAL
- en: A user either from the UI (Horizon) or from the CLI decides to spawn a VM.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: She/he clicks a button or types a command such as `nova boot ...`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Keystone checks authorization and authentication for this user in his/her tenant,
    by checking in the user''s database or in LDAP (depends on how OpenStack is configured)
    and generates a token that will be used throughout the whole session: `Here is
    your token: gAAAAABX78ldEiY2`*.*'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If authentication succeeds and the user is authorized to spawn a VM, Nova is
    invoked by using the authorization token: "We are launching a VM, can you please
    find a suitable physical host where to?"'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If such an host exists, Nova takes the image of choice of the user from Glance:
    "Glance, please pass me an Ubuntu Xenial bootable qcow2 file"'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'On the compute host where to physically launch the VM, a `nova-compute` process,
    which talks to the configured plugin, for example, says to Libvirt: "We are starting
    a VM on this host"'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Neutron allocates private (and public, if required) network ports for the VM:
    "Please create these ports on the designated networks, in these subnet pools"'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If the user wants to, Cinder allocates volume/s on the hosts designed by its
    scheduler. That is. Let's create additional volume/s and let's attach them to
    the VM.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If KVM is used, a suitable XML is generated with all the information above,
    and Libvirt starts the VM on the compute host
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When the VM is started, some variables are injected via cloud-init, for example,
    an SSH key to allow passwordless SSH logins
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This is (except for step 8 on Cinder) exactly how the OpenStack driver of Docker
    Machine behaves: when you create a Docker Host with Machine using `-d openstack`,
    you have to specify an existing glance image, an existing private (and optionally
    a public) network, and (optionally, otherwise is automatically generated) specify
    an SSH image, stored in the Nova database. And, of course, you have to pass to
    Machine the authorization variables to your OpenStack environment, or alternatively,
    source them as exported shell variables.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A Machine command creating a Docker Host on OpenStack will then look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: OpenStack Nova
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'So, the classical way to go for Docker Swarm on OpenStack would be starting
    creating instances, say 10 VMs from Ubuntu 16.04 images, on a dedicated network:'
  prefs: []
  type: TYPE_NORMAL
- en: From the web UI, specifying 10 as the number of instances
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Or from the CLI, using `nova boot ... --max-count 10 machine-`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Or by using Docker Machine
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The last is the more promising way because Machine automatically installs Docker,
    without later having to hack or have to use other tools on the newly created instances
    (such as Machine with the generic driver, Belt, Ansible, Salt or other scripts).
    But at the time of writing (Machine 0.8.2), Machine does not support bulk-host
    creations, so you will have to loop a `docker-machine` command with some basic
    shell logic:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This is not a good UX at all, also because Machine scales still very bad when
    we speak of dozens of hosts.
  prefs: []
  type: TYPE_NORMAL
- en: The (deprecated) nova-docker driver
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Once, there was a driver for Nova, to plug Docker containers as final destinations
    for Nova (instead of creating KVM or VmWare VMs, for example, these drivers allowed
    to create and manage Docker containers from Nova). If using such a tool for the
    *old* Swarm makes sense (since everything is orchestrated as containers), this
    is of no interest for Swarm Mode, which needs Docker Hosts rather than bare containers.
  prefs: []
  type: TYPE_NORMAL
- en: The reality - OpenStack the friendly way
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Luckily, OpenStack is a very vibrant project, and now that it has reached release
    **O** (**Ocata**), it is enriched by many optional modules. From the Docker Swarm
    perspective, the most interesting ones are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Heat:** This is the orchestrator system, which can create VMs configurations
    from templates.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Murano:** This is the application catalog that can run applications from
    a catalog maintained by the open source community, including Docker and Kubernetes
    containers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Magnum:** This is the Container as a Service solution from Rackspace.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kuryr:** This is the networking abstractor. With Kuryr, you can link Neutron
    tenant networks and Docker networks created with Docker Libnetwork (such as the
    Swarm ones), and connect OpenStack instances with Docker containers as if they
    were connected to the same network.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenStack Heat
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'OpenStack Heat resembles Docker Compose a bit, allowing you to start systems
    by a template, but it''s much more powerful: You can not only boot a set of instances
    from an image, say Ubuntu 16.04, but you can orchestrate them, which means create
    networks, attach VMs interfaces to networks, place load balancers and execute
    later tasks on the instances, such as installing Docker. Roughly, Heat is the
    equivalent of Amazon''s CloudFormation for OpenStack.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In Heat everything starts from YAML templates, thanks to which you can model
    your infrastructure, before firing it up, in the same fashion as you do with Compose.
    For example, you create a template file like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Then, you can launch a stack from it (`heat stack-create -f configuration.hot
    dockerhosts`). Heat will call Nova, Neutron, Cinder and all the necessary OpenStack
    services to orchestrate resources and make them available.
  prefs: []
  type: TYPE_NORMAL
- en: Here we're not going to show how to start a Docker Swarm infrastructure through
    Heat, rather we'll see here Magnum, which uses Heat underneath to manipulate OpenStack
    objects.
  prefs: []
  type: TYPE_NORMAL
- en: OpenStack Magnum
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Magnum, announced in late 2015 and developed by the OpenStack Containers Team,
    aims to make **Container Orchestration Engines** (**COEs**) such as Docker Swarm
    and **Kubernetes** available as first class resources in OpenStack. There were
    and there will be many projects inside the OpenStack arena focused to provide
    containers support, but Magnum goes further, because it's designed to support
    *containers orchestration*, not bare containers management.
  prefs: []
  type: TYPE_NORMAL
- en: '![OpenStack Magnum](images/image_10_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'So far, the focus has been put especially on Kubernetes, but we''re speaking
    of **Magnum** here because it''s the most promising open source technology for
    providing a convenient way of running CaaS orchestration on the private cloud.
    Magnum does not support the newest Swarm Mode yet, at the time of writing: This
    feature must be addressed. There is a Launchpad blueprint opened by the author,
    who eventually might start working on after the book is published: [https://blueprints.launchpad.net/magnum/+spec/swarm-mode-support](https://blueprints.launchpad.net/magnum/+spec/swarm-mode-support).'
  prefs: []
  type: TYPE_NORMAL
- en: Architecture and core concepts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Magnum has two main components, running on controller nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The first process, `magnum-api` is the typical OpenStack API provider, invoked
    by the magnum Python client or by other processes for operations, such as creating
    a cluster. The latter, `magnum-conductor`, is invoked by `magnum-api` (more or
    less, it has the same functions of `nova-conductor`) through an AMQP server, such
    as Rabbit, and its goal is to interface to the Kubernetes or Docker APIs. In practice,
    these two binaries work together to provide a sort of orchestration abstraction.
  prefs: []
  type: TYPE_NORMAL
- en: '![Architecture and core concepts](images/image_10_009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'On the OpenStack cluster compute nodes, nothing special is necessary to run,
    apart from `nova-compute` processes: Magnum conductor exploits Heat directly to
    create stacks, which creates networks and instantiates VMs in Nova.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The Magnum terminology is evolving alongside with the project. But these are
    the main concepts:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Containers** are Docker containers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **Cluster** (formerly a Bay) is a collection of node objects where work is
    scheduled, for example, Swarm nodes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **ClusterTemplate** (formerly BayModel) is the template storing information
    about the cluster types. For example, a ClusterTemplate defines *a Swarm cluster
    with 3 managers and 5 workers*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pods** are a collection of containers running on the same physical or virtual
    machine.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As for advanced options, such as storage, new COEs support, and scaling, Magnum
    is a very active project and we recommend you to follow its evolution on [http://docs.openstack.org/developer/magnum/](http://docs.openstack.org/developer/magnum/).
  prefs: []
  type: TYPE_NORMAL
- en: Install HA Magnum on Mirantis OpenStack
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Installing Magnum is not so trivial, especially if you want to warranty some
    failover typical of OpenStack HA deployments. There are many tutorials on the
    Internet on how to configure Magnum in DevStack (the developer's 1-node staging
    setup), but none showing how to work on real production systems with more than
    one controller. Here we show how to install Magnum on a real setup..
  prefs: []
  type: TYPE_NORMAL
- en: 'Typically, production OpenStack installations count a number of nodes dedicated
    to different goals. In a minimal HA deployment, there usually are:'
  prefs: []
  type: TYPE_NORMAL
- en: Three or more (odd number for quorum reasons) **controller nodes**, responsible
    of hosting the OpenStack program's APIs and configuration services, such as Rabbit,
    MySQL, and HAproxy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An arbitrary number of **compute nodes**, where workloads run physically (where
    VMs are hosted)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optionally, there may be dedicated storage, monitoring, database, network, and
    other nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our setup here, based on **Mirantis OpenStack** running Newton with Heat
    installed, we have three controllers and three computes plus storage nodes. HA
    is configured with Pacemaker, which keeps resources as MySQL, Rabbitmq, and HAproxy
    in high availability. APIs are proxied by HAproxy. This is a screenshot showing
    the resources configured into Pacemaker. They all are started and working properly:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Install HA Magnum on Mirantis OpenStack](images/image_10_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: All nodes in the cluster run Ubuntu 16.04 (Xenial), for which the stable Magnum
    2.0 packages exist, so it's enough to consume them from upstream and install with
    `apt-get install`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before installing Magnum, however, it''s necessary to prepare the environment.
    First, a database is required. Enter the MySQL console from any controller by
    just typing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'In MySQL, create the magnum database and user, and grant the correct privileges:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Now, it's necessary to create the service credentials in Keystone, beginning
    with defining a magnum OpenStack user, who must be added to the services group.
    The services group is a special group, which includes the OpenStack services running
    across the cluster, such as Nova, Neutron, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'After that, a new service must be created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: OpenStack programs are invoked and talk through their APIs. An API is accessed
    with an endpoint, that is a pair URL and port, which in HA setups is proxied by
    HAproxy. In our setup, HAproxy receives HTTP requests on `10.21.22.2`. and balances
    them across the controller IPs, that are `10.21.22.4, 5`, and `6`.
  prefs: []
  type: TYPE_NORMAL
- en: '![Install HA Magnum on Mirantis OpenStack](images/image_10_011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We have to create such endpoints for Magnum, which listens by default on port
    9511, for each zone (public, internal, and admin):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Also, Magnum needs an additional configuration to organize its workloads internally
    in domains, so a dedicated domain plus a domain user must be added:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Now everything is in place to finally run `apt-get`. On all three controllers,
    run the following command and in the ncurses interface, always answer No, to not
    change the environment, or keep the default configurations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Configure an HA Magnum installation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The configuration of Magnum is pretty straightforward. What''s needed to be
    done to have it in an up and running state is:'
  prefs: []
  type: TYPE_NORMAL
- en: Configure it through the `magnum.conf` file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Restart the magnum binaries.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Open port `tcp/9511`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Configure HAproxy to accept and balance magnum APIs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reload HAproxy.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The crucial configurations that must be done on each controller follow. First,
    on every controller the host parameter should be the IP of the interface on the
    management network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'If **Barbican** (the OpenStack project dedicated to the management of secrets
    such as password) is not installed, certificates must be handled by the `**x509keypair**`
    plugin:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, a database connection string is required. In this HA setup, MySQL answers
    on the VIP `10.21.22.2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The Keystone authentication is configured as follow (the options are rather
    self-explanatory):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Oslo (the message broker) must be configured to messaging:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The Rabbitmq configuration is this, specifying the Rabbit cluster hosts (since
    Rabbit runs on controllers, the IPs of all controllers'' management network):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, an additional configuration of the trustee is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'After this reconfiguration, the Magnum services must be restarted:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Magnum uses by default port `tcp/9511`, so traffic to this port must be accepted
    in iptables: Modify iptables to add this rule:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Just after the other OpenStack services, right after `116 openvswitch db`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, it''s time to configure HAproxy to accept magnum. Add an `180-magnum.cfg`
    file into `/etc/haproxy/conf.d` on all controllers with this content:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: This configures the magnum-api to listen on the VIP `10.21.22.2:9511`, backing
    on the three controllers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Just right after, HAproxy must be restarted from Pacemaker. From any controller,
    run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Wait until no HAproxy processes are running on all controllers (you can check
    with `ps aux`), but this should be very fast, less than 1 second, then:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'After that, Magnum will be available with services up:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '![Configure an HA Magnum installation](images/image_10_012.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Create a Swarm cluster on Magnum
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Creating a Swarm cluster, when the COE will be added to Magnum, will require
    these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a Swarm Mode template.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Launch a cluster from the template.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We''re not diving too much into something that doesn''t exist yet, but the
    commands will be something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, a cluster template of type swarm-mode based on Ubuntu Xenial with `m1.medium`
    flavors is defined: VMs will be injected the fuel keypair, will have an additional
    external public IP. The UX for creating a cluster based on such a template might
    be expected to be:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Here, a Swarm cluster is instantiated with three managers and five workers.
  prefs: []
  type: TYPE_NORMAL
- en: Magnum is a great project, at the highest level of abstraction for running container
    orchestration on OpenStack. It's running on the Rackspace cloud, and it's available
    for public usage through Carina, refer to [http://blog.rackspace.com/carina-by-rackspace-simplifies-containers-with-easy-to-use-instant-on-native-container-environment](http://blog.rackspace.com/carina-by-rackspace-simplifies-containers-with-easy-to-use-instant-on-native-container-environment)
    .
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored the alternative platforms on which we can run Docker
    Swarm clusters. We worked with the newest Docker tools--Docker for AWS and Docker
    for Azure--and we used them to demonstrate how to install Swarm in a new fashion.
    After introducing Docker Datacenter, we moved to the private cloud part. We worked
    on OpenStack, showing how to run Docker hosts on it, how to install OpenStack
    Magnum, and how to create Swarm objects on it. We've almost finished our travel.
  prefs: []
  type: TYPE_NORMAL
- en: The next and last chapter will sketch the future of Docker orchestration.
  prefs: []
  type: TYPE_NORMAL
