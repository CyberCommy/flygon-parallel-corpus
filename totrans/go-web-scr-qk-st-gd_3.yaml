- en: Web Scraping Etiquette
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before jumping into too much code, there are a few points you will need to keep
    in mind as you begin running a web scraper. It is important to remember that we
    all must be good citizens of the internet in order for everyone to get along.
    Keeping that in mind, there are many tools and best practices to follow in order
    to ensure that you are being fair and respectful when adding a load to an outside
    web server. Stepping outside of these guidelines could put your scraper at risk
    of being blocked by the web server, or in extreme cases, you could find yourself
    in legal trouble.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: What is a robots.txt file?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is a User-Agent string?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How can you throttle your web scraper?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do you use caching?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is a robots.txt file?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most of the pages on a website are free to be accessed by web scrapers and bots.
    Some of the reasons for allowing this are in order to be indexed by search engines
    or to allow pages to be discovered by content curators. Googlebot is one of the
    tools that most websites would be more than happy to give access to their content.
    However, there are some sites that may not want everything to show up in a Google
    search result. Imagine if you could google a person and instantly obtain all of
    their social media profiles, complete with contact information and address. This
    would be bad news for the person, and certainly not a good privacy policy for
    the company hosting the site. In order to control access to different parts of
    a website, you would configure a `robots.txt` file.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `robots.txt` file is typically hosted at the root of the website in the
    `/robots.txt` resource. This file contains definitions of who can access which
    pages in this website. This is done by describing a bot that matches a `User-Agent`
    string, and specifying which paths are allowed and disallowed. Wildcards are also
    supported in the `Allow` and `Disallow` statements. The following is an example
    `robots.txt` file from Twitter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the most restrictive `robots.txt` file you will encounter. It states
    that no web scraper can access any part of [twitter.com](http://twitter.com).
    Violating this will put your scraper at risk of being blacklisted by Twitter''s
    servers. On the other hand, websites like Medium are a little more permissive.
    Here is their `robots.txt` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Looking into this, you can see that editing profiles is disallowed by the following
    directives:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Disallow: /*/edit$`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Disallow: /*/*/edit$`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The pages that are related to logging in and signing up, which could be used
    for automated account creation, are also disallowed by `Disallow: /m/`.'
  prefs: []
  type: TYPE_NORMAL
- en: If you value your scraper, do not access these pages. The `Allow` statements
    provide explicit permission to paths in the in `/_/` routes, as well as some `api`
    related resources. Outside of what is defined here, if there is no explicit `Disallow`
    statement, then your scraper has permission to access the information. In the
    case of Medium, this includes all of the publicly available articles, as well
    as public information about the authors and publications. This `robots.txt` file
    also includes a `sitemap`, which is an XML-encoded file listing all of the pages
    available on the website. You can think of this as a giant index, which can come
    in very handy.
  prefs: []
  type: TYPE_NORMAL
- en: 'One more example of a `robots.txt` file shows how a site defines rules for
    different `User-Agent` instances. The following `robots.txt` file is from Adidas:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This example explicitly disallows access to a few paths for all web scrapers,
    as well as a special note for `bingbot`. The `bingbot` must respect the `Crawl-delay`
    of `1` second, meaning it cannot access any pages more than once per second. `Crawl-delays`
    are very important to take note of, as they will define how quickly you can make
    web requests. Violating this may generate more errors for your web scraper, or
    it may be permanently blocked.
  prefs: []
  type: TYPE_NORMAL
- en: What is a User-Agent string?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When an HTTP client makes a request to a web server, they identify who they
    are. This holds true for web scrapers and normal browsers alike. Have you ever
    wondered why a website knows that you are a Windows or a Mac user? This information
    is contained inside your `User-Agent` string. Here is an example `User-Agent`
    string for a Firefox browser on a Linux computer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'You can see that this string identifies the family, name, and version of the
    web browser, as well as the operating system. This string will be sent with every
    request from this browser inside of a request header, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Not all `User-Agent` strings contain this much information. HTTP clients that
    are not web browsers are typically much smaller. Here are some examples:'
  prefs: []
  type: TYPE_NORMAL
- en: 'cURL: `curl/7.47.0`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Go: `Go-http-client/1.1`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Java: `Apache-HttpClient/4.5.2`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Googlebot (for images): `Googlebot-Image/1.0`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`User-Agent` strings are a good way of introducing your bot and taking responsibility
    for following the rules set in a `robots.txt` file. By using this mechanism, you
    will be held accountable for any violations.'
  prefs: []
  type: TYPE_NORMAL
- en: Example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are open source tools available that help parse `robots.txt` files and
    validate website URLs against them to see if you have access or not. One project
    that I would recommend is available on GitHub called `robotstxt` by user `temoto`.
    In order to download this library, run the following command in your terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The `$GOPATH` referred to here is the one you set up during the installation
    of the Go programming language back in [Chapter 1](005f17ec-d83f-4acc-9132-6ee89bc86f1e.xhtml), *Introducing
    Web Scraping and Go*. This is the directory with the `src/ bin/` and `pkg/ directories`.
  prefs: []
  type: TYPE_NORMAL
- en: 'This will install the library on your machine at `$GOPATH/src/github/temoto/robotstxt`.
    If you would like, you can read the code to see how it all works. For the sake
    of this book, we will just be using the library in our own project. Inside your
    `$GOPATH/src` folder, create a new folder called `robotsexample`. Create a `main.go`
    file inside the `robotsexample` folder. The following code for `main.go` shows
    you a simple example of how to use the `temoto/robotstxt` package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This example uses Go for each loop using the `range` operator. The `range` operator
    returns two variables, the first is the `index` of the `iteration` (which we ignore
    by assigning it to `_`), and the second is the value at that index.
  prefs: []
  type: TYPE_NORMAL
- en: This code checks six different paths against the `robots.txt` file for [https://www.packtpub.com/](https://www.packtpub.com/),
    using the default `User-Agent` string for the Go HTTP client. If the `User-Agent`
    is allowed to access a page, then the `Test()` method returns `true`. If it returns
    `false`, then your scraper should not access this section of the website.
  prefs: []
  type: TYPE_NORMAL
- en: How to throttle your scraper
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Part of good web scraping etiquette is making sure you are not putting too much
    load on your target web server. This means limiting the number of requests you
    make within a certain period of time. For smaller servers, this is especially
    true, as they have a much more limited pool of resources. As a good rule of thumb,
    you should only access the same web page as often as you think it will change.
    For example, if you were looking at daily deals, you would probably only need
    to scrape once per day. As for scraping multiple pages from the same website,
    you should first follow the `Crawl-Delay` in a `robots.txt` file. If there is
    no `Crawl-Delay` specified, then you should manually delay your requests by one
    second after every page.
  prefs: []
  type: TYPE_NORMAL
- en: There are many different ways to incorporate delays into your crawler, from
    manually putting your program to sleep to using external queues and worker threads.
    This section will explain a few basic techniques. We will revisit more complicated
    examples when we discuss the Go programming language concurrency model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The simplest way to add throttle to your web scraper is to track the timestamps
    for requests that are made, and ensure that the elapsed time is greater than your
    desired rate. For example, if you were to scrape at a rate of one page per `5`
    seconds, it would look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'This example has many instances of `:=` when defining variables. This is a
    shorthand way, in Go, of simultaneously declaring and instantiating variables.
    It replaces the need to say the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`var a string`'
  prefs: []
  type: TYPE_NORMAL
- en: '`a = "value"`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead, it becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '`a := "value"`'
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we make requests to [http://www.example.com/index.html](http://www.example.com/index.html)
    once every five seconds. We know how long it has been since our last request,
    as we update the `lastRequestTime` variable and check it before we make each request.
    This is all you need to scrape a single website, even if you were scraping multiple
    pages.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you were scraping multiple websites, you would need to separate `lastRequestTime`
    into one variable per website. The simplest way to do this would be with a `map`,
    Go''s key-value structure, where the key would be the host and the value would
    be the timestamp for the last request. This would replace the definition with
    something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Our `for` loop would also change slightly and set the value of the map to the
    current scrape time, but only for the website, we are scraping. For example, if
    we were to scrape the pages in an alternating manner, it might look something
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, to update the map with the last known request time, we would use a
    similar block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: You can find the full source code for this example on GitHub.
  prefs: []
  type: TYPE_NORMAL
- en: If you look at the output in the terminal, you will see that the first requests
    to either site have no delay and each sleep period is slightly less than five
    seconds now. This shows that the crawler is respecting each site's rate independently.
  prefs: []
  type: TYPE_NORMAL
- en: How to use caching
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One last technique that can benefit your scraper, as well as reducing load on
    the website, is by only requesting new content when it changes. If your scraper
    is downloading the same old content from a web server, then you aren't getting
    any new information and the web server is doing unnecessary work. For this reason,
    most web servers implement techniques to provide the client with instructions
    on caching.
  prefs: []
  type: TYPE_NORMAL
- en: 'A website that supports caching, will give the client information on what it
    can store, and how long to store it. This is done through response headers such
    as `Cache-Control`, `Etag`, `Date`, `Expires`, and `Vary`. Your web scraper should
    be aware of these directives to avoid making unnecessary requests to the web server,
    saving you, and the server, time and computing resources. Let''s take a look at
    our [http://www.example.com/index.html](http://www.example.com/index.html) response
    one more time, given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The body of the response is not included in this example.
  prefs: []
  type: TYPE_NORMAL
- en: There are a few response headers used to communicate caching instructions that
    you should follow in order to increase the efficiency of your web scraper. These
    headers will inform you of what information to cache, for how long, and a few
    other helpful pieces of information to make life easier.
  prefs: []
  type: TYPE_NORMAL
- en: Cache-Control
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `Cache-Control` header is used to indicate whether this content is cacheable,
    and for how long. Some of the common values for this header are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`no-cache`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`no-store`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`must-revalidated`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max-age=<seconds>`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`public`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cache directives such as `no-cache`, `no-store`, and `must-revalidate` exist
    to prevent the client from caching the response. Sometimes, the server is aware
    that the content on this page changes frequently, or is dependent on a source
    outside of its control. If none of these directives are sent, you should be able
    to cache the response using the provided `max-age` directive. This defines the
    number of seconds you should consider this content as fresh. After this time,
    the response is said to be stale and a new request should be made to the server.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the response from the previous example, the server sends a `Cache-Control`
    header:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: This indicates that you should cache this page for up to `604880` seconds (seven
    days).
  prefs: []
  type: TYPE_NORMAL
- en: Expires
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `Expires` header is another way of defining how long to retain cached information.
    This header defines an exact date and time from which the content will be considered
    stale and should be refreshed. This time should coincide with the `max-age` directive
    from the `Cache-Control` header, if one is provided.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our example, the `Expires` header matches the 7-day expiration based on
    the `Date` header, which defines when the request was received by the server:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Etag
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `Etag` is also important in keeping cached information. This is a key unique
    to this page, and will only change if the content of the page changes. After the
    cache expires, you can use this tag to check with the server if there is actually
    new content, without downloading a fresh copy. This works by sending an `If-None-Match`
    header containing the `Etag` value. When this happens, the server will check if
    the `Etag` on the current resource matches the `Etag` in the `If-None-Match` header.
    If it does match, then there have been no updates and the server responds with
    a status code of 304 Not Modified, with some headers to extend your cache. The
    following is an example of a `304` response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The server, in this case, validates the `Etag` and provides a new `Expires`
    time, still matching the `max-age`, from the time this second request was fulfilled.
    This way, you still save time by not needing to read more data over the network.
    You can still use your cached pages to fulfill your needs.
  prefs: []
  type: TYPE_NORMAL
- en: Caching content in Go
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The storage and retrieval of cached pages can be implemented by hand using your
    local filesystem, or a database to hold the data and the cached information. There
    are also open source tools available to help simplify this technique. One such
    project is `httpcache` by GitHub user `gregjones`.
  prefs: []
  type: TYPE_NORMAL
- en: The `httpcache` follows the caching requirements set by the **Internet Engineering
    Task Force** (**IETF**), the governing body for internet standards. The library
    provides a module that can store and retrieve web pages from your local machine,
    as well as a plugin for the Go HTTP client to automatically handle all cache-related
    HTTP request and response headers. It also provides multiple storage backends
    where you can store the cached information, such as Redis, Memcached, and LevelDB.
    This will allow you to run a web scraper on different machines, but connect to
    the same cached information.
  prefs: []
  type: TYPE_NORMAL
- en: As your scraper grows in size and you need to design a distributed architecture,
    a feature like this will be critical to ensure that time and resources are not
    wasted on duplicated work. Stable communication between all of your scrapers is
    key!
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at an example using `httpcache`. First, install `httpcache`
    by typing the following commands into your terminal, shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`go get github.com/gregjones/httpcache`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`go get github.com/peterbourgon/diskv`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `diskv` project is used by `httpcache` to store the web page on your local
    machine.
  prefs: []
  type: TYPE_NORMAL
- en: 'Inside your `$GOPATH/src`, create a folder called `cache` with a `main.go`
    inside it. Use the following code for your `main.go` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: This program uses the local disk cache to store the response from [http://www.example.com/index.html](http://www.example.com/index.html).
    Under the hood, it reads all of the cache-related headers to determine if it can
    store the page and includes the expiration date with the data. On the second request,
    `httpcache` checks if the content is expired and returns the cached data instead
    of making another HTTP request. It also adds an extra header, `X-From-Cache`,
    to indicate that this is being read from the cache. If the page had expired, it
    would make the HTTP request with the `If-None-Match` header and handle the response,
    including updating the cache in case of a 304 Not Modified response.
  prefs: []
  type: TYPE_NORMAL
- en: Using a client that is automatically set up to handle caching content will make
    your scraper run faster, as well as reducing the likelihood that your web scraper
    will be flagged as a bad citizen. When this is done in combination with respecting
    a website's `robots.txt` file and properly throttling your requests, you can scrape
    confidently, knowing that you are a respectable member of the web community.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned the basic etiquette for respectfully crawling the
    web. You learned what a `robots.txt` file is and why it is important to obey it.
    You also learned how to properly represent yourself using `User-Agent` strings. Controlling
    your scraper via throttling and caching was also covered. With these skills, you
    are one step closer to building a fully functional web scraper.
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 4](e6f2de6a-420b-4691-9ba1-969ed6ad32ea.xhtml), *Parsing HTML*,
    we will look at how to extract information from HTML pages using various techniques.
  prefs: []
  type: TYPE_NORMAL
