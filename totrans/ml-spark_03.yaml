- en: Designing a Machine Learning System
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will design a high-level architecture for an intelligent,
    distributed machine learning system that uses Spark as its core computation engine.
    The problem we will focus on will be taking the existing architecture for a web-based
    business and redesigning it to use automated machine learning systems to power
    key areas of the business.
  prefs: []
  type: TYPE_NORMAL
- en: Before we dig deeper into our scenario, we will spend some time understanding
    what machine learning is.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then we will:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduce a hypothetical business scenario
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Provide an overview of the current architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explore various ways in which machine learning systems can enhance or replace
    certain business functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Provide a new architecture based on these ideas
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A modern large-scale data environment includes the following requirements:'
  prefs: []
  type: TYPE_NORMAL
- en: It must integrate with the other components of the system, especially with data
    collection and storage systems, analytics and reporting, and frontend applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It should be easily scalable and independent of the rest of the architecture.
    Ideally, this should be in the form of horizontal as well as vertical scalability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It should allow efficient computation with respect to the type of workload in
    mind, that is, machine learning and iterative analytics applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If possible, it should support both batch and real-time workload
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As a framework, Spark meets these criteria. However, we must ensure that the
    machine learning systems designed on Spark also meet this criteria. There is no
    good in implementing an algorithm that ends up having bottlenecks that cause our
    system to fail in terms of one or more of these requirements.
  prefs: []
  type: TYPE_NORMAL
- en: What is Machine Learning?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine learning is a subfield of data mining. While data mining has been around
    for more than 50+ years, machine learning is a subset where a large cluster of
    machines is used to analyze and extract knowledge from large datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning is closely related to computational statistics. It has strong
    ties to mathematical optimization; it provides methods, theory, and application
    domains to the field. Machine learning is employed in various types of computing
    tasks where designing and programming explicit algorithms are infeasible. Example
    applications are spam filtering, **optical character recognition** (**OCR**),
    search engine, and computer vision. Machine learning is sometimes combined with
    data mining, which focuses more on exploratory data analysis and is known as unsupervised
    learning.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning systems can be classified into three categories, depending
    on the nature of the learning signal available to a learning system. Learning
    algorithm discovers structure from the input provided. It can have a goal (hidden
    patterns), or it could be a means try to find features.
  prefs: []
  type: TYPE_NORMAL
- en: '**Unsupervised learning**: No labels of outputs are given to the learning system.
    It finds structure on its own from the inputs given to'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Supervised learning**: The system is presented with inputs and desired outputs
    by a human and the goal is to learn a model to map inputs to outputs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reinforcement learning**: The system interacts with an environment in which
    it performs a stated goal without a human explicitly telling it whether it has
    come close to its goal'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the later sections, we will map supervised and unsupervised learning to various
    chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing MovieStream
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To better illustrate the design of our architecture, we will introduce a practical
    scenario. Let's assume that we have just been appointed to head the data science
    team of MovieStream, a fictitious Internet business that streams movies and television
    shows to its users.
  prefs: []
  type: TYPE_NORMAL
- en: 'MovieStream system is outlined in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_03_001.png)'
  prefs: []
  type: TYPE_IMG
- en: MovieStream's current architecture
  prefs: []
  type: TYPE_NORMAL
- en: As we can see in the preceding diagram, currently, MovieStream's content editorial
    team is responsible for deciding which movies and shows are promoted and shown
    in various parts of the site. They are also responsible for creating the content
    for MovieStream's bulk marketing campaigns, which include e-mail and other direct
    marketing channels. Currently, MovieStream collects basic data on what titles
    are viewed by users on an aggregate basis and has access to some demographic data
    collected from users when they sign up to the service. In addition, they have
    access to some basic metadata about the titles in their catalog.
  prefs: []
  type: TYPE_NORMAL
- en: MovieStream can handle many of the functions currently handled by the content
    team in an automated manner.
  prefs: []
  type: TYPE_NORMAL
- en: Business use cases for a machine learning system
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Perhaps the first question we should answer is, *Why* *to use machine learning
    at all?*
  prefs: []
  type: TYPE_NORMAL
- en: 'Why doesn''t MovieStream simply continue with human-driven decisions? There
    are many reasons to use machine learning (and certainly some reasons not to),
    but the most important ones are mentioned here:'
  prefs: []
  type: TYPE_NORMAL
- en: The scale of data involved means that full human involvement quickly becomes
    infeasible as MovieStream grows
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model-driven approaches such as machine learning and statistics can often benefit
    from uncovering patterns that cannot be seen by humans (due to the size and complexity
    of the datasets)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model-driven approaches can avoid human and emotional biases (as long as the
    correct processes are carefully applied)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, there is no reason why both model-driven and human-driven processes
    and decision making cannot coexist. For example, many machine learning systems
    rely on receiving labeled data in order to train models. Often, labeling such
    data is costly, time consuming, and requires human input. A good example of this
    is classifying textual data into categories or assigning a sentiment indicator
    to the text. Many real-world systems use some form of human-driven system to generate
    labels for such data (or at least part of it) to provide training data to models.
    These models are then used to make predictions in the live system at a larger
    scale.
  prefs: []
  type: TYPE_NORMAL
- en: In the context of MovieStream, we need not fear that our machine learning system
    will make the content team redundant. Indeed, we will see that our aim is to lift
    the burden of time-consuming tasks where machine learning might be able to perform
    better while providing tools to allow the team to better understand the users
    and content. This might, for example, help them in selecting which new content
    to acquire for the catalog (which involves a significant amount of cost and is,
    therefore, a critical aspect of the business).
  prefs: []
  type: TYPE_NORMAL
- en: Personalization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Perhaps one of the most important potential applications of machine learning
    in MovieStream's business is personalization. Generally speaking, personalization
    refers to adapting the experience of a user and the content presented to them
    based on various factors, which might include user behavior data as well as external
    factors.
  prefs: []
  type: TYPE_NORMAL
- en: '**Recommendations** are essentially a subset of personalization. Recommendation
    generally refers to presenting a user with a list of items that we hope the user
    will be interested in. Recommendations might be used in web pages (for example,
    recommendation-related products), via e-mails or other direct marketing channels,
    via mobile apps, and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: Personalization is very similar to recommendations, but while recommendations
    are usually focused on an *explicit* presentation of products or content to the
    user, personalization is more generic and, often, more *implicit*. For example,
    applying personalization to search on the MovieStream site might allow us to adapt
    the search results for a given user, based on the data available about that user.
    This might include recommendation-based data (in the case of a search for products
    or content) but might also include various other factors such as geolocation and
    past search history. It might not be apparent to the user that the search results
    are adapted to their specific profile; this is why personalization tends to be
    more implicit.
  prefs: []
  type: TYPE_NORMAL
- en: Targeted marketing and customer segmentation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In a manner similar to recommendations, targeted marketing uses a model to select
    what to target at users. While generally recommendations and personalization are
    focused on a one-to-one situation, segmentation approaches might try to assign
    users into groups based on characteristics and, possibly, behavioral data. The
    approach might be fairly simple or might involve approaches that try to assign
    users into groups based on characteristics and, possibly, behavioral data. The
    approach might be fairly simple or might involve a machine-learning model such
    as clustering. Either way, the result is a set of segment assignments that might
    allow us to understand the broad characteristics of each group of users, what
    makes them similar to each other within a group, and what makes them different
    from others in different groups.
  prefs: []
  type: TYPE_NORMAL
- en: This could help MovieStream to better understand the drivers of user behavior
    and might also allow a broader targeting approach where groups are targeted as
    opposed to (or more likely, in addition to) direct one-to-one targeting with personalization.
  prefs: []
  type: TYPE_NORMAL
- en: These methods can also help when we don't necessarily have labeled data available
    (as is the case with certain user and content profile data), but we still wish
    to perform more focused targeting than a complete one-size-fits-all approach.
  prefs: []
  type: TYPE_NORMAL
- en: Predictive modeling and analytics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A third area where machine learning can be applied is in predictive analytics.
    This is a very broad term, and in some ways, it encompasses recommendations, personalization,
    and targeting too. In this context, since recommendations and segmentation are
    somewhat distinct, we use the term **predictive modeling** to refer to other models
    that seek to make predictions. An example of this can be a model that predicts
    the potential viewing activity and revenue of new titles before any data is available
    on how popular the title might be. MovieStream can use past activity and revenue
    data, together with content attributes, to create a regression model that can
    be used to make predictions for brand new titles.
  prefs: []
  type: TYPE_NORMAL
- en: As another example, we can use a **classification model** to automatically assign
    tags, keywords, or categories to new titles for which we only have partial data.
  prefs: []
  type: TYPE_NORMAL
- en: Types of machine learning models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While we have one example, there are many other examples, some of which we will
    touch on in the relevant chapters when we introduce each machine learning task.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, we can broadly divide the preceding use cases and methods into two
    categories of machine learning:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Supervised learning**: These types of models use labeled data to learn. Recommendation
    engines, regression, and classification are examples of supervised learning methods.
    The labels in these models can be user--movie ratings (for the recommendation),
    movie tags (in the case of the preceding classification example), or revenue figures
    (for regression). We will cover supervised learning models in [Chapter 4](d3bf76a8-26be-4db7-8310-b936d220407e.xhtml),
    *Building a Recommendation Engine with Spark*, [Chapter 6](7bd5bfd3-6301-49dc-ba28-5d6553b57e01.xhtml),
    *Building a Classification Model with Spark*, and [Chapter 7](5df7d44a-d025-448b-826d-75c44ee7a165.xhtml),
    *Building a Regression Model with Spark*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unsupervised learning**: When a model does not require labeled data, we refer
    to unsupervised learning. These types of models try to learn or extract some underlying
    structure in the data or reduce the data down to its most important features.
    Clustering, dimensionality reduction, and some forms of feature extraction, such
    as text processing, are all unsupervised techniques and will be dealt with in
    [Chapter 8](7b684d30-421c-4874-a5e6-6f6d57d93405.xhtml), *Building a Clustering
    Model with Spark*, [Chapter 9](a164696f-86f1-4ed6-919d-d24a2f29385f.xhtml), *Dimensionality
    Reduction with Spark*, and [Chapter 10](789e8b8c-28e8-444d-92a6-aace3a4dfdd6.xhtml),
    *Advanced Text Processing with Spark*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The components of a data-driven machine learning system
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The high-level components of our machine learning system are outlined in the
    following diagram. This diagram illustrates the machine learning pipeline from
    which we obtain data and in which we store data. We then transform it into a form
    that is usable as input to a machine learning model; train, test, and refine our
    model; and then, deploy the final model to our production system. The process
    is then repeated as new data is generated.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_03_002.png)'
  prefs: []
  type: TYPE_IMG
- en: A general machine-learning pipeline
  prefs: []
  type: TYPE_NORMAL
- en: Data ingestion and storage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first step in our machine learning pipeline will be taking in the data that
    we require for training our models. Like many other businesses, MovieStream's
    data is typically generated by user activity, other systems (this is commonly
    referred to as machine-generated data), and external sources (for example, the
    time of day and weather during a particular user's visit to the site).
  prefs: []
  type: TYPE_NORMAL
- en: This data can be ingested in various ways, for example, gathering user activity
    data from the browser and mobile application event logs or accessing external
    web APIs to collect data on geolocation or weather.
  prefs: []
  type: TYPE_NORMAL
- en: Once the collection mechanisms are in place, the data usually needs to be stored.
    This includes the raw data, data resulting from intermediate processing, and final
    model results to be used in production.
  prefs: []
  type: TYPE_NORMAL
- en: Data storage can be complex and involve a wide variety of systems, including
    HDFS, Amazon S3, and other filesystems; SQL databases such as MySQL or PostgreSQL;
    distributed NoSQL data stores such as HBase, Cassandra, and DynamoDB; and search
    engines such as Solr or Elasticsearch to stream data systems such as Kafka, Flume,
    or Amazon Kinesis.
  prefs: []
  type: TYPE_NORMAL
- en: For the purposes of this book, we will assume that the relevant data is available
    to us, so we will focus on the processing and modeling steps in the following
    pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Data cleansing and transformation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The majority of machine learning algorithms operate on features, which are typically
    numerical representations of the input variables that will be used for the model.
  prefs: []
  type: TYPE_NORMAL
- en: While we might want to spend the majority of our time exploring machine learning
    models, data collected via various systems and sources in the preceding ingestion
    step is, in most cases, in a raw form. For example, we might log user events such
    as details of when a user views the information page for a movie, when they watch
    a movie, or when they provide some other feedback. We might also collect external
    information such as the location of the user (as provided through their IP address,
    for example). These event logs will typically contain some combination of textual
    and numeric information about the event (and also, perhaps, other forms of data
    such as images or audio).
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to use this raw data in our models, in almost all cases, we need to
    perform preprocessing, which might include:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Filtering data**: Let''s assume that we want to create a model from a subset
    of raw data, such as only the most recent few months of activity data or only
    events that match certain criteria.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dealing with missing, incomplete, or corrupted data**: Many real-world datasets
    are incomplete in some way. This might include data that is missing (for example,
    due to a missing user input) or data that is incorrect or flawed (for example,
    due to an error in data ingestion or storage, technical issues or bugs, or software
    or hardware failure). We might need to filter out bad data or alternatively decide
    a method to fill in missing data points (such as using the average value from
    the dataset for missing points, for example).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dealing with potential anomalies, errors, and outliers**: Erroneous or outlier
    data might skew the results of model training, so we might wish to filter these
    cases out or use techniques that are able to deal with outliers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Joining together disparate data sources**: For example, we might need to
    match up the event data for each user with different internal data sources, such
    as user profiles, as well as external data, such as geolocation, weather, and
    economic data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Aggregating data**: Certain models might require input data that is aggregated
    in some way, such as computing the sum of a number of different event types per
    user.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Once we have performed initial preprocessing on our data, we often need to
    transform the data into a representation that is suitable for machine learning
    models. For many model types, this representation will take the form of a vector
    or matrix structure that contains numerical data. Common challenges during data
    transformation and feature extraction include:'
  prefs: []
  type: TYPE_NORMAL
- en: Taking categorical data (such as country for geolocation or category for a movie)
    and encoding it in a numerical representation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extracting useful features from text data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dealing with image or audio data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Converting numerical data into categorical data to reduce the number of values
    a variable can take on. An example of this is converting a variable for age into
    buckets (such as 25-35, 45-55, and so on).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transforming numerical features; for example, applying a log transformation
    to a numerical variable can help deal with variables that take on a very large
    range of values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Normalizing and standardizing numerical features ensures that all the different
    input variables for a model have a consistent scale. Many machine learning models
    require standardized input to work properly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature engineering, which is the process of combining or transforming the existing
    variables to create new features. For example, we can create a new variable that
    is the average of some other data, such as the average number of times a user
    watches a movie.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will cover all of these techniques through the examples in this book.
  prefs: []
  type: TYPE_NORMAL
- en: This data-cleansing, exploration, aggregation, and transformation steps can
    be carried out using both Spark's core API functions as well as the SparkSQL engine,
    not to mention other external Scala, Java, or Python libraries. We can take advantage
    of Spark's Hadoop compatibility to read data from and write data to the various
    storage systems mentioned earlier.
  prefs: []
  type: TYPE_NORMAL
- en: We can also leverage Spark streaming in case the streaming input is involved.
  prefs: []
  type: TYPE_NORMAL
- en: Model training and testing loop
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once we have our training data in a form that is suitable for our model, we
    can proceed with the model's training and testing phase. During this phase, we
    are primarily concerned with model selection. This can refer to choosing the best
    modeling approach for our task, or the best parameter settings for a given model.
    In fact, the term model selection often refers to both of these processes, as,
    in many cases, we might wish to try out various models and select the best performing
    model (with the best performing parameter settings for each model). It is also
    common to explore the application of combinations of different models (known as
    ensemble methods) in this phase.
  prefs: []
  type: TYPE_NORMAL
- en: This is typically a fairly straightforward process of running our chosen model
    on our training dataset and testing its performance on a test dataset (that is,
    a set of data that is held out for the evaluation of the model that the model
    has not seen in the training phase). This process is referred to as cross-validation.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, the model tends to overfit or doesn't converge fully depending on
    the type of the dataset and the number of Iterations used.
  prefs: []
  type: TYPE_NORMAL
- en: Using Ensemble methods such as Gradient Boosted Trees and Random forest are
    techniques used in ML and Spark to avoid overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: However, due to the large scale of data we are typically working with, it is
    often useful to carry out this initial train-test loop on a smaller representative
    sample of our full dataset or perform model selection using parallel methods where
    possible.
  prefs: []
  type: TYPE_NORMAL
- en: For this part of the pipeline, Spark's built-in machine learning library, MLlib,
    is a perfect fit. We will focus most of our attention in this book on the model
    training, evaluation, and cross-validation steps for various machine learning
    techniques, using MLlib and Spark's core features.
  prefs: []
  type: TYPE_NORMAL
- en: Model deployment and integration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once we have found the optimal train-test loop, we might still face the task
    of deploying the model to a production system so that it can be used to make actionable
    predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Usually, this process involves exporting the trained model to a central data
    store from where the production-serving system can obtain the latest version.
    Thus, the live system *refreshes* the model periodically as a new model is trained.
  prefs: []
  type: TYPE_NORMAL
- en: Model monitoring and feedback
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is critically important to monitor the performance of our machine learning
    system in production. Once we deploy our optimal-trained model, we wish to understand
    how it is doing in the "wild". Is it performing as we expect on new, unseen data?
    Is its accuracy good enough? The reality is, regardless of how much model selection
    and tuning we try to do in the earlier phases, the only way to measure true performance
    is to observe what happens in our production system.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the batch mode model creation, there are also models built with
    Spark streaming which are real-time in nature.
  prefs: []
  type: TYPE_NORMAL
- en: Also, bear in mind that model accuracy and predictive performance is only one
    aspect of a real-world system. Usually, we are concerned with other metrics related
    to business performance (for example, revenue and profitability) or user experience
    (such as the time spent on our site and how active our users are overall). In
    most cases, we cannot easily map model-predictive performance to these business
    metrics. The accuracy of a recommendation or targeting system might be important,
    but it relates only indirectly to the true metrics we are concerned about, namely,
    whether we are improving user experience, activity, and ultimately, revenue.
  prefs: []
  type: TYPE_NORMAL
- en: So, in real-world systems, we should monitor both model-accuracy metrics as
    well as business metrics. If possible, we should be able to experiment with different
    models running in production to allow us to optimize against these business metrics
    by making changes to the models. This is often done using live split tests. However,
    doing this correctly is not an easy task, and live testing and experimentation
    is expensive, in the sense that mistakes, poor performance, and using baseline
    models (they provide a control against which we test our production models) can
    negatively impact user experience and revenue.
  prefs: []
  type: TYPE_NORMAL
- en: Another important aspect of this phase is **model feedback**. This is the process
    where the predictions of our model feed through into user behavior; this, in turn,
    feeds through into our model. In a real-world system, our models are essentially
    influencing their own future training data by impacting decision-making and potential
    user behavior.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if we have deployed a recommendation system, then, by making recommendations,
    we might be influencing user behavior because we are only allowing users a limited
    selection of choices. We hope that this selection is relevant for our model; however,
    this feedback loop, in turn, can influence our model's training data. This, in
    turn, feeds back into real-world performance. It is possible to get into an ever-narrowing
    feedback loop; ultimately, this can negatively affect both model accuracy and
    our important business metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, there are mechanisms by which we can try to limit the potential
    negative impact of this feedback loop. These include providing some unbiased training
    data by having a small portion of data coming from users who are not exposed to
    our models or by being principled in the way we balance exploration, to learn
    more about our data, and exploitation, to use what we have learned to improve
    our system's performance.
  prefs: []
  type: TYPE_NORMAL
- en: We will briefly cover in [Chapter 11](80797b8a-39cd-42af-861b-401da1bf728d.xhtml),
    *Real-time Machine Learning with Spark Streaming*.
  prefs: []
  type: TYPE_NORMAL
- en: Batch versus real time
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous sections, we outlined the common batch processing approach,
    where the model is retrained using all data or a subset of all data, periodically.
    As the preceding pipeline takes some time to complete, it might not be possible
    to use this approach to update models immediately as new data arrives.
  prefs: []
  type: TYPE_NORMAL
- en: While we will be mostly covering batch machine learning approaches in this book,
    there is a class of machine learning algorithms known as **online learning**;
    they update immediately as new data is fed into the model, thus enabling a real-time
    system. A common example is an online-optimization algorithm for a linear model,
    such as stochastic gradient descent. We can learn this algorithm using examples.
    The advantages of these methods are that the system can react very quickly to
    new information and also that the system can adapt to changes in the underlying
    behavior (that is, if the characteristics and distribution of the input data are
    changing over time, which is almost always the case in real-world situations).
  prefs: []
  type: TYPE_NORMAL
- en: However, online-learning models come with their own unique challenges in a production
    context. For example, it might be difficult to ingest and transform data in real-time.
    It can also be complex to properly perform model selection in a purely online
    setting. The latency of the online training and the model selection and deployment
    phases might be too high for true real-time requirements (for example, in online
    advertising, latency requirements are measured in single-digit milliseconds).
    Finally, batch-oriented frameworks might make it awkward to handle real-time processes
    of a streaming nature.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, Spark's real-time stream processing is a good potential fit for
    real-time machine learning workflows. We will explore Spark Streaming and online
    learning in [Chapter 11](80797b8a-39cd-42af-861b-401da1bf728d.xhtml), *Real-time
    Machine Learning with Spark Streaming*
  prefs: []
  type: TYPE_NORMAL
- en: Due to the complexities inherent in a true real-time machine learning system,
    in practice, many systems target near real-time operations. This is essentially
    a hybrid approach where models are not necessarily updated immediately as new
    data arrives; instead, the new data is collected into mini batches of a small
    set of training data. These mini batches can be fed to an online-learning algorithm.
    In many cases, this approach is combined with a periodic batch process that might
    recompute the model on the entire dataset and perform more complex processing
    and model selection. This can help ensure that the real-time model does not degrade
    over time.
  prefs: []
  type: TYPE_NORMAL
- en: Another similar approach involves making approximate updates to a more complex
    model as new data arrives while recomputing the entire model in a batch process
    periodically. In this way, the model can learn from new data, with a short delay
    (usually measured in seconds or, perhaps, a few minutes), but will become more
    and more inaccurate over time due to the approximation applied. The periodic recomputation
    takes care of this by retraining the model on all available data.
  prefs: []
  type: TYPE_NORMAL
- en: Data Pipeline in Apache Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we have seen the movie lens use case, it is quite common to run a sequence
    of machine learning algorithms to process and learn from data. Another example
    is a simple text document processing workflow, which can include several stages:'
  prefs: []
  type: TYPE_NORMAL
- en: Split the document's text into words
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Convert the document's words into a numerical feature vector
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learn a prediction model from feature vectors and labels
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark MLlib represents such a workflow as a Pipeline; it consists of Pipeline
    Stages in sequence (Transformers and Estimators), which are run in a specific
    order.
  prefs: []
  type: TYPE_NORMAL
- en: A Pipeline is specified as a sequence of stages. Each stage is a Transformer
    or an Estimator. Transform converts one data frame into another. Estimator, on
    the other hand, is a learning algorithm. Pipeline stages are run in order, and
    the input DataFrame is transformed as it passes through each stage.
  prefs: []
  type: TYPE_NORMAL
- en: In Transformer stages, the `transform()` method is called on the DataFrame.
    For Estimator stages, the `fit()` method is called to produce a Transformer (which
    becomes part of the PipelineModel or fitted Pipeline). The transformer's `transform()`
    method is executed on the DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: An architecture for a machine learning system
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have explored how our machine learning system might work in the
    context of MovieStream, we can outline a possible architecture for our system:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_03_003.png)'
  prefs: []
  type: TYPE_IMG
- en: MovieStream's future architecture
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can see, our system incorporates the machine learning pipeline outlined
    in the preceding diagram; this system also includes:'
  prefs: []
  type: TYPE_NORMAL
- en: Collecting data about users, their behavior, and our content titles
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transforming this data into features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training our models, including our training-testing and model-selection phases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying the trained models to both our live model-serving system as well as
    using these models for offline processes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feeding back the model results into the MovieStream website through recommendation
    and targeting pages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feeding back the model results into MovieStream's personalized marketing channels
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the offline models to provide tools to MovieStream's various teams to
    better understand user behavior, characteristics of the content catalogue, and
    drivers of revenue for the business
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next section, we digress a little from Movie Stream and give an overview
    of MLlib-Spark's machine learning module.
  prefs: []
  type: TYPE_NORMAL
- en: Spark MLlib
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apache Spark is an open-source platform for large dataset processing. It is
    well suited for iterative machine learning tasks as it leverages in-memory data
    structures such as RDDs. MLlib is Spark's machine learning library. MLlib provides
    functionality for various learning algorithms-supervised and unsupervised. It
    includes various statistical and linear algebra optimizations. It is shipped along
    with Apache Spark and hence saves on installation headaches like some other libraries.
    MLlib supports several higher languages such as Scala, Java, Python and R. It
    also provides a high-level API to build machine-learning pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: MLlib's integration with Spark has quite a few benefits. Spark is designed for
    iterative computation cycles; it enables efficient implementation platform for
    large machine learning algorithms, as these algorithms are themselves iterative.
  prefs: []
  type: TYPE_NORMAL
- en: Any improvement in Spark's data structures results in direct gains for MLlib.
    Spark's large community contributions have helped bring new algorithms to MLlib
    faster.
  prefs: []
  type: TYPE_NORMAL
- en: Spark also has other APIs such as Pipeline APIs GraphX, which can be used in
    conjunction with MLlib; it makes building interesting use cases on top of MLlib
    easier.
  prefs: []
  type: TYPE_NORMAL
- en: Performance improvements in Spark ML over Spark MLlib
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spark 2.0 uses Tungsten Engine, which is built using ideas of modern compilers
    and MPP databases. It emits optimized bytecode at runtime, which collapses the
    query into a single function. Hence, there is no need for virtual function calls.
    It also uses CPU registers to store intermediate data. This technique has been
    called whole stage code generation.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_03_004.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Reference : https://databricks.com/blog/2016/05/11/apache-spark-2-0-technical-preview-easier-faster-and-smarter.htmlSource:
    https://databricks.com/blog/2016/05/11/apache-spark-2-0-technical-preview-easier-faster-and-smarter.html'
  prefs: []
  type: TYPE_NORMAL
- en: 'The upcoming table and graph show single function improvements between Spark
    1.6 and Spark 2.0:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_03_005.png)'
  prefs: []
  type: TYPE_IMG
- en: Chart comparing Performance improvements in Single line functions between Spark
    1.6 and Spark 2.0
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_03_006.png)'
  prefs: []
  type: TYPE_IMG
- en: Table comparing Performance improvements in Single line functions between Spark
    1.6 and Spark 2.0.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing algorithms supported by MLlib
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we look at various algorithms supported by MLlib versions.
  prefs: []
  type: TYPE_NORMAL
- en: Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In 1.6, there are over 10 algorithms supported for classification, whereas when
    Spark MLversion 1.0 was announced, only 3 algorithms were supported.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_03_007.png)'
  prefs: []
  type: TYPE_IMG
- en: Clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There has been quite a bit of investment in Clustering algorithms, moving from
    1 algo support in 1.0.0 to supporting 6 implementations in 1.6.0.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_03_008.png)'
  prefs: []
  type: TYPE_IMG
- en: Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Traditionally, regression was not the main area of focus but has become of late
    with 3-4 new algorithms from 1.2.0 version to 1.3.0 version.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_03_009.png)'
  prefs: []
  type: TYPE_IMG
- en: MLlib supported methods and developer APIs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: MLlib provides fast and distributed implementations of learning algorithms,
    including various linear models, Naive Bayes, SVM, and Ensembles of Decision Trees
    (also known as Random Forests) for classification and regression problems, alternating.
  prefs: []
  type: TYPE_NORMAL
- en: '**Least Squares** (explicit and implicit feedback) are used for collaborative
    filtering. It also supports k-means clustering and **principal component analysis**
    (**PCA**) for clustering and dimensionality reduction.'
  prefs: []
  type: TYPE_NORMAL
- en: The library provides some low-level primitives and basic utilities for convex
    optimization ([http://spark.apache.org/docs/latest/mllib-optimization.html](http://spark.apache.org/docs/latest/mllib-optimization.html)),
    distributed linear algebra (with support for Vectors and Matrix), statistical
    analysis (using Breeze and also native functions), and feature extraction, and
    supports various I/O formats, including native support for LIBSVM format.
  prefs: []
  type: TYPE_NORMAL
- en: 'It also supports data integration via Spark SQL as well as PMML ([https://en.wikipedia.org/wiki/Predictive_Model_Markup_Language](https://en.wikipedia.org/wiki/Predictive_Model_Markup_Language))
    (Guazzelli et al., 2009). You can find more information about PMML support at
    this link: [https://spark.apache.org/docs/1.6.0/mllib-pmml-model-export.html](https://spark.apache.org/docs/1.6.0/mllib-pmml-model-export.html).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Algorithmic Optimizations** involves MLlib that includes many optimizations
    to support efficient distributed learning and prediction.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The ALS algorithm for recommendation makes use of blocking to reduce JVM garbage
    collection overhead and to utilize higher-level linear algebra operations. Decision
    trees use ideas from the PLANET project (reference: [http://dl.acm.org/citation.cfm?id=1687569](http://dl.acm.org/citation.cfm?id=1687569)),
    such as data-dependent feature discretization to reduce communication costs, and
    tree ensembles parallelize learning both within trees and across trees.'
  prefs: []
  type: TYPE_NORMAL
- en: Generalized linear models are learned using optimization algorithms, which parallelize
    gradient computation, using fast C++-based linear algebra libraries for worker.
  prefs: []
  type: TYPE_NORMAL
- en: '**Computations**. Algorithms benefit from efficient communication primitives.
    In particular, tree-structured aggregation prevents the driver from being a bottleneck.'
  prefs: []
  type: TYPE_NORMAL
- en: Model updates are combined partially on a small set of executors. These are
    then sent to the driver. This implementation reduces the load the driver has to
    handle. Tests showed that these functions reduce the aggregation time by an order
    of magnitude, especially on datasets with a large number of partitions.
  prefs: []
  type: TYPE_NORMAL
- en: '(Reference: [https://databricks.com/blog/2014/09/22/spark-1-1-mllib-performance-improvements.html](https://databricks.com/blog/2014/09/22/spark-1-1-mllib-performance-improvements.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pipeline API** includes practical machine learning pipelines that often involve
    a sequence of data preprocessing, feature extraction, model fitting, and validation
    stages.'
  prefs: []
  type: TYPE_NORMAL
- en: Most of the machine learning libraries do not provide native support for the
    diverse set of functionalities for pipeline construction. When handling large-scale
    datasets, the process of wiring together an end-to-end pipeline is both labor-intensive
    and expensive from the perspective of network overheads.
  prefs: []
  type: TYPE_NORMAL
- en: '**Leveraging Spark''s ecosystem**: MLlib includes a package aimed to address
    these concerns.'
  prefs: []
  type: TYPE_NORMAL
- en: The `spark.ml` package eases the development and tuning of multistage learning
    pipelines by providing a uniform set of high-level APIs ([http://arxiv.org/pdf/1505.06807.pdf](http://arxiv.org/pdf/1505.06807.pdf)).
    It includes APIs that enable users to swap out a standard learning approach in
    place of their specialized algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Spark Integration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: MLlib benefits from the components within the Spark ecosystem. Spark core provides
    an execution engine with over 80 operators for transforming data (data cleaning
    and featurization).
  prefs: []
  type: TYPE_NORMAL
- en: MLlib uses other high-level libraries packaged with Spark-like Spark SQL. It
    provides integration data functionality, SQL, and structured data processing,
    which simplifies data cleaning and preprocessing. It supports the DataFrame abstraction,
    which is fundamental to the `spark.ml` package.
  prefs: []
  type: TYPE_NORMAL
- en: '**GraphX** ([https://www.usenix.org/system/files/conference/osdi14/osdi14-paper-gonzalez.pdf](https://www.usenix.org/system/files/conference/osdi14/osdi14-paper-gonzalez.pdf))
    supports large-scale graph processing and has a powerful API for implementing
    learning algorithms that can be viewed as large sparse graph problems, for example,
    LDA.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Spark Streaming** ([https://www.cs.berkeley.edu/~matei/papers/2013/sosp_spark_streaming.pdf](https://www.cs.berkeley.edu/~matei/papers/2013/sosp_spark_streaming.pdf))
    allows processing of real-time data streams and enabling the development of learning
    algorithms which are online, as in Freeman (2015). We will cover streaming in
    some of the later chapters of this book.'
  prefs: []
  type: TYPE_NORMAL
- en: MLlib vision
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: MLlib's vision is to provide a scalable machine learning platform, which can
    handle large datasets at scale and fastest processing time as compared to the
    existing systems such as Hadoop.
  prefs: []
  type: TYPE_NORMAL
- en: It also strives to provide support for as many algorithms as possible in the
    domain of supervised and unsupervised learning classification, regression such
    as Classification, Regression, and clustering.
  prefs: []
  type: TYPE_NORMAL
- en: MLlib versions compared
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will compare various versions of MLlib and new functionality,
    which has been added.
  prefs: []
  type: TYPE_NORMAL
- en: Spark 1.6 to 2.0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The DataFrame-based API will be the primary API.
  prefs: []
  type: TYPE_NORMAL
- en: The RDD-based API is entering maintenance mode. The MLlib guide ([http://spark.apache.org/docs/2.0.0/ml-guide.html](http://spark.apache.org/docs/2.0.0/ml-guide.html))
    provides more details.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the new features introduced in Spark 2.0:'
  prefs: []
  type: TYPE_NORMAL
- en: '**ML persistence**: The DataFrames-based API provides support for saving and
    loading ML models and Pipelines in Scala, Java, Python, and R'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MLlib in R**: SparkR offers MLlib APIs for generalized linear models, naive
    Bayes, k-means clustering, and survival regression in this release'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Python**: PySpark in 2.0 supports new MLlib algorithms, LDA, Generalized
    Linear Regression, Gaussian Mixture Model, among others'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Algorithms added to DataFrames-based API are GMM, Bisecting K-Means clustering,
    MaxAbsScaler feature transformer.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learnt about the components that are inherent in a data-driven,
    automated machine learning system. We also outlined how a possible high-level
    architecture for such a system might look in a real-world situation. We also got
    an overview of MLlib-Spark's machine learning library-compared to other machine
    learning implementations from a performance perspective. In the end, we looked
    at new features in various versions of Spark starting from Spark 1.6 to Spark
    2.0.
  prefs: []
  type: TYPE_NORMAL
- en: In next chapter, we shall discuss how to obtain publicly-available datasets
    for common machine learning tasks. We will also explore general concepts to process,
    clean, and transform data so that it can be used to train a machine learning model.
  prefs: []
  type: TYPE_NORMAL
