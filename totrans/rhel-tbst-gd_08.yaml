- en: Chapter 8. Hardware Troubleshooting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the last chapter, we identified that the filesystems on our NFS were mounted
    as **Read-Only**. In order to identify the cause, we performed quite a bit of
    troubleshooting around NFS and filesystems. We used commands such as `showmount`
    to see what NFS shares are available and the `mount` command to show the mounted
    filesystems.
  prefs: []
  type: TYPE_NORMAL
- en: Once we identified the issue, we were able to use the `fsck` command to perform
    a filesystem check and recover the filesystems.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will continue down the path from [Chapter 7](part0044_split_000.html#19UOO2-8ae10833f0c4428b9e1482c7fee089b4
    "Chapter 7. Filesystem Errors and Recovery"), *FileSystem Errors and Recovery*
    and investigate a hardware device failure. This chapter will cover many of the
    log files and tools necessary to determine not only whether a hardware failure
    has occurred, but why it has occurred as well.
  prefs: []
  type: TYPE_NORMAL
- en: Starting with a log entry
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In [Chapter 7](part0044_split_000.html#19UOO2-8ae10833f0c4428b9e1482c7fee089b4
    "Chapter 7. Filesystem Errors and Recovery"), *FileSystem Errors and Recovery*
    while looking through the `/var/log/messages` log file to identify issues with
    the NFS servers filesystems, we noticed the following messages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The preceding messages indicate that the RAID device `/dev/md127` had a failure.
    Since the previous chapter was solely focused on the issues with the filesystem
    itself, we did not investigate the RAID device failure further. In this chapter,
    we will investigate to determine the cause and resolution.
  prefs: []
  type: TYPE_NORMAL
- en: To start the investigation, we should first review the original log messages
    as these can tell us quite a bit about the state of the RAID device.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a start, let''s break down the messages into smaller sections as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The first log message is actually quite telling. The first key piece of information
    shown is the RAID device that the message is about `(md/raid1:md127)`.
  prefs: []
  type: TYPE_NORMAL
- en: By the name of this device, we already know quite a bit. The first thing we
    know is that this RAID device is created by Linux's software raid system **multiple
    device driver** (**md**). This system allows Linux to take two independent disks
    and apply a RAID to them.
  prefs: []
  type: TYPE_NORMAL
- en: Since we will be working primarily with a RAID in this chapter, we should first
    understand what RAID is and how it works.
  prefs: []
  type: TYPE_NORMAL
- en: What is a RAID?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Redundant Array of Independent Disks** (**RAID**) is often either a software-
    or hardware-based system that allows users to take multiple disks and use them
    as one device. The RAID can be configured in multiple ways, allowing for either
    greater data redundancy or performance.'
  prefs: []
  type: TYPE_NORMAL
- en: This configuration is commonly referred to as a RAID level. The different types
    of RAID levels provide different functionality to get a better idea of the RAID
    levels. Let's explore a few that are commonly used.
  prefs: []
  type: TYPE_NORMAL
- en: RAID 0 – striping
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: RAID 0 is one of the simplest RAID levels to understand. The way RAID 0 works
    is by taking multiple disks and combining them to act as one. When data is written
    to the RAID device, the data is split and parts are written on each disk. To understand
    this better, let's put together a simple scenario.
  prefs: []
  type: TYPE_NORMAL
- en: If we had a simple RAID 0 device that consisted of five 500 GB drives, our RAID
    device would be the size of all the five drives together—2500 GB or 2.5 TB. If
    we were to write a 50 MB file to the RAID device, 10 MB of the file's data would
    be written to each disk at the same time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This process is commonly referred to as **striping**. In the same context, when
    that 50 MB file is read from the RAID device, the read request will be processed
    by each disk at the same time as well.
  prefs: []
  type: TYPE_NORMAL
- en: The ability to split a file and process parts of it to each disk at the same
    time provides better performance of the write or read requests. In fact, because
    we have five disks, the requests are faster by a multiple of 5.
  prefs: []
  type: TYPE_NORMAL
- en: A simple analogy to this would be if you had five people building a wall at
    equal speed, they would be five times faster than a single person building the
    same wall.
  prefs: []
  type: TYPE_NORMAL
- en: While RAID 0 provides performance, it does not provide any data protection.
    If a single drive in this RAID fails, the data from that drive is not available
    and such a failure could result in complete data loss with RAID 0.
  prefs: []
  type: TYPE_NORMAL
- en: RAID 1 – mirroring
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: RAID 1 is another simple RAID level. Unlike RAID 0 where the drives are combined,
    in RAID 1 the drives are mirrored. RAID 1 generally consists of two or more drives.
    When data is written to the RAID device, the data is written to each device in
    its entirety.
  prefs: []
  type: TYPE_NORMAL
- en: 'This process is referred to as **mirroring**, since the data is essentially
    mirrored on all drives:'
  prefs: []
  type: TYPE_NORMAL
- en: Using the same scenario as before, if we had five 500 GB disk drives in a RAID
    1 configuration, the total disk size would be 500 GB. When we write the same 50
    MB file to the RAID device, each drive will get its own copy of that 50 MB file.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This also means that the write request will only be as fast as the slowest drive
    in the RAID. With RAID 1, every drive must complete the write request before it
    is considered complete.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Read requests, however, can be served by any one of the RAID 1 drives. Because
    of this, a RAID 1 can sometimes provide read requests faster, as each request
    can be performed by a different drive in the RAID.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RAID 1 provides the highest level of data resiliency, as it only requires one
    disk drive to remain active during failures. Using our five-disk scenario, we
    could lose four of the five disks and still rebuild and use the RAID. This is
    the reason why RAID 1 should be used when data protection is more important than
    disk performance.
  prefs: []
  type: TYPE_NORMAL
- en: RAID 5 – striping with distributed parity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**RAID 5** is an example of a difficult-to-understand RAID level. RAID 5 works
    by striping data across multiple disks such as RAID 0, but it also includes parity.
    Parity data is special data that is generated by performing an exclusive OR operation
    on the data written to the RAID device. The resulting data can be used to rebuild
    the missing data from another drive.'
  prefs: []
  type: TYPE_NORMAL
- en: Using the same example as we did earlier, where we have five 500 GB hard drives
    in a RAID 5 configuration, if we were to yet again write a 50 MB file, each disk
    will receive 10 MB of data; this is exactly like RAID 0\. However, unlike RAID
    0, parity data is also written to each disk. Because of the additional parity
    data, the total data size available to the RAID is the total of the four drives,
    with one drive's worth of data allocated to parity. In our case, this would mean
    2 TB of available disk space with 500 GB used for parity.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Often, there is a misconception that the parity data is written to a dedicated
    drive with RAID 5\. This is not the case. It is simply that the parity data size
    is a full disk's worth of space. This data, however, is distributed against all
    disks.
  prefs: []
  type: TYPE_NORMAL
- en: A reason to use RAID 5 over RAID 0 is the fact that it is possible for the data
    to be rebuilt if a single drive fails. The only problem with RAID 5 is if two
    drives fail, the RAID cannot be rebuilt and may result in data loss.
  prefs: []
  type: TYPE_NORMAL
- en: RAID 6 – striping with double distributed parity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**RAID 6** is essentially the same type of RAID as RAID 5; however, the parity
    data is doubled. By doubling the parity data, the RAID can survive up to two disk
    failures. Since the parity is doubled if we were to take five 500 GB hard drives
    and place them into a RAID 6 configuration, the available disk space would be
    1.5 TB, the sum of 3 drives; the other 1 TB of data space would be occupied by
    two sets of parity data.'
  prefs: []
  type: TYPE_NORMAL
- en: RAID 10 – mirrored and striped
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**RAID 10** (commonly known as RAID 1 + 0) is another very common RAID level.
    RAID 10 is essentially a combination of both RAID 1 and RAID 0\. With RAID 10
    each disk has a mirror and data is striped across all of the mirrored drives.
    To explain this we will use a similar example as above; however, we will do this
    with six 500 GB drives.'
  prefs: []
  type: TYPE_NORMAL
- en: If we were to write a 30 MB file, it will be broken into 10 MB chunks and striped
    to three RAID devices. These RAID devices are RAID 1 mirrors. Essentially, a RAID
    10 is numerous RAID 1 devices striped together in a RAID 0 configuration.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The RAID 10 configuration is a good balance between performance and data protection.
    In order for a complete failure to occur, both sides of a mirror must fail; this
    means 2 sides of a RAID 1.
  prefs: []
  type: TYPE_NORMAL
- en: Considering the number of disks in the RAID the chances of this are less likely
    than those of RAID 5\. From a performance standpoint, RAID 10 still benefits from
    the striping methodology and is able to write different chunks of a single file
    to each disk, by increasing the write speed.
  prefs: []
  type: TYPE_NORMAL
- en: RAID 10 also benefits from having two disks with the same data; as with RAID
    1, when a read request is made either disk may serve that request allowing for
    concurrent read requests to be handled by each disk independently.
  prefs: []
  type: TYPE_NORMAL
- en: The downside to RAID 10 is while it often can meet or exceed the performance
    of RAID 5 it often takes more hardware to do this as each disk is mirrored and
    you lose half of the total disk space to the RAID.
  prefs: []
  type: TYPE_NORMAL
- en: With our preceding example, our usable space for six 500 GB drives in a RAID
    10 configuration would be 1.5 TB. Simply put, it is 50 percent of our disk capacity.
    This same capacity is available with 4 drives for RAID 5.
  prefs: []
  type: TYPE_NORMAL
- en: Back to troubleshooting our RAID
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have a better understanding of RAID and the different configurations,
    let's go back to investigating our errors.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: From the preceding error, we can see that our RAID device is **md127**. We can
    also see that this device is a RAID 1 device (`md/raid1`). The message stating
    *Operation continuing on 1 devices* means the second part of the mirror is still
    operational.
  prefs: []
  type: TYPE_NORMAL
- en: The good thing is that, if both sides of the mirror were unavailable, the RAID
    would completely fail and result in worse issues.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we now know the RAID device affected, the type of RAID used, and even
    the hard disk that failed, we have quite a bit of information about this failure.
    If we continue looking at the log entries from `/var/log/messages`, we can find
    out even more:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding messages are interesting as they indicate that MD the Linux software
    RAID service attempted to recover the RAID:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'In the first line of this section of logs, it seems that the device `sdb1`
    was removed from the RAID:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The third line, however, is stating that the device `sdb1` has been re-added
    to the RAID or "**bound**" to the RAID.
  prefs: []
  type: TYPE_NORMAL
- en: 'The fourth and fifth lines show that the RAID started recovery steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: How RAID recovery works
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Earlier we discussed how various RAID levels are able to rebuild and recover
    data from lost devices. This happens either via parity data or mirrored data.
  prefs: []
  type: TYPE_NORMAL
- en: When a RAID device loses one of its drives and that drive is either replaced
    or re-added to the RAID, the RAID manager, whether it is a software or hardware
    RAID, will start rebuilding the data. The goal of this rebuild is to recreate
    the data that should be on the missing drive.
  prefs: []
  type: TYPE_NORMAL
- en: If the RAID is a mirrored RAID, the data from the available mirrored disk will
    be read and written to the replaced disk.
  prefs: []
  type: TYPE_NORMAL
- en: For parity-based RAIDs, the rebuild will be based on the surviving data that
    has been striped across the RAID and the parity data within the RAID.
  prefs: []
  type: TYPE_NORMAL
- en: During the rebuild process for parity-based RAIDs, any additional failure could
    result in a failed rebuild. With mirror-based RAIDs, the failure can occur on
    any disk as long as there is one full copy of the data being used for the rebuild.
  prefs: []
  type: TYPE_NORMAL
- en: 'At the end of our captured log messages, we can see that the rebuild was successful:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: It appears that the RAID `device /dev/md127` is healthy based on the end of
    the log messages found in the previous chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Checking the current RAID status
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While `/var/log/messages` is a great way to see what has happened on the server,
    it doesn't necessarily mean that those log messages are accurate with regard to
    the current state of the RAID.
  prefs: []
  type: TYPE_NORMAL
- en: In order to see the current status of the RAID devices, we can run a few commands.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first command we will use is the `mdadm` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The `mdadm` command is used to manage Linux MD based RAIDs. In the preceding
    command, we specified the flag `--detail` followed by a RAID device. This tells
    `mdadm` to print the details of the specified RAID device.
  prefs: []
  type: TYPE_NORMAL
- en: The `mdadm` command can perform more than just printing status; it can also
    be used to perform RAID activities such as creating, destroying, or modifying
    a RAID device.
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand the output of the `--detail` flag, let''s break down the output
    from above as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The first section tells us quite a bit about the RAID itself. Important items
    to note are the `Creation Time`, which in this case is `Wed April 15th` at 9:39
    A.M. This tells us when the RAID was first created.
  prefs: []
  type: TYPE_NORMAL
- en: The `Raid Level` is also noted, which as we saw in `/var/log/messages` is RAID
    1\. We can also see the `Array Size`, which tells us the total available disk
    space the RAID device will provide (524 MB) and the number of `Raid Devices` used
    in this RAID array, which in this case is two devices.
  prefs: []
  type: TYPE_NORMAL
- en: The number of devices that make up this RAID is important as it can help us
    understand the state of this RAID.
  prefs: []
  type: TYPE_NORMAL
- en: Since our RAID is made up of a total of two devices, if any one device fails,
    we know that our RAID will be at risk of a complete failure if the leftover disk
    is lost. If our RAID consisted of three devices, however, we would know that even
    the loss of two disks would not cause a complete RAID failure.
  prefs: []
  type: TYPE_NORMAL
- en: 'Just from the first half of the `mdadm` command, we can see quite a bit of
    information about this RAID. From the second half, we will find even more key
    information as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The `Update Time` is useful as it shows the last time this RAID changed status,
    whether that status change was the addition of a disk or a rebuild.
  prefs: []
  type: TYPE_NORMAL
- en: This timestamp can be useful, especially if we are trying to correlate it with
    log entries in `/var/log/messages` or other system events.
  prefs: []
  type: TYPE_NORMAL
- en: Another key piece of information is the `RAID Device State`, which, for our
    example, is clean, degraded. The degraded state means that while the RAID has
    a failed device, the RAID itself is still functional. Degraded simply means functional
    but suboptimal.
  prefs: []
  type: TYPE_NORMAL
- en: If our RAID device was actively rebuilding or recovering right now, we would
    also see those statuses listed.
  prefs: []
  type: TYPE_NORMAL
- en: Under the current state output, we can see four device categories that tell
    us about the hard disks used for this RAID. The first being `Active Devices`;
    this tells us the number of drives that are currently active in the RAID.
  prefs: []
  type: TYPE_NORMAL
- en: The second is `Working Devices`; this tells us the number of working drives.
    Often, the number of `Working Devices` and `Active Devices` will be the same.
  prefs: []
  type: TYPE_NORMAL
- en: The fourth item in this list is `Failed Devices`; this is the number of devices
    currently marked as failed. Even though our RAID currently has a failed device,
    this number is `0`. There is a valid reason for this, but we will cover that reason
    in a bit.
  prefs: []
  type: TYPE_NORMAL
- en: The last item in our list is the number of `Spare Devices`. In some RAID systems,
    you can create spare devices, which are used to rebuild a RAID in events such
    as drive failure.
  prefs: []
  type: TYPE_NORMAL
- en: These spare devices can come in handy, as the RAID system will usually automatically
    rebuild the RAID, which reduces the likelihood of complete failure of the RAID.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the final two lines of the output of `mdadm`, we can see information about
    the drives that make up the RAID:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: From the output, we can see that we have one disk device `/dev/sda1` that is
    currently in an active sync state. We can also see that another device has been
    removed from the RAID.
  prefs: []
  type: TYPE_NORMAL
- en: Summarizing the key information
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: From the output of `mdadm --detail`, we can see that `/dev/md127` is a RAID
    device that has a RAID level of 1 and is currently in a degraded state. We can
    see from the details that the degraded state is due to the fact that one of the
    drives that make up the RAID is currently removed.
  prefs: []
  type: TYPE_NORMAL
- en: Looking at md status with /proc/mdstat
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Another useful place to look for the current status of MD is `/proc/mdstat`;
    this file, like many files in `/proc`, is constantly updated by the kernel. If
    we use the `cat` command to read this file, we can take a quick look at this server''s
    current RAID status:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The contents of `/proc/mdstat` are somewhat cryptic, but if we break them down
    it contains quite a lot of information.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The first line `Personalities` tells us what RAID levels the kernel on this
    system currently supports. For our example, it is RAID 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The next set of lines is the current status of `/dev/md126`, another RAID device
    on this system that we have not yet looked at. These three lines can actually
    give us quite a bit of information about `md126`; in fact, they give us much the
    same information as `mdadm --detail` would tell us.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Within the first line itself, we can see the device name `md126`. We can see
    the current state of the RAID, which is active. We can also see the RAID level
    of this RAID device RAID 1\. Finally, we can also see the disk devices that make
    up this RAID; which, in our example, is only `sda2`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second line also contains key information as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Specifically, the last two values are the most useful for our current task,
    `[2/1]` shows how many disk devices are allocated to this RAID and how many are
    available. From the value in the example we can see that 2 drives are expected
    but only 1 drive is available.
  prefs: []
  type: TYPE_NORMAL
- en: The last value `[U_]` shows the current status of the drives that make up this
    RAID. The status U is for up and the "`_`" is for down.
  prefs: []
  type: TYPE_NORMAL
- en: In our example we can see that one disk device is up, and the other is down.
  prefs: []
  type: TYPE_NORMAL
- en: Given the above information, we were able to determine that the RAID device
    `/dev/md126` is currently in an active state; it is using RAID level 1 and currently
    has one of two disks unavailable.
  prefs: []
  type: TYPE_NORMAL
- en: If we keep looking through the `/proc/mdstat` file we can see a similar status
    for `md127`.
  prefs: []
  type: TYPE_NORMAL
- en: Using both /proc/mdstat and mdadm
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: After going through `/proc/mdstat` and `mdadm --detail` we can see that both
    provide similar information. From my experience I've found using both `mdstat`
    and `mdadm` can be useful. The `/proc/mdstat` file is generally where I go for
    a quick and easy snapshot of all RAID devices on the system, whereas the `mdadm`
    command is generally what I use for deeper RAID device details (details such as
    the number of spare drives, creation time and the last update time).
  prefs: []
  type: TYPE_NORMAL
- en: Identifying a bigger issue
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Earlier while using `mdadm` to look at the current status of `md127`, we could
    see that the RAID device `md127` had a disk removed from service. While looking
    through `/proc/mdstat` we discovered that there is another RAID device `/dev/md126`,
    and that too has a disk removed from service.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another interesting item that we can see is that the RAID device `/dev/md126`
    is a surviving disk: `/dev/sda1`. This is interesting because the surviving disk
    for `/dev/md127` is `/dev/sda2`. If we remember from the earlier chapter `/dev/sda1`
    and `/dev/sda2` are simply 2 partitions from the same physical disk. Given the
    fact that both RAID devices have a missing drive and that our logs state that
    `/dev/md127` had `/dev/sdb1` removed and re-added. It is likely that both `/dev/md127`
    and `/dev/md126` are using partitions from `/dev/sdb`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Since `/proc/mdstat` only has two statuses for RAID devices, up or down, we
    can confirm whether the second disk has actually been removed from `/dev/md126`
    using the `--detail` flag:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: From the output, we can see that the current status and configuration for `/dev/md126`
    is exactly the same as `/dev/md127`. Given this information, we can make an assumption
    that `/dev/md126` once had `/dev/sdb2` as part of its RAID.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we suspect that the problem may simply be that a single hard drive is
    having an issue, we need to validate if that is truly the case or not. The first
    step is to identify whether or not there truly is a `/dev/sdb` device; the fasted
    way to do this is to perform a directory listing in `/dev` with the `ls` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: We can see from the results of this `ls` command that there is in fact an `sdb`,
    `sdb1`, and `sdb2` device. Before going further, let's get a clearer understanding
    of `/dev`.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding /dev
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `/dev` directory is a special directory where the contents are created by
    the kernel at installation time. This directory contains special files that allow
    users or applications to interact with physical and sometimes logical devices.
  prefs: []
  type: TYPE_NORMAL
- en: If we look at the previous `ls` command's results, we can see that within the
    `/dev` directory there are several files that begin with `sd`.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous chapter, we learned that files that start with `sd` are actually
    seen as SCSI or SATA drives. In our case, we have both `/dev/sda` and `/dev/sdb`;
    this means, on this system, there are two physical SCSI or SATA drives.
  prefs: []
  type: TYPE_NORMAL
- en: The additional devices `/dev/sda1`, `/dev/sda2`, `/dev/sdb1`, and `/dev/sdb2`
    are simply partitions of those disks. In fact, with disk drives, a device name
    that ends with a numeric value is often a partition of another device, just as
    `/dev/sdb1` is a partition of `/dev/sdb`. While there are of course some exceptions
    to this rule, it is often safe to make this assumption when troubleshooting disk
    drives.
  prefs: []
  type: TYPE_NORMAL
- en: More than just disk drives
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `/dev/` directory contains far more than just disk drives. If we look in
    `/dev/`, we can actually see quite a few common devices.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: From the results of this `ls`, we can see that there are numerous files, directories,
    and symlinks in the `/dev` directory.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a list of common devices or directories that are useful to
    know and understand:'
  prefs: []
  type: TYPE_NORMAL
- en: '**/dev/cdrom**: This is often a symlink to the `cdrom` device. The CD-ROM''s
    actual device follows a naming convention similar to hard disks, where it starts
    with `sr` and is followed by the number of the device. We can see where the `/dev/cdrom`
    symlink points with the `ls` command:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '**/dev/console**: This device is not necessarily linked to a specific hardware
    device like `/dev/sda` or `/dev/sr0`. The console device is used for interacting
    with the systems console, which may or may not be an actual monitor.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**/dev/cpu**: This is actually a directory that contains additional directories
    within it for each CPU on the system. Within those directories is a `cpuid` file
    used to query information about the CPU:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '**/dev/md**: This is another directory that contains symlinks with user-friendly
    names that link to actual RAID devices. If we use `ls`, we can see the available
    RAID devices on this system:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '**/dev/random** and **/dev/urandom**: These two devices are used for generating
    random data. The `/dev/random` and `/dev/urandom` devices will both pull random
    data from the kernel''s entropy pool. One difference between these two is that
    when the system''s entropy count is low, the `/dev/random` device will wait until
    sufficient entropy has been re-added.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As we learned earlier, the `/dev/` directory has quite a few useful files and
    directories. Getting back to our original issue, however, we have identified that
    `/dev/sdb` exists and there are two partitions `/dev/sdb1` and `/dev/sdb2`.
  prefs: []
  type: TYPE_NORMAL
- en: We have not, however, identified whether `/dev/sdb` was originally part of the
    two RAID devices currently in a degraded state. To do this, we can utilize the
    `dmesg` facility.
  prefs: []
  type: TYPE_NORMAL
- en: Device messages with dmesg
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `dmesg` command is a great command for troubleshooting hardware issues.
    When a system initially boots, the kernel will identify the various hardware devices
    available to that system.
  prefs: []
  type: TYPE_NORMAL
- en: As the kernel identifies these devices, the information is written to the kernel's
    ring buffer. This ring buffer is essentially an internal log for the kernel. The
    `dmesg` command can be used to print this ring buffer.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is an example output from the `dmesg` command; in this example,
    we will use the `head` command to shorten the output to only the first 15 lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The reason we limited the output to just 15 lines is because the `dmesg` command
    will output quite a bit of data. To put it in perspective, we can run the command
    again, but this time send the output to `wc -l`, which will count the number of
    lines printed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, the `dmesg` command returns `597` lines. Reading all the 597
    lines of the kernel's ring buffer is not a quick process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since our goal was to find out information about `/dev/sdb`, we can run the
    `dmesg` command again, this time using the `grep` command to filter the output
    to `/dev/sdb` related information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: When executing the preceding example, the `–C` (context) flag was used to tell
    `grep` to include five lines of context to the output. Generally, when `grep`
    is run with no flags, only lines that contain the search string ( "`sdb`" , in
    this case) are printed. With the context flag set to five, the `grep` command
    will print 5 lines before and 5 lines after each line that contains the search
    string.
  prefs: []
  type: TYPE_NORMAL
- en: This use of `grep` allows us to see not only the lines that include the string
    `sdb`, but also the lines before and after, which may contain additional information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have this additional information, let''s break it down to understand
    what it is telling us better:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The preceding information seems to be standard information about `/dev/sdb`.
    We can see some basic information about `/dev/sda` and `/dev/sdb` from these messages.
  prefs: []
  type: TYPE_NORMAL
- en: 'One useful thing we can see from the preceding information is the size of these
    drives:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see that each drive is `8.58` GB in size. While the information is useful
    in general, it is not useful for our current situation. What is useful, however,
    is the last four lines from the preceding code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: These last four lines are showing the available partitions on both `/dev/sda`
    and `/dev/sdb` as well as a message stating that each disk has been `Attached`.
  prefs: []
  type: TYPE_NORMAL
- en: This information is quite useful as it tells us at the most basic level that
    these two drives are working. This is something that is in question for `/dev/sdb`,
    since we suspect that the RAID system has removed it from service.
  prefs: []
  type: TYPE_NORMAL
- en: So far, the `dmesg` command has already given us a bit of useful information;
    let's keep looking through the data to better understand these disks.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: The preceding three lines would be useful if we troubleshoot an issue with our
    CD-ROM device. However, for our disk issue, they are not useful and are only included
    due to grep's context being set to 5.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following lines, however, will tell us quite a bit about our disk drives:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The last section of dmesg''s output tells us quite a bit about the RAID devices
    and `/dev/sdb`. Since there is quite a bit of data, we will need to break this
    down to really understand it all:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding messages look very similar to the messages from `/dev/md127`;
    however, there are a few lines that were not present in the messages at `/dev/md127`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: If we look at these messages, we can see that `/dev/md126` attempted to use
    `/dev/sdb2` in the RAID array; however, it found the drive to be non-fresh. The
    non-fresh message is interesting as it might explain why `/dev/sdb` is not being
    included into the RAID devices.
  prefs: []
  type: TYPE_NORMAL
- en: Summarizing what dmesg has provided
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In a RAID set, each disk maintains an event count for every write request. The
    RAID uses this event count to ensure that each disk has received the appropriate
    amount of write requests. This allows the RAID to validate the consistency of
    the entire RAID.
  prefs: []
  type: TYPE_NORMAL
- en: When a RAID is restarting, the RAID manager will check the event count of each
    disk and ensure that they are consistent.
  prefs: []
  type: TYPE_NORMAL
- en: From the preceding messages, it appears that `/dev/sda2` may have a higher event
    count than `/dev/sdb2`. This would indicate that some writes occurred on `/dev/sda1`
    that never occurred on `/dev/sdb2`. This would be abnormal for a mirrored array
    and would indicate an issue with `/dev/sdb2`.
  prefs: []
  type: TYPE_NORMAL
- en: How do we check whether the event counts are different? With the `mdadm` command,
    we can display the event count for each disk device.
  prefs: []
  type: TYPE_NORMAL
- en: Using mdadm to examine the superblock
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To view the event count, we will use the `mdadm` command with the `--examine`
    flag to examine the disk devices:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: The `--examine` flag is very similar to `--detail` except where `--detail` is
    used to print the RAID device details. `--examine` is used to print RAID details
    from the individual disks that make up the RAID. The details that `--examine`
    prints are actually from the superblock details on the disk.
  prefs: []
  type: TYPE_NORMAL
- en: When the Linux RAID is utilizing a disk as part of a RAID device, the RAID system
    will reserve some space on the disk for a **superblock**. This superblock is simply
    used to store metadata about the disk and the RAID.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding command, we simply printed the RAID superblock information
    from `/dev/sda1`. To get a better understanding of the RAID superblock, let''s
    take a look at the details the `--examine` flag provides:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: The first section of this output provides quite a bit of useful information.
    The magic number, for instance, is used as a superblock header. This is a value
    that is used to indicate the beginning of the superblock.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another useful piece of information is the `Array UUID`. This is a unique identifier
    for the RAID that this disk belongs to. If we print the details of RAID `md127`,
    we can see that the Array UUID from `/dev/sda1` and the UUID from `md127` match:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: This can be useful when a device name has changed and you need to identify the
    disks that belong to a specific RAID. An example of this would be if someone accidently
    put drives into the wrong slot during hardware maintenance. If the drives still
    contain the UUID, it is possible to identify the RAID the misplaced drives belong
    to.
  prefs: []
  type: TYPE_NORMAL
- en: The bottom three lines `Creation Time`, `RAID Level`, and `RAID Devices` are
    also very useful when used with the output of `--detail`.
  prefs: []
  type: TYPE_NORMAL
- en: 'This second snippet of information is useful for determining information about
    the disk device:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The next section of information from the `--examine` output is very useful
    for our issue:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: In this section, we can see the `Events` information, which is showing the current
    event count value on this disk. We can also see the `Array State` value of `/dev/sda1`.
    The value of `A`. indicates that from the perspective of `/dev/sda1`, its mirrored
    partner is missing.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we examine the details of the superblock under `/dev/sdb1`, we will see
    an interesting difference in the `Array State` and `Events` values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: From the results, we have answered quite a few questions about `/dev/sdb1`.
  prefs: []
  type: TYPE_NORMAL
- en: The first question that we had is whether /`dev/sdb1` was part of a RAID or
    not. From the fact that this device has a RAID superblock and that information
    is printable via `mdadm`, we can safely say `yes`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'By looking at the `Array UUID`, we can also determine whether this device was
    a part of `/dev/md127`, as we suspected:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: As it appears, `/dev/sdb1` was at some point part of `/dev/md127`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The final question we need to answer is whether or not the `Events` values
    differ between `/dev/sda1` and `/dev/sdb1`. From the `--examine` information from
    `/dev/sda1`, we can see the event count is set to 60\. In the preceding code,
    `--examine` results from `/dev/sdb1`; we can see that the event count is much
    lower—`48`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Given the difference, we can safely say that `/dev/sdb1` is 12 events behind
    `/dev/sda1`. This is a very significant difference and a sensible reason for MD
    to reject adding `/dev/sdb1` to the RAID array.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an interesting side note, if we look at the `Array State` of `/dev/sdb1`,
    we can see that it still believes that it is an active disk in the `/dev/md127`
    array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'This is due to the fact that since the device is no longer part of the RAID,
    it is not being updated with the current status. We can see this in the update
    time as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: The `Update Time` for `/dev/sda1` is much more recent; thus, it should be trusted
    above the disk `/dev/sdb1`.
  prefs: []
  type: TYPE_NORMAL
- en: Checking /dev/sdb2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we know the reasons behind `/dev/sdb1` not being added to `/dev/md127`,
    we should determine whether the same situation is true for `/dev/sdb2` and `/dev/md126`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we already know that `/dev/sda2` is healthy and part of the `/dev/md126`
    array, we will focus solely on capturing its `Events` value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: The event count of `/dev/sda2` is quite high in comparison to `/dev/sda1`. From
    this, we can determine that `/dev/md126` is probably a very active RAID device.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have the event count, let''s take a look at the details of /`dev/sdb2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Again, from the fact that we were able to print superblock information from
    `/dev/sdb2`, we have determined that this device is in fact part of a RAID:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'If we compare the `Array UUID` of `/dev/sdb2` with the `UUID` of `/dev/md126`,
    we will also see that it was in fact part of that RAID array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'This answers our question as to whether `/dev/sdb2` was part of the `md126`
    RAID. If we look at the event count of `/dev/sdb2`, we can also answer the question
    as to why it is not currently part of that RAID:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: It appears that this device has missed write events that were sent to the `md126`
    RAID, given that the `Events` count from `/dev/sda2` was 7517 and the `Events`
    count from `/dev/sdb2` is 541.
  prefs: []
  type: TYPE_NORMAL
- en: What we have learned so far
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'From the troubleshooting steps that we have taken so far, we have collected
    quite a few key pieces of data. Let''s walk through what we have learned and what
    we can infer from these findings:'
  prefs: []
  type: TYPE_NORMAL
- en: On our system, we have two RAID devices.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the `mdadm` command and the contents of `/proc/mdstat`, we were able to
    determine that this system has two RAID devices`—/dev/md126` and `/dev/md127`.
  prefs: []
  type: TYPE_NORMAL
- en: Both RAID devices are a RAID 1 and missing a mirrored device.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With the `mdadm` command and output of `dmesg`, we were able to identify that
    both RAID devices are set up as a RAID 1 device. On top of that, we were also
    able to see that both RAID devices were missing a disk; both the missing devices
    were partitions from the `/dev/sdb` hard disk.
  prefs: []
  type: TYPE_NORMAL
- en: Both `/dev/sdb1` and `/dev/sdb2` have mismatched event counts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With the `mdadm` command, we were able to inspect the `superblock` details of
    the `/dev/sdb1` and `/dev/sdb2` devices. During this, we were able to see that
    the event counts for those devices are not matching the active partitions on `/dev/sda`.
  prefs: []
  type: TYPE_NORMAL
- en: For this reason, the RAID will not re-add the `/dev/sdb` devices to their respective
    RAID arrays.
  prefs: []
  type: TYPE_NORMAL
- en: The disk `/dev/sdb` seems to be functional.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While the RAID hasn't added `/dev/sdb1` or `/dev/sdb2` to their respective RAID
    arrays, it does not mean that the device `/dev/sdb` is faulty.
  prefs: []
  type: TYPE_NORMAL
- en: From the messages in `dmesg`, we did not see any errors for the `/dev/sdb` device
    itself. We also were able to use `mdadm` to inspect the partitions on those drives.
    From everything we have done so far, these drives appear to be functional.
  prefs: []
  type: TYPE_NORMAL
- en: Re-adding the drives to the arrays
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `/dev/sdb` disk seems to be functional and, outside the event count difference,
    we cannot see any reason the RAID would reject the devices. Our next step will
    be an attempt to re-add the removed devices to their RAID arrays.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first RAID we will attempt this with is `/dev/md127`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: The simplest way to re-add the drive is to simply use the `-a` (add) flag with
    `mdadm`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: The preceding command will tell `mdadm` to add the device `/dev/sdb1` to the
    RAID device `/dev/md127`. Since `/dev/sdb1` was already part of the RAID array,
    the MD service simply re-adds the disk and re-syncs the missing events from `/dev/sda1`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see this in action if we look at the RAID details with the `--detail`
    flag:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'From the preceding output, we can see a few differences from the earlier examples.
    One very important difference is the `Rebuild Status`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: With `mdadm --detail`, we can see the completion status of the drives re-syncing.
    If there were any errors in this process, we will also be able to see this. If
    we look at the bottom three lines, we can also see which devices are active and
    which are being rebuilt.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'After a few seconds, if we run `mdadm --detail` again, we should see that the
    RAID device has re-synced:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: Now we can see that both drives are listed as `active sync` state and that the
    RAID `State` is simply `clean`.
  prefs: []
  type: TYPE_NORMAL
- en: The preceding output is what a functional RAID 1 device should look like. At
    this point, we can consider the issue with `/dev/md127` resolved.
  prefs: []
  type: TYPE_NORMAL
- en: Adding a new disk device
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sometimes you will find yourself in a situation where your disk drive was actually
    faulty and the actual physical hardware must be replaced. In situations like this,
    once the partitions `/dev/sdb1` and `/dev/sdb2` are recreated, the device can
    simply be added to the RAID using the same steps as we used earlier.
  prefs: []
  type: TYPE_NORMAL
- en: When the command `mdadm <raid device> -a <disk device>` is executed, `mdadm`
    first checks to see whether the disk device was ever once part of the RAID.
  prefs: []
  type: TYPE_NORMAL
- en: It does this by reading the superblock information on the disk device. If the
    device was previously part of the RAID, it simply re-adds it and starts a rebuild
    to re-sync the drives.
  prefs: []
  type: TYPE_NORMAL
- en: If the disk device was never part of the RAID, it will be added as a spare device,
    and if the RAID is degraded, the spare device will be used to get the RAID back
    into a clean state.
  prefs: []
  type: TYPE_NORMAL
- en: When disks are not added cleanly
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In a previous work environment, when we replaced hard drives the drives were
    always quality tested before being used to replace faulty drives in production
    environments. Often, this quality testing involved creating partitions and adding
    those partitions to an existing RAID.
  prefs: []
  type: TYPE_NORMAL
- en: 'Because those devices already had a RAID superblock on them, `mdadm` would
    reject the addition of the devices to the RAID. It is possible to clear an existing
    RAID `superblock` using the `mdadm` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding command will tell `mdadm` to remove the RAID `superblock` information
    from the specified disk—in this case, `/dev/sdb2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: Using `--examine`, we can see that there is now no superblock on the device
    that had one before.
  prefs: []
  type: TYPE_NORMAL
- en: The `--zero-superblock` flag should be used with caution and only when the device
    data is no longer required. Once this superblock information is removed, the RAID
    sees this disk as a blank disk, and during any re-sync process, the existing data
    will be overwritten.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the superblock is removed, the same steps can be performed to add it to
    a RAID array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: Another way to watch the rebuild status
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Earlier we used `mdadm --detail` to show the rebuild status of `md127`. Another
    way to see this information is via `/proc/mdstat`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'After a bit, the RAID will finish re-syncing; now, both the RAID arrays are
    in a healthy status:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, [Chapter 7](part0044_split_000.html#19UOO2-8ae10833f0c4428b9e1482c7fee089b4
    "Chapter 7. Filesystem Errors and Recovery"), *FileSystem Errors and Recovery*
    we noticed a simple RAID failure message in our `/var/log/messages` log file.
    In this chapter, we used a `Data Collector` approach to investigate the cause
    of that failure message.
  prefs: []
  type: TYPE_NORMAL
- en: After investigating with the RAID management command `mdadm`, we found several
    RAID devices in a degraded state. Using `dmesg`, we were able to determine which
    hard drive devices were affected and that the disks at some point were removed
    from service. We also found that the disk **event counts** were mismatched, preventing
    the disks from being re-added automatically.
  prefs: []
  type: TYPE_NORMAL
- en: We verified that the devices were not physically faulty with `dmesg` and choose
    to re-add them to the RAID array.
  prefs: []
  type: TYPE_NORMAL
- en: While this chapter focused heavily on RAID and disk failures, both `/var/log/messages`
    and `dmesg` can be used to troubleshoot other device failures. For devices other
    than hard disks, however, the solution is often a simple replacement. Of course,
    like most things, this depends on the type of failure experienced.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will show how to troubleshoot custom user applications
    and the use of system tools to perform some advanced troubleshooting.
  prefs: []
  type: TYPE_NORMAL
