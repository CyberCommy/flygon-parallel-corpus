- en: The Future of Microservices and Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The software systems of tomorrow will be bigger, more complicated, will be able
    to handle more data, and will have an even bigger impact on our world. Just think
    about self-driving cars and ubiquitous robots. The human capacity to deal with
    complexity will not scale. That means that we will have to use a divide-and-conquer
    approach to build those complex software systems. Microservice-based architectures
    will continue to replace monoliths. Then, the challenge will shift into coordinating
    all those microservices into a coherent whole. This is where Kubernetes comes
    in as the standard orchestration solution.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will discuss the near future of microservices and Kubernetes.
    We will focus on the near future, because the innovation pace is amazing and trying
    to look much further ahead is futile. The long-term vision is that AI will probably
    advance to the point where most software development can be automated. At this
    point, human limits of handling complexity may not apply, and the software developed
    by AI will not be understandable by humans.
  prefs: []
  type: TYPE_NORMAL
- en: So, let's leave the far future alone and, in the spirit of being hands-on, discuss
    the emerging technologies, standards, and trends that will be relevant in the
    next few years, of which you may want to become aware.
  prefs: []
  type: TYPE_NORMAL
- en: 'The topics we will cover include some microservices themes, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Microservices versus serverless functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Microservices, containers, and orchestration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: gRPC/gRPC-Web
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: HTTP/3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GraphQL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will also discuss some Kubernetes themes:'
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes extensibility
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Service mesh integration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Serverless computing on Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubernetes and VMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cluster autoscaling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using operators
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's start with microservices.
  prefs: []
  type: TYPE_NORMAL
- en: The future of microservices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Microservices are the dominant approach for building modern, large-scale systems
    today. But, are they going to remain the top choice? Let's find out.
  prefs: []
  type: TYPE_NORMAL
- en: Microservices versus serverless functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the biggest questions regarding the future of microservices is whether
    serverless functions are going to make microservices obsolete. The answer is absolutely
    not. There are are many great benefits to serverless functions, as well as some
    serious limitations, such as cold start and time limits. Those limitations accumulate
    when you have functions invoking other functions. The execution time limits for
    function are very problematic if you want to apply retry logic with exponential
    backoff. A long-running service can keep local state and connections to data stores,
    and respond quicker to requests. But, for me, the biggest issue with serverless
    functions is that they represent a single function, which equates to a single
    endpoint of a service. I find a lot of value in the abstraction of a service that
    encapsulates a complete domain. If you try to port a service with 10 methods to
    serverless functions, then you'll run into management issues.
  prefs: []
  type: TYPE_NORMAL
- en: All those 10 functions will need access to the same data store, and multiple
    functions might need to be modified. All the functions will need similar access,
    configuration, and credentials to access various dependencies. Microservices will
    remain the backbone of large, cloud-native distributed systems. However, a lot
    of work will be offloaded to serverless functions, which makes sense. We will
    probably see some systems composed solely from serverless functions, but these
    will be forced and will have to make compromises.
  prefs: []
  type: TYPE_NORMAL
- en: Let's see the symbiotic relationship between microservices and containers.
  prefs: []
  type: TYPE_NORMAL
- en: Microservices, containers, and orchestration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When you break up a monolith into microservices, or start building a microservice-based
    system from scratch, you end up with a lot of services. You need to package, deploy,
    upgrade, and configure all those microservices. Containers address the packaging
    problem. Without containers, it is very difficult to scale microservice-based
    systems. As the number of microservices in the system grows orchestrating, the
    various containers and optimal scheduling requires a dedicated solution. This
    is where Kubernetes excels. The future of distributed systems is more microservices,
    packaged into more containers, which require Kubernetes to manage them. I say
    Kubernetes here, because in 2019, Kubernetes won the container orchestration wars.
  prefs: []
  type: TYPE_NORMAL
- en: Another aspect of many microservices is that they need to communicate with each
    other over a network. Where as in a monolith, most interactions are just function
    calls, in a microservice environment, a lot of interactions require hitting an
    endpoint or making a remote procedure call. Enter gRPC.
  prefs: []
  type: TYPE_NORMAL
- en: gRPC and gRPC-Web
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: gRPC is Google's remote procedure call protocol. Over the years, there were
    many RPC protocols. I still remember the days of CORBA and DCOM, and Java RMI.
    Fast-forward to the modern web, where REST beat SOAP to become the big gorilla
    in the arena of web APIs. But, these days, gRPC is beating REST. gRPC provides
    a contract-based model with strong typing, efficient payload based on protobuf,
    and is automatically generating client code. The combination is very powerful.
    The last refuge of REST was its ubiquity and the ease of calling REST APIs bearing
    JSON payloads from web applications running in the browser.
  prefs: []
  type: TYPE_NORMAL
- en: But, even this advantage is fading away. You could always put a REST-compatible
    gRPC gateway in front of your gRPC service, but I consider it a kludge. On the
    other hand, gRPC-web is a full-fledged JavaScript library that lets web applications
    simply invoke gRPC services. See [https://github.com/grpc/grpc-web/tree/master/packages/grpc-web](https://github.com/grpc/grpc-web/tree/master/packages/grpc-web).
  prefs: []
  type: TYPE_NORMAL
- en: GraphQL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If gRPC is the REST killer inside the cluster, then GraphQL is the REST killer
    at the edge. GraphQL is simply a superior paradigm. It gives the frontend developers
    a lot of freedom to evolve their designs. It decouples the needs of the frontend
    from the rigid APIs of the backend and serves as the perfect BFF (backends-for-frontends)
    pattern. See [https://samnewman.io/patterns/architectural/bff/](https://samnewman.io/patterns/architectural/bff/).
  prefs: []
  type: TYPE_NORMAL
- en: Similar to gRPC contracts, the structured schema of a GraphQL service is very
    enticing for large-scale systems.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, GraphQL solves the dreaded *N+1* problems of traditional REST APIs,
    where you first fetch a list of *N* resources from a REST endpoint and then you
    have to make *N* more calls (one per resource) to get related resources on each
    of the *N* items in the list.
  prefs: []
  type: TYPE_NORMAL
- en: I expect GraphQL to gain more and more mindshare as developers become more comfortable,
    awareness grows, tooling improves, and learning materials become more available.
  prefs: []
  type: TYPE_NORMAL
- en: HTTP/3 is coming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The web is built on HTTP. There is no question about it. It''s pretty amazing
    how well this protocol fared. Here is a quick recap: in 1991, Tim-Berneres-Lee
    proposes HTTP 0.9 to support his idea for a World Wide Web. In 1996, The HTTP
    Working Group publishes HTTP 1.0 as the informational RFC 1945 to enable the internet
    boom of the late 1990s.  In 1997, the first official RFC 2068 for HTTP 1.1 is
    published. In 1999, RFC 2616 adds a number of improvements to HTTP 1.1 and remains
    the dominant standard for two decades. In 2015, HTTP/2 is published, based on
    the SPDY protocol by Google, and all major browsers add support for it.'
  prefs: []
  type: TYPE_NORMAL
- en: 'gRPC is built on top of HTTP/2, which fixes a lot of issues with previous revisions
    of the HTTP and provides the following features:'
  prefs: []
  type: TYPE_NORMAL
- en: Binary framing and compression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multiplexing using streams (multiple requests on the same TCP connection)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Better flow control
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Server push
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: That sounds great. What will HTTP/3 give us? It offers the same feature set
    of HTTP/2\. However, HTTP/2 is based on TCP, which doesn't offer streams. That
    means that streams are implemented at the HTTP/2 level. HTTP/3 is based on QUIC,
    a reliable transport over UDP. The details are out of scope, but the bottom line
    is that HTTP/3 will have much better performance and is always secure.
  prefs: []
  type: TYPE_NORMAL
- en: It may still take a while for broad HTTP/3 adoption, because many enterprises
    block or rate limit UDP on their networks. However, the benefits are compelling
    and gRPC over HTTP/3 will have even a bigger edge in performance compared to REST
    APIs.
  prefs: []
  type: TYPE_NORMAL
- en: Those are the primary future trends that will impact microservices. Let's see
    what is next for Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: The future of Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes is here to stay. I will make a bold prediction and say that it will
    be around for decades. It is undeniably the current leader in the container orchestration
    space, but more importantly, it is designed in a super-extensible way. Any potential
    improvement can be built on top of the nice building blocks that Kubernetes provides
    (for example, service mesh) or replace those building blocks (such as network
    plugins, storage plugins, and custom schedulers). It is hard to imagine a brand
    new platform that will make Kubernetes obsolete, as opposed to improving and integrating
    it.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, the industry momentum behind Kubernetes and the way it is developed
    in the open and managed by the CNCF is inspiring. Even though it originated from
    Google, there is no sentiment that it is Google's project. It is perceived as
    a true open source project that benefits everyone.
  prefs: []
  type: TYPE_NORMAL
- en: Now, consider that Kubernetes caters to the needs of the entire spectrum, from
    hobbyists playing with local Kubernetes on their laptops, through developers,
    testing locally or in the cloud, all the way to large enterprises that require
    certification and support for their own on-premises data centers.
  prefs: []
  type: TYPE_NORMAL
- en: Pretty much the only criticism there is against Kubernetes is that it is hard
    to learn. This is true at the moment, but it will get easier and easier. There
    is a lot of good training material. Developers and operators will gain experience.
    It's easy to find information and the community is large and vibrant.
  prefs: []
  type: TYPE_NORMAL
- en: A lot of people say that Kubernetes will become boring soon and will become
    an invisible infrastructure layer. I don't subscribe to this point of view. Some
    hard parts of the Kubernetes experience, such as setting up a cluster and installing
    a lot of additional software into the cluster, will become boring, but I think
    we'll see a lot of innovation across the board in the next 5 years.
  prefs: []
  type: TYPE_NORMAL
- en: Let's dive into specific technologies and trends.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes extensibility
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is an easy call. Kubernetes was always designed as an extensible platform.
    But, some of the extension mechanisms required merging into the main Kubernetes
    repository. The Kubernetes developers recognized early the limitations and, across
    the board, introduced more loosely coupled mechanisms to extend Kubernetes and
    replace pieces that were considered core components in the past.
  prefs: []
  type: TYPE_NORMAL
- en: Abstracting the container runtime
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Docker used to be the only container runtime that Kubernetes supported. Then
    it added special support to the now-defunct RKT runtime. However, later, it introduced
    the **Container Runtime Interface** (**CRI**) as a way to integrate any container
    runtime through a standard interface. Here are some of the runtimes that implement
    CRI and can be used in Kubernetes:'
  prefs: []
  type: TYPE_NORMAL
- en: Docker (of course)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CRI-O (supports any OCI image)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Containerd (became an CNCF graduate in February 2019)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Frakti (Kata containers)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PouchContainer (P2P image distribution, optional VM-based)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Abstracting networking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes networking always required a **Container Networking Interface** (**CNI**)
    plugin. It is yet another CNCF project. It allows a lot of innovation in the networking
    and network security space.
  prefs: []
  type: TYPE_NORMAL
- en: You can find here a long list of platforms that support CNI (beyond Kubernetes)
    and an even longer list of plugins at [https://github.com/containernetworking/cni.](https://github.com/containernetworking/cni)
  prefs: []
  type: TYPE_NORMAL
- en: I expect the CNI to remain the standard interface for networking solutions.
    A very interesting project is Cilium, which utilizes the **extended Berkeley Packet
    Filter** (**eBPF**) to provide very high-performance networking and security at
    the Linux-kernel level, which may offset some of the overhead of service mesh
    sidecar proxies.
  prefs: []
  type: TYPE_NORMAL
- en: Abstracting storage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes has an abstract storage model, based on volumes and persistent volume
    claims. It supports a large number of storage solution in-trees. This means those
    storage solutions had to be built into the Kubernetes code base.
  prefs: []
  type: TYPE_NORMAL
- en: Early on (in Kubernetes 1.2), the Kubernetes team introduced a special type
    of plugin called FlexVolume that provided an interface for out-of-tree plugins.
    Storage providers could provide their own drivers that implement the FlexVolume
    interface and could serve as a storage layer without modifying Kubernetes itself.
    But, the FlexVolume approach was still pretty clunky. It required installing special
    drivers on each node and, in some cases, on the master too.
  prefs: []
  type: TYPE_NORMAL
- en: In Kubernetes 1.13, the **Container Storage Interface** (**CSI**) matured to
    **generally available** (**GA**) status and provides a modern gRPC-based interface
    for implementing out-of-tree storage plugins. Soon, Kubernetes will even support
    raw block storage via CSI (introduced as beta in Kubernetes 1.14).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram illustrates the place of CSI in the Kubernetes cluster
    and how it neatly isolates storage providers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/3676e7ad-049a-4841-8362-bdfc53134579.png)'
  prefs: []
  type: TYPE_IMG
- en: Container Storage Interface
  prefs: []
  type: TYPE_NORMAL
- en: The trend is to replace all the in-tree and FlexVolume plugins with CSI-based
    implementations, which will allow removing a significant chunk of functionality
    from the core Kubernetes code base.
  prefs: []
  type: TYPE_NORMAL
- en: The cloud provider interface
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes has seen a lot of success with cloud platforms, such as Google's
    GKE, Microsoft's AKS, Amazon's EKS, Alibaba's AliCloud, IBM's cloud Kubernetes
    service, DigitalOcean's Kubernetes service, VMware's Cloud PKS, and Oracle's container
    engine for Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: In the early days, integrating Kubernetes into a cloud platform required a lot
    of effort and involved customizing multiple Kubernetes control plane components,
    such as the API server, the kubelet, and the controller manager.
  prefs: []
  type: TYPE_NORMAL
- en: 'To make things easier on cloud platform providers, Kubernetes introduced the
    **Cloud Controller Manager** (**CCM**). The CCM abstracts away, through a set
    of stable interfaces, all the parts that a cloud provider needs to implement.
    Now, the touch points between Kubernetes and the cloud provider are formalized
    and it''s simpler to reason about and ensure that the integration is successful.
    Let''s have a look at the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/c15d71e6-a1fe-458b-aae3-345691eebbc4.png)'
  prefs: []
  type: TYPE_IMG
- en: Cloud Controller Manager
  prefs: []
  type: TYPE_NORMAL
- en: The preceding diagram illustrates the interactions between a Kubernetes cluster
    and a host cloud platform.
  prefs: []
  type: TYPE_NORMAL
- en: Service mesh integration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I mentioned at the end of the [Chapter 13](b39834c8-859c-42a5-846a-e48b76dfd6cc.xhtml),* Service
    Mesh - Working with Istio*, that service meshes are important. They complement
    Kubernetes and add a lot of value. While Kubernetes provides the management and
    scheduling of resources and the extensible API, a service mesh provides the next
    layer of managing the traffic flowing between the containers in the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: This symbiosis is very powerful. On GKE, Istio is already just a button-click
    away. I expect most Kubernetes distribution to provide the option to install Istio
    (or maybe the AWS app mesh, in the case of EKS) as part of the initial setup.
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, I expect a lot of other solutions to consider Istio as a standard
    component and build on top of it. An interesting project to watch in this space
    is Kyma ([https://kyma-project.io/](https://kyma-project.io/)), which aims to
    easily install a slew of best-of-breed cloud-native components. Kyna takes the
    extensible and open Kubernetes and adds an opinionated set of well-integrated
    components, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Helm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dex
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Istio
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Knative
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prometheus
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Grafana
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jeager
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubeless
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loki
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Velero (formerly, Ark)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Minio
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Serverless computing on Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we discussed in [Chapter 9](eae34b9d-8cc9-484a-87b1-82487d0a30dc.xhtml),
    *Running Serverless Tasks on Kubernetes*, serverless computing is all the rage.
    There are many solutions out there. Let''s distinguish between two separate solutions
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Function as a Service** (**FaaS**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Server as a Service** (**SaaS**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| **FaaS** | **SaaS** |'
  prefs: []
  type: TYPE_TB
- en: '| **FaaS** means that you launch a function either as source code that gets
    packaged into an image, or as a pre-packaged image you build. This image then
    gets scheduled on your cluster and it runs to completion. You still need to manage
    and scale the nodes in your cluster and make sure you have enough capacity for
    your long-running services and your functions. | **SaaS** means that you don''t
    need to provision and manage the nodes in your cluster. Your cluster auto-magically
    grows and shrinks according to the load. The Kubernetes cluster autoscaler provides
    this capability on Kubernetes. |'
  prefs: []
  type: TYPE_TB
- en: Obviously, you can mix and match, and run the Kubernetes cluster autoscaler
    and also run some function as a service framework to get the benefits of both.
  prefs: []
  type: TYPE_NORMAL
- en: So far, so good. But, Kubernetes is often deployed on public cloud platforms
    that have their own non-Kubernetes solutions to the same problem. For example,
    in AWS, you have Lambda functions (FaaS) as well as Fargate (SaaS). Microsoft
    Azure has Azure Functions and container instances that use a virtual kubelet and
    you can elastically grow your AKS cluster. Google has Cloud Functions and Cloud
    Run.
  prefs: []
  type: TYPE_NORMAL
- en: It will interesting to see how the public cloud providers integrate their offerings
    with Kubernetes. Google Cloud Run is built on top on Knative and can already run
    either on your GKE cluster or on Google's infrastructure (so it's independent
    of Kubernetes).
  prefs: []
  type: TYPE_NORMAL
- en: I predict that Knative will become yet another standard component that other
    FaaS solutions use as a building block on Kubernetes because it is so portable
    and supported by major players, such as Google and Pivotal. It is designed from
    the get-go as a loosely coupled collection of pluggable components that let you
    swap in your preferred components.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes and VMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes started as an orchestration platform for Docker containers. A lot
    of Docker-specific assumptions were built in. Kubernetes 1.3 added special support
    for CoreOS rkt and started the journey toward a decoupled runtime experience.
    Kubernetes 1.5 introduced the CRI, where the kubelet talks to the container runtime
    engine via gRPC. The CRI graduated to stable in Kubernetes 1.6.
  prefs: []
  type: TYPE_NORMAL
- en: As I mentioned earlier when discussing the abstraction of the container runtime,
    the CRI opened the door to multiple runtime implementations. One class of runtime
    extensions are lightweight or micro VMs. This may seem a little counter-productive
    because one of the biggest motivations for the container movement was that VMs
    are too heavyweight for dynamic cloud applications.
  prefs: []
  type: TYPE_NORMAL
- en: It turned out that containers are not fool-proof when it comes to isolation.
    Security concerns override any other concern for many use cases. The solution
    is to bring back VMs, but with a lighter touch. Now that the industry has some
    decent experience with containers, it is possible to design the next generation
    of VMs that will find the sweet spot between iron-clad isolation and high performance/low
    resource.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are some of the most prominent projects:'
  prefs: []
  type: TYPE_NORMAL
- en: gVisor
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Firecracker
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kata containers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: gVisor
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: gVisor is an open source project from Google. It is a user-space kernel sandbox
    that sits in front of the host kernel. It exposes an **Open Container Initiative**
    (**OCI**) interface called runsc. It also has a CRI plugin to interface directly
    with Kubernetes. The protection offered by gVisor is only partial. If there is
    a container breach, then the user kernel and a special secomp policy provide extra
    layers of security, but it is not a complete isolation. gVisor is used by Google
    AppEngine.
  prefs: []
  type: TYPE_NORMAL
- en: Firecracker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Firecracker is an open source project from AWS. It is a VM monitor using KVM
    to manage micro VMs. It is designed specifically to run secure multi-tenant containers
    and functions as as a service. It currently runs only on Intel CPUs, but support
    is planned for AMD and ARM.
  prefs: []
  type: TYPE_NORMAL
- en: AWS Lambda and AWS Fargate use Firecracker already. Currently, Firecracker can't
    be used easily on Kubernetes. The plan is to provide container integration via
    containerd. Refer to the link: [https://github.com/firecracker-microvm/firecracker-containerd/](https://github.com/firecracker-microvm/firecracker-containerd/).
  prefs: []
  type: TYPE_NORMAL
- en: Kata containers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is another open source solution managed by the **OpenStack Foundation**
    (**OSF**), in the form of Kata containers. It combines technology from Intel's
    clear containers and Hyper.sh RunV. It supports multiple hypervisors, such QEMU,
    NEMU, and even Firecracker. The goal of the Kata containers is to build a secure
    container runtime based on hardware virtualization for workload isolation. Kata
    containers can already be used on Kubernetes via containerd.
  prefs: []
  type: TYPE_NORMAL
- en: It's hard to tell how it will all shake up. There was already some consolidation.
    There is strong demand for safe and secure container runtimes. All the projects
    can either be used on Kubernetes already, or there are plans to integrate them
    soon. This will probably be one of the most important, yet invisible, improvements
    to the cloud-native landscape. The main concern is that those lightweight VMs
    might introduce too much of a performance overhead for some use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Cluster autoscaling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you deal with fluctuating load (and it''s safe to say that any non-trivial
    system does), then you have three options:'
  prefs: []
  type: TYPE_NORMAL
- en: Over provision your cluster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Try to find a magic ideal size and deal with outages, timeout, and slow performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Grow and shrink your cluster based on demand.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s discuss the preceding options in more detail:'
  prefs: []
  type: TYPE_NORMAL
- en: Option 1 is expensive. You pay for resources, you don't fully utilize most of
    the time. It does buy you some peace and quiet, but eventually, you may run into
    a spike of demand that temporarily exceeds even your over provisioned capacity.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Option 2 is not really an option. You may find yourself there if you opted for
    over provisioning and underestimated.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Option 3 is where you want to be. Your cluster's capacity matches your workload.
    You can always satisfy your SLOs and SLAs and you don't pay for unused capacity.
    However, trying to elastically manage your cluster manually is a no-starter.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The solution is to do it automatically. This is where the cluster autoscaler
    comes in. I believe that, for large-scale clusters, the cluster autoscaler will
    become a standard component. There may be additional custom controllers that also
    adjust the cluster size based on custom metrics, or adjust other resources beyond
    nodes.
  prefs: []
  type: TYPE_NORMAL
- en: I fully expect all the large cloud providers to invest and address all the current
    gotchas and issues related to the cluster autoscaler and ensure it works flawlessly
    on their platforms.
  prefs: []
  type: TYPE_NORMAL
- en: Another prominent trend in the Kubernetes community that became a best practice
    is to provide complex components through Kubernetes operators.
  prefs: []
  type: TYPE_NORMAL
- en: Using operators
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A Kubernetes operator is a controller that encapsulates operational knowledge
    of some application. It can manage installation, configuration, updates, fail-overs,
    and more. Operators often rely on CRDs to keep their own state and can automatically
    respond to events. Providing an operator is quickly becoming the way to release
    new, complicated software.
  prefs: []
  type: TYPE_NORMAL
- en: Helm charts are fine for installing the bits onto the cluster (and operators
    may use Helm charts for that purpose), but there is a lot of ongoing management
    associated with complex components, such as data stores, monitoring solutions,
    CI/CD pipelines, message brokers, and serverless frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: 'The trend here is very clear: complex projects will provide operators as a
    standard feature.'
  prefs: []
  type: TYPE_NORMAL
- en: There are two interesting projects that support this trend.
  prefs: []
  type: TYPE_NORMAL
- en: 'OperatorHub ([https://operatorhub.io/](https://operatorhub.io/)) is a curated
    index of Kubernetes operators, where people can go and find well-packaged software
    to install on their cluster. OperatorHub was started by RedHat (now part of IBM),
    Amazon, Microsoft, and Google. It is very well-organized by category and provider
    and is easily searchable. Here is a screenshot of the main page:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/f41b2327-e9ac-4389-9524-55f84386c6ab.png)'
  prefs: []
  type: TYPE_IMG
- en: Operator hub
  prefs: []
  type: TYPE_NORMAL
- en: Operators are very useful, but they require pretty good knowledge of how Kubernetes
    works, controllers, the concept of reconciliation logic, how to create CRDs, and
    how to interact with the Kubernetes API server. It is not rocket science, but
    it's not trivial either. If you want to develop your own operators, there is a
    project called the Operator Framework ([https://github.com/operator-framework](https://github.com/operator-framework)).
    The Operator Framework provides an SDK to make it easy to start with your operators.
    There are guides for writing operators in Go, using Ansible or Helm.
  prefs: []
  type: TYPE_NORMAL
- en: Operators significantly reduce complexity, but what if you need to manage many
    clusters? This is where cluster federation comes in.
  prefs: []
  type: TYPE_NORMAL
- en: Federation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Managing a single large Kubernetes cluster is not simple. Managing multiple
    geo-distributed clusters is much harder. It is especially difficult if you try
    to treat multiple clusters as one big logical cluster. Many challenges arise around
    high availability, fail-over, load balancing, security, and latency.
  prefs: []
  type: TYPE_NORMAL
- en: 'For many very large systems, multiple clusters are a necessity. Sometimes,
    it is necessary for smaller systems too. The following are some use cases:'
  prefs: []
  type: TYPE_NORMAL
- en: Hybrid on-premises/cloud
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Geo-distributed redundancy and availability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multi-provider redundancy and availability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Very large systems (more nodes than a single Kubernetes cluster can handle)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubernetes attempted to address the problem with the Kubernetes Federation V1
    proposal and implementation. It failed and never made it to GA. But, then came
    V2, at [https://github.com/kubernetes-sigs/federation-v2.](https://github.com/kubernetes-sigs/federation-v2)
  prefs: []
  type: TYPE_NORMAL
- en: 'All big cloud providers have products for a hybrid on-premises/cloud systems.
    These include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Google Anthos
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GKE on-premises - AWS Outposts: Microsoft Azure Stack'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition, many third-party Kubernetes solutions offer cross-cloud and even
    bare-metal management of multiple clusters. One of the most promising projects
    in this area is Gardener ([https://gardener.cloud/](https://gardener.cloud/))
    that lets you manage potentially thousands of clusters. It operates by having
    a garden cluster that manages many seed clusters (as custom resources) that can
    have shoot clusters.
  prefs: []
  type: TYPE_NORMAL
- en: I see it as a natural progression. Once the industry masters the art of managing
    a single cluster, then mastering a collection of clusters will become the next
    challenge.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we looked at where microservices and Kubernetes are going next.
    All the indicators show that both microservices and Kubernetes will continue to
    be major factors when designing, building, evolving, and operating cloud-native,
    large-scale, distributed systems. This is good news. Small programs, scripts,
    and mobile apps will not disappear, but the backend systems will become large,
    deal with more data, and be responsible for managing larger and larger aspects
    of our lives. Technologies such as virtual reality, sensors, and AI will require
    ever-growing amounts of data to be processed and stored.
  prefs: []
  type: TYPE_NORMAL
- en: The short-term development in the microservices world will see gRPC emerge as
    a popular transport for inter-service communication, as well as a public interface.
    Web clients will be able to consume gRPC via the gRPC for web. GraphQL is another
    innovation that is a major improvement compared to the REST API. The industry
    still needs some time to understand how to design and build microservice-based
    architectures. Building a single microservice is simple. Building a whole system
    of coordinated microservices is a whole other story.
  prefs: []
  type: TYPE_NORMAL
- en: Containers and Kubernetes solve some of the hard problems that microservice-based
    architectures present. New technologies, such as service mesh, will gain mindshare
    very quickly. Serverless computing (both SaaS and FaaS) will help developers to
    deploy and update applications even faster. The merging of containers and virtualization
    will result in more secure systems. Operators will make bigger and more useful
    building blocks a reality. Cluster federation will be the new frontier of scalable
    systems.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, you should have a good idea of what is coming down the line and
    what to expect. This knowledge will allow you to plan ahead and make your own
    assessments regarding which technologies to invest in right now, and which technologies
    need to mature some more.
  prefs: []
  type: TYPE_NORMAL
- en: In short, we are at the beginning of an exciting, new era, where we will learn
    how to create reliable systems at an unprecedented scale. Keep learning, stay
    on top of all the amazing technologies available to you, build your own systems,
    and contribute back to the community.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The reading list is quite extensive because we discussed a lot of up-and-coming
    projects and technologies that are worth monitoring and following up on:'
  prefs: []
  type: TYPE_NORMAL
- en: '**gRPC**: [https://grpc.io/](https://grpc.io/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The Frakti runtime**: [https://github.com/kubernetes/frakti](https://github.com/kubernetes/frakti)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Containerd**: [https://containerd.io/](https://containerd.io/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**PouchContainer**: [https://github.com/alibaba/pouch](https://github.com/alibaba/pouch)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kata Containers**: [https://katacontainers.io/](https://katacontainers.io/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kubernetes and Cloud Providers**: [https://medium.com/@the.gigi/kubernetes-and-cloud-providers-b7a6227d3198](https://medium.com/@the.gigi/kubernetes-and-cloud-providers-b7a6227d3198)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Extending Kubernetes**: [https://www.youtube.com/watch?v=qVZnU8rXAEU](https://www.youtube.com/watch?v=qVZnU8rXAEU)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Azure Functions**: [https://azure.microsoft.com/en-us/services/functions/](https://azure.microsoft.com/en-us/services/functions/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Azure Container Instances**: [https://azure.microsoft.com/en-us/services/container-instances/](https://azure.microsoft.com/en-us/services/container-instances/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Google Cloud Run**: [https://cloud.google.com/blog/products/serverless/announcing-cloud-run-the-newest-member-of-our-serverless-compute-stack](https://cloud.google.com/blog/products/serverless/announcing-cloud-run-the-newest-member-of-our-serverless-compute-stack)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**gVisor**: [https://gvisor.dev/](https://gvisor.dev/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Firecracker:** [https://firecracker-microvm.github.io/](https://firecracker-microvm.github.io/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kata Containers**: [https://katacontainers.io/](https://katacontainers.io/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gardener**: [https://gardener.cloud/](https://gardener.cloud/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The Operator Framework**: [https://github.com/operator-framework/operator-sdk](https://github.com/operator-framework/operator-sdk)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**HTTP/3 explained**: [https://http3-explained.haxx.se](https://http3-explained.haxx.se)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
