- en: Chapter 6. Deploy Real Applications on Swarm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'With a Swarm infrastructure we can put up various types of load to deploy.
    We''ll work on the application stack in this and the next chapter. In this chapter
    we''ll:'
  prefs: []
  type: TYPE_NORMAL
- en: Discover Swarm's services and tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploy Nginx containers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploy a complete WordPress
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploy a tiny scale Apache Spark architecture.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Microservices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The IT industry has always been keen on decoupling and reusing its creations,
    either source code or applications. Modeling applications at the architectural
    level is not an exception. Modularization was earlier called **service-oriented
    architecture** (**SOA**) and was kept glued by open source protocols based on
    XML. However, with the advent of containers, everyone is now speaking of micro
    services.
  prefs: []
  type: TYPE_NORMAL
- en: Micro services are small and self-contained autonomous modules that work together
    to accomplish an architectural goal.
  prefs: []
  type: TYPE_NORMAL
- en: The most inflated example of a micro service architecture is a web-application
    stack, for example WordPress, where web server might be one service, others being
    the database, cache engine, and the service containing the application itself.
    Modeling micro services through Docker containers can be done immediately and
    that's how the industry is moving ahead right now.
  prefs: []
  type: TYPE_NORMAL
- en: '![Microservices](images/image_06_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'There are many advantages of using microservices and they are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Reusability**: You just pull the images of services you want (nginx, MySQL)
    in case you customize them'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Heterogeneity**: You link existing modules embracing the different technologies.
    If, sometime later in the future, you decide to switch from MySQL to MariaDB,
    you plug off MySQL and plug in MariaDB'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Focus on small**: Detached modules are easy to troubleshoot separately'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scale**: You can easily scale the web servers to 10 front ends, the cache
    servers to three, and architect the database replicas on five nodes, and one day
    scale-up or scale-down depending on the application load and needs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Resilience**: If you have three memcached servers and one fails, you can
    have mechanisms that try to resurrect it or just forget it and immediately fire
    up another one'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploy a replicated nginx
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We get in touch with how to use services on Swarm by starting with a simple
    sample: Deploy and scale Nginx.'
  prefs: []
  type: TYPE_NORMAL
- en: A minimal Swarm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To make this chapter self-sufficient and useful for developers who are reading
    it as a stand-alone chapter. Let''s quickly create a minimal Swarm Mode architecture
    locally, made of one manager and three workers:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We spawn up four Docker hosts:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We then take control of `node-1`, which we elect as our static manager, and
    initialize it on a Swarm:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Docker generates a token, for us, to join our three workers. So we just copy-paste
    that output to iterate through the other three workers to join them to the nodes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Swarm Mode architecture is always connected to `node-1` by Docker Machine-shell
    environment variables that are filled by the previous `eval` command. We need
    to check whether all the nodes, including the leader manager, are active and successfully
    joined to the Swarm:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A minimal Swarm](images/image_06_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, we can check the status of this Swarm cluster using `docker info` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A minimal Swarm](images/image_06_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The important information here is that Swarm is active, and then some Raft details
    follow.
  prefs: []
  type: TYPE_NORMAL
- en: Docker service
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A new command introduced in Docker 1.12 is `docker service` and that's what
    we're going to see now. Service is the primary way by which you'll operate applications
    on Docker Swarm mode; it's how you will create, destroy, scale and roll update
    services.
  prefs: []
  type: TYPE_NORMAL
- en: Services are made of tasks. An nginx service is made-up of nginx container tasks.
    The service mechanism spins-up the tasks on (typically) worker nodes. So, when
    you create a service, you have to mandatorily specify, among its options, a service
    name and the container that will be the base of the service.
  prefs: []
  type: TYPE_NORMAL
- en: .
  prefs: []
  type: TYPE_NORMAL
- en: '![Docker service](images/image_06_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The syntax to create the services is very immediate: You just use the `docker
    service create` command, specifying options, such as the exposed ports, and select
    the container to use. Here we execute'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![Docker service](images/image_06_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This command starts nginx, exposes the container's port `80` to the host's port
    `80`, so that they can be reached from outside, and specifies a replica factor
    of three.
  prefs: []
  type: TYPE_NORMAL
- en: Replica factor is the way you scale containers on Swarm. If you specify three,
    Swarm will create three nginx tasks (containers) on three nodes and try to preserve
    this number, in case one or more of these containers die, by rescheduling nginx
    on other available hosts (where possible).
  prefs: []
  type: TYPE_NORMAL
- en: If a no `--replicas` option is given, then the default replica factor is `1`.
  prefs: []
  type: TYPE_NORMAL
- en: 'After some time, Swarm needs to pull the image from the hub, or any registry
    locally, to the hosts and create the appropriate container (and exposing the port);
    we see that three nginx are in place on our infrastructure with the command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![Docker service](images/image_06_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'These tasks are actually scheduled on three nodes, as shown using the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![Docker service](images/image_06_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The `fsoppelsa/swarm-nginx` container used here is a trivial modification of
    `richarvey/nginx-php-fpm`, which is a nginx image empowered by PHP. We used PHP
    to output on the Nginx welcome page the address of the current server, by adding
    a PHP command with the purpose of showing the load balancing mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![Docker service](images/image_06_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Now if you point your browser to the manager IP and reload several times, you'll
    see that a load balancer is actually redirecting you to different containers sometimes.
  prefs: []
  type: TYPE_NORMAL
- en: 'First page that will load will be similar to the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Docker service](images/image_06_009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following screenshot shows another page loaded, with a different node selected
    by the load balancer, 10.255.0.9:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Docker service](images/image_06_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following screenshot is of another page loaded when the load balancer redirects
    to node 10.255.0.10:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Docker service](images/image_06_011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Overlay networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If instead of just replicating, you want to connect containers running on different
    hosts to your Swarm infrastructure, you have to use networks. For example, you
    need to connect your web servers to your database containers so that they can
    communicate.
  prefs: []
  type: TYPE_NORMAL
- en: The answer to this, in Swarm Mode, is to use overlay networks. They are implemented
    with Docker's libnetwork and libkv. These networks are VxLAN networks built on
    top of another network (in the standard setup, the physical host network).
  prefs: []
  type: TYPE_NORMAL
- en: VxLAN is an extension of the VLAN protocol, aiming at increasing its scalability.
    Containers on different hosts, connected to Docker VxLAN networks, can communicate
    as if they are on the same host.
  prefs: []
  type: TYPE_NORMAL
- en: Docker Swarm Mode includes a routing mesh table that enables this multi-host
    networking, by default, called **ingress**.
  prefs: []
  type: TYPE_NORMAL
- en: Integrated load balancing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How does the load balancing work on Swarm Mode 1.12? The routing works in two
    different ways. Firstly, it works through the port exposed by the Virtual IP service.
    Any requests to the port are distributed among the hosts hosting the service tasks.
    Secondly, the service is given a Virtual IP address that is routable only inside
    the Docker Network. When requests are made to this VIP address, they are distributed
    to the underlying containers. This Virtual IP is registered inside the DNS server
    included in Docker Swarm. When a DNS query is done on the service name (for example
    nslookup mysql), the Virtual IP is returned.
  prefs: []
  type: TYPE_NORMAL
- en: 'Connecting services: A WordPress example'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The possibility of launching a bunch of replicated and load balanced containers
    is already a good start, but how about more complex application stacks, made of
    different interconnected containers?
  prefs: []
  type: TYPE_NORMAL
- en: In this case, you can link containers by calling them by name. As we just saw,
    the internal Swarm DNS server will guarantee a reliable name resolution mechanism.
    If you instantiate a service called `nginx`, you can just reference it as `nginx`
    and other services will resolve to the `nginx` Virtual IP (load balanced), hence
    accessing the distributed containers.
  prefs: []
  type: TYPE_NORMAL
- en: 'To demonstrate this with an example, we''re now going to deploy the more classical
    of classics on Swarm: WordPress. You can run WordPress as a container, in fact
    a ready image is available on the Docker Hub, however it requires an external
    database (in this case MySQL) to store its data.'
  prefs: []
  type: TYPE_NORMAL
- en: So, as a start, we'll create a new dedicated overlay network on Swarm, called
    WordPress, and run one MySQL container on top of it as a Swarm service and three
    load balanced WordPress containers (web containers) also as a Swarm service. MySQL
    will expose port 3306, while WordPress will expose port `80`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start by defining our overlay network. When connected to the Swarm manager,
    we issue the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![Connecting services: A WordPress example](images/image_06_012.jpg)'
  prefs: []
  type: TYPE_IMG
- en: So, what happens behind the curtain? The command creates an overlay network
    with libnetwork, which becomes available on the Swarm nodes when they get scheduled
    tasks requiring it. It will always be present if you connect to `node-2` and list
    networks.
  prefs: []
  type: TYPE_NORMAL
- en: 'We now create a MySQL service, made of just one container (no MySQL native
    replicas nor Galera or other replication mechanisms) with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We want to pull MySQL 5.6 from the hub, call the service (later accessible
    via resolved name pointing to its VIP) `mysql`, set replicas to one for clarity,
    expose port `3306`, specify the dedicated network WordPress, and the root password,
    in our case it''s `dockerswarm`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Connecting services: A WordPress example](images/image_06_013.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'It is necessary to pull the MySQL image from the hub after a few seconds, we
    can check and see that in our case a `mysql` container was downloaded and placed
    on `node-1` (actually, masters can run containers if not specified differently),
    and the VIP is `10.255.0.2`, on the WordPress network. We can get this information
    with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![Connecting services: A WordPress example](images/image_06_014.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We now have a running MySQL, we just need to launch and connect it to WordPress.
  prefs: []
  type: TYPE_NORMAL
- en: Swarm scheduling strategies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'It just happened that we started a service and Swarm scheduled the container
    to be run on `node-1`. Swarm mode (as of now, at the time of writing Docker 1.12,
    and 1.13-dev) has only one possible strategy: spread. Spread counts the number
    of containers on each host and attempts to place newly created containers on the
    less loaded hosts (that is, hosts with less containers). Despite the fact that
    there is only one spread strategy available on this day, Swarm comes with options
    that allow us to filter the hosts, on which the tasks will be launched, with good
    precision.'
  prefs: []
  type: TYPE_NORMAL
- en: These options are called **constraints** and may be passed as an optional argument
    when services are instantiated with `--constraint`.
  prefs: []
  type: TYPE_NORMAL
- en: We now want to start WordPress. We decide that we want to forcibly execute three
    containers on the three workers and not on the master, so we specify a constraint.
  prefs: []
  type: TYPE_NORMAL
- en: Constraints are of the form of `--constraint``node.KEY == VALUE` or `--constraint``node.KEY
    != VALUE` and there are several variants. An operator can filter by node id, role,
    and hostname. More interesting, as we saw in [Chapter 5](ch04.html "Chapter 4. Creating
    a Production-Grade Swarm"), *Administer a Swarm Cluster*, is the possibility to
    specify custom labels by adding it to the node attributes with the `docker node
    update --label-add` command.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Key** | **Meaning** | **Example** |'
  prefs: []
  type: TYPE_TB
- en: '| `node.id` | ID of node | `node.id == 3tqtddj8wfyd1dl92o1l1bniq` |'
  prefs: []
  type: TYPE_TB
- en: '| `node.role` | Node role (manager, worker) | `node.role != manager` |'
  prefs: []
  type: TYPE_TB
- en: '| `node.hostname` | Node hostname | `node.hostname == node-1` |'
  prefs: []
  type: TYPE_TB
- en: '| `node.labels` | Labels | `node.labels.type == database` |'
  prefs: []
  type: TYPE_TB
- en: Now, WordPress
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here we want to start `wordpress` on all workers, so we say that the constraint
    is `node.role != manager` (or `node.role == worker`). Also, we call the service,
    just `wordpress,` set the replica factor to `3`, expose port `80,` and say to
    WordPress that MySQL is located on host mysql (this is resolved internally in
    Swarm and points to the MySQL VIP):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![Now, WordPress](images/image_06_015.jpg)'
  prefs: []
  type: TYPE_IMG
- en: After some time, we need to download WordPress images to the workers so that
    we can check if everything is up and running.
  prefs: []
  type: TYPE_NORMAL
- en: '![Now, WordPress](images/image_06_016.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We now connect to one of the hosts on port `80` and we're welcomed by the WordPress
    installer.
  prefs: []
  type: TYPE_NORMAL
- en: '![Now, WordPress](images/image_06_017.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'WordPress is ready after a few steps, such as selecting an admin username and
    a password, are performed in the browser:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Now, WordPress](images/image_06_018.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Docker Compose and Swarm mode
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many developers enjoy using Compose to model their applications, for example
    applications similar to WordPress. We do the same and think that it's a fantastic
    way to describe and manage micro services on Docker. However, at the time of writing
    this book, no support for Docker Swarm Mode is available in Compose yet and all
    containers are scheduled on the current node. To deploy an application across
    the swarm, we need to use the new bundle feature of stacks.
  prefs: []
  type: TYPE_NORMAL
- en: At the time of writing, stacks are available only experimentally, but we're
    showing them here just to give you a taste of what it will be like to deploy microservices
    on Docker in the (near) future.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Docker stacks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For Docker, stacks will be the standard way of packaging applications made
    by multiple containers. Consider the hyper inflated WordPress example: You need
    a minimum of one web server and one database.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Developers usually describe these applications with Compose, by creating a
    YAML, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, they launch this application with a command such as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Here, `mysql` and `wordpress` containers are scheduled, pulled, and started
    as daemons on the host to which the developer is connected. Starting from Docker
    1.12 (experimental in 1.12), it will be possible to package `mysql + wordpress`
    in a single file package, called **Distributed Application Bundle** (**DAB**).
  prefs: []
  type: TYPE_NORMAL
- en: Distributed Application Bundles
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'So, instead of `docker-compose up` command, you will run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: This command will output another JSON, called `wordpress.dab`, which will be
    the starting point for deploying services described as Swarm services by Compose
    on Swarm.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this example, the content of `wordpress.dab` looks similar to:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Docker deploy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Starting from the generated `wordpress.dab` file, when connected to a Swarm
    manager, the developer can start a stack with the deploy command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Now you will have two services called `wordpress1_wordpress` and `wordpress1_db`,
    conventionally following the syntax traditions of Compose.
  prefs: []
  type: TYPE_NORMAL
- en: '![Docker deploy](images/image_06_019.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This is a very primitive demo of what it will be. As an experimental feature,
    the support features in Compose are still not completely defined, but we expect
    it to change (even radically) in the future to meet the developer, Swarm, and
    Compose needs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another app: Apache Spark'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have acquired some practice using services, we step up to the next
    level. We'll deploy Apache Spark on Swarm. Spark is an open source cluster computing
    framework from the Apache foundation, which is mainly used for data processing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Spark may be (but not limited to) used for things, such as:'
  prefs: []
  type: TYPE_NORMAL
- en: Analysis of big data (Spark Core)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fast and scalable data structured console (Spark SQL)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Streaming analytics (Spark Streaming)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Graph processing (Spark GraphX)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here we will focus mainly on the infrastructural part of Swarm. If you want
    to learn how to program or use Spark in detail, read Packt's selection of books
    on Spark. We suggest starting with *Fast Data Processing with Spark 2.0 - Third
    Edition*.
  prefs: []
  type: TYPE_NORMAL
- en: Spark is a neat and clear alternative for Hadoop, it is a more agile and efficient
    substitute for the complexity and magnitude of Hadoop.
  prefs: []
  type: TYPE_NORMAL
- en: The theoretical topology of Spark is immediate and can reckon the Swarm mode
    on one or more managers leading the cluster operations and a certain number of
    workers who are executing real tasks.
  prefs: []
  type: TYPE_NORMAL
- en: As for managers, Spark can use its own managers called standalone managers (as
    we'll do here) or use Hadoop YARN or even exploit Mesos features.
  prefs: []
  type: TYPE_NORMAL
- en: Then, Spark can delegate storage to an internal HDFS (Hadoop Distributed Filesystem)
    or to external storage services, such as Amazon S3, OpenStack Swift, or Cassandra.
    Storage is used by Spark to get data to elaborate and then to save the elaborated
    results.
  prefs: []
  type: TYPE_NORMAL
- en: Why Spark on Docker
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We''ll show you how to start a Spark cluster on a Docker Swarm cluster, as
    an alternative to start Spark with virtual machines. The example defined in this
    chapter can get many benefits from containers:'
  prefs: []
  type: TYPE_NORMAL
- en: Starting containers is much more quicker
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scaling containers in a pet model is more immediate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can get Spark images without having to create VMs, to write custom scripts,
    adapt Ansible Playbooks. Just `docker pull`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can create a dedicated overlay network with Docker Networking features,
    without physically compromising or invoking a networking team
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark standalone without Swarm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's start defining a tiny Apache Spark cluster built with the classical Docker
    tools, which are basically Docker commands on a Docker host. Before understanding
    the big picture, we need to start familiarizing ourselves with Swarm concepts
    and terminologies on the field.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''ll work with the `google_container` images, specifically
    with Swarm version 1.5.2\. Many improvements are included in the 2.0 version,
    but these images are proven to be very stable and reliable. So, we can start by
    pulling them for the master and the workers from the Google repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Spark can run on the top of YARN, Mesos, or Hadoop. In the following examples
    and chapters, we're going to use its standalone mode, because it is the easiest
    and requires no additional prerequisites. In a standalone Spark cluster mode,
    Spark allocates resources based on cores. By default, an application will grab
    all the cores in the cluster, so we're going to limit the resources dedicated
    to the workers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our architecture will be very straightforward: one master, which will be responsible
    for managing the cluster, and three workers for the nodes running Spark jobs.
    For our purpose, the master has to publish port `8080` (the Web UI we''ll use
    for convenience) and we''ll call it spark-master. By default, the worker containers
    attempt to connect to the URL `spark://spark-master:7077`, so apart from linking
    them to the master, no further customization are required.'
  prefs: []
  type: TYPE_NORMAL
- en: 'So, let''s pass it to the practical part and initialize a Spark master with
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: This runs in the daemon mode (`-d`), a container from the `gcr.io/google_containers/spark-master`
    image, assigns the name (`--name`) spark-master to the container and configures
    its hostname (`-h`) to spark-master.
  prefs: []
  type: TYPE_NORMAL
- en: We can connect now a browser to the Docker host, at port `8080,` to verify that
    Spark is up and running.
  prefs: []
  type: TYPE_NORMAL
- en: '![Spark standalone without Swarm](images/image_06_020.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'It still has no Alive Workers, which we''re going to spawn now. We start the
    workers with the following commands just before we take note of the ID of the
    Spark master container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'This starts a container in the daemon mode, links it to the master, limits
    the memory-in-use to a maximum of 256M, exposes port 8081 to web (worker) management,
    and assigns it to the container name `worker-1`. Similarly, we start the other
    two workers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'We can check on the master if everything is connected and running:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Spark standalone without Swarm](images/image_06_021.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Spark standalone on Swarm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, we have discussed the not so important part. We're now going now to
    transfer the concepts already discussed to Swarm architecture, so we'll instantiate
    the Spark master and workers as Swarm services, instead of single containers.
    We'll create an architecture with a replica factor of one for the master, and
    a replica factor of three for the workers.
  prefs: []
  type: TYPE_NORMAL
- en: Spark topology
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this example, we'll create a Spark cluster made of one master and three workers.
  prefs: []
  type: TYPE_NORMAL
- en: Storage
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We'll define a real storage and start some real Spark tasks in [Chapter 7](ch07.html
    "Chapter 7. Scaling Up Your Platform"), S*caling Up Your Platform*.
  prefs: []
  type: TYPE_NORMAL
- en: Prerequisites
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We begin by creating a new dedicated overlay network for Spark:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we set some labels onto nodes to be able to filter later. We want to
    host the Spark master on the Swarm manager (`node-1`) and Spark workers on Swarm
    workers (node-2, 3 and 4):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We''re adding here the ''sparkworker'' type tags for extreme clarity. With
    only two variants, it''s possible in fact to write the same constraint as:'
  prefs: []
  type: TYPE_NORMAL
- en: '**--constraint ''node.labels.type == sparkworker''**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Or:'
  prefs: []
  type: TYPE_NORMAL
- en: '**--constraint ''node.labels.type != sparkmaster''**'
  prefs: []
  type: TYPE_NORMAL
- en: Start Spark on Swarm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will now define our Spark services in Swarm, similar to what we did for Wordpress
    in the preceding section, but this time we will drive the scheduling strategy
    by defining where to start the Spark master and the Spark workers with the maximum
    precision.
  prefs: []
  type: TYPE_NORMAL
- en: 'We begin with the master as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'A Spark master exposes port `8080` (the web UI) and optionally, for the clarity
    of the example, here we also expose port `7077` used by the Spark workers to connect
    to the master and port 6066, the Spark API port. Also, we limit the memory to
    1G with --limit-memory. Once the Spark master is up, we can create the service
    hosting the workers, sparkworker:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, we expose port `8081` (the workers web UI), but it''s optional.
    Here, all the Spark containers are scheduled on spark worker nodes, as we defined
    earlier. It will take some time to pull the images to the hosts. As a result,
    we have the minimal Spark infrastructure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Start Spark on Swarm](images/image_06_022.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The Spark cluster is up and running, even if there is a little note to add:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Start Spark on Swarm](images/image_06_023.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Despite the fact that we limited the memory to 256M for each worker, in the
    UI we still see that Spark reads 1024M. This is because of the Spark internal
    default configuration. If we connect to any of the hosts, where one of the workers
    is running, and check its statistics with the `docker stats a7a2b5bb3024` command,
    we see that the container is actually limited:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Start Spark on Swarm](images/image_06_024.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we started working on the application stack and deploying
    real things on Swarm. We did some practice in defining Swarm services and we launched
    a cluster of nginx, as well as a load-balanced WordPress on a dedicated overlay
    network. Then, we moved on to something more real: Apache Spark. We deployed Spark
    on Swarm at a small scale, by defining our own scheduling strategies. We are going
    to expand Swarm and scale it to a bigger size, with more real storage and networking
    options, in [Chapter 7](ch07.html "Chapter 7. Scaling Up Your Platform"), S*caling
    Up Your Platform*.'
  prefs: []
  type: TYPE_NORMAL
