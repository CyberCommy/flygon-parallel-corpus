- en: Chapter 8. Advanced Solr – Grouping, the MoreLikeThis Query, and Distributed
    Search
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter we will look at some of the advanced concepts of Solr. We will
    look at grouping results based on certain criteria. We will also look at finding
    results similar to a particular document based on some terms within the document.
    We will explore distributed search that can be used to horizontally scale the
    Solr search infrastructure. Topics that will be covered in this chapter are
  prefs: []
  type: TYPE_NORMAL
- en: Grouping results by field
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Grouping results by query
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running morelikethis query using PHP
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tuning parameters of morelikethis query
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distributed search
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up distributed search
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Executing distributed search using PHP
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up Solr master-slave
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Load balancing Solr queries using PHP
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Grouping results by fields
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Result grouping is a feature where results are clubbed together based on certain
    criteria. Solr provides us grouping based on field or based on queries. Let us
    search for all books and group results based on author name and genre. Grouping
    should be done on non-tokenized fields as the grouping output makes more sense
    for the complete field value rather than individual tokens. The non-tokenized
    string fields for author name and genre are `author_s` and `genre_s`. Why? Remember
    we discussed a concept known as dynamic fields in [Chapter 2](ch02.html "Chapter 2. Inserting,
    Updating, and Deleting Documents from Solr"), *Inserting, Updating, and Deleting
    Documents from Solr*. The dynamic fields of type `*_s` is defined as `string`
    as shown in the following code, which is not tokenized in Solr:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We will have to get the grouping component and add the fields we need to group
    by as shown in the following query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s also set the number of items that Solr should return for each group
    with the following query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'And also return the total number of groups with the following query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'To display the grouping information we need to first get the grouping component
    from the result set with the following query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'As we have done grouping on multiple fields we would be getting multiple groups
    in the result set. For each group in the groups array, we will get the number
    of matches and the number of group elements with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We are iterating over the number of group elements to find the name/heading
    and number of elements in the group. And for each group element get the documents
    from that group element.
  prefs: []
  type: TYPE_NORMAL
- en: 'The query in the Solr query logs is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The parameters being passed are `group=true` to enable grouping and for each
    field we have a `group.field=<field_name>` parameter. The `group.limit` parameter
    is used to specify the number of documents to be retrieved for each group element.
    The output is as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Grouping results by fields](graphics/4920OS_08_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Output of group by field.
  prefs: []
  type: TYPE_NORMAL
- en: Grouping results by queries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Instead of grouping by a field, we can also group by query. Let us create groups
    for different price ranges. Instead of using the `addField()` function on the
    grouping component, we have to use the `addQuery()` function and specify our query
    as a parameter in the function as shown in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Here we have created 3 groups for price ranges $0 to $5, $5.01 to $10 and more
    than $10.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also set sorting within a group using the `setSort()` function as shown
    in the following query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Code to display the groups is similar to that discussed earlier. Output from
    our code is shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Grouping results by queries](graphics/4920OS_08_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: From Solr logs we can see that instead of `group.field` parameter we have got
    multiple `group.query` parameters.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Running more like this query using PHP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**More like this** feature of Solr can be used to construct a query based on
    the terms within a document. This feature helps us in retrieving documents similar
    to those in our query results. We have to specify the fields on which the more
    like this functionality is run. For efficiency purposes, it is recommended that
    we have `termVectors=true` for these fields. Let us understand how this functionality
    works by looking at an example. Suppose we want books that are similar to those
    appearing in the result. Similarity of books is derived from the author and the
    series to which they belong. So we would have to tell Solr to get books similar
    to the currently selected book based on the fields'' `author` and `series`. Solr
    (Lucene) internally compares all the tokens within the specified fields in all
    documents (books in our case) in the index with the fields of the currently selected
    book. And based on how many tokens are matching, it retrieves the results and
    ranks them so that the documents with maximum token matches are on top.'
  prefs: []
  type: TYPE_NORMAL
- en: Let us add `termVectors=true` to our fields `author`&`*_t` (for `series_t`).
    Term vector is a collection of term frequency pairs optionally with positional
    information. Term vectors are basic building blocks for a Solr/Lucene index. We
    will need to index the documents again.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For more information on how Lucene index works check out the documentation at
    [http://lucene.apache.org/core/4_5_1/core/org/apache/lucene/codecs/lucene45/package-summary.html](http://lucene.apache.org/core/4_5_1/core/org/apache/lucene/codecs/lucene45/package-summary.html)
  prefs: []
  type: TYPE_NORMAL
- en: 'In our PHP code, we will have to get the `MoreLikeThis` component from the
    query and add the fields on which we want to run this feature as shown in the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'This code says that we want to run the more like this similarity feature on
    fields'' `author` and `series_t`. This should work for fairly large data sets
    but let''s give it a few tweaks to make it run for our small books index using
    the following query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: This sets the minimum limits for classifying a document as a similar document.
    We will discuss more about these parameters in the 'tuning more like this query
    parameters' section.
  prefs: []
  type: TYPE_NORMAL
- en: 'After running the query, we need to get the more like this component from the
    result set. And then while processing the documents in the result set, we need
    to get the similar documents from the more like this `resultset` component as
    shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we get the similar documents for a document, we can loop through the documents
    and get the details of similar documents. The following screenshot shows the output
    of this program:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Running more like this query using PHP](graphics/4920OS_08_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We can see in the Solr logs that the two main parameters that are passed to
    Solr are `mlt=true` & `mlt.fl=author,series_t`. To see the same result directly
    from Solr we can use the following query at `http://localhost:8080/solr/collection1/select/?mlt=true&rows=10&mlt.count=2&mlt.mindf=1&mlt.fl=author,series_t&fl=id,name,author,series_t,score,price&start=0&q=cat:book&mlt.mintf=1`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here the parameters are explained as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**mlt.count**: This specifies the number of similar documents that we want
    to fetch for each document in the result set'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**mlt.mindf**: This is the minimum document frequency at which words will be
    ignored which do not occur in at least these many documents'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**mlt.mintf**: This is the minimum term frequency after which terms will be
    ignored in the source document'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More like this tuning parameters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let us see some additional functionality that can be used to tune the more
    like this feature. We can use the following functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '**setMinimumDocumentFrequency()** and **setMinimumTermFrequency()**: These
    are to set the minimum document frequency and minimum term frequency that we saw
    earlier. If the variables are not set they are not passed to Solr and Solr uses
    the default parameters of `minimumDocumentFrequency` as `5` and `minimumTermFrequency`
    as `2`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**setMinimumWordLength()**: This can be used to set the minimum word length
    below which words will be ignored.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**setMaximumWordLength()**: This can be used to set the maximum word length
    above which words will be ignored.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**setMaximumQueryTerms()**: This can be used to set the maximum number of query
    terms that will be included in any generated query. If it is not set it is not
    passed to Solr and in that case the default value of Solr which is 25 is applied.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**setMaximumNumberOfTokens()**: This can be used to set the maximum number
    of tokens to parse in each doc field that is not stored with `TermVector` support.
    Default is 5000 which will be applied if we do not pass any parameter to Solr.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**setBoost()**: If `true` it boosts the query by the interesting term relevance.
    Default is `false`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**setCount()**: This can be used to set the `mlt.count` parameter in Solr.
    It specifies how many similar documents to fetch for each document in the result
    set.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**setQueryFields()**: This can be used to specify the query fields and their
    boosts. These fields must also be set in `setFields()` function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distributed search
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When an index becomes too large to fit in a single machine, we can shard it
    and distribute it across multiple machines. Sharding requires a strategy that
    decides the shard to which a document is to be indexed based on certain values
    in certain fields. This strategy can be based on date, type of documents and so
    on. Though indexing has to happen separately for multiple shards, the search has
    to happen through a single interface. We should be able to specify the shards
    and the query should be run on all shards and return results for all shards. Solarium
    makes searching across multiple shards easy. Solarium supports distributed search
    through the `DistributedSearch` component. This allows us to query multiple shards
    using a single interface and get results from all shards.
  prefs: []
  type: TYPE_NORMAL
- en: Another way of scaling your search infrastructure is to create a master-slave
    Solr architecture. Master can be used to add documents in your index and slaves
    can be used to provide search. This architecture can help scale the search over
    a large number of servers. It is generally recommended to create an infrastructure
    that has both replicas and shards. The Solr cloud is a solution that provides
    such an infrastructure with easy management and monitoring.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up a distributed search
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let us create a new Solr instance that we will use as a separate shard. Go to
    the directory where Solr is installed and create a copy of the `example` folder,
    `example2`. This creates a new instance of Solr where the index and schema are
    copied. Now start the Solr server inside the `example2` folder. We have a new
    Solr instance running on port 8983\. Our earlier instance is running on port 8080.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The corresponding commands for Linux users are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: For Windows users, you can simply copy the `example` folder using Windows Explorer
    and then run `java-jar start.jar –Djetty.port=8983` in command prompt after `cd`
    to `example2` folder.
  prefs: []
  type: TYPE_NORMAL
- en: To kill an existing server press *Ctrl* + *C* on your command prompt where the
    Solr server is running.
  prefs: []
  type: TYPE_NORMAL
- en: To check the Solr instance go to `http://localhost:8983/solr/`.
  prefs: []
  type: TYPE_NORMAL
- en: Since we have copied the index as well, the new instance of Solr will have all
    the documents that we had. Let us delete all the documents from the index and
    push some new documents to the index.
  prefs: []
  type: TYPE_NORMAL
- en: '`http://localhost:8983/solr/update?stream.body=<delete><query>*:*</query></delete>`'
  prefs: []
  type: TYPE_NORMAL
- en: '`http://localhost:8983/solr/update?stream.body=<commit/>`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Add more books from the `books2.csv` file with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Executing a distributed search using PHP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To do a search on multiple shards, first we need to get the distributed search
    component from Solr. And then add shards to search as given in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: After executing the search we get a result set that has results from both the
    shards and can use it as we have used result sets earlier.
  prefs: []
  type: TYPE_NORMAL
- en: When we execute the search, we can see that Solr logs on both servers receive
    entries pertaining to this search. The parameters that are passed are `shard.url`
    (which contains the URL for Solr) and `isShard=true`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of adding shard one after another, we can add multiple shards in a
    single go using the `addShards()` function as given in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Solr has come out with the Solr cloud. So instead of using shards, we can set
    up a Solr cloud where data is automatically sharded and we can get in touch with
    any Solr instance to get the results of our query.
  prefs: []
  type: TYPE_NORMAL
- en: The Solr cloud has a concept of collections. So instead of adding cores, we
    will be required to add collections using the `addCollection()` or the `addCollections()`
    function. This functionality is available in Solarium 3.1 and higher.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up Solr master-slave
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can set up Solr replication where the master Solr server is used for indexing
    and both master and slave Solr servers can be used for searches. Setting up replication
    is pretty simple in Solr. Check out the `requestHandler` named/`replication`.
    Simply create a copy of the `example` folder inside Solr as `example3` and empty
    the index files from the `example3` folder with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'In the master server (`example` folder), change the `solrconfig.xml` file to
    add configuration parameters for the replication master with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Here, we have specified that replication should happen after a start up and
    after a commit on Solr. And the `schema.xml` and `stopwords.txt` files should
    be replicated.
  prefs: []
  type: TYPE_NORMAL
- en: 'On `slave` Solr (`example3` folder), change the `solrconfig.xml` file to add
    slave Solr configuration parameters. The parameters to be specified are the master
    Solr server URL and the poll interval for checking Solr. Here the poll interval
    is defined as HH:MM:SS (hours:minutes:seconds) as illustrated in the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Restart Tomcat and start up the Solr server in `example3` folder using the
    `java –jar start.jar` command we have used earlier. This will start Solr on port
    8983\. This Solr server will act as the slave and poll the master for updates.
    All the index files are replicated on the slave Solr as shown in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Setting up Solr master-slave](graphics/4920OS_08_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Solr interface for master server.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the Solr interface for the master server as shown in the preceding screenshot,
    we can see that it is defined as the replication master with a version number
    for the index. In the Solr slave server as shown in the following, we can see
    that there is a slave and reference to master. Note that the version numbers are
    matching in the screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Setting up Solr master-slave](graphics/4920OS_08_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Solr interface for slave server.
  prefs: []
  type: TYPE_NORMAL
- en: Load balancing Solr queries using PHP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Solarium comes with a load balancer plugin that can be used to build redundancy
    among multiple Solr servers. The load balancer plugin has the following features:'
  prefs: []
  type: TYPE_NORMAL
- en: Support for multiple servers, each with their own weight.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ability to use a failover mode—try another Solr server if a query fails.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Block certain query types. Updates are by default blocked.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Force a specific server for the next query.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Add all Solr servers where you want to distribute your queries to your Solarium
    client configuration. In our case, we will be adding the master and slave Solr
    servers that we have recently set up with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we need to create the endpoints from the Solarium client, get the load
    balancer plugin from the client and add the endpoints to the load balancer with
    respective weights as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Enable failover to another Solr server after query fails on any server. Limit
    maximum number of retries to 2 before declaring the Solr server as unreachable
    and failing over to another server with the following queries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Here we have set the weight of master server as `1` and slave server as `5`.
    So on average 1 out of 6 queries will go to the master. If we want to force a
    query on the master, we can use the `setForcedEndpointForNextQuery()` function
    as shown in the following query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed some of the advanced functionalities available
    in Solr and the Solarium library. We saw grouping of results based on field and
    query. And we also saw a functionality known as more like this, which can be used
    to extract results similar to a particular document from Solr based on some fields
    of the original document. We saw how to scale Solr beyond a single machine using
    replication and sharding. And we saw the functionalities provided by Solarium
    for scaling Solr.
  prefs: []
  type: TYPE_NORMAL
