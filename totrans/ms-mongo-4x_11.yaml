- en: Monitoring, Backup, and Security
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Monitoring, backup, and security should not be an afterthought, but a necessary
    process before deploying MongoDB in a production environment. In addition, monitoring
    can (and should) be used to troubleshoot and improve performance at the development
    stage.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will discuss the operational aspects of MongoDB. Having
    a backup strategy that produces correct and consistent backups, as well as making
    sure that our backup strategy will work in the unfortunate case that a backup
    is needed, will be covered in this chapter. Finally, we will discuss security
    for MongoDB for many different aspects, such as authentication, authorization,
    network-level security, and how to audit our security design.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will focus on the following three areas:'
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Backup
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Security
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we are designing a software system, we undertake many explicit and implicit
    assumptions. We always try to make the best decisions based on our knowledge,
    but there may be some parameters that we have underestimated or didn't take into
    account.
  prefs: []
  type: TYPE_NORMAL
- en: Using monitoring, we can validate our assumptions and verify that our application
    performs as intended and scales as expected. Good monitoring systems are also
    vital for detecting software bugs and to help us detect early potential security
    incidents.
  prefs: []
  type: TYPE_NORMAL
- en: What should we monitor?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By far, the most important metric to monitor in MongoDB is memory usage. MongoDB
    (and every database system, for what it's worth) uses system memory extensively
    to increase performance. No matter whether we use MMAPv1 or WiredTiger storage
    engines, the memory that's used is the first thing that we should keep our eyes
    on.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding how computer memory works can help us to evaluate metrics from
    our monitoring system. These are the most important concepts related to computer
    memory.
  prefs: []
  type: TYPE_NORMAL
- en: Page faults
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: RAM is fast, yet expensive. Hard disk drives, or solid state drives, are relatively
    cheaper and slower, and also provide durability for our data in the case of system
    and power failures. All of our data is stored on the disk, and when we perform
    a query, MongoDB will try to fetch data from memory. If the data is not in the
    memory, then it will fetch the data from the disk and copy it to the memory. This
    is a **page fault event**, because the data in the memory is organized in pages.
  prefs: []
  type: TYPE_NORMAL
- en: As page faults happen, the memory gets filled up, and eventually, some pages
    need to be cleared for more recent data to come into the memory. This is called a **page
    eviction event**. We cannot completely avoid page faults unless we have a really
    static dataset, but we do want to try to minimize page faults. This can be achieved
    by holding our working set in memory.
  prefs: []
  type: TYPE_NORMAL
- en: Resident memory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **resident memory** size is the total amount of memory that MongoDB owns
    in the RAM. This is the base metric to monitor, and it should be less than 80%
    of the available memory.
  prefs: []
  type: TYPE_NORMAL
- en: Virtual and mapped memory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When MongoDB asks for a memory address, the operating system will return a virtual
    address. This may or may not be an actual address in the RAM, depending on where
    the data resides. MongoDB will use this virtual address to request the underlying
    data. When we have journaling enabled (which should be almost always), MongoDB
    will keep another address on record for the journaled data. The virtual memory
    refers to the size of all of the data requested by MongoDB, including the journaling.
  prefs: []
  type: TYPE_NORMAL
- en: The mapped memory excludes journaling references.
  prefs: []
  type: TYPE_NORMAL
- en: What all of this means is that over time, our mapped memory will be roughly
    equal to our working set, and the virtual memory will be around twice the amount
    of our mapped memory.
  prefs: []
  type: TYPE_NORMAL
- en: Working sets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The working set is the data size that MongoDB uses. In the case of a transactional
    database, this will end up being the data size that MongoDB holds, but there may
    be cases where we have collections that are not used at all and will not contribute
    to our working set.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring memory usage in WiredTiger
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Understanding the memory usage in MMAPv1 is relatively straightforward. MMAPv1
    uses the `mmap()` system call under the hood to pass on the responsibility of
    the memory page to the underlying operating system. That is why when we use MMAPv1,
    the memory usage will grow unbounded, as the operating system is trying to fit
    as much of our dataset into the memory as possible.
  prefs: []
  type: TYPE_NORMAL
- en: With WiredTiger, on the other hand, we define the internal cache memory usage
    on startup. By default, the internal cache will be, at a maximum, between half
    of our RAM, which is in-between 1 GB or 256 MB.
  prefs: []
  type: TYPE_NORMAL
- en: On top of the internal cache, there is also memory that MongoDB can allocate
    for other operations, like maintaining connections and data processing (in-memory
    sort, MapReduce, aggregation, and more).
  prefs: []
  type: TYPE_NORMAL
- en: MongoDB processes will also use the underlying operating system's filesystem
    cache, just like in MMAPv1\. The data in the filesystem cache is compressed.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can view the settings for the WiredTiger cache via the mongo shell, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: We can adjust its size by using the `storage.wiredTiger.engineConfig.cacheSizeGB`
    parameter.
  prefs: []
  type: TYPE_NORMAL
- en: The generic recommendation is to leave the WiredTiger internal cache size at
    its default. If our data has a high compression ratio, it may be worth reducing
    the internal cache size by 10% to 20% to free up more memory for the filesystem
    cache.
  prefs: []
  type: TYPE_NORMAL
- en: Tracking page faults
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The number of page faults can remain fairly stable and not affect performance
    significantly. However, once the number of page faults reaches a certain threshold,
    our system will be quickly and severely degraded. This is even more evident for
    HDDs, but it affects **solid-state drives** (**SSDs**) as well.
  prefs: []
  type: TYPE_NORMAL
- en: The way to ensure that we don't run into problems regarding page faults is to
    always have a staging environment that is identical to our production in setup.
    This environment can be used to stress test how many page faults our system can
    handle, without deteriorating performance. Comparing the actual number of page
    faults in our production system with the maximum number of page faults that we
    calculated from our staging system, we can find out how much leeway we have left.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another way to view page faults is via the shell, looking at the `extra_info`
    field of the `serverStatus` output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: As the `note` states, these fields may not be present in every platform.
  prefs: []
  type: TYPE_NORMAL
- en: Tracking B-tree misses
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you saw in the previous chapter, proper indexing is the best way to keep
    MongoDB responsive and performant. B-tree misses refer to page faults that happen
    when we try to access a B-tree index. Indexes are usually used frequently, and
    are relatively small compared to our working set and the memory available, so
    they should be in the memory at all times.
  prefs: []
  type: TYPE_NORMAL
- en: If we have an increasing number of B-tree misses or ratio of B-tree hits, or
    if there is a decrease in the number of B-tree misses, it's a sign that our indexes
    have grown in size and/or are not optimally designed. B-tree misses can also be
    monitored via MongoDB Cloud Manager, or in the shell.
  prefs: []
  type: TYPE_NORMAL
- en: In the shell, we can use collection stats to locate it.
  prefs: []
  type: TYPE_NORMAL
- en: I/O wait
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**I/O wait** refers to the time that the operating system waits for an I/O
    operation to complete. It has a strongly positive correlation with page faults.
    If we see an I/O wait increasing over time, it''s a strong indication that page
    faults will follow, as well. We should aim to keep the I/O wait at less than 60%
    to 70% for a healthy operational cluster. Aiming for a threshold like this will
    buy us some time to upgrade in the case of a suddenly increased load.'
  prefs: []
  type: TYPE_NORMAL
- en: Read and write queues
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another way to look at I/O wait and page faults is via read and write queues.
    When we have page faults and I/O wait, requests will inevitably start to queue
    for either reads or writes. Queues are the effect, rather than the root cause,
    so by the time the queues start building up, we know we have a problem to solve.
  prefs: []
  type: TYPE_NORMAL
- en: Lock percentage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This is more of an issue with earlier versions of MongoDB, and less of an issue
    when using the WiredTiger storage engine. The **lock percentage** shows the percentage
    of time that the database is locked up, waiting for an operation that uses an
    exclusive lock to release it. It should generally be low: 10% to 20%, at the most.
    Over 50% means it''s a cause for concern.'
  prefs: []
  type: TYPE_NORMAL
- en: Background flushes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: MongoDB will flush data to the disk every minute, by default. The **background
    flush** refers to the time it takes for the data to persist to the disk. It should
    not be more than 1 second for every 1 minute period.
  prefs: []
  type: TYPE_NORMAL
- en: Modifying the flush interval may help with the background flush time; by writing
    to the disk more frequently, there will be less data to write. This could, in
    some cases, make writes faster.
  prefs: []
  type: TYPE_NORMAL
- en: The fact that the background flush time gets affected by the write load means
    that if our background flush time starts to get too high, we should consider sharding
    our database to increase the write capacity.
  prefs: []
  type: TYPE_NORMAL
- en: Tracking free space
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A common issue when using MMAPv1 (less frequent with WiredTiger) is free disk
    space. Like the memory, we need to track the disk space usage and be proactive,
    rather than reactive, with it. Keep monitoring the disk space usage, with proper
    alerts when it reaches 40%, 60%, or 80% of the disk space, especially for datasets
    that grow quickly.
  prefs: []
  type: TYPE_NORMAL
- en: Disk space issues are often the ones that cause the most headaches for administrators,
    DevOps, and developers, because of the time it takes to move data around.
  prefs: []
  type: TYPE_NORMAL
- en: The `directoryperdb` option can help with data sizing, as we can split our storage
    into different physically mounted disks.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring replication
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Replica sets use the **operations log** (**oplog**) to keep the synced state.
    Every operation gets applied on the primary server, and then gets written in the
    primary server's oplog, which is a capped collection. Secondaries read this oplog
    asynchronously and apply the operations one by one.
  prefs: []
  type: TYPE_NORMAL
- en: If the primary server gets overloaded, then the secondaries won't be able to
    read and apply the operations fast enough, generating replication lag. **Replication
    lag** is counted as the time difference between the last operation applied on
    the primary and the last operation applied on the secondary, as stored in the
    oplog capped collection.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if the time is 4:30:00 PM and the secondary just applied an operation
    that was applied on our primary server at 4:25:00 PM, this means that the secondary
    is lagging five minutes behind our primary server.
  prefs: []
  type: TYPE_NORMAL
- en: In our production cluster, the replication lag should be close to (or equal
    to) zero.
  prefs: []
  type: TYPE_NORMAL
- en: Oplog size
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Every member in a replica size will have a copy of the oplog in `db.oplog.rs()`.
    The reason for this is that if the primary steps down, one of the secondaries
    will get elected, and it needs to have an up-to-date version of the oplog for
    the new secondaries to track.
  prefs: []
  type: TYPE_NORMAL
- en: The oplog size is configurable, and we should set it to be as large as possible.
    The oplog size doesn't affect the memory usage, and can make or break the database
    in cases of operational issues.
  prefs: []
  type: TYPE_NORMAL
- en: The reason for this is that if the replication lag increases over time, we will
    eventually get to the point where the secondaries will fall so behind that the
    primary server won't be able to read from the primary's oplog, as the oldest entry
    in the primary's oplog will be later than the latest entry that was applied in
    our secondary server.
  prefs: []
  type: TYPE_NORMAL
- en: In general, the oplog should be at least one to two days' worth of operations.
    The oplog should be longer than the time it takes for the initial sync, for the
    same reason that was detailed previously.
  prefs: []
  type: TYPE_NORMAL
- en: Working set calculations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The working set is the strongest indicator of our memory requirements. Ideally,
    we would like to have our entire dataset in the memory, but most of the time, this
    is not feasible. The next best thing is to have our working set in memory. The
    working set can be calculated directly or indirectly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Directly, we had the `workingSet` flag in `serverStatus`, which we can invoke
    from the shell, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Unfortunately, this was removed in version 3.0, so we will focus on the indirect
    method of calculating a working set.
  prefs: []
  type: TYPE_NORMAL
- en: Indirectly, our working set is the size of data that we need to satisfy 95%
    or more of our user's requests. To calculate this, we need to identify the queries
    that the users make and which datasets they use from the logs. Adding 30% to 50%
    to it for index memory requirements, we can arrive at the working set calculation.
  prefs: []
  type: TYPE_NORMAL
- en: Another indirect way of estimating the working size is through the number of
    page faults. If we don't have page faults, then our working set fits in the memory.
    Through trial and error, we can estimate the point at which the page faults start
    to happen and understand how much more of a load our system can handle.
  prefs: []
  type: TYPE_NORMAL
- en: If we can't have the working set in memory, then we should have at least enough
    memory so that the indexes can fit in memory. In the previous chapter, we described
    how we can calculate index memory requirements, and how we can use this calculation
    to size our RAM accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring tools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are several options for monitoring. In this section, we will discuss how
    we can monitor by using MongoDB's own tools or third-party tools.
  prefs: []
  type: TYPE_NORMAL
- en: Hosted tools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: MongoDB, Inc.'s own tool, MongoDB Cloud Manager (formerly MongoDB Monitoring
    Service), is a robust tool for monitoring all of the metrics that were described
    previously. MongoDB Cloud Manager has a limited free tier and a 30 day trial period.
  prefs: []
  type: TYPE_NORMAL
- en: Another option for using MongoDB Cloud Manager is via MongoDB Atlas, MongoDB,
    Inc.'s DBaaS offering. This also has a limited free tier, and is available in
    all three major cloud providers (Amazon, Google, and Microsoft).
  prefs: []
  type: TYPE_NORMAL
- en: Open source tools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All major open source tools, like **Nagios**, **Munin**, **Cacti**, and others,
    provide plugin support for MongoDB. Although it is beyond the scope of this book,
    operations and DevOps should be familiar with both setting up and understanding
    the metrics that were described previously in order to effectively troubleshoot
    MongoDB and preemptively resolve issues before they grow out of proportion.
  prefs: []
  type: TYPE_NORMAL
- en: The `mongotop` and `mongostat` commands and scripts in the mongo shell can also
    be used for ad hoc monitoring. One of the risks with such manual processes, however,
    is that any failure of the scripts may jeopardize our database. If there are well-known
    and tested tools for your monitoring needs, please avoid writing your own.
  prefs: []
  type: TYPE_NORMAL
- en: Backups
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A quote from a well-known maxim is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '"Hope for the best, plan for the worst."'
  prefs: []
  type: TYPE_NORMAL
- en: – John Jay (1813)
  prefs: []
  type: TYPE_NORMAL
- en: This should be our approach when designing our backup strategy for MongoDB.
    There are several distinct failure events that can happen.
  prefs: []
  type: TYPE_NORMAL
- en: Backups should be the cornerstone of our disaster recovery strategy, in case
    something happens. Some developers may rely on replication for disaster recovery,
    as it seems that having three copies of our data is more than enough. We can always
    rebuild the cluster from the other two copies, in case one of the copies is lost.
  prefs: []
  type: TYPE_NORMAL
- en: This is the case in the event of disks failing. Disk failure is one of the most
    common failures in a production cluster, and will statistically happen once the
    disks start reaching their **mean time between failures** (**MTBF**) time.
  prefs: []
  type: TYPE_NORMAL
- en: However, it is not the only failure event that can happen. Security incidents,
    or purely human errors, are just as likely to happen, and should be a part of
    our plan, as well. Catastrophic failures by means of losing all replica set members
    at once, from a fire, a flood, an earthquake, or a disgruntled employee, are events
    that should not lead to production data loss.
  prefs: []
  type: TYPE_NORMAL
- en: A useful interim option, in the middle ground between replication and implementing
    proper backups, could be setting up a delayed replica set member. This member
    can lag several hours or days behind the primary server so that it will not be
    affected by malicious changes in the primary. The important detail to take into
    account is that the oplog needs to be configured so that it can hold several hours
    of delay. Also, this solution is only an interim, as it doesn't take into account
    the full range of reasons why we need disaster recovery, but can definitely help
    with a subset of them.
  prefs: []
  type: TYPE_NORMAL
- en: This is called **disaster recovery**. Disaster recovery is a class of failures
    that require backups to be taken not only regularly, but also by using a process
    that isolates them (both geographically and in terms of access rules) from our
    production data.
  prefs: []
  type: TYPE_NORMAL
- en: Backup options
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Depending on our deployment strategy, we can choose different options for backups.
  prefs: []
  type: TYPE_NORMAL
- en: Cloud-based solutions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The most straightforward solution arises if we are using a cloud DBaaS solution.
    In the example of MongoDB, Inc.'s own MongoDB Atlas, we can manage backups from
    the GUI.
  prefs: []
  type: TYPE_NORMAL
- en: If we host MongoDB in our own servers, we can then use MongoDB, Inc.'s MongoDB
    Cloud Manager. Cloud Manager is a SaaS that we can point to our own servers to
    monitor and back up our data. It uses the same oplog that replication uses, and
    can back up both replica sets and sharded clusters.
  prefs: []
  type: TYPE_NORMAL
- en: If we don't want to (or can't, for security reasons) point our servers to an
    external SaaS service, we can use MongoDB Cloud Manager's functionality on-premises,
    using MongoDB Ops Manager. To get MongoDB Ops Manager, we need to get a subscription
    to the Enterprise Advanced edition of MongoDB for our cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Backups with filesystem snapshots
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The most common backup method in the past, and one that is still widely used,
    relies on the underlying filesystem point-in-time snapshots functionality to back
    up our data.
  prefs: []
  type: TYPE_NORMAL
- en: EBS on EC2, and **Logical Volume Manager** (**LVM**) on Linux, support point-in-time
    snapshots.
  prefs: []
  type: TYPE_NORMAL
- en: If we use WiredTiger with the latest version of MongoDB, we can have volume-level
    backups, even if our data and journal files reside in different volumes.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can make a backup of a replica set as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: To make a backup of a replica set, we need to have a consistent state for our
    database. This implies that we have all of our writes either committed to the
    disk or in our journal files.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we use WiredTiger storage, our snapshot will be consistent as of the latest
    checkpoint, which is either 2 GB of data or the last minute backup.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensure that you store the snapshot in an offsite volume for disaster recovery
    purposes. You need to have enabled journaling to use point-in-time snapshots.
    It's a good practice to enable journaling regardless.
  prefs: []
  type: TYPE_NORMAL
- en: Making a backup of a sharded cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If we want to make a backup of an entire sharded cluster, we need to stop the
    balancer before starting. The reason is that if there are chunks migrating between
    different shards at the time that we take our snapshot, our database will be in
    an inconsistent state, having either incomplete or duplicate data chunks that
    were in flight at the time we took our snapshot.
  prefs: []
  type: TYPE_NORMAL
- en: Backups from an entire sharded cluster will be approximate-in-time. If we need
    point-in-time precision, we need to stop all of the writes in our database, something
    that is generally not possible for production systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to disable the balancer by connecting to our mongos through
    the mongo shell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Then, if we don't have journaling enabled in our secondaries, or if we have
    journal and data files in different volumes, we need to lock our secondary mongo
    instances for all shards and the config server replica set.
  prefs: []
  type: TYPE_NORMAL
- en: We also need to have a sufficient oplog size in these servers so that they can
    catch up to the primaries once we unlock them; otherwise, we will need to resync
    them from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given that we don''t need to lock our secondaries, the next step is to back
    up the config server. In Linux (and using LVM), this would be similar to doing
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Then, we need to repeat the same process for a single member from each replica
    set in each shard.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we need to restart the balancer using the same mongo shell that we
    used to stop it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Without going into too much detail here, it's evident that making a backup of
    a sharded cluster is a complicated and time-consuming procedure. It needs prior
    planning and extensive testing to make sure that it not only works with minimal
    disruption, but also that our backups are usable and can be restored back to our
    cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Making backups using mongodump
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `mongodump` tool is a command-line tool that can make a backup of the data
    in our MongoDB cluster. As such, the downside is that all of the indexes need
    to be recreated on restore, which may be a time-consuming operation.
  prefs: []
  type: TYPE_NORMAL
- en: The major downside that the `mongodump` tool has is that in order to write data
    to the disk, it needs to bring data from the internal MongoDB storage to the memory
    first. This means that in the case of production clusters running under strain,
    `mongodump` will invalidate the data residing in the memory from the working set
    with the data that would not be residing in the memory under regular operations.
    This degrades the performance of our cluster.
  prefs: []
  type: TYPE_NORMAL
- en: On the plus side, when we use `mongodump`, we can continue taking writes in
    our cluster, and if we have a replica set, we can use the `--oplog` option to
    include the entries that occur during the `mongodump` operation in its output
    oplog.
  prefs: []
  type: TYPE_NORMAL
- en: If we go with that option, we need to use `--oplogReplay` when we use the `mongorestore`
    tool to restore our data back to the MongoDB cluster.
  prefs: []
  type: TYPE_NORMAL
- en: '`mongodump` is a great tool for single-server deployments, but once we get
    to larger deployments, we should consider using different (and better planned)
    approaches to back up our data.'
  prefs: []
  type: TYPE_NORMAL
- en: Backing up by copying raw files
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If we don''t want to use any of the preceding options that were outlined, our
    last resort is to copy the raw files using `cp`/`rsync`, or something equivalent.
    This is generally not recommended, for the following reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: We need to stop all of the writes before copying files
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The backup size will be larger, since we need to copy indexes and any underlying
    padding and fragmentation storage overhead
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We cannot get point-in-time recovery by using this method for replica sets,
    and copying data from sharded clusters in a consistent and predictable manner
    is extremely difficult
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Making a backup by copying raw files should be avoided, unless no other option
    really exists.
  prefs: []
  type: TYPE_NORMAL
- en: Making backups using queuing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another strategy that's used in practice is utilizing a queuing system, intercepting
    our database and the frontend software system. Having something like an ActiveMQ
    queue before the inserts/updates/deletes in our database means that we can safely
    send out data to different sinks, which are MongoDB servers or log files in a
    separate repository. Like the delayed replica set method, this method can be useful
    for a class of backup problems, but can fail for some others.
  prefs: []
  type: TYPE_NORMAL
- en: This is a useful interim solution, but it should not be used as a permanent
    one.
  prefs: []
  type: TYPE_NORMAL
- en: EC2 backup and restore
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: MongoDB Cloud Manager can automate making backups from EC2 volumes; and, since
    our data is in the cloud, why not use the Cloud Manager anyway?
  prefs: []
  type: TYPE_NORMAL
- en: 'If we can''t use it for some reason, we can write a script to make backup by
    implementing the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Assuming that we have journaling enabled (and we really should) and we have
    already mapped `dbpath`, containing data and journal files to a single EBS volume,
    we first need to find the EBS block instances associated with the running instance
    by using `ec2-describe-instances`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The next step is to find the logical volumes that `dbpath` of our MongoDB database
    is mapped to using `lvdisplay`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once we have identified the logical devices from the logical volumes, we can
    use `ec2-create-snapshot` to create new snapshots. We need to include each and
    every logical device that maps to our `dbpath` directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To verify that our backups work, we need to create new volumes based on the
    snapshots and mount the new volumes there. Finally, the `mongod` process should
    be able to start mounting the new data, and we should connect by using MongoDB
    to verify these.
  prefs: []
  type: TYPE_NORMAL
- en: Incremental backups
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Making full backups every time may be viable for some deployments, but as the
    size reaches a certain threshold, full backups take too much time and space.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, we will want to make full backups every once in a while (maybe
    one per month, for example) and incremental backups in-between (for example, nightly).
  prefs: []
  type: TYPE_NORMAL
- en: Both Ops Manager and Cloud Manager support incremental backups, and if we get
    to this size, it may be a good idea to use a tool to make our backups instead
    of rolling out our own.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we don''t want to (or can''t) use these tools, we have the option of restoring
    via the oplog, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Make a full backup with any method that was described previously
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Lock writes on the secondary server of our replica set
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note the latest entry in the oplog
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Export the entries in the oplog after the latest entry in the oplog:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Unlock writes on the secondary server
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To restore, we can use the `oplog.rs` file that we just exported, and use `mongorestore`
    with the option `--oplogReplay`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: This method requires locking writes, and may not work in future versions.
  prefs: []
  type: TYPE_NORMAL
- en: An even better solution is to use the **L****ogical Volume Management (LVM)** filesystem
    with incremental backups, but this depends on the underlying LVM implementation,
    which we may or may not be able to tweak.
  prefs: []
  type: TYPE_NORMAL
- en: Security
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Security is a multifaceted goal in a MongoDB cluster. For the rest of this chapter,
    we will examine different attack vectors and how we can protect against them.
    In addition to these best practices, developers and administrators must always
    use common sense so that security interferes only as much as is required for operational
    goals.
  prefs: []
  type: TYPE_NORMAL
- en: Authentication
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Authentication** refers to verifying the identity of a client. This prevents
    the impersonation of someone in order to gain access to their data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The simplest way to authenticate is by using a `username` and `password` pair.
    This can be done via the shell in two ways, the first of which is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Passing in a comma-separated `username` and `password` will assume the default
    values for the rest of the fields:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: If we pass a document object, we can define more parameters than `username`/`password`.
  prefs: []
  type: TYPE_NORMAL
- en: The (authentication) `mechanism` parameter can take several different values,
    with the default being `SCRAM-SHA-1`. The parameter value `MONGODB-CR` is used
    for backwards compatibility with versions earlier than 3.0.
  prefs: []
  type: TYPE_NORMAL
- en: MONGODB-x.509 is used for TLS/SSL authentication. Users and internal replica
    set servers can be authenticated by using SSL certificates, which are self-generated
    and signed, or comes from a trusted third-party authority.
  prefs: []
  type: TYPE_NORMAL
- en: To configure x.509 for internal authentication of replica set members, we need
    to supply one of the following parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is for the configuration file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is used on the command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'MongoDB Enterprise Edition, the paid offering from MongoDB, Inc., adds two
    more options for authentication, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The first added option is **Generic Security Service Application Program Interface**
    (**GSSAPI**) Kerberos. Kerberos is a mature and robust authentication system that
    can be used for Windows-based Active Directory deployments, among others.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The second added option is PLAIN (LDAP SASL). LDAP is just like Kerberos: a
    mature and robust authentication mechanism. The main consideration when using
    the PLAIN authentication mechanism is that the credentials are transmitted in
    plain text over the wire. This means that we should secure the path between the
    client and server via VPN or a TSL/SSL connection to avoid a man in the middle
    stealing our credentials.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Authorization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After we have configured the authentication to verify that the users are who
    they claim they are when connecting to our MongoDB server, we need to configure
    the rights that each one of them will have in our database.
  prefs: []
  type: TYPE_NORMAL
- en: This is the **authorization** aspect of permissions. MongoDB uses role-based
    access control to control permissions for different user classes.
  prefs: []
  type: TYPE_NORMAL
- en: Every role has permissions to perform some actions on a resource.
  prefs: []
  type: TYPE_NORMAL
- en: A resource can be a collection/collections or a database/databases.
  prefs: []
  type: TYPE_NORMAL
- en: 'The command''s format is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'If we specify `""` (an empty string) for either `db` or `collection`, it means
    any `db` or `collection`. An example of this is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: This would apply our action in every `collection` in the `mongo_books` database.
  prefs: []
  type: TYPE_NORMAL
- en: If the database is not the `admin` database, this will not include the system
    collections. System collections, such as `<db>.system.profile`, `<db>.system.js`,
    `admin.system.users`, and `admin.system.roles`, need to be defined explicitly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similar to the preceding option, we can define the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: We define this to apply our rule to all of the collections across all of the
    databases, except for system collections, of course.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also apply rules across an entire cluster, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The preceding example grants privileges for the `addShard` action (adding a
    new shard to our system) across the entire cluster. The cluster resource can only
    be used for actions that affect the entire cluster, rather than a collection or
    database (for example, `shutdown`, `replSetReconfig`, `appendOplogNote`, `resync`,
    `closeAllDatabases`, and `addShard`).
  prefs: []
  type: TYPE_NORMAL
- en: What follows is an extensive list of cluster-specific actions, and some of the
    most widely used actions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The list of the most widely used actions is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`find`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`insert`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`remove`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`update`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bypassDocumentValidation`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`viewRole`/`viewUser`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`createRole`/`dropRole`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`createUser`/`dropUser`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`inprog`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`killop`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`replSetGetConfig`/`replSetConfigure`/`replSetStateChange`/`resync`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`getShardMap`/`getShardVersion`/`listShards`/`moveChunk`/`removeShard`/`addShard`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dropDatabase`/`dropIndex`/`fsync`/`repairDatabase`/`shutDown`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`serverStatus`/`top`/`validate`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cluster-specific actions are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`unlock`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`authSchemaUpgrade`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cleanupOrphaned`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cpuProfiler`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`inprog`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`invalidateUserCache`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`killop`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`appendOplogNote`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`replSetConfigure`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`replSetGetConfig`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`replSetGetStatus`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`replSetHeartbeat`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`replSetStateChange`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`resync`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`addShard`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`flushRouterConfig`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`getShardMap`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`listShards`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`removeShard`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`shardingState`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`applicationMessage`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`closeAllDatabases`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`connPoolSync`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fsync`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`getParameter`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hostInfo`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logRotate`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`setParameter`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`shutdown`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`touch`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`connPoolStats`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cursorInfo`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`diagLogging`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`getCmdLineOpts`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`getLog`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`listDatabases`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`netstat`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`serverStatus`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`top`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If this sounds too complicated, that's because it is! The flexibility that MongoDB
    allows for configuring different actions on resources means that we need to study
    and understand the extensive lists, as described previously.
  prefs: []
  type: TYPE_NORMAL
- en: Thankfully, some of the most common actions and resources are bundled in built-in
    roles.
  prefs: []
  type: TYPE_NORMAL
- en: We can use these built-in roles to establish the baseline of permissions that
    we will give to our users, and then fine-grain these based on the extensive list.
  prefs: []
  type: TYPE_NORMAL
- en: User roles
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are two different generic user roles that we can specify, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`read`: A read-only role across non-system collections and the following system
    collections: `system.indexes`, `system.js`, and `system.namespaces` collections'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`readWrite`: A read and modify role across non-system collections and the `system.js`
    collection'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Database administration roles
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are three database-specific administration roles, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`dbAdmin`: The basic admin user role that can perform schema-related tasks,
    indexing, and gathering statistics. A `dbAdmin` cannot perform user and role management.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`userAdmin`: Create and modify roles and users. This is complementary to the
    `dbAdmin` role.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A `userAdmin` can modify itself to become a superuser in the database, or, if
    scoped to the `admin` database, the MongoDB cluster.
  prefs: []
  type: TYPE_NORMAL
- en: '`dbOwner`: Combining `readWrite`, `dbAdmin`, and `userAdmin` roles, this is
    the most powerful admin user role.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cluster administration roles
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are the cluster-wide administration roles that are available:'
  prefs: []
  type: TYPE_NORMAL
- en: '`hostManager`: Monitor and manage servers in a cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`clusterManager`: Provides management and monitoring actions on the cluster.
    A user with this role can access the config and local databases, which are used
    in sharding and replication, respectively.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`clusterMonitor`: Read-only access for monitoring tools provided by MongoDB,
    such as MongoDB Cloud Manager and the Ops Manager agent.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`clusterAdmin`: Provides the greatest cluster management access. This role
    combines the privileges that are granted by the `clusterManager`, `clusterMonitor`,
    and `hostManager` roles. Additionally, the role provides the `dropDatabase` action.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Backup and restore roles
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Role-based authorization roles can be defined in the backup and restore granularity
    level, as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '`backup`: Provides privileges that are needed to back up the data. This role
    provides sufficient privileges to use the MongoDB Cloud Manager backup agent,
    the Ops Manager backup agent, or `mongodump`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`restore`: Provides the privileges that are needed to restore data with `mongorestore`,
    without the `--oplogReplay` option or `system.profile` collection data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Roles across all databases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Similarly, the following is the set of available roles across all databases:'
  prefs: []
  type: TYPE_NORMAL
- en: '`readAnyDatabase`: Provides the same read-only permissions as `read`, except
    it applies to all but the local and config databases in the cluster. The role
    also provides the `listDatabases` action on the cluster as a whole.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`readWriteAnyDatabase`: Provides the same read and write permissions as `readWrite`,
    except it applies to all but the local and config databases in the cluster. The
    role also provides the `listDatabases` action on the cluster as a whole.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`userAdminAnyDatabase`: Provides the same access to user administration operations
    as `userAdmin`, except it applies to all but the local and config databases in
    the cluster. Since the `userAdminAnyDatabase` role allows users to grant any privilege
    to any user, including themselves, the role also indirectly provides superuser
    access.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dbAdminAnyDatabase`: Provides the same access to database administration operations
    as `dbAdmin`, except it applies to all but the local and config databases in the
    cluster. The role also provides the `listDatabases` action on the cluster as a
    whole.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Superuser
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Finally, the following are the superuser roles that are available:'
  prefs: []
  type: TYPE_NORMAL
- en: '`root`: Provides access to the operations and all of the resources of the `readWriteAnyDatabase`,
    `dbAdminAnyDatabase`, `userAdminAnyDatabase`, `clusterAdmin`, `restore`, and `backup`
    combined'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`__internal`: Similar to the root user, any `__internal` user can perform any
    action against any object across the server'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Superuser roles should be avoided, as they can have potentially destructive
    permissions across all of the databases on our server.
  prefs: []
  type: TYPE_NORMAL
- en: Network-level security
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Apart from MongoDB-specific security measures, there are best practices that
    have been established for network-level security:'
  prefs: []
  type: TYPE_NORMAL
- en: Only allow communication between servers, and only open the ports that are used
    for communicating between them.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Always use TLS/SSL for communication between servers. This prevents man-in-the-middle
    attacks from impersonating a client.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Always use different sets of development, staging, and production environments
    and security credentials. Ideally, create different accounts for each environment,
    and enable two-factor authentication in both staging and production environments.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Auditing security
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: No matter how much we plan our security measures, a second or third pair of
    eyes from someone outside of our organization can give a different view of our
    security measures and uncover problems that we may have underestimated or overlooked.
    Don't hesitate to involve security experts and white hat hackers to do penetration
    testing in your servers.
  prefs: []
  type: TYPE_NORMAL
- en: Special cases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Medical or financial applications require added levels of security for data
    privacy reasons.
  prefs: []
  type: TYPE_NORMAL
- en: If we are building an application in the healthcare space, accessing users'
    personally identifiable information, we may need to get HIPAA certified.
  prefs: []
  type: TYPE_NORMAL
- en: If we are building an application that interacts with payments and manages cardholder
    information, we may need to become PCI/DSS compliant.
  prefs: []
  type: TYPE_NORMAL
- en: The specifics of each certification are outside the scope of this book, but
    it is important to know that MongoDB has use cases in these fields that fulfill
    the requirements, and, as such, it can be the right tool with proper design beforehand.
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Summing up the best practice recommendations involving security, we have the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Enforce authentication**: Always enable authentication in production environments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Enable access control**: First, create a system administrator, and then use
    that administrator to create more limited users. Give as few permissions as are
    needed for each user role.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Define fine-grained roles in access control**: Do not give more permissions
    than are needed for each user.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Encrypt communication between clients and servers**: Always use TLS/SSL for
    communication between clients and servers in production environments. Always use
    TLS/SSL for communication between `mongod` and `mongos` or config servers, as
    well.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Encrypt data at rest**: MongoDB Enterprise Edition offers the functionality
    to encrypt data when stored, using WiredTiger encryption at rest.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alternatively, we can encrypt data using filesystem, device, or physical encryption.
    In the cloud, we often get the option for encryption, as well (for example, with
    EBS on Amazon EC2).
  prefs: []
  type: TYPE_NORMAL
- en: '**Limit network exposure**: MongoDB servers should only be connected to the
    application servers and any other servers that are needed for operations. Ports
    other than the ones that we set up for MongoDB communications should not be open
    to the outside world. If we want to debug MongoDB usage, it''s important to have
    a proxy server with controlled access set up to communicate with our database.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Audit servers for unusual activity**: MongoDB Enterprise Edition offers a
    utility for auditing. By using it, we can output events to the console, a JSON
    file, a BSON file, or the syslog. In any case, it''s important to make sure that
    audit events are stored in a partition that is not available to the system''s
    users.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use a dedicated operating system user to run MongoDB. Make sure that the dedicated
    operating system user can access MongoDB, but doesn't have unnecessary permissions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Disable JavaScript server-side scripts if they are not needed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MongoDB can use JavaScript for server-side scripts with the following commands: `mapReduce()`,
    `group()`, and `$where`. If we don't need these commands, we should disable server-side
    scripting by using the `--noscripting` option on the command line.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, you learned about three operational aspects of MongoDB: monitoring,
    backup, and security.'
  prefs: []
  type: TYPE_NORMAL
- en: We discussed the metrics that we should monitor in MongoDB, and how to monitor
    them. Following that, we discussed how to make backups and ensure that we can
    use them to restore our data. Finally, you learned about security with the authentication
    and authorization concepts, as well as network-level security and how to audit
    it.
  prefs: []
  type: TYPE_NORMAL
- en: As important as it is to design, build, and extend our application as needed,
    it is equally important to make sure that we have peace of mind during operations
    and are safeguarded from unexpected events, such as human error and internal or
    external malicious users.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will learn about pluggable storage engines, a new concept
    that was introduced in version 3.0 of MongoDB. Pluggable storage engines allow
    different use cases to be served, especially in application domains that have
    specific and stringent requirements concerning data handling and privacy.
  prefs: []
  type: TYPE_NORMAL
