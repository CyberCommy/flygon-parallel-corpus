- en: Scraping Challenges and Solutions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover:'
  prefs: []
  type: TYPE_NORMAL
- en: Retrying failed page downloads
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Supporting page redirects
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Waiting for content to be available in Selenium
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Limiting crawling to a single domain
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Processing infinitely scrolling pages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Controlling the depth of a crawl
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Controlling the length of a crawl
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handling paginated websites
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handling forms and form-based authorization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handling basic authorization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preventing bans by scraping via proxies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Randomizing user agents
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Caching responses
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Developing a reliable scraper is never easy, there are so many *what ifs* that
    we need to take into account. What if the website goes down? What if the response
    returns unexpected data? What if your IP is throttled or blocked? What if authentication
    is required? While we can never predict and cover all *what ifs*, we will discuss
    some common traps, challenges, and workarounds.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that several of the recipes require access to a website that I have provided
    as a Docker container. They require more logic than the simple, static site we
    used in earlier chapters. Therefore, you will need to pull and run a Docker container
    using the following Docker commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Retrying failed page downloads
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Failed page requests can be easily handled by Scrapy using retry middleware.
    When installed, Scrapy will attempt retries when receiving the following HTTP
    error codes:'
  prefs: []
  type: TYPE_NORMAL
- en: '`[500, 502, 503, 504, 408]`'
  prefs: []
  type: TYPE_NORMAL
- en: 'The process can be further configured using the following parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`RETRY_ENABLED` (True/False - default is True)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RETRY_TIMES` (# of times to retry on any errors - default is 2)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RETRY_HTTP_CODES` (a list of HTTP error codes which should be retried - default
    is [500, 502, 503, 504, 408])'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `06/01_scrapy_retry.py` script demonstrates how to configure Scrapy for
    retries. The script file contains the following configuration for Scrapy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: How it works
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Scrapy will pick up the configuration for retries as specified when the spider
    is run. When encountering errors, Scrapy will retry up to three times before giving
    up.
  prefs: []
  type: TYPE_NORMAL
- en: Supporting page redirects
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Page redirects in Scrapy are handled using redirect middleware, which is enabled
    by default. The process can be further configured using the following parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`REDIRECT_ENABLED`: (True/False - default is True)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`REDIRECT_MAX_TIMES`: (The maximum number of redirections to follow for any
    single request - default is 20)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The script in `06/02_scrapy_redirects.py` demonstrates how to configure Scrapy
    to handle redirects. This configures a maximum of two redirects for any page. Running
    the script reads the NASA sitemap and crawls that content. This contains a large
    number of redirects, many of which are redirects from HTTP to HTTPS versions of
    URLs. There will be a lot of output, but here are a few lines demonstrating the
    output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This particular URL was processed after one redirection, from an HTTP to an
    HTTPS version of the URL. The list defines all of the URLs that were involved
    in the redirection.
  prefs: []
  type: TYPE_NORMAL
- en: 'You will also be able to see where redirection exceeded the specified level
    (2) in the output pages.  The following is one example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: How it works
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The spider is defined as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This is identical to our previous NASA sitemap based crawler, with the addition
    of one line printing the `redirect_urls`. In any call to `parse`, this metadata
    will contain all redirects that occurred to get to this page.
  prefs: []
  type: TYPE_NORMAL
- en: 'The crawling process is configured with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Redirect is enabled by default, but this sets the maximum number of redirects
    to 2 instead of the default of 20.
  prefs: []
  type: TYPE_NORMAL
- en: Waiting for content to be available in Selenium
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A common problem with dynamic web pages is that even after the whole page has
    loaded, and hence the `get()` method in Selenium has returned, there still may
    be content that we need to access later as there are outstanding Ajax requests
    from the page that are still pending completion. An example of this is needing
    to click a button, but the button not being enabled until all data has been loaded
    asyncronously to the page after loading.
  prefs: []
  type: TYPE_NORMAL
- en: 'Take the following page as an example: [http://the-internet.herokuapp.com/dynamic_loading/2](http://the-internet.herokuapp.com/dynamic_loading/2).
    This page finishes loading very quickly and presents us with a Start button:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/08dd65a4-9018-4136-9bb9-0b7f74e17aff.png)The Start button presented
    on screen'
  prefs: []
  type: TYPE_NORMAL
- en: 'When pressing the button, we are presented with a progress bar for five seconds:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/530d1355-e1cc-4551-ab0f-9374c029b03e.png)The status bar while waiting'
  prefs: []
  type: TYPE_NORMAL
- en: And when this is completed, we are presented with Hello World!
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/d37ade1b-f857-4a5f-a7ce-5157238e9e09.png)After the page is completely
    rendered'
  prefs: []
  type: TYPE_NORMAL
- en: Now suppose we want to scrape this page to get the content that is exposed only
    after the button is pressed and after the wait? How do we do this?
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can do this using Selenium. We will use two features of Selenium. The first
    is the ability to click on page elements. The second is the ability to wait until
    an element with a specific ID is available on the page.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we get the button and click it. The button''s HTML is the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'When the button is pressed and the load completes, the following HTML is added
    to the document:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: We will use the Selenium driver to find the Start button, click it, and then
    wait until a `div` with an ID of `'finish'` is available. Then we get that element
    and return the text in the enclosed `<h4>` tag.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You can try this by running `06/03_press_and_wait.py`.  It''s output will be
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Now let's see how it worked.
  prefs: []
  type: TYPE_NORMAL
- en: How it works
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let us break down the explanation:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by importing the required items from Selenium:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we load the driver and the page:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'With the page loaded, we can retrieve the button:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'And then we can click the button:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Next we create a `WebDriverWait` object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'With this object, we can request Selenium''s UI wait for certain events. This
    also sets a maximum wait of 10 seconds. Now using this, we can wait until we meet
    a criterion; that an element is identifiable using the following XPath:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'When this completes, we can retrieve the h4 element and get its enclosing text:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Limiting crawling to a single domain
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can inform Scrapy to limit the crawl to only pages within a specified set
    of domains. This is an important task, as links can point to anywhere on the web,
    and we often want to control where crawls end up going. Scrapy makes this very
    easy to do. All that needs to be done is setting the `allowed_domains` field of
    your scraper class.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The code for this example is `06/04_allowed_domains.py`. You can run the script
    with your Python interpreter. It will execute and generate a ton of output, but
    if you keep an eye on it, you will see that it only processes pages on nasa.gov.
  prefs: []
  type: TYPE_NORMAL
- en: How it works
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The code is the same as previous NASA site crawlers except that we include
    `allowed_domains=[''nasa.gov'']`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The NASA site is fairly consistent with staying within its root domain, but
    there are occasional links to other sites such as content on boeing.com. This
    code will prevent moving to those external sites.
  prefs: []
  type: TYPE_NORMAL
- en: Processing infinitely scrolling pages
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many websites have replaced "previous/next" pagination buttons with an infinite
    scrolling mechanism. These websites use this technique to load more data when
    the user has reached the bottom of the page. Because of this, strategies for crawling
    by following the "next page" link fall apart.
  prefs: []
  type: TYPE_NORMAL
- en: While this would seem to be a case for using browser automation to simulate
    the scrolling, it's actually quite easy to figure out the web pages' Ajax requests
    and use those for crawling instead of the actual page. Let's look at `spidyquotes.herokuapp.com/scroll`
    as an example.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Open [http://spidyquotes.herokuapp.com/scroll](http://spidyquotes.herokuapp.com/scroll)
    in your browser. This page will load additional content when you scroll to the
    bottom of the page:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/5aedcf3b-b3dd-4e67-8328-7093d70c2db4.png)Screenshot of the quotes
    to scrape'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the page is open, go into your developer tools and select the network
    panel. Then, scroll to the bottom of the page. You will see new content in the
    network panel:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/b8f8c31d-706b-4f11-bab8-901263f7fdfc.png)Screenshot of the developer
    tools options'
  prefs: []
  type: TYPE_NORMAL
- en: 'When we click on one of the links, we can see the following JSON:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: This is great because all we need to do is continually generate requests to
    `/api/quotes?page=x`, increasing `x` until the `has_next` tag exists in the reply
    document. If there are no more pages, then this tag will not be in the document.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `06/05_scrapy_continuous.py` file contains a Scrapy agent, which crawls
    this set of pages. Run it with your Python interpreter and you will see output
    similar to the following (the following is multiple excerpts from the output):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: When this gets to page 10 it will stop as it will see that there is no next
    page flag set in the content.
  prefs: []
  type: TYPE_NORMAL
- en: How it works
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s walk through the spider to see how this works. The spider starts with
    the following definition of the start URL:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The parse method then prints the response and also parses the JSON into the
    data variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Then it loops through all the items in the quotes element of the JSON objects.
    For each item, it yields a new Scrapy item back to the Scrapy engine:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'It then checks to see if the data JSON variable has a `''has_next''` property,
    and if so it gets the next page and yields a new request back to Scrapy to parse
    the next page:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It is also possible to process infinite, scrolling pages using Selenium. The
    following code is in `06/06_scrape_continuous_twitter.py`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The output would be similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: This code starts by loading the page from Twitter. The call to `.get()` will
    return when the page is fully loaded. The `scrollHeight` is then retrieved, and
    the program scrolls to that height and waits for a moment for the new content
    to load. The `scrollHeight` of the browser is retrieved again, and if different
    than `last_height`, it will loop and continue processing. If the same as `last_height`,
    no new content has loaded and you can then continue on and retrieve the HTML for
    the completed page.
  prefs: []
  type: TYPE_NORMAL
- en: Controlling the depth of a crawl
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The depth of a crawl can be controlled using Scrapy `DepthMiddleware` middleware.
    The depth middleware limits the number of follows that Scrapy will take from any
    given link. This option can be useful for controlling how deep you go into a particular
    crawl. This is also used to keep a crawl from going on too long, and useful if
    you know that the content you are crawling for is located within a certain number
    of degrees of separation from the pages at the start of your crawl.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The depth control middleware is installed in the middleware pipeline by default. An
    example of depth limiting is contained in the `06/06_limit_depth.py` script. This
    script crawls the static site provided with the source code on port 8080, and
    allows you to configure the depth limit. This site consists of three levels: 0,
    1, and 2, and has three pages at each level. The files are named `CrawlDepth<level><pagenumber>.html`.
    Page 1 on each level links to the other two pages on the same level, as well as
    to the first page on the next level. Links to higher levels end at level 2\. This
    structure is great for examining how depth processing is handled in Scrapy.'
  prefs: []
  type: TYPE_NORMAL
- en: How it works
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The limiting of depth can be performed by setting the `DEPTH_LIMIT` parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'A depth limit of 1 means we will only crawl one level, which means it will
    process the URLs specified in `start_urls`, and then any URLs found within those
    pages. With `DEPTH_LIMIT` we get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The crawl starts with `CrawlDepth0-1.html`. That page has two lines, one to
    `CrawlDepth0-2.html` and one to `CrawlDepth1-1.html`. They are then requested
    to be parsed. Considering that the start page is at depth 0, those pages are at
    depth 1, the limit of our depth. Therefore, we will see those two pages being
    parsed. However, note that all the links from those two pages, although requesting
    to be parsed, are then ignored by Scrapy as they are at depth 2, which exceeds
    the specified limit.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now change the depth limit to 2:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The output then becomes as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Note that the three pages previously ignored with `DEPTH_LIMIT` set to 1 are
    now parsed. And now, links found at that depth, such as for the page `CrawlDepth1-3.html`,
    are now ignored as their depth exceeds 2.
  prefs: []
  type: TYPE_NORMAL
- en: Controlling the length of a crawl
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The length of a crawl, in terms of number of pages that can be parsed, can be
    controlled with the `CLOSESPIDER_PAGECOUNT` setting.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will be using the script in `06/07_limit_length.py`. The script and scraper
    are the same as the NASA sitemap crawler with the addition of the following configuration
    to limit the number of pages parsed to 5:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'When this is run, the following output will be generated (interspersed in the
    logging output):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: How it works
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Note that we set the page limit to 5, but the example actually parsed 7 pages. 
    The value for `CLOSESPIDER_PAGECOUNT` should be considered a value that Scrapy
    will do as a minimum, but which may be exceeded by a small amount.
  prefs: []
  type: TYPE_NORMAL
- en: Handling paginated websites
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Pagination breaks large sets of content into a number of pages. Normally, these
    pages have a previous/next page link for the user to click. These links can generally
    be found with XPath or other means and then followed to get to the next page (or
    previous). Let's examine how to traverse across pages with Scrapy. We'll look
    at a hypothetical example of crawling the results of an automated internet search.
    The techniques directly apply to many commercial sites with search capabilities,
    and are easily modified for those situations.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will demonstrate handling pagination with an example that crawls a set of
    pages from the website in the provided container.  This website models five pages
    with previous and next links on each page, along with some embedded data within
    each page that we will extract.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first page of the set can be seen at `http://localhost:5001/pagination/page1.html`. 
    The following image shows this page open, and we are inspecting the Next button:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/2c9af8d6-9b76-47e9-965e-1875830119d4.png)Inspecting the Next button'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two parts of the page that are of interest. The first is the link
    for the Next button. It''s a fairly common practice that this link has a class
    that identifies the link as being for the next page. We can use that info to find
    this link. In this case, we can find it using the following XPath:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: The second item of interest is actually retrieving the data we want from the
    page. On these pages, this is identified by a `<div>` tag with a `class="data"`
    attribute. These pages only have one data item, but in this example of crawling
    the pages resulting in a search, we will pull multiple items.
  prefs: []
  type: TYPE_NORMAL
- en: Now let's go and actually run a scraper for these pages.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There is a script named `06/08_scrapy_pagination.py`. Run this script with
    Python and there will be a lot of output from Scrapy, most of which will be the
    standard Scrapy debugging output. However, within that output you will see that
    we extracted the data items on all five pages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: How it works
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The code begins with the definition of `CrawlSpider` and the start URL:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Then the rules field is defined, which informs Scrapy how to parse each page
    to look for links. This code uses the XPath discussed earlier to find the Next
    link in the page. Scrapy will use this rule on every page to find the next page
    to process, and will queue that request for processing after the current page.
    For each page that is found, the callback parameter informs Scrapy which method
    to call for processing, in this case `parse_result_page`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'A single list variable named `all_items` is declared to hold all the items
    we find:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Then the `parse_start_url` method is defined. Scrapy will call this to parse
    the initial URL in the crawl. The function simply defers that processing to `parse_result_page`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The `parse_result_page` method then uses XPath to find the text inside of the
    `<h1>` tag within the `<div class="data">` tag. It then appends that text to the
    `all_items` list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Upon the crawl being completed, the `closed()` method is called and writes
    out the content of the `all_items` field:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The crawler is run using Python as a script using the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Note the use of the `CLOSESPIDER_PAGECOUNT` property being set to `10`. This
    exceeds the number of pages on this site, but in many (or most) cases there will
    likely be thousands of pages in a search result. It's a good practice to stop
    after an appropriate number of pages. This is good behavior a crawler, as the
    relevance of items to your search drops dramatically after a few pages, so crawling
    beyond the first few pages has greatly diminishing returns and it's generally
    best to stop after a few pages.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As mentioned at the start of the recipe, this is easy to modify for various
    automatic searches on various content sites. This practice can push the limits
    of acceptable use, so it has been generalized here. But for more actual examples,
    visit my blog at: `www.smac.io`.'
  prefs: []
  type: TYPE_NORMAL
- en: Handling forms and forms-based authorization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We are often required to log into a site before we can crawl its content. This
    is usually done through a form where we enter a user name and password, press
    *Enter*, and then granted access to previously hidden content. This type of form
    authentication is often called cookie authorization, as when we authorize, the
    server creates a cookie that it can use to verify that you have signed in.  Scrapy
    respects these cookies, so all we need to do is somehow automate the form during
    our crawl.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will crawl a page in the containers web site at the following URL: `http://localhost:5001/home/secured`. 
    On this page, and links from that page, there is content we would like to scrape.
    However, this page is blocked by a login. When opening the page in a browser,
    we are presented with the following login form, where we can enter `darkhelmet`
    as the user name and `vespa` as the password:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/518cef3e-91c8-47a3-b978-020504dcc4ca.png)Username and password credentials
    are entered'
  prefs: []
  type: TYPE_NORMAL
- en: Upon pressing *Enter* we are authenticated and taken to our originally desired
    page.
  prefs: []
  type: TYPE_NORMAL
- en: There's not a great deal of content there, but the message is enough to verify
    that we have logged in, and our scraper knows that too.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We proceed with the recipe as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you examine the HTML for the sign-in page, you will have noticed the following
    form code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'To get the form processors in Scrapy to work, we will need the IDs of the username
    and password fields in this form. They are `Username` and `Password` respectively.
    Now we can create a spider using this information. This spider is in the script
    file, `06/09_forms_auth.py`. The spider definition starts with the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: We define two fields in the class, `login_user` and `login_pass`, to hold the
    username we want to use. The crawl will also start at the specified URL.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The `parse` method is then changed to examine if the page contains a login
    form. This is done by using XPath to see if there is an input form of type password
    and with an `id` of `Password`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'If that field is found, we then return a `FormRequest` to Scrapy, generated
    using its `from_response` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'This function is passed the response, and then a dictionary specifying the
    IDs of fields that need data inserted along with those values. A callback is then
    defined to be executed after this FormRequest is executed by Scrapy, and to which
    is passed the content of the resulting form:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'This callback simply looks for the words `This page is secured`, which are
    only returned if the login is successful. When running this successfully, we will
    see the following output from our scraper''s print statements:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: How it works
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When you create a `FormRequest`, your are instructing Scrapy to construct a
    form POST request on behalf of your process, using the data in the specified dictionary
    as the form parameters in the POST request. It constructs this request and sends
    it to the server.  Upon receipt of the answer in that POST, it calls the specified
    callback function.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This technique is also useful in form entries of many other kinds, not just
    login forms. This can be used to automate, then execute, any type of HTML form
    request, such as making orders, or those used for executing search operations.
  prefs: []
  type: TYPE_NORMAL
- en: Handling basic authorization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Some websites use a form of authorization known as *basic authorization*. This
    was popular before other means of authorization, such as cookie auth or OAuth.
    It is also common on corporate intranets and some web APIs. In basic authorization,
    a header is added to the HTTP request. This header, `Authorization`, is passed
    the Basic string and then a base64 encoding of the values `<username>:<password>`. 
    So in the case of darkhelmet, this header would look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that this is no more secure than sending it in plain-text, (although when
    performed over HTTPS it is secure.) However, for the most part, is has been subsumed
    for more robust authorization forms, and even cookie authorization allows for
    more complex features such as claims:'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Supporting basic auth in Scrapy is straightforward. To get this to work for
    a spider and a given site the spider is crawling, simply define the `http_user`,
    `http_pass`, and `name` fields in your scraper. The following demonstrates:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: How it works
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When the spider crawls any pages on the given site specified by the name, it
    will use the values of `http_user` and `http_pass` to construct the appropriate
    header.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Note, this task is performed by the `HttpAuthMiddleware` module of Scrapy. More
    info on basic authorization is also available at:[ https://developer.mozilla.org/en-US/docs/Web/HTTP/Authentication](https://developer.mozilla.org/en-US/docs/Web/HTTP/Authentication).
  prefs: []
  type: TYPE_NORMAL
- en: Preventing bans by scraping via proxies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sometimes you may get blocked by a site that your are scraping because you are
    identified as a scraper, and sometimes this happens because the webmaster sees
    the scrape requests coming from a uniform IP, at which point they simply block
    access to that IP.
  prefs: []
  type: TYPE_NORMAL
- en: To help prevent this problem, it is possible to use proxy randomization middleware
    within Scrapy. There exists a library, `scrapy-proxies`, which implements a proxy
    randomization feature.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can get `scrapy-proxies` from GitHub at [https://github.com/aivarsk/scrapy-proxies](https://github.com/aivarsk/scrapy-proxies)
    or by installing it using `pip install scrapy_proxies`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Use of `scrapy-proxies` is done by configuration. It starts by configuring `DOWNLOADER_MIDDLEWARES`,
    and making sure they have `RetryMiddleware`, `RandomProxy`, and `HttpProxyMiddleware`
    installed. The following would be a typical configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'The the `PROXY_LIST` setting is configured to point to a file containing a
    list of proxies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we need to let Scrapy know the `PROXY_MODE`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'If `PROXY_MODE` is `2`, then you must specify a `CUSTOM_PROXY`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: How it works
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This configuration essentially tells Scrapy that if a request for a page fails
    with any of the `RETRY_HTTP_CODES`, and for up to `RETRY_TIMES` per URL, then
    use a proxy from within the file specified by `PROXY_LIST`, and by using the pattern
    defined by `PROXY_MODE`. With this, you can have Scrapy fail back to any number
    of proxy servers to retry the request from a different IP address and/or port.
  prefs: []
  type: TYPE_NORMAL
- en: Randomizing user agents
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Which user agent you use can have an effect on the success of your scraper.
    Some websites will flat out refuse to serve content to specific user agents. This
    can be because the user agent is identified as a scraper that is banned, or the
    user agent is for an unsupported browser (namely Internet Explorer 6).
  prefs: []
  type: TYPE_NORMAL
- en: Another reason for control over the scraper is that content may be rendered
    differently by the web server depending on the specified user agent. This is currently
    common for mobile sites, but it can also be used for desktops, to do things such
    as delivering simpler content for older browsers.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, it can be useful to set the user agent to other values than the defaults.
    Scrapy defaults to a user agent named `scrapybot`. This can be configured by using
    the `BOT_NAME` parameter. If you use Scrapy projects, Scrapy will set the agent
    to the name of your project.
  prefs: []
  type: TYPE_NORMAL
- en: 'For more complicated schemes, there are two popular extensions that can be
    used: `scrapy-fake-agent` and `scrapy-random-useragent`.'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We proceed with the recipe as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`scrapy-fake-useragent` is available on GitHub at [https://github.com/alecxe/scrapy-fake-useragent](https://github.com/alecxe/scrapy-fake-useragent),
    and `scrapy-random-useragent` is available at [https://github.com/cnu/scrapy-random-useragent](https://github.com/cnu/scrapy-random-useragent). 
    You can include them using `pip install scrapy-fake-agent` and/or  `pip install
    scrapy-random-useragent`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`scrapy-random-useragent` will select a random user agent for each of your
    requests from a file. It is configured in two settings:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'This disables the existing `UserAgentMiddleware`, and replaces it with the
    implementation provided in `RandomUserAgentMiddleware`. Then, you configure a
    reference to a file containing a list of user agent names:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: Once configured, each request will use a random user agent from the file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`scrapy-fake-useragent` uses a different model. It retrieves user agents from
    an online database tracking the most common user agents in use. Configuring Scrapy
    for its use is done with the following settings:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: It also has the ability to set the type of user agent used, to values such as
    mobile or desktop, to force selection of user agents in those two categories.
    This is performed using the `RANDOM_UA_TYPE` setting, which defaults to random.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If using `scrapy-fake-useragent` with any proxy middleware, then you may want
    to randomize per proxy. This can be done by setting `RANDOM_UA_PER_PROXY` to True.
    Also, you will want to set the priority of `RandomUserAgentMiddleware` to be greater
    than `scrapy-proxies`, so that the proxy is set before being handled.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Caching responses
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Scrapy comes with the ability to cache HTTP requests. This can greatly reduce
    crawling times if pages have already been visited. By enabling the cache, Scrapy
    will store every request and response.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There is a working example in the `06/10_file_cache.py` script. In Scrapy,
    caching middleware is disabled by default. To enable this cache, set `HTTPCACHE_ENABLED` to `True` and `HTTPCACHE_DIR` to
    a directory on the file system (using a relative path will create the directory
    in the project''s data folder). To demonstrate, this script runs a crawl of the
    NASA site, and caches the content. It is configured using the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: We ask Scrapy to cache using files and to create a sub-directory in the current
    folder. We also instruct it to limit the crawl to roughly 500 pages. When running
    this, the crawl will take roughly a minute (depending on your internet speed),
    and there will be roughly 500 lines of output.
  prefs: []
  type: TYPE_NORMAL
- en: 'After the first execution, you can see that there is now a `.scrapy` folder
    in your directory that contains the cache data.  The structure will look like
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/5c6f662d-595c-46d0-95f7-c118ffe6afc4.png)'
  prefs: []
  type: TYPE_IMG
- en: Running the script again will only take a few seconds, and will produce the
    same output/reporting of pages parsed, except that this time the content will
    come from the cache instead of HTTP requests.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are many configurations and options for caching in Scrapy. By default,
    the cache expiration, specified by `HTTPCACHE_EXPIRATION_SECS`, is set to 0\.
    0 means the cache items never expire, so once written, Scrapy will never request
    that item via HTTP again. Realistically, you will want to set this to some value
    that does expire.
  prefs: []
  type: TYPE_NORMAL
- en: File storage for the cache is only one of the options for caching. Items can
    also be cached in DMB and LevelDB by setting the `HTTPCACHE_STORAGE` setting to `scrapy.extensions.httpcache.DbmCacheStorage` or
    `scrapy.extensions.httpcache.LeveldbCacheStorage`, respectively. You could also
    write your own code, to store page content in another type of database or cloud
    storage if you feel so inclined.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we come to cache policy. Scrapy comes with two policies built in:
    Dummy (the default), and RFC2616\. This can be set by changing the `HTTPCACHE_POLICY`
    setting to `scrapy.extensions.httpcache.DummyPolicy` or `scrapy.extensions.httpcache.RFC2616Policy`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The RFC2616 policy enables HTTP cache-control awareness with operations including
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Do not attempt to store responses/requests with no-store cache-control directive
    set
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do not serve responses from cache if no-cache cache-control directive is set
    even for fresh responses
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute freshness lifetime from max-age cache-control directive
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute freshness lifetime from Expires response header
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute freshness lifetime from Last-Modified response header (heuristic used
    by Firefox)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute current age from Age response header
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute current age from Date header
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Revalidate stale responses based on Last-Modified response header
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Revalidate stale responses based on ETag response header
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Set Date header for any received response missing it
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Support max-stale cache-control directive in requests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
