- en: Networking and Indicators of Compromise Recipes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following recipes are covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Getting a jump start with IEF
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Coming into contact with IEF
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It's a beautiful soup
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Going hunting for viruses
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gathering intel
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Totally passive
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Technology has come a long way and, with it, the extent to which tools are made
    widely available has changed too. As a matter of fact, being cognizant of the
    tools' existence is half the battle due to the sheer volume of tools available
    on the internet. Some of these tools are publicly available and can be bent toward
    forensic purposes. In this chapter, we will learn how to interact with websites
    and identify malware through Python, including an automated review of potentially
    malicious domains, IP addresses, or files.
  prefs: []
  type: TYPE_NORMAL
- en: We start out by taking a look at how to manipulate **Internet Evidence Finder**
    (**IEF**) results and perform additional processing outside of the context of
    the application. We also explore using services such as VirusShare, PassiveTotal,
    and VirusTotal to create HashSets of known malware, query suspicious domain resolutions,
    and identify known bad domains or files, respectively. Between these scripts,
    you will become familiar with using Python to interact with APIs.
  prefs: []
  type: TYPE_NORMAL
- en: 'The scripts in this chapter focus on solving particular problems and are ordered
    by complexity:'
  prefs: []
  type: TYPE_NORMAL
- en: Learning to extract data from IEF results
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Processing cached Yahoo contacts data from Google Chrome
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preserving web pages with Beautiful Soup
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating an X-Ways-compatible HashSet from VirusShare
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using PassiveTotal to automate the review of sketchy domains or IP addresses
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Automating identification of known bad files, domains, or IPs with VirusTotal
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visit [www.packtpub.com/books/content/support](http://www.packtpub.com/books/content/support)
    to download the code bundle for this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Getting a jump start with IEF
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Recipe Difficulty: Easy'
  prefs: []
  type: TYPE_NORMAL
- en: 'Python Version: 3.5'
  prefs: []
  type: TYPE_NORMAL
- en: 'Operating System: Any'
  prefs: []
  type: TYPE_NORMAL
- en: This recipe will act as a quick means of dumping all reports from IEF to a CSV
    file and an introduction to interacting with IEF results. IEF stores data in a
    SQLite database, which we explored rather thoroughly in [Chapter 3](part0097.html#2SG6I0-260f9401d2714cb9ab693c4692308abe),
    *A Deep Dive into Mobile Forensic Recipes*. As IEF can be configured to scan specific
    categories of information, it is not so simple as dumping out set tables for each
    IEF database. Instead, we must determine this information dynamically and then
    interact with said tables. This recipe will dynamically identify result tables
    within the IEF database and dump them to respective CSV files. This process can
    be performed on any SQLite database to quickly dump its contents to a CSV file
    for review.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All libraries used in this script are present in Python's standard library.
    For this script, make sure to have an IEF results database generated after executing
    the program. We used IEF version 6.8.9.5774 to generate the database used to develop
    this recipe. After IEF finishes processing the forensic image, for example, you
    should see a file named `IEFv6.db`. This is the database we will interact with
    in this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will employ the following steps to extract data from the IEF results database:'
  prefs: []
  type: TYPE_NORMAL
- en: Connect to the database.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Query the database to identify all tables.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write result tables to an individual CSV file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First, we import the required libraries to handle argument parsing, writing
    spreadsheets, and interacting with SQLite databases.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This recipe's command-line handler is relatively straightforward. It accepts
    two positional arguments, `IEF_DATABASE` and `OUTPUT_DIR`, representing the file
    path to the `IEFv6.db` file and the desired output location, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We perform the input validation steps as usual prior to calling the `main()`
    function of the script. First, we check the output directory and create it if
    it does not exist. Then, we confirm that the IEF database exists as expected.
    If all is as expected, we execute the `main()` function and supply it with the
    two user-supplied inputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The `main()` function starts out simply enough. We print a status message to
    the console and create the `sqlite3` connection to the database to execute the
    necessary SQLite queries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Next, we need to query the database to identify all tables present. Notice the
    rather complex query we execute to perform this. If you are familiar with SQLite,
    you may shake your head and wonder why we have not executed the `.table` command.
    Unfortunately, in Python, this cannot be done so easily. Rather, one must execute
    the following command to achieve the desired goal.
  prefs: []
  type: TYPE_NORMAL
- en: As we have seen previously, the `Cursor` returns results as a list of tuples.
    The command we have executed returns a number of details about each table in the
    database. In this case, we are only interested in extracting the name of the table.
    We accomplish this using list comprehension by first fetching all results from
    the cursor object and then appending the second element of each result to the
    tables list if the name matches certain criteria. We have elected to ignore table
    names that start with `_` or end with `_DATA`. From a review of these tables,
    they contained actual cached file content rather than the metadata IEF presents
    for each record.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: With the list of table names in hand, we can now iterate through each one and
    extract their contents into a variable. Prior to that, we print an update status
    message to the console to inform the user of the current execution status of the
    script. In order to write the CSVs, we need to first determine the column names
    for a given table. This is performed, as we saw in Chapter 3, using the `pragma
    table_info` command. With some simple list comprehension, we extract just the
    names of the columns and store them in a variable for later.
  prefs: []
  type: TYPE_NORMAL
- en: 'With that accomplished, we execute the favorite and simplest SQL query and
    select all (`*`) data from each table. Using the `fetchall()` method on the cursor
    object, we store the list of tuples containing the table''s data in its entirety
    in the `table_data` variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We can now begin to write the data for each table to its appropriate CSV file.
    To keep things simple, the name of each CSV file is simply the table name and
    an appended `.csv` extension. We use `os.path.join()` to combine the output directory
    with the desired CSV name.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we print a status update to the console and begin the process to write
    each CSV file. This is accomplished by first writing the table column names as
    the header of the spreadsheet followed by the contents of the table. We use the
    **`writerows()`** method to write the list of tuples in one line rather than create
    an unnecessary loop and execute `writerow()` repeatedly for each tuple.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'When we run this script, we can see the discovered artifacts and extract CSV
    reports of the text information:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00049.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Once we have completed the script, we can see information about an artifact
    as seen in the following snippet of a report:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00050.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Coming into contact with IEF
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Recipe Difficulty: Medium'
  prefs: []
  type: TYPE_NORMAL
- en: 'Python Version: 3.5'
  prefs: []
  type: TYPE_NORMAL
- en: 'Operating System: Any'
  prefs: []
  type: TYPE_NORMAL
- en: We can take further advantage of the IEF results in the SQLite database by manipulating
    and gleaning, even more, information from artifacts that IEF does not necessarily
    support. This can be particularly important when new artifacts are discovered
    and are unsupported. As the internet, and many businesses using the internet change
    constantly, it is unrealistic for software to keep up with every new artifact.
    In this case, we will look at cached Yahoo Mail contacts that get stored on the
    local system as a byproduct of using Yahoo Mail.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All libraries used in this script are present in Python's standard library.
    Again, as in the previous recipe, if you would like to follow along, you will
    need an IEF results database. We used IEF version 6.8.9.5774 to generate the database
    used to develop this recipe. In addition to that, you will likely need to generate
    Yahoo Mail traffic to create the necessary situation where Yahoo Mail contacts
    are cached. In this example, we used the Google Chrome browser to use Yahoo Mail
    and will, therefore, be looking at Google Chrome cache data. This recipe, while
    specific to Yahoo, illustrates how you can use the IEF results database to further
    process artifacts and identify additional relevant information.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The recipe follows these basic principles:'
  prefs: []
  type: TYPE_NORMAL
- en: Connect to the input database.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Query the Google Chrome cache table for Yahoo Mail contact records.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Process contact cache JSON data and metadata.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write all relevant data to a CSV.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First, we import the required libraries to handle argument parsing, writing
    spreadsheets, processing JSON data, and interacting with SQLite databases.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: This recipe's command-line handler does not differ from the first recipe. It
    accepts two positional arguments, `IEF_DATABASE` and `OUTPUT_DIR,` representing
    the file paths to the `IEFv6.db` file and the desired output location, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: And again, we perform the same data validation steps as executed in the first
    recipe of this chapter. If it ain't broke, why fix it? After validation, we execute
    the `main()` function and supply it with the two validated inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The `main()` function starts again by creating a connection to the input SQLite
    database (we promise this recipe isn''t identical to the first one: keep reading).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: We can now begin scouring the database for all instances of Yahoo Mail contact
    cache records. Notice that the URL fragment we are looking for is rather specific
    to our purpose. This should ensure that we do not get any false positives. The
    percent sign (`%`) at the end of the URL is the SQLite wildcard equivalent character.
    We execute the query in a `try` and `except` statement in the event the input
    directory does not have the Chrome cache records table, is corrupt, or is encrypted.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: If we were able to execute the query successfully, we store the returned list
    of tuples into the `contact_cache` variable. This variable serves as the only
    input to the `process_contacts()` function, which returns a nested list structure
    convenient for the CSV writer.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The `process_contacts()` function starts by printing a status message to the
    console, setting up the `results` list, and iterating through each contact cache
    record. Each record has a number of metadata elements associated with it beyond
    the raw data. This includes the URL, the location of the cache on the filesystem,
    and the timestamps for the first visit, last visit, and last sync time.
  prefs: []
  type: TYPE_NORMAL
- en: We use the `json.loads()` method to store the JSON data extracted from the table
    into the `contact_json` variable for further manipulation. The `total` and `count`
    keys from the JSON data, store the total number of Yahoo Mail contacts and the
    count of them present in the JSON cache data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Before we extract contact data from contact JSON, we need to ensure that it
    has contacts in the first place. If it does not, we continue onto the next cache
    record in the hopes that we find contacts there. If on the other hand, we do have
    contacts, we initialize a number of variables to an empty string. This is achieved
    in one line by bulk-assigning variables to a tuple of empty strings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: With these variables initialized, we begin looking for each of them in each
    of the contacts. Sometimes the particular cache record will not retain full contact
    details such as the `"anniversary"` key. For this reason, we initialized these
    variables to avoid referring to variables that do not exist if that particular
    key isn't present in a given cache record.
  prefs: []
  type: TYPE_NORMAL
- en: For the `name`, `"anniversary"`, and `"birthday"` keys, we need to perform some
    string concatenation so that they are in a convenient format. The `emails`, `phones`,
    and `links` variables could have more than one result and we, therefore, use list
    comprehension and the `join()` method to create a comma-separated list of those
    respective elements. The great thing about that line of code is that if there
    is only one email, phone number, or link, it will not place a comma after that
    one element unnecessarily.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: We handle the `company`, `jobTitle`, and `notes` sections differently by using
    the `get()` method instead. Because these are simple key and value pairs, we do
    not need to do any additional string processing on them. Instead, with the `get()`
    method, we can extract the key's value or, if it isn't present, set the default
    value to an empty string.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: After we have processed the contact data, we append a list of the metadata and
    extracted data elements to the `results` list. Once we have processed each contact
    and each cache record, we return the `results` list back to the `main()` function,
    which gets passed onto the CSV writer function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The `write_csv()` method takes the nested `results` list structure and the output
    file path as its inputs. After we print a status message to the console, we employ
    the usual strategy to write the results to the output file. Namely, we first write
    the headers of the CSV followed by the actual contact data. Thanks to the nested
    list structure, we can just use the `writerows()` method to write all of the results
    to the file in one line.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'This screenshot illustrates an example of the type of data that this script
    can extract:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00051.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Beautiful Soup
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Recipe Difficulty: Medium'
  prefs: []
  type: TYPE_NORMAL
- en: 'Python Version: 3.5'
  prefs: []
  type: TYPE_NORMAL
- en: 'Operating System: Any'
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we create a website preservation tool leveraging the **Beautiful
    Soup** library. This is a library meant to process markup languages, such as HTML
    or XML, and can be used to easily process these types of data structures. We will
    use it to identify and extract all links from a web page in a few lines of code.
    This script is meant to showcase a very simplistic example of a website preservation
    script; it is by no means intended to replace existing software out there on the
    market.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This recipe requires the installation of the third-party library `bs4`. This
    module can be installed via the following command. All other libraries used in
    this script are present in Python's standard library.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Learn more about the `bs4` library; visit [https://www.crummy.com/software/BeautifulSoup/bs4/doc/](https://www.crummy.com/software/BeautifulSoup/bs4/doc/).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will perform the following steps in this recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: Access index web page and identify all initial links.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Recurse through all known links to:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find additional links and add them to the queue.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate `SHA-256` hash of each web page.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write and then verify web page output to the destination directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Log relevant activity and hash results.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First, we import the required libraries to handle argument parsing, parsing
    HTML data, parsing dates, hashing files, logging data, and interacting with web
    pages. We also setup a variable used to later construct the recipe's logging component.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: This recipe's command-line handler takes two positional inputs, `DOMAIN` and
    `OUTPUT_DIR`, which represent the website URL to preserve and the desired output
    directory, respectively. The optional `-l` argument can be used to specify the
    location of the log file path.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: We will now setup the logging for the script, using the default or user-specified
    path. Using the logging format in *Chapter 1*, we specify a file and stream handler
    to keep the user in the loop and document the acquisition process.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: After setting up the log, we log a few details about the execution context of
    the script, including the supplied arguments and OS details.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: We perform some additional input validation on the desired output directory.
    After these steps, we call the `main()` function and pass it the website URL and
    the output directory.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The `main()` function is used to perform a few tasks. First, it extracts the
    base name of the website by removing any unnecessary elements before the actual
    name. For example, [https://google.com](https://google.com) becomes [google.com](https://google.com).
    We also create the set, `link_queue`, which will hold all unique links found on
    the web page.
  prefs: []
  type: TYPE_NORMAL
- en: We perform some additional validation on the input URL. During development,
    we ran into some errors when URLs were not preceded by `https://` or `http://`,
    so we check whether that is the case here and exit the script and inform the user
    of the requirement if they are not present. If everything checks out, we are ready
    to access the base web page. To do that, we create the unverified SSL context
    to avoid errors when accessing the web page.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Next, in a `try-except` block, we open a connection to the website with the
    unverified SSL context using the `urlopen()` method and read in the web page data.
    If we receive an error when attempting to access the web page, we print and log
    a status message prior to exiting the script. If we are successful, we log a success
    message and continue script execution.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: With this first web page, we call the `write_output()` function to write it
    to the output directory and the `find_links()` function to identify all links
    on the web page. Specifically, this function attempts to identify all internal
    links on the website. We will explore both of these functions momentarily.
  prefs: []
  type: TYPE_NORMAL
- en: After identifying links on the first page, we print two status messages to the
    console and then call the `recurse_pages()` method to iterate through and discover
    all links on the discovered web pages and add them to the queue set. That completes
    the `main()` function; let's now take a look at the supporting cast of functions,
    starting with the `write_output()` method.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The `write_output()` method takes a few arguments: the URL of the web page,
    its page data, the output directory, and an optional counter argument. By default
    this argument is set to zero if it is not supplied in the function call. The counter
    argument is used to append a loop iteration number to the output file to avoid
    writing over identically named files. We start by removing some unnecessary characters
    in the name of the output file that may cause it to create unnecessary directories.
    We also join the output directory with the URL directories and create them with
    `os.makedirs()`.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Now, we log a few details about the web page we are writing. First, we log the
    name and output destination for the file. Then, we log the hash of the data as
    it was read from the web page with the `hash_data()` method. We create the path
    variable for the output file and append the counter string to avoid overwriting
    resources. We then open the output file and write the web page content to it.
    Finally, we log the output file hash by calling the `hash_file()` method.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: The `hash_data()` method is really quite simple. We read in the UTF-8 encoded
    data and then generate the `SHA-256` hash of it using the same methodology as
    seen in previous recipes.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: The `hash_file()` method is just a little more complicated. Before we can hash
    the data, we must first open the file and read its contents into the `SHA-256`
    algorithm. With this complete, we call the `hexdigest()` method and return the
    generated `SHA-256` hash. Let's now shift to the `find_links()` method and how
    we leverage `BeautifulSoup` to quickly find all relevant links.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: The `find_links()` method accomplishes a few things in its initial `for` loop.
    First of all, we create a `BeautifulSoup` object out of the web page data. Secondly,
    while creating that object, we specify that we only want to process part of the
    document, specifically, `<a href>` tags. This helps limit CPU cycles and memory
    usage and allows us to focus on only what is relevant. The `SoupStrainer` object
    is a fancy name for a filter and, in this case, filters only `<a href>` tags.
  prefs: []
  type: TYPE_NORMAL
- en: With the list of links set up, we then create some logic to test whether they
    are part of this domain. In this case, we accomplish this by checking whether
    the website's URL is part of the link. Any link that passes that test must then
    not start with a "`#`" symbol. During testing, on one of the websites, we found
    this would cause internal page references, or named anchors, to get added as a
    separate page, which was not desirable. After a link passes those tests, it is
    added to the set queue (unless it is already present in the set object). After
    we process all such links, the queue is returned to the calling function. The
    `recurse_pages()` function makes multiple calls to this function to find all links
    in every page we index.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: The `recurse_pages()` function takes as its inputs the website URL, current
    link queue, the unverified SSL context, and the output directory. We start by
    creating a processed list to keep track of the links we have already explored.
    We also set up the loop counter, which we later pass into the `write_output()`
    function to uniquely name the output files.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we begin the dreaded `while True` loop, always a somewhat dangerous way
    of iteration, but it is used in this instance to continue iterating over the queue,
    which becomes progressively larger as we discover more pages. In this loop, we
    increment the counter by `1`, but more importantly, check whether the processed
    list length matches the length of all found links. If that is the case, this loop
    will be broken. However, until that scenario is met, the script will continue
    iterating over all links, looking for more internal links and writing them to
    the output directory.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: We start iterating through a copy of the queue to process each link. We use
    the set `copy()` command so that we can update the queue without generating errors
    during its iterative loops. If the link has already been processed, we continue
    onto the next link to avoid performing redundant tasks. If this is the first time
    the link is being processed, the `continue` command is not executed, and instead,
    we append this link to the processed list so it will not be processed again in
    the future.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: We attempt to open and read in the data for each link. If we cannot access the
    web page, we print and log that and continue executing the script. This way, we
    preserve all of the pages that we can access and have a log with details on links
    we were unable to access and preserve.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Finally, for each link we are able to access, we write its output to a file
    by passing the link name, page data, output directory, and the counter. We also
    set the `queue` object equal to the new set, which will have all elements from
    the old `queue` and any additional new links from the `find_links()` method. Eventually,
    and it may take some time based on the size of the website, we will have processed
    all items in the link queue and will exit the script after printing a status message
    to the console.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'When we execute this script, we provide the URL for the website, the output
    folder, and a path to the log file as seen here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00052.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can then open the output file in a browser and view the preserved content:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00053.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can extend this script in many ways, including:'
  prefs: []
  type: TYPE_NORMAL
- en: Collecting CSS, images, and other resources
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Screenshotting rendered pages in a browser with selenium
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting the user-agent to disguise collections
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Going hunting for viruses
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Recipe Difficulty: Medium'
  prefs: []
  type: TYPE_NORMAL
- en: 'Python Version: 3.5'
  prefs: []
  type: TYPE_NORMAL
- en: 'Operating System: Any'
  prefs: []
  type: TYPE_NORMAL
- en: VirusShare is the largest privately owned collection of malware samples, with
    over 29.3 million samples and counting. One of the great benefits of VirusShare,
    besides the literal cornucopia of malware that is every malware researcher's dream,
    is the list of malware hashes which is made freely available. We can use these
    hashes to a create a very comprehensive hash set and leverage that in casework
    to identify potentially malicious files.
  prefs: []
  type: TYPE_NORMAL
- en: To learn more about and use `VirusShare`, visit the website [https://virusshare.com/](https://virusshare.com/).
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we demonstrate how to automate downloading lists of hashes from
    VirusShare to create a newline-delimited hash list. This list can be used by forensic
    tools, such as X-Ways, to create a HashSet. Other forensic tools, EnCase, for
    example, can use this list as well but require the use of an EnScript to successfully
    import and create the HashSet.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This recipe uses the `tqdm` third-party library to create an informative progress
    bar. The `tqdm` module can be installed via the following command. All other libraries
    used in this recipe are native to Python.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Learn more about the `tqdm` library; visit [https://github.com/noamraph/tqdm](https://github.com/noamraph/tqdm).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will perform the following steps in this recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: Read the VirusShare hashes page and dynamically identify the most recent hash
    list.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize progress bar and download hash lists in the desired range.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First, we import the required libraries to handle argument parsing, creating
    progress bars, and interacting with web pages.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: This recipe's command-line handler takes one positional argument, `OUTPUT_HASH`,
    the desired file path for the hash set we will create. An optional argument, `--start`,
    captured as an integer, is the optional starting location for the hash lists.
    VirusShare maintains a page of links to malware hashes, where each link contains
    a list of between `65,536` and `131,072` `MD5` hashes. Rather than downloading
    all hash lists (which can take some time), the user can specify the desired starting
    location. For example, this may come in handy if an individual has previously
    downloaded hashes from VirusShare and now wishes to download the latest few hash
    lists that have been released.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: We perform the standard input validation steps to ensure the supplied inputs
    will not cause any unexpected errors. We use the `os.path.dirname()` method to
    separate the directory path from the file path and check that it exists. If it
    doesn't, we create the directory now rather than encountering issues trying to
    write to a directory that does not exist. Lastly, we use an `if` statement and
    supply the `main()` function with the `start` argument as a keyword, if it was
    supplied.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: The `main()` function is the only function in this recipe. While it is long,
    the task is relatively straightforward, making additional functions somewhat unnecessary.
    Notice the `**kwargs` argument in the definition of the function. This creates
    a dictionary we can refer to support supplied keyword arguments. Prior to accessing
    the VirusShare website, we set up a few variables and print a status message to
    the console first. We use `ssl._create_unverified_context()` in order to bypass
    an SSL verification error received in Python 3.X.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: We use a `try` and `except` block to open the VirusShare hashes page using the
    `urllib.request.urlopen()` method with the unverified SSL context. We use the
    `read()` method to read the page data and decode it to UTF-8\. If we receive an
    error attempting to access this page, we print a status message to the console
    and exit the script accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: The first task with the downloaded page data is to identify the latest hash
    list. We do this by looking for the last instance of an HTML `href` tag to a VirusShare
    hash list. For instance, an example link may look like "`hashes/VirusShare_00288.md5`".
    We use string slicing and methods to separate the hash number (`288` in the previous
    example) from the link. We now check the `kwargs` dictionary to see whether the
    `start` argument was supplied. If it wasn't, we set the `start` variable to zero
    to download the first hash list and all intervening hash lists, up to and including
    the last one, to create the hash set.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Before we begin downloading the hash lists, we perform a sanity check and validate
    the `start` variable. Specifically, we check whether it is less than zero or greater
    than the latest hash list. We are using the `start` and `stop` variables to initialize
    the `for` loop and progress bar and therefore must validate the `start` variable
    to avoid unexpected outcomes. If the user supplied a bad `start` argument, we
    print a status message to the console and exit the script.
  prefs: []
  type: TYPE_NORMAL
- en: After the last sanity check, we print a status message to the console and set
    the `hashes_downloaded` counter to zero. We use this counter in a later status
    message to record how many hashes were downloaded and written to the hash list.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: As discussed in [Chapter 1](part0029.html#RL0A0-260f9401d2714cb9ab693c4692308abe),
    *Essential Scripting and File Information Recipes*, we can use the `tqdm.trange()`
    method as a substitute for the built-in `range()` method to create a loop and
    also a progress bar. We supply it with the desired `start` and `stop` integers
    and set a scale and a description for the progress bar. We must add `1` to the
    `stop` integer, due to the way `range()` works, to actually download the last
    hash list.
  prefs: []
  type: TYPE_NORMAL
- en: In the `for` loop, we create a base URL and insert a five-digit number to specify
    the appropriate hash list. We accomplish this by converting the integer to a string
    and using `zfill()` to ensure the digit has five characters by prepending zeroes
    to the front of the string until it is five digits long. Next, as before, we use
    a `try` and `except` to open, read, and decode the hash list. We split on any
    new line characters to quickly create a list of hashes. If we encounter an error
    accessing the web page, we print a status message to the console and continue
    executing rather than exiting from the script.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Once we have the hash list, we open the hash set text file in "`a+`" mode to
    append to the bottom of the text file and create the file if it does not already
    exist. Afterward, we only need to iterate through the downloaded hash list and
    write each hash to the file. Note that each hash list starts with a few commented
    lines (denoted by the `#` symbol) and so we implement logic to ignore those lines
    in addition to empty lines. After all hashes have been downloaded and written
    to the text file, we print a status message to the console and indicate the number
    of hashes downloaded.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'When we run this script the hashes start downloading locally and are stored
    in the specified file as seen here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00054.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: When previewing the output file, we can see the `MD5` hash values saved as plain
    text. As previously mentioned, we can import this into the forensic tools either
    directly, as with X-Ways, or through a script, as with EnCase ([http://www.forensickb.com/2014/02/enscript-to-create-encase-v7-hash-set.html](http://www.forensickb.com/2014/02/enscript-to-create-encase-v7-hash-set.html)).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00055.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Gathering intel
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Recipe Difficulty: Medium'
  prefs: []
  type: TYPE_NORMAL
- en: 'Python Version: 3.5'
  prefs: []
  type: TYPE_NORMAL
- en: 'Operating System: Any'
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we use **VirusTotal**, a free online virus, malware, and URL
    scanner, to automate the review of potentially malicious websites or files. VirusTotal
    maintains detailed documentation of their API on their website. We will demonstrate
    how to perform basic queries against their system using their documented API and
    store returned results into a CSV file.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To follow this recipe, you need to first create an account with VirusTotal
    and decide between the free public API or the private API. The public API has
    request limitations, which the private API does not. For example, with the public
    API, we are limited to 4 requests per minute and 178,560 requests per month. More
    details about the different API types can be found on VirusTotal''s website. We
    will make these API calls with the `requests` library. This library can be installed
    using:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: To learn more about and use `VirusTotal`, visit the website at [https://www.virustotal.com/](https://www.virustotal.com/).
    [](https://www.virustotal.com/) Learn more about the `VirusTotal` Public API;
    visit [https://www.virustotal.com/en/documentation/public-api/](https://www.virustotal.com/en/documentation/public-api/).
    [](https://www.virustotal.com/en/documentation/public-api/) Learn more about the
    `VirusTotal` Private API; visit [https://www.virustotal.com/en/documentation/private-api/](https://www.virustotal.com/en/documentation/private-api/).
  prefs: []
  type: TYPE_NORMAL
- en: To view your API key, which you will need for the script, click on your account
    name in the top-right corner and navigate to My API key. Here you can view details
    of your API key and request a private key. Take a look at the following screenshot
    for additional details. All libraries used in this script are present in Python's
    standard library.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00056.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We use the following methodology to accomplish our objective:'
  prefs: []
  type: TYPE_NORMAL
- en: Read in the list of signatures, as either domains and IPs or file paths and
    hashes, to research.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Query VirusTotal using the API for domain and IPs or files.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Flatten results into a convenient format.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write results to a CSV file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First, we import the required libraries to handle argument parsing, creating
    spreadsheets, hashing files, parsing JSON data, and interacting with web pages.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: This recipe's command-line handler is a little more complicated than normal.
    It takes three positional arguments, `INPUT_FILE`, `OUTPUT_CSV`, and `API_KEY`,
    which represent the input text file of domains and IPs or file paths, the desired
    output CSV location, and a text file containing the API key to use, respectively.
    In addition to this, there are a few optional arguments, `-t` (or `--type`) and
    `--limit`, to specify the type of data in the input file and file paths or domains
    and to limit requests to comply with public API limitations. By default, the `type`
    argument is configured to the domain value. If the `limit` switch is added, it
    will have the Boolean value of `True`; otherwise, it will be `False`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: Next, we perform the standard data validation process on the input file and
    output CSV. If the inputs pass the data validation steps, we pass all arguments
    to the `main()` function or otherwise exit the script.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: The `main()` function starts by reading the input file into a set called `objects`.
    A set was used here to cut down on duplicate lines and duplicate calls to the
    API. In this manner, we can try to prolong hitting the limitations of the public
    API unnecessarily.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: After we have read in the data, we check whether the type of data we read in
    is in the domain and IP category or file paths. Depending on the type, we send
    the set of data to the appropriate function, which will return VirusTotal query
    results to the `main()` function. We will then send these results to the `write_csv()`
    method to write the output. Let's look at the `query_domain()` function first.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: This function first performs additional input validation, this time on the API
    key file, to ensure the file exists prior to trying to make calls with said key.
    If the file does exist, we read it into the `api` variable. The `json_data` list
    will store returned JSON data from the VirusTotal API calls.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: After we print a status message to the console, we begin to loop through each
    domain or IP address in the set. For each item, we increment `count` by one to
    keep track of how many API calls we have made. We create a parameter dictionary
    and store the domain or IP to search and API key and set `scan` to `1`. By setting
    `scan` to `1`, we will automatically submit the domain or IP for review if it
    is not already in the VirusTotal database.
  prefs: []
  type: TYPE_NORMAL
- en: We make the API call with the `requests.post()` method, querying the appropriate
    URL with the parameter dictionary to obtain the results. We use the `json()` method
    on the returned requests object to convert it into easily manipulated JSON data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: If the API call was successful and the data was found in the VirusTotal database,
    we append the JSON data to the list. If the data was not present in the VirusTotal
    database, we can use the API to retrieve the report after it has been created.
    Here, for simplicity, we assume the data is already present in their database
    and only add results if they were found rather than waiting for the report to
    be generated if the item does not already exist.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: Next, we check whether `limit` is `True` and the `count` variable is equal to
    3\. If so, we need to wait a minute before continuing the queries to comply with
    the public API limitations. We print status messages to the console so the user
    is aware of what the script is doing and use the `time.sleep()` method to halt
    script execution for a minute. After we have waited a minute, we reset the count
    back to zero and begin querying the remaining domain or IPs in the list. Once
    we have finished this process, we return the list of JSON results back to the
    `main()` function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: The `query_file()` method is similar to the `query_domain()` method we just
    explored. First, we validate that the API key file exists or exit the script otherwise.
    Once validated, we read in the API key and store it in the `api` variable and
    instantiate the `json_data` list to store the API JSON data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: Unlike the `query_domain()` function, we need to perform some additional validation
    and processing on each file path before we can use it. Namely, we need to validate
    that each file path is valid and then we must hash each file, or use the hash
    provided in the signatures file. We hash these files as this is how we will look
    them up in the VirusTotal database. Recall that we are assuming the file is already
    present in the database. We can use the API to submit samples and retrieve reports
    after the file is scanned.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: Let's take a quick look at the `file_hash` function. The `hash_file()` method
    is relatively straightforward. This function takes a file path as its only input
    and returns the `SHA-256` hash for the said file. We accomplish this, similar
    to how we did so in [Chapter 1](part0029.html#RL0A0-260f9401d2714cb9ab693c4692308abe),
    *Essential Scripting and File Information Recipes*, by creating a `hashlib` algorithm
    object, reading the file data into it `1,024` bytes at a time, and then calling
    the `hexdigest()` method to return the calculated hashes. With that covered, let's
    look at the remainder of the `query_file()` method.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: The `query_file()` method continues by creating a parameter dictionary with
    the API key and file hash to look up. Again, we use the `requests.post()` and
    `json()` methods to make the API call and convert it into JSON data, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: If the API call was successful and the file was already present in the VirusTotal
    database, we append the JSON data to the list. Once more, we perform checks on
    the count and limit to ensure we comply with the public API limitations. After
    we have completed all of the API calls, we return the list of JSON data back to
    the `main()` function for output.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: The `write_csv()` method first checks that the output data actually contains
    API results. If it does not, the script will exit rather than write an empty CSV
    file.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: If we do have results, we print a status message to the console and begin by
    flattening the JSON data into a convenient output format. We create a `flatten_data`
    list, which will store each flattened JSON dictionary. The field list maintains
    the list of keys in the flattened JSON dictionary and the desired column headers.
  prefs: []
  type: TYPE_NORMAL
- en: We use a few `for` loops to get to the JSON data and append a dictionary with
    this data to the list. After this process is completed, we will have a very simple
    list of dictionary structures to work with. We can use the `csv.DictWriter` class
    as we have previously to easily handle this type of data structure.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: With the data set ready for output, we open the CSV file and create the `DictWriter`
    class instance. We supply it the file object and the list of headers in the dictionary.
    We write the headers to the spreadsheet before writing each dictionary to a row.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot reflects when we run the script against files and
    hashes, and a second for running against domains and IPs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00057.jpeg)![](../images/00058.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Looking at the output, we can learn about the malware classifications for the
    files and hashes and the domain or IP ranking in CSV format:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00059.jpeg)![](../images/00060.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Totally passive
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Recipe Difficulty: Medium'
  prefs: []
  type: TYPE_NORMAL
- en: 'Python Version: 3.5'
  prefs: []
  type: TYPE_NORMAL
- en: 'Operating System: Any'
  prefs: []
  type: TYPE_NORMAL
- en: This recipe explores the PassiveTotal API and how to use it to automate the
    review of domains and IP addresses. This service is particularly useful in viewing
    historical resolution details for a given domain. For example, you may have a
    suspected phishing website and, based on historical resolution patterns, can identify
    how long it has been active and what other domains used to share that IP. This
    then gives you additional domains to review and search for, in your evidence as
    you identify the different means and methods of how the attackers maintained persistence
    as they compromised multiple users across the environment.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To use the PassiveTotal API, you need to first create a free account on their
    website. Once you are logged in, you can view your API key by navigating to your
    account settings and clicking on the User Show button under the API ACCESS section.
    See the following screenshot for a visual representation of this page.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00061.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: All libraries used in this script are present in Python's standard library.
    However, we do install the PassiveTotal Python API client and follow the installation
    and setup instructions in the README found at [https://github.com/passivetotal/python_api](https://github.com/passivetotal/python_api)
    or with `pip install passivetotal==1.0.30`. We do this to use the PassiveTotal
    command-line `pt-client` application. In this script, we make the API calls through
    this client rather than performing this at a more manual level as we did in the
    previous recipe. More details on the PassiveTotal API, especially if you are interested
    in developing something more advanced, can be found on their website.
  prefs: []
  type: TYPE_NORMAL
- en: To learn more about and use `PassiveTotal`, visit the website [https://www.passivetotal.org](https://www.passivetotal.org).
    [](https://www.passivetotal.org) Learn more about the `PassiveTotal` API; visit
    [https://api.passivetotal.org/api/docs](https://api.passivetotal.org/api/docs).
    [](https://api.passivetotal.org/api/docs) Learn more about the `PassiveTotal`
    Python API; visit [https://github.com/passivetotal/python_api](https://github.com/passivetotal/python_api).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We use the following methodology to accomplish our objective:'
  prefs: []
  type: TYPE_NORMAL
- en: Read in the list of domains to review.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Call the command-line `pt-client` using `subprocess` and return results to our
    script for each domain.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write results to a CSV file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First, we import the required libraries to handle argument parsing, creating
    spreadsheets, parsing JSON data, and spawning subprocesses.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: This recipe's command-line handler takes two positional arguments, `INPUT_DOMAINS`
    and `OUTPUT_CSV`, for the input text file containing domains and/or IPs and the
    desired output CSV, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: We perform the standard input validation steps on each of the inputs to avoid
    unexpected errors in the script. With the inputs validated, we called the `main()`
    function and pass it the two inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: The `main()` function is rather straightforward, and similar, to that in the
    previous recipe. We again use a set to read in the objects in the input file.
    Once again, this is to avoid redundant API calls to the PassiveTotal API as there
    are daily limitations to the free API. After we read in these objects, we call
    the `query_domains()` function, which uses the `pt-client` application to make
    API calls. Once we have all of the returned JSON data from the API calls, we call
    the `write_csv()` method to write the data to the CSV file.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: The `query_domains()` function starts by creating a `json_data` list to store
    the returned JSON data and printing a status message to the console. We then begin
    to iterate through each object in the input file and remove any "`https://`" or
    "`http://`" substrings. While testing `pt-client`, it was observed to generate
    an internal server error if that substring was present. For example, instead of
    [https://www.google.com](https://www.google.com), the query should just be [www.google.com](https://www.google.com).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: With the domain or IP address ready to be queried, we use the `subprocess.Popen()`
    method to open a new process and execute the `pt-client` application. Arguments
    to be executed in this process are in a list. The command that will be executed,
    if the domain is [www.google.com](https://www.google.com), would look like `pt-client
    pdns -q www.gooogle.com`. Supplying the `stdout` keyword argument as `subprocess.PIPE`
    creates a new pipe for the process so that we can retrieve results from the query.
    We do exactly that in the following line by calling the `communicate()` method
    and then converting the returned data into a JSON structure that we can then store.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: If the `quota_exceeded` message is in the JSON results, then we have exceeded
    the daily API limit and print that to the console and continue executing. We continue
    executing rather than exiting so that we can write any results we did retrieve
    before exceeding the daily API quota.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: Next, we set the `result_count` and check if it is equal to zero. If results
    were found for the query, we append the results to the JSON list. We return the
    JSON list, after performing this operation on all domains and/or IPs in the input
    file.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: The `write_csv()` method is pretty straightforward. Here we first check that
    we have data to write to the output file. Then, we print a status message to the
    console and create the list of headers and the order in which they should be written.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: After we have created the list of headers, we use the `csv.DictWriter` class
    to set up the output CSV file, write the header row, and iterate through each
    dictionary in the JSON results and write them to their respective rows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'Running the script provides insight to the number of responses per item in
    the PassiveTotal lookup:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00062.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The CSV report displays the collected information as seen here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00063.jpeg)'
  prefs: []
  type: TYPE_IMG
