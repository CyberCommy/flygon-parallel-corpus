- en: '*Chapter 12*: Multithreading and Asynchronous Programming'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since the very first personal computer, we have benefitted from the constant
    increase of CPU power—a phenomenon that heavily influenced developers' choices
    of tools, languages, and application design, while historically not putting much
    effort into programming to take advantage of multithreading.
  prefs: []
  type: TYPE_NORMAL
- en: On the hardware side, the prediction made by Moore's law that the density of
    the transistors in processors should double every 2 years, thus providing more
    computing power, worked for some decades, but we can already observe it slowing
    down. Even if the CPU manufacturers started producing multi-core CPUs roughly
    20 years ago, the ability to execute code concurrently was primarily used by the
    **operating systems** (**OSes**) to make executing multiple processes smoother.
  prefs: []
  type: TYPE_NORMAL
- en: This doesn't mean that code was unable to leverage the power of concurrency,
    but just that only a small quantity of applications fully embraced the *multithreading
    paradigm*. The primary reason for this is because all the code we write is executed
    sequentially from a single thread provided by the OS infrastructure unless we
    explicitly request the creation of other threads and orchestrate their execution.
  prefs: []
  type: TYPE_NORMAL
- en: This trend is mostly due to the fact that many programming languages do not
    provide constructs to automatically generate multithreading code. This is because
    it is extremely difficult to provide the semantics that fit any use case and efficiently
    take advantage of the concurrent processing capabilities offered by modern CPUs.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, there are times where we don't really need to execute the
    application code concurrently, but we can't continue the execution because it
    is necessary to wait for some outstanding I/O operation. At the same time, blocking
    the code execution is also not acceptable and therefore a different strategy is
    required. This domain of problems is categorized under *asynchronous programming*
    and requires slightly different tools.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will learn the basics of multithreading and asynchronous
    programming and look specifically at the following:'
  prefs: []
  type: TYPE_NORMAL
- en: What is a thread?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating threads in .NET
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding synchronization primitives
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The task paradigm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you will be familiar with multithreading techniques,
    using primitives to synchronize code execution, tasks, continuations, and cancellation
    tokens. You will also understand what the potentially dangerous operations are
    and the basic patterns to use to avoid problems when sharing resources among multiple
    threads.
  prefs: []
  type: TYPE_NORMAL
- en: We will now begin familiarizing ourselves with the basic concepts needed to
    operate with multithreading and asynchronous programming.
  prefs: []
  type: TYPE_NORMAL
- en: What is a thread?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Every OS provides abstractions to allow multiple programs to share the same
    hardware resources, such as CPU, memory, and input and output devices. The process
    is one of those abstractions, providing a reserved virtual address space that
    its running code cannot escape from. This basic sandbox avoids the process code
    interfering with other processes, establishing the basis for a balanced ecosystem.
    The process has nothing to do with code execution, but primarily with memory.
  prefs: []
  type: TYPE_NORMAL
- en: The abstraction that takes care of code execution is the **thread**. Every process
    has at least one thread, but any process code may request the creation of more
    threads that will all share the same virtual address space, delimited by the owning
    process. Running multiple threads in a single process is roughly equivalent to
    a group of woodworking friends working on the same project –they need to be coordinated,
    paying attention to each other's progress, and taking care not to block each other's
    activity.
  prefs: []
  type: TYPE_NORMAL
- en: All modern OSes offer the preemptive multitasking strategy as opposed to cooperative
    multitasking. This means that a special component of the OS schedules the amount
    of time each thread can run, without needing any cooperation from the running
    code.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: Earlier version of Windows, such as Windows 3.x and Windows 9x, used cooperative
    multitasking, meaning that any application could hang the entire operating system
    with a simple infinite loop. This was mostly because of CPU power and capabilities
    limitations. All the later operating systems, such as Windows versions starting
    from the very first **NT 3.1 Advanced Server** and all the Unix-like OSes, have
    always used preemptive multitasking, making the OS more robust and providing a
    better user experience.
  prefs: []
  type: TYPE_NORMAL
- en: You can see the number of threads used in each running process with either the
    Task Manager, Process Explorer, or Process Hacker tools. You will immediately
    notice that many applications, including all the .NET ones, use more than one
    single thread. This information doesn't tell us much about how the application
    is designed because modern runtimes such as the .NET CLR use background threads
    for internal processing, such as the **garbage collector**, the **finalization
    queue**, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: In order to see the number of threads used by the running processes, open the
    **Task Manager**(*Ctrl* + *Shift* + *Esc*), click on the **Details** tab, and
    add the **Threads** column. Columns can be added by right-clicking one of the
    grid headers, selecting the **Select Columns** menu item, and finally checking
    the **Threads** voice.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows a C++ console application where the user''s
    code uses a single thread and the other three threads are created by the C++ runtime:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.1 - The Task Manager showing NativeConsole.exe process with four
    threads'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_12.1_B12346.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.1 – The Task Manager showing NativeConsole.exe process with four threads
  prefs: []
  type: TYPE_NORMAL
- en: The namespace containing the primitives dealing with threads is `System.Threading`
    but later in this chapter, we will also introduce `System.Threading.Tasks` namespace.
  prefs: []
  type: TYPE_NORMAL
- en: When a .NET application starts, the .NET runtime prepares our process, allocating
    memory and creating some threads, including the one that will spin the execution
    of our code, starting from the `Main` entry point.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following console application accesses the current thread and prints the
    current thread `Id` on the screen:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The `ManagedThreadId` property is important when diagnosing multithreading code
    because it correlates the execution of some code with a specific thread.
  prefs: []
  type: TYPE_NORMAL
- en: 'This `Id` can only be used within the running process and is different from
    the OS thread identifier. Should you ever require access to the native identifier,
    you need to use interoperability, as demonstrated in the following Windows-only
    snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The native `Id` is the one that you can see in **Process Explorer** and the
    **Process Hacker** tools and is the one needed to interop with other native APIs.
    In the following screenshot, you can see the results printed in the console on
    the left, and the Process Hacker threads window on the right:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.2 - The console application side by side with Process Hacker showing
    the same native thread Id'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_12.2_B12346.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.2 – The console application side by side with Process Hacker showing
    the same native thread Id
  prefs: []
  type: TYPE_NORMAL
- en: 'Threads can also be created from either the OS, the .NET runtime, or some library
    without our code explicitly requesting it. For example, the following class shows
    a `FileSystemWatcher` class in action and prints the `ManagedThreadId` property
    for each filesystem operation: the `Run` method prints the ID associated with
    the main thread, while the `Wacher_Deleted` and `Watcher_Created` methods are
    executed from a thread created by the OS or the infrastructure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'You can experiment with this code by creating a console application and adding
    the following code to the `Main` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, if you start creating and deleting some `.txt` files in the console folder,
    you will see something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The `TID` numbers you see will likely change every time you rerun the application:
    they are neither predictable nor used in the same order.'
  prefs: []
  type: TYPE_NORMAL
- en: We will now see how we can create a new thread, execute some code concurrently,
    and examine the main characteristics of a thread.
  prefs: []
  type: TYPE_NORMAL
- en: Creating threads in .NET
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Creating a raw thread is something that mostly makes sense only when you have
    a long-running operation that depends on the CPU alone. As an example, let''s
    say we want to compute prime numbers, without really caring about the possible
    optimizations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The `Primes` class implements `IEnumerable<long>` so that we can easily enumerate
    the prime numbers in a `Max` argument is used to limit the resulting sequence,
    which is otherwise restricted by `long.MaxValue`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Calling the preceding code is very easy to do but, as the calculation can take
    a very long time, it totally blocks the executing thread:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: What happens here is that the main thread is busy calculating the prime numbers.
    Thanks to preemptive multitasking, this thread will be interrupted by the OS scheduler
    to give other process' threads the opportunity to run their code. However, since
    our application has no other threads executing application code, we can only wait.
  prefs: []
  type: TYPE_NORMAL
- en: In any desktop application, be it a console or a GUI, the user experience is
    frustrating as any interaction with the mouse and keyboard is *blocked*. Even
    worse, the GUIs cannot even redraw the content of the screen as the only thread
    was stolen by the prime number's computation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The very first step is to move the blocking code into a separate method so
    that we can execute it in a new and separate thread:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The `Thread.Sleep` method is used only to make some observations on the CPU
    usage. Then, `Sleep` tells the OS to suspend the current thread execution for
    the given amount of time, expressed in *milliseconds*. Generally, calling `Sleep`
    is not recommended in production code because it prevents that thread from being
    reused. Later in this chapter, we will discover better ways to insert delays in
    our code.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `Worker` method has nothing special and it may optionally get an object
    parameter that can be used to initialize the local variables. Instead of invoking
    it directly, we just ask the infrastructure to invoke it in the context of a new
    thread:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: As you can see from the preceding code, the `Thread` object is created but the
    thread is not started yet. We have to explicitly call the `Start` method to make
    it happen. This is important because the `Thread` class has other important properties
    that can be set only before the thread is started.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, the main thread''s details are printed by using the `PrintThreadInfo`
    method. Please note that some properties are not always available. For this reason,
    we have to check whether the thread is running before printing `Priority` or `IsBackground`.
    Since the `ThreadState` enumeration has the `Flags` attribute and the `Running`
    state is zero, the official documentation (https://docs.microsoft.com/en-us/dotnet/api/system.threading.threadstate?view=netframework-4.8#remarks)
    reminds us to check if the `Stopped` and `Unstarted` bits are not set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The result of the executing the preceding code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Even if this is a trivial example, we must observe a few things:'
  prefs: []
  type: TYPE_NORMAL
- en: The first is that we have no guarantees about the *output order* regarding the
    `Primes calculation …` and `Id:5 …` lines. They may appear in *reversed order*.
    In order to obtain a *deterministic behavior*, you need to apply a synchronization
    technique that we will discuss later in the *Understanding synchronization primitives*
    section.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Another important consideration is the *CPU usage*. If you open **Task Manager**,
    under the **Performance** tab, you can set the view to show a separate graph for
    each logical CPU. In the following screenshot, you can see a four-core CPU that
    has eight logical cores (thanks to the Intel Hyper-Threading technology!). You
    may also want to show kernel times (shown in a darker color) because the kernel
    mode only executes code for the OS and drivers, while the user mode (shown in
    a lighter color) just executes the code we write. This distinction will allow
    you to immediately see which application code is being executed:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 12.3 - The Task Manager showing all the logical processors'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_12.3_B12346.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.3 – The Task Manager showing all the logical processors
  prefs: []
  type: TYPE_NORMAL
- en: 'If we now execute our code without the `Sleep` call, we will see that one of
    the CPUs will show a higher amount of CPU usage as one thread keeps consuming
    the full amount of execution time granted by the OS. This single thread impacts
    the total (100%) amount of CPU time by *100% / 8 CPUs = 12.5%*. In fact, during
    the computation, the **Details** tab of **Task Manager** will show your process
    consuming roughly 12% of the CPU:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.4 - The Task Manager showing the execution time distributed across
    all the available logical CPUs](img/Figure_12.4_B12346.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.4 – The Task Manager showing the execution time distributed across
    all the available logical CPUs
  prefs: []
  type: TYPE_NORMAL
- en: The thread computation is *distributed* across multiple logical CPUs. Every
    time the OS interrupts the thread, schedules some other work of another process,
    and then gets back to our thread, the thread may be executed on any other logical
    CPU.
  prefs: []
  type: TYPE_NORMAL
- en: 'Just as an experiment, you can force the execution to take place on a specific
    logical CPU by adding the following code at the very beginning of the `Worker`
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'This code requires the following declaration inside the class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Those new lines of code retrieve a list of all the `ProcessThread` objects for
    our process and then filter the `ProcessThread` object whose native ID matches
    the one that is doing the execution.
  prefs: []
  type: TYPE_NORMAL
- en: 'After setting `ProcessorAffinity`, the new execution fully loads the logical
    CPU `2` with our computation, as shown in the following screenshot (the light
    blue section of CPU `2` entirely fills the rectangle):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.5 – The Task Manager showing CPU 2 fully loaded with the execution
    of the sample code'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_12.5_B12346.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.5 – The Task Manager showing CPU 2 fully loaded with the execution
    of the sample code
  prefs: []
  type: TYPE_NORMAL
- en: 'Immediately before starting the thread, we have the possibility to shape the
    thread characteristics by setting one or more of these properties:'
  prefs: []
  type: TYPE_NORMAL
- en: The `Priority` property is used from the OS scheduler to decide the slot of
    time the thread can run. Giving it a high priority will reduce the amount of time
    the thread stays suspended.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `Name` property is useful when debugging because you can see it in the Visual
    Studio Thread window.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We briefly discussed the `ThreadState` property, which can assume many different
    values. One of them—`WaitSleepJoin`—represents a thread that is inside a `Wait`
    method or sleeping.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `CurrentCulture` and `CurrentUICulture` properties are read by certain APIs
    that are *region-dependent*. For example, when you convert a number or a date
    into a string (using the `ToString` method) or the `Parse` static method for the
    opposite conversion, the current culture settings are used.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `IsBackground` property specifies whether the thread should prevent the
    process from terminating when it is still active. When true, the process will
    not wait for the thread to finish its work. In our example, if you set it to true,
    then you can end the process by pressing any key.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You may have noticed the `Thread` class has the `Abort` method. It should never
    be used because it may corrupt the state of the memory or prevent the correct
    disposal of managed resources.
  prefs: []
  type: TYPE_NORMAL
- en: The correct way to terminate a thread is to exit normally from the method it
    initially started. In our case, this is the `Worker` method. A simple `return`
    statement is all you need.
  prefs: []
  type: TYPE_NORMAL
- en: We have seen how to create a thread manually, but there is a more convenient
    way to run some code in a separate thread—the `ThreadPool` class.
  prefs: []
  type: TYPE_NORMAL
- en: Using the ThreadPool class
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We spent some time investigating the characteristics of threads and this was
    indeed very useful because the thread is the fundamental code-execution building
    block. Manually creating a thread is correct as long as it is executing CPU-dependent
    and long-running code. Anyway, since the cost of the thread is dependent on the
    OS, it is wiser to create an adequate amount of threads and reuse them. Their
    number is very dependent on the available logical CPUs and other factors, and
    this is the reason why it is far better to use the `ThreadPool` abstraction.
  prefs: []
  type: TYPE_NORMAL
- en: The static `ThreadPool` class provides a pool of threads that can be used to
    run some concurrent computation. As soon as the code terminates, the thread comes
    back to the pool, becoming available for a future operation without needing to
    be destroyed and recreated.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: Be warned not to modify any property of the thread picked from `ThreadPool`.
    For example, if you modify `ProcessorAffinity`, this setting will continue to
    be valid, even if the thread is reused for different purposes. If you need to
    modify the thread's properties, then manual creation is still the best choice.
  prefs: []
  type: TYPE_NORMAL
- en: 'Running our `Worker` using the `ThreadPool` class is straightforward:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Please note that the delegate parameter accepted by the `Thread` class constructor
    and `QueueUserWorkItem` are different, but the one taking an object parameter
    is compatible with both.
  prefs: []
  type: TYPE_NORMAL
- en: We have seen how to start a parallel computation, but we are still not able
    to orchestrate their execution. Should an algorithm be run on a different thread,
    we need to be aware of its termination and how to access the result.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: '`ThreadPool` is used from many popular libraries, including the base class
    library shipped with the .NET runtime. Whenever you need to access a resource
    requiring an I/O operation that may take a while to succeed or fail, most of the
    time, `ThreadPool` comes into play. Those resources include, among others, databases,
    filesystem objects, or anything that can be accessed through the network.'
  prefs: []
  type: TYPE_NORMAL
- en: Every time you need to access a resource concurrently, be it a resource retrieved
    by means of an I/O operation or an instance of an object in memory, you may need
    to synchronize its access. In the next section, we will see how to synchronize
    thread execution.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding synchronization primitives
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Every time you write single-threaded code, any method execution occurs sequentially
    and requires no special action from the developer. On the other hand, when some
    code is executed on a separate thread, synchronization is needed to ensure that
    we avoid two dangerous concurrency conditions—**race** and **deadlock**. These
    categories of problems must be carefully avoided during design because their detection
    is difficult and they may occur occasionally.
  prefs: []
  type: TYPE_NORMAL
- en: A **race condition** is a situation where two or more threads access an unprotected
    shared resource or when the threads' executions behave differently, depending
    on the timing and the underlying process architecture.
  prefs: []
  type: TYPE_NORMAL
- en: A *deadlock condition* happens when two or more threads have a circular dependency
    on each other to access a resource.
  prefs: []
  type: TYPE_NORMAL
- en: 'The general recommendations when writing some code that may be executed from
    multiple threads are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Avoid shared resources as much as possible. Their access must be synchronized
    with a lock that will affect the execution performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The stack is your friend. Every time you call a method, the local stack is private,
    ensuring the local variables will not be shared with other callers and threads.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Every time you need to share a resource among multiple threads, use the documentation
    to verify whether it is thread-safe or not. Whenever it is not thread-safe, a
    lock must protect the resource or the code sequence.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Even when the shared resource is thread-safe, you must consider whether a number
    of statements need to be executed atomically to guarantee their reliability.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The thread library has many primitives available to protect the resources, but
    we will focus more on those that are more likely to be used in the asynchronous
    context, which is the most important topic that will be covered in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two sets of synchronization primitives:'
  prefs: []
  type: TYPE_NORMAL
- en: The ones implemented in *kernel mode* by the OS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ones in *user mode* provided by the .NET class library
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The distinction is very important because every time you transition to the kernel
    mode with a system call, the OS has to save the local call and stack, which will
    be restored right after, impacting the performance of the operation. The advantage
    of kernel mode primitives is the ability to give them a name and make them shared
    across processes, providing a powerful machine-wide synchronization mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following example shows two threads from `ThreadPool` printing `Ping` and
    `Pong`. Each thread synchronizes with the other by waiting for the matching `ManualResetEventSlim`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: After having created the two events, the two threads are run and print the ID
    of the thread they are running on. Inside those threads, each execution is suspended
    in the `Wait` method, which avoids the thread consuming any CPU power. At the
    end of the listing, the `pong.Set` method starts the game and unblocks the first
    thread. Since the events are *manual*, they must be reset to the unsignaled state
    for the next hit. At this point, a message is printed, a delay simulates some
    hard work and, finally, the other event is signaled, which will cause the second
    thread to unblock.
  prefs: []
  type: TYPE_NORMAL
- en: 'Alternatively, we can use the `ManualResetEvent` kernel event, whose usage
    is very similar. For example, in place of `Wait`, it has the `WaitOne` method.
    But were we to use these events in a high-performance synchronization algorithm,
    there would be a huge difference. The following table shows a comparison of the
    two synchronization primitives measured with the popular Benchmark.NET micro-benchmark
    library. Both tests simply call `Set()`, followed by the `Reset()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: There is a difference of roughly two orders of magnitude, which is not negligible
    at all.
  prefs: []
  type: TYPE_NORMAL
- en: 'Beyond the ability to use kernel events to synchronize code running in different
    processes, they can be used in conjunction with the powerful `WaitHandle.WaitAny`
    and `WaitAll` methods, as shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: You can play with the three timeouts expressed in milliseconds to see the different
    results. The main idea is to exit the wait as soon as any of the events or the
    timeout expires, whichever comes first.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: The kernel objects of the Windows OS can be all used inside the wait primitives.
    For example, if you want to wait for multiple processes to exit, you can just
    use the `WaitHandle` primitives, shown in the preceding code block, with the process
    handles.
  prefs: []
  type: TYPE_NORMAL
- en: We've only just scratched the surface, but the official documentation has many
    samples showing various synchronization objects in action. Instead, we will continue
    to focus on those that are more relevant for this book, such as accessing a shared
    resource from multiple threads.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, we have a shared variable called `_shared`, a `ManualResetEvent`
    object that is used to start all the threads together, and a simple object. The
    `Shared` property makes use of `Thread.Sleep`, causing an explicit thread context
    switch on the setter. The switch is what normally happens when the OS scheduler
    preemptively gives control to another thread in the system. It''s not a trick;
    it just increases the probability that the getter and the setter are not executed
    consecutively by each thread:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The following method initializes the shared variable to `0` and creates 10
    threads, all executing the same code in the lambda:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: All the threads start immediately and block the execution in the `WaitOne` event
    that is unblocked by the `Set` method. This gives more chances for many threads
    to execute the code in the lambda with the same timing. Finally, we call `Join`
    to wait for the end of the execution of each thread and print the results.
  prefs: []
  type: TYPE_NORMAL
- en: The synchronization problem of this code exists because the threads will read
    a value, increment the number in a CPU register, and write back the result in
    the variable. Since many threads will read the same value, the value written back
    to the variable is old and its real *current* value gets lost.
  prefs: []
  type: TYPE_NORMAL
- en: By uncommenting the lock statement, we instruct the compiler to surround the
    statements in the curly braces with a **Critical Section**, the fastest user mode
    synchronization object available. This results in serializing the access to that
    code, which has a very significant impact on the performance that is necessary
    and unavoidable.
  prefs: []
  type: TYPE_NORMAL
- en: The empty object instance we created at the beginning should not change; otherwise,
    different threads would wait for different critical sections. Please note that
    the `lock` argument can be any reference type. For example, should you need to
    protect a collection, you can lock it directly without the help of an external
    object. Anyway, in our example, `Shared` is a value type and must be protected
    with the help of a separate reference type.
  prefs: []
  type: TYPE_NORMAL
- en: If you replace the `Shared` property with a simple field, the problem will be
    less likely to occur. Also, the compiler configuration (debug versus release)
    will make a great difference because *inlining* and other optimizations make it
    even more likely that a thread context switch can't happen when accessing a field
    or a simple property. The physical hardware configuration and the CPU architecture
    are other variables that may greatly influence the outcome of these tests.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: Unit testing is *not appropriate* to ensure the absence of issues such as race
    conditions or deadlocks. Also, be aware that virtual machines are the worst environment
    to test concurrent code in because the scheduler is more predictable than an OS
    running on the physical hardware.
  prefs: []
  type: TYPE_NORMAL
- en: We have seen how we can ensure that a number of statements are executed atomically,
    with no interference. But if it was just for ensuring an atomic increment of the
    underlying `_shared` field, there is a more convenient tool—the `Interlocked`
    class.
  prefs: []
  type: TYPE_NORMAL
- en: '`Interlocked` is a static class that exposes a few useful methods to ensure
    the atomicity of certain operations. For example, instead of the `lock` statement,
    we could use the following code, which is much faster, even if limited to the
    operations exposed by `Interlocked`. The following code shows how to atomically
    increment the `_shared` variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Among other things, we can use it for writing a variable and getting back the
    old value atomically (the `Exchange` method) or reading variables whose size is
    larger than the available native registers (the `Read` method).
  prefs: []
  type: TYPE_NORMAL
- en: We have seen why synchronization is needed and what the main tools are that
    we can use to protect against these concurrent access problems. But now, it is
    time to introduce an abstraction that will make every developer's life easier—the
    task paradigm.
  prefs: []
  type: TYPE_NORMAL
- en: The task paradigm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Concurrency is primarily about designing algorithms with very loosely coupled
    units of work, which is often not possible or extends the complexity beyond any
    possible benefit.
  prefs: []
  type: TYPE_NORMAL
- en: Asynchronous programming is, instead, related to the asynchronous nature of
    the OS and the devices, whether because they fire events or because it takes time
    to fulfill the requested operation. Every time the user moves the mouse, types
    keys on the keyboard, or retrieves some data from the internet, the OS presents
    data to our process in a separate thread and our code must be ready to consume
    it.
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the simplest possible examples is loading a text file from disk and
    computing the string length, which can be different from the file length, depending
    on the encoding:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: As soon as you invoke this method, the calling thread gets blocked until the
    OS and the library completes reading it. The operation may be lightning-fast or
    very slow, depending on its size and technology. The text file may be on **Network-Attached
    Storage** (**NAS**), a local disk, a corrupted USB key, or on a remote server
    accessed through a **Virtual Private Network** (**VPN**).
  prefs: []
  type: TYPE_NORMAL
- en: In the context of a desktop application, any blocking thread will cause an unpleasant
    user experience because the main thread is already responsible for redrawing the
    user interface and responding to the events coming from the input devices.
  prefs: []
  type: TYPE_NORMAL
- en: Server applications are no different because any blocking thread is a resource
    that cannot be used efficiently with other requests, preventing the application
    from scaling and serving other users.
  prefs: []
  type: TYPE_NORMAL
- en: For decades, the solution to this problem was to execute the long-lasting code
    by manually creating a separate thread, but more recently, the .NET runtimes introduced
    the task paradigm and the C# language introduced the `async` and `await` keywords.
    Since then, the whole .NET library has been revised to embrace this paradigm,
    providing methods that return task-based operations.
  prefs: []
  type: TYPE_NORMAL
- en: The Task Library, available in the `System.Threading.Tasks` namespace, and the
    language integration provide an abstraction that dramatically simplifies the management
    of asynchronous operations. A task represents a unit of work that performs a well-defined
    job. No matter whether you deal with concurrency or asynchronous events, a task
    defines a given job and its life cycle, going from its creation to its completion,
    the options for which include success, failure, or cancellation.
  prefs: []
  type: TYPE_NORMAL
- en: Tasks can be composed by defining what other tasks should be executed right
    after a given operation. The chained task is called **continuation** and is automatically
    scheduled from the libraries by means of the **Task Scheduler**.
  prefs: []
  type: TYPE_NORMAL
- en: By default, the Task Library provides a default implementation (the `TaskScheduler.Default`
    static property), which most developers will never need to dig into. The default
    implementation orchestrates the task's execution using `ThreadPool` and uses the
    *work-stealing* technique to redistribute the task queue over multiple threads
    to provide load balancing and to prevent tasks from being stalled for too long.
    Be aware that this default implementation is smart enough to eventually make the
    decision to schedule the execution of tasks directly on the main thread instead
    of picking one from the pool. The bravest can experiment with the creation of
    custom schedulers to change the scheduling strategy, but it is something not many
    developers really need to do.
  prefs: []
  type: TYPE_NORMAL
- en: Later, in the *Synchronization context* section, we will talk about **synchronization
    context**, which allows continuations to be executed in the calling thread and
    avoids the need to use the synchronization primitives described in the previous
    section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s begin investigating tasks with the asynchronous version of reading a
    text file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: This new version of the method *immediately completes* and, instead of returning
    the content of the file, returns an object representing the *ongoing* operation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we just initiated the operation that didn''t complete yet, the steps
    required to manage the completion are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Refactor out the code following the asynchronous operation (getting the string
    length) in a separate method. This method is equivalent to the old-style callback
    that mustn't be called before the asynchronous operation has completed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Monitor the ongoing task and provide a notification when it has completed or
    failed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once completed, retrieve the result and synchronize the execution (by means
    of **synchronization context**) on the main thread, or throw an exception if something
    has gone wrong. This step is crucial if we don't want to mess with potential race
    conditions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Invoke the callback that we refactored out at the first point.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Of course, we don''t have to manually manage all this machinery. The first
    interesting advantage of the Task Library is its support for continuations, which
    allow the developer to specify the code to be executed as soon as the task completes
    successfully:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: This new version is better than creating threads and manually writing the synchronization
    code, even if it can be further improved. The `ContinueWith` method contains the
    code that determines the other code to be executed as soon as the file has been
    successfully read.
  prefs: []
  type: TYPE_NORMAL
- en: The `t` variable contains the task, which is either failed or completed successfully.
    If it was successful, `t.Result` contains the string content obtained from the
    `ReadAllTextAsync` method.
  prefs: []
  type: TYPE_NORMAL
- en: Anyway, we still don't have the length; we just expressed how to retrieve the
    length in the *future* once the result of `ReadAllTextAsync` has been retrieved.
    This is the reason why the `lengthTask` variable is a `Task<int>`, that is, the
    promise of an integer.
  prefs: []
  type: TYPE_NORMAL
- en: Tasks and continuations are the building blocks that I strongly recommend experimenting
    with because there are times they need to be managed directly.
  prefs: []
  type: TYPE_NORMAL
- en: But the C# language also introduced two precious keywords that further simplify
    the code we need to write. The `await` keyword is used to indicate that the result
    of the operation and everything that comes after it is part of a continuation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thanks to the `await` keyword, the compiler refactors and generates new **Intermediate
    Language** (**IL**) code to provide the appropriate management of asynchronous
    operations and the continuation. The final code to asynchronously load the content
    of the file and return the string length is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The highlighted portions of code are refactored by the compiler with more than
    just a continuation. The compiler generates a *class* to take care of the state
    machine responsible for monitoring the task progress, and a method for calling
    the appropriate code or throwing an exception, as soon as the state of the task
    changes.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: If you want to dig into more details on the generated code, you can use the
    **ILSpy** tool (https://github.com/icsharpcode/ILSpy/releases) and see the generated
    IL code.
  prefs: []
  type: TYPE_NORMAL
- en: Apparently, the compiler could get rid of the promise and let us work on the
    returned content, right? Not really – this code is refactored and the code we
    wrote is an artifact expressing our expectations rather than what normally and
    sequentially happens in a method.
  prefs: []
  type: TYPE_NORMAL
- en: In fact, the preceding code looks contradictory, as the `content.Length` integer
    will only be available in the future, but we return it directly from a method
    with the return type of `Task<int>`.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is where the `async` keyword comes into play:'
  prefs: []
  type: TYPE_NORMAL
- en: The `async` keyword is a modifier that must be specified every time we want
    to use `await` inside a method.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `async` keyword informs us that the `return` statement specifies a future
    object or value. In our case, we return `int` but `async` informs us that it is
    really a `Task<int>`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Should an `async` method return `void`, the return type becomes the non-generic
    `Task`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We now have a method that is processing the file asynchronously, but we had
    to change the signature from `int` to `Task<int>`.
  prefs: []
  type: TYPE_NORMAL
- en: 'When you write a lambda using the `await` keyword in its body, the `async`
    keyword is required as well. For example, let''s look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Using `async` on a method implies that all the callers must embrace the task
    paradigm as well because otherwise, they could not know when the operation completes.
  prefs: []
  type: TYPE_NORMAL
- en: Synchronous implementations of asynchronous methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have seen how the task paradigm impacts the method signature and we know
    how important a method signature is. When it appears on a public API or in interfaces,
    it is a contract that, in most cases, we can't change. From a design perspective,
    it can be very valuable for anticipating the possibility of a given method being
    implemented with tasks, but there are cases where asynchronicity is not needed.
  prefs: []
  type: TYPE_NORMAL
- en: 'For those cases, the `Task` class exposes a static method allowing us to directly
    build a completed task with or without results. In the following example, the
    asynchronous method synchronously returns a completed task:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The `CompletedTask` property is created only once for the entire application
    domain; therefore, it is extremely lightweight and should not be any cause for
    concern regarding performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'If, instead, we need to return a value, we can use the static `FromResult`
    method, which internally creates a new completed `Task` every time it is invoked:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Creating an object every time we add two numbers is definitely a performance
    concern because it directly impacts the amount of work the garbage collector has
    to do. For this reason, more recently, Microsoft introduced the `ValueTask` classes.
  prefs: []
  type: TYPE_NORMAL
- en: Occasionally asynchronous methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `ValueTask` immutable struct is a convenient wrapper around either a synchronous
    result or `Task`. This further abstraction is meant to simplify those cases where
    the method is required to have an asynchronous signature, but its implementation
    is just occasionally asynchronous.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `AddAsync` method we defined with tasks in the previous section can easily
    be converted to use the `ValueTask` struct instead:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The overhead of using `Task` is clear for a trivial sum; therefore, whenever
    such a method should be called in a hot path (some performance-critical code),
    it would certainly be a performance concern.
  prefs: []
  type: TYPE_NORMAL
- en: Anyway, there are cases where you may need to convert `ValueTask` into `Task`
    in order to benefit from all the utilities we will continue to discuss in the
    rest of this chapter. The conversion is available with the `AsTask` method, which
    returns the wrapped task, if any, or creates a fresh new `Task` if not.
  prefs: []
  type: TYPE_NORMAL
- en: Breaking the task chain – blocking the thread
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Given a task, if you call the `Wait` method or access the `Result` getter property,
    they will block the thread execution until the task is either completed or canceled.
    The rationale behind the task paradigm is to avoid blocking threads so that they
    can be reused for other purposes. But blocking may also provoke very bad side
    effects.
  prefs: []
  type: TYPE_NORMAL
- en: Since the default source for threads in asynchronous programming is `ThreadPool`
    (should it exhaust its threads), any further request will be automatically blocked.
    This phenomenon is known as **thread starvation**.
  prefs: []
  type: TYPE_NORMAL
- en: The general advice is to avoid waiting and use the `await` keyword or the continuations
    to complete some work.
  prefs: []
  type: TYPE_NORMAL
- en: Manually creating a task
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are times when a library does not offer an asynchronous behavior but
    you don''t want to keep the current thread busy for that long. In this case, you
    can use the `Task.Run` method, which schedules the execution of the lambda, which
    will most probably occur in a separate thread. The following example shows how
    to read the length of a file if the asynchronous `ReadAllTextAsync` method that
    we used previously were not available:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: You should always prefer the provided asynchronous version instead of using
    the `Run` method because the thread where this task is scheduled will block until
    the end of the synchronous execution.
  prefs: []
  type: TYPE_NORMAL
- en: We will now look at what the best course of action is whenever there is a very
    large amount of work to be done inside a task.
  prefs: []
  type: TYPE_NORMAL
- en: Long-running tasks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Even if you don't block the thread, there is still the risk of starvation whenever
    the asynchronous stack never awaits and becomes a long-running job, keeping the
    thread busy.
  prefs: []
  type: TYPE_NORMAL
- en: 'These cases can be treated with two different strategies:'
  prefs: []
  type: TYPE_NORMAL
- en: The first is manually *creating a thread*, which we already discussed at the
    beginning of this chapter. This is the best strategy when you need more control
    or need to modify the thread properties.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The second possibility is *informing the task scheduler* that the task is going
    to run for a long time. This way, the scheduler will take a different strategy
    and avoid the `ThreadPool` altogether. The following code shows how to run a long-running
    task:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: The essential recommendation is to try splitting the long jobs into smaller
    units of work that can be easily transformed into tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Breaking the task chain – fire and forget
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have seen that embracing the task paradigm requires modifying the entire
    chain of callers. But there are times when this is not possible and also not desirable.
    For example, in the context of a desktop WPF application, you may have to write
    to a file inside a button-click event handler:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'We can''t change its signature to return a `Task`; moreover, this would not
    make sense for two reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: The calling library has been designed before the tasks and it would not be able
    to manage the task progress.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is one of the events designed as a **Fire-and-Forget** operation, meaning
    that you don't really care how long they will take or which result they are going
    to compute.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For these cases, you can embrace the `async`/`await` keywords while not using
    the returning `Task` at all:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: But remember, when you break the task chain, you lose the possibility to know
    whether the operation will ever complete or fail.
  prefs: []
  type: TYPE_NORMAL
- en: Information box
  prefs: []
  type: TYPE_NORMAL
- en: Every time you see `async void` in your code, you should wonder whether it could
    be a potential bug, or just that you really don't want to know what will happen
    to that task in the end. Over the years, the habit to use `async void` instead
    of `async Task` has been the primary source of bugs in asynchronous code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, if you just invoke an asynchronous method without awaiting it (or
    using one of the `ContinueWith` methods), you will lose control of the invocation
    obtaining the same *fire-and-forget* behavior, because asynchronous methods return
    immediately after starting the asynchronous operation. Also, all the code following
    a not-awaited asynchronous operation will be executed concurrently, incurring
    the risk of race conditions or accessing data that is not yet available:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: We have seen how simple it is to manage an asynchronous operation when everything
    completes successfully, but the code can throw exceptions and we need to catch
    them appropriately.
  prefs: []
  type: TYPE_NORMAL
- en: Task and exceptions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are two kinds of exceptions that can happen when something goes wrong.
    The first is before any asynchronous method gets called, while the second is related
    to exceptions happening in the asynchronous code.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following example shows these two cases:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'In the first case, we are telling the caller that we will return a `Task<int>`
    but that no asynchronous operation has begun yet. This situation is exactly what
    happens in synchronous methods and can be caught accordingly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'On the other hand, if the exception occurs in the continuation, the exception
    will not happen immediately; it will only happen as soon as the task is *consumed*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: As soon as `resultTask` has completed as *faulted*, the exception has already
    happened but the compiler-generated code caught it and assigned it to the `Task.Exception`
    property. Since there may be multiple exceptions happening at the same time inside
    `Task`, the generated code encapsulates all the captured exceptions inside a single
    `AggregateException`. The `InnerException` and `InnerExceptions` properties in
    `AggregateException` contain the original exception.
  prefs: []
  type: TYPE_NORMAL
- en: 'Whenever you want to handle the exceptions and resolve them immediately, you
    may want to use the continuations instead of the `await` keyword:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: As we mentioned previously, the exception in a *faulted* task is thrown as soon
    as the result gets *consumed*, which we previously mentioned in the context of
    using `await`. However, this can also occur in the case where the `t.Result` property
    is accessed.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: The `Task` class exposes the `GetAwaiter` method, which returns the inner struct
    representing the asynchronous operation. You can get the result of the asynchronous
    operation with `task.GetAwaiter().GetResult()`, as well as `task.Result`, but
    there is a small difference. In fact, in the case of an exception, the former
    returns the original exception, while the latter returns an `AggregateException`
    containing the original exception.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, it is worth mentioning that we can rewrite the `CrashAfterAsync` method
    with the static `Task.FromException<T>` method instead:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Similar to what we saw with `FromResult<T>`, a new `Task` is created, but this
    time, its state is initialized to *faulted* and it contains the desired exception.
  prefs: []
  type: TYPE_NORMAL
- en: The preceding example is quite abstract but succinct enough to give you an idea
    of how to properly handle exceptions, depending on when they are thrown. There
    are many scenarios where this regularly happens. A real example of this duality
    would be an serialization exception occurring when preparing the JSON parameters
    or during the HTTP rest call as a result of a network failure.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to transitioning to the faulted state, tasks can also be canceled,
    thanks to a built-in standard mechanism provided by the task paradigm.
  prefs: []
  type: TYPE_NORMAL
- en: Canceling a task
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unlike faults, cancellation is requested from the callers to interrupt the execution
    of one or more tasks. Cancellation can be imperative, or simply a timeout, which
    is very useful when a given task should not take more than a given amount of time.
  prefs: []
  type: TYPE_NORMAL
- en: 'From the caller''s perspective, the cancellation pattern originates from the
    `CancellationTokenSource` class, which provides three different constructors:'
  prefs: []
  type: TYPE_NORMAL
- en: The default constructor is used when you are willing to cancel tasks by imperatively
    calling the `Cancel` method.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The other constructors take either an `int` or a `TimeSpan`, which determine
    the maximum amount of time before a cancellation gets triggered, unless the tasks
    complete beforehand.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the following example, we will experiment with canceling one of the three
    worker methods using a `CancellationToken` that has been obtained from a timed
    `CancellationTokenSource`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: The `Token` property returns a read-only struct that can be used by multiple
    consumers without impacting on the garbage collector or even being copied, as
    it is immutable.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first consumer being examined here takes `CancellationToken` and correctly
    propagates it to any other method that accepts cancellations. In our example,
    there is just `Task.Delay`, a very convenient method used to instruct the infrastructure
    to trigger the continuation after 5 seconds:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'The result of the preceding code''s execution is the cancellation of the task,
    which is transformed into a `TaskCanceledException` by the code generated from
    the `await` keyword:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Another possibility is when a worker is executing only *synchronous* code and
    still needs to be canceled:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Please note the use of `Thread.Sleep` instead of the `Delay` method, which was
    necessary because we wanted a synchronous implementation.
  prefs: []
  type: TYPE_NORMAL
- en: The `Thread.Sleep` method is very different because it blocks the thread entirely
    and prevents the thread from being reused anywhere else, while `Task.Delay` spawns
    a request to call the following code as a continuation as soon as the specified
    amount of time has expired.
  prefs: []
  type: TYPE_NORMAL
- en: The more interesting part is testing the `IsCancellationRequested` Boolean property
    to allow a collaborative cancellation of the task. Being collaborative by explicitly
    checking that property is necessary because you may not need to interrupt the
    execution before having disposed of some resource, be it written on a database
    or anywhere else.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once again, the result of executing the preceding method will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'The third and final case is when you don''t want to throw any exception, but
    just to return from the execution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: In this case, we carefully avoided propagating `CancellationToken` to the underlying
    calls, because, by using `await`, it would have triggered the exception.
  prefs: []
  type: TYPE_NORMAL
- en: 'The execution of this final `WorkForever3Async` method does not raise any exceptions
    and lets the execution continue normally:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: The downside of this implementation is that the cancellation may not happen
    immediately. `Task.Delay` will need to complete regardless of the cancellation,
    which, in the worst case, can't happen before 5 seconds.
  prefs: []
  type: TYPE_NORMAL
- en: We have seen how the task paradigm makes running asynchronous operations dramatically
    easier, but how can we run multiple asynchronous requests at the same time? They
    can potentially be run in parallel to avoid useless waits.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring the progress of a task
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After the user starts a long-running operation, providing feedback is very important
    to avoid the user becoming frustrated. This is possible when you are in control
    of what is happening, such as with some a time-expensive algorithm. Instead, when
    the long-running operation depends on a call to an external library, monitoring
    the progress is not possible.
  prefs: []
  type: TYPE_NORMAL
- en: The Task Library does not have specific support for monitoring progress, but
    the .NET library provides `IProgress<T>`, which can easily be used for this goal.
    This interface just provides a single member—`void Report(T value)`—which leaves
    total freedom on the implementation details. In the simplest cases, `T` would
    be an integer value representing the progress as a percentage.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, a load operation could be implemented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: The method, which in our case simulates an asynchronous operation by just calling
    `Task.Delay`, must have a prediction of the total number of steps that relates
    to 100% of the progress. After each step, the `Report` method is called to inform
    us about the current percentage, but ensure the code is protected from the progress
    being null, as the consumer may not be interested in receiving such feedback.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the consumer side, the first thing to do is create the progress provider,
    which is simply a class implementing `IProgress<int>`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, the caller should just pass the provider instance to the `Load` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'As you may expect, the output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: The generic argument of `IProgress<T>` can potentially be used to pause the
    execution or trigger more complex logic such as pausing/resuming behavior.
  prefs: []
  type: TYPE_NORMAL
- en: Parallelizing tasks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A common programming task is retrieving some resources from the internet. For
    example, the essential code to download a resource via HTTP is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: Thanks to `EnsureSuccessStatusCode`, any failure will trigger an exception,
    leaving the responsibility of catching it to the caller. Also, we didn't even
    set any header, but it's enough for our purposes.
  prefs: []
  type: TYPE_NORMAL
- en: 'We already know how to invoke this asynchronous method to download an image,
    but now the challenge is choosing the right strategy to download many of them:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first question is: *how can we download multiple images in parallel?* If
    we need to download 10 images, we don''t want to sum the times needed to download
    each of them. Anyway, we will not enter into the discussion of how much we can
    scale if, let''s say, you need to download millions of images. This would be out
    of the scope of a discussion about asynchronous machinery.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The second question is: *do we need them all at the same time?* In this case,
    we can use the `Task.WhenAll` helper method, which takes an array of tasks and
    returns a single task representing the overall operation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For these samples, we are going to use the online free service called *Lorem
    PicSum* ([https://picsum.photos/](https://picsum.photos/)). Every time you make
    a request to the URI you see in the code, a new and different image sized 200
    x 200 will be retrieved. You can, of course, use any URI of your choice:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: The use of `Enumerable.Range` is a nice way to repeat an action for the given
    number of times. We don't really care about the generated numbers; in fact, we
    use the `discard (_)` token instead of a variable in the `Select` method.
  prefs: []
  type: TYPE_NORMAL
- en: The `Select` lambda just initiates the download operations returning the corresponding
    tasks that we don't await yet. Instead, we ask the `WhenAll` method to create
    a new `Task` that will be signaled as soon as all the tasks are completed successfully.
    Should any task fail, the code generated from the `await` keyword will cause an
    exception to be thrown.
  prefs: []
  type: TYPE_NORMAL
- en: 'The task obtained from the `WhenAll` method cannot be used to retrieve the
    results, but it guarantees that we can access the `Result` properties for all
    the tasks. Therefore, after awaiting `allTask`, we iterate the `tasks` array retrieving
    the `byte[]` array for all the downloaded images. Here is the output obtained
    by awaiting all the downloads at the same time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: In many cases, this is a good strategy because we may need all the resources
    before continuing. An alternative is to wait for the first download so that we
    can start processing it, but we still want to download them all concurrently to
    save time.
  prefs: []
  type: TYPE_NORMAL
- en: 'This alternative strategy can be pursued with the help of the `WaitAny` method.
    In the following example, starting the downloads is no different. We just add
    a `Stopwatch` class to show the time taken in milliseconds at the end of the downloads:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'The `while` loop is used to process all the unfinished tasks. Initially, the
    `tasks` array contains all of them, but every time `WhenAny` completes, it means
    that at least one task has completed. The completed ones are immediately printed
    on screen, together with the milliseconds elapsed since the beginning of the operation.
    The other ones are reassigned to the `tasks` variable so that we can loop back
    and process the completed tasks until the very last one. The output of this new
    method is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Of course, you can combine these two methods together. For example, if you
    want to get as many downloaded images as possible in no more than 100 milliseconds,
    just replace the `WhenAny` line with the following one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'In other words, we are asking to wait for any task (at least one) but not before
    100 milliseconds. The `while` loop will repeat the operation, as we did previously,
    by consuming all the remaining tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: When you test these code snippets, be sure to run them in a loop because the
    first run can be heavily influenced by the **Just-in-Time** compiler.
  prefs: []
  type: TYPE_NORMAL
- en: We have seen how the `Task` class provides a very powerful building block to
    consume asynchronous operations, but this requires libraries providing asynchronous
    behavior. In the next section, we will see how we can expose a manual task and
    trigger its completion.
  prefs: []
  type: TYPE_NORMAL
- en: Signaling tasks with the TaskCompletionSource object
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Going back to the file watcher sample in the *What is a thread?* section at
    the beginning of this chapter, you may remember `FileSystemWatcher` exposing events
    and not embracing the task paradigm. You may wonder whether we write some sort
    of adapter to leverage the power of all the nice tools offered by the Task Library,
    and the answer is *yes*.
  prefs: []
  type: TYPE_NORMAL
- en: The `TaskCompletionSource` object provides an important building block that
    we can use to expose asynchronous behavior. It is created and used on the producer
    side to signal the completion of an operation, be it a success or a failure. It
    provides, via the `Task` property, the task object that must be used from the
    client to await the notification.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following class uses `FileSystemWatcher` to monitor the filesystem in the
    current folder. The `Deleted` event stops the notifications and notifies the completion
    source about the successful deletion of a file. Similarly, the `Error` event sets
    the exception that will be eventually triggered on the consumer side of the `await`
    statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: Every time the `WhenDeleted` method is called, a new completion source is created,
    the file watcher is started, and the `Task` responsible for the notification is
    returned to the client.
  prefs: []
  type: TYPE_NORMAL
- en: 'From the consumer perspective, this solution is awesome because it removes
    any complexity:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: The downside of this solution is that only a single deletion can be detected
    at one time.
  prefs: []
  type: TYPE_NORMAL
- en: Also, since the code inside the `Deleted` event turns off the notifications,
    calling the `WhenDeleted` method inside a loop could cause missing deletions.
  prefs: []
  type: TYPE_NORMAL
- en: But we can fix that problem! The slightly more complex solution is to buffer
    the events in a thread-safe queue and change the `WhenDeleted` method strategy
    by dequeuing the available event, if any.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the revised code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: Once again, we could solve the problem with just the Task Library tools. Depending
    on the use case, this strategy requires recreating a new `TaskCompletionSource<T>`
    every time and since it is a reference type, it may affect the performance being
    subject to garbage collection. Should we need to reuse the same notification object,
    we can do so by creating a custom notification object.
  prefs: []
  type: TYPE_NORMAL
- en: In fact, the `await` keyword just needs an object implementing a method called
    `GetAwaiter`, returning an object that implements the `INotifyCompletion` interface.
    This object, in turn, must implement an `IsCompleted` property and all the required
    machinery miming the `TaskCompletionSource` behavior.
  prefs: []
  type: TYPE_NORMAL
- en: In the *Further reading* section, you will find an interesting article called
    *await anything* from the Microsoft official blog that deep dives into this topic.
  prefs: []
  type: TYPE_NORMAL
- en: Synchronization context
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Depending on the application we are writing, not all threads are created equal.
    Desktop applications have a main thread that is the only one allowed to draw on
    screen and deal with graphics controls. The GUI libraries work around the concept
    of a queue of messages where every request is posted. The main thread is responsible
    for dequeueing those messages and dispatching them into the user-defined handlers
    that implement the desired behavior.
  prefs: []
  type: TYPE_NORMAL
- en: Every time something happens on a thread different than the UI one, a marshaling
    operation must occur, which will cause a message to be posted in the queue managed
    by the main thread. Two popular examples of marshalling messages in the UI thread
    are `Control.Invoke` in the context of Windows Forms applications and `Dispatcher.Invoke`
    for Window Presentation Foundation.
  prefs: []
  type: TYPE_NORMAL
- en: Information box
  prefs: []
  type: TYPE_NORMAL
- en: The very first prerelease version of WPF was multithreaded. But the code complexity
    required users to deal with multithreading, and the consequent possible bugs in
    the user's code were raising the bar too much. Even many C++ libraries, like DirectX
    and OpenGL, are mostly single-threaded to cut down the complexity.
  prefs: []
  type: TYPE_NORMAL
- en: On the server side, ASP.NET applications also have the context of the main thread,
    but there isn't just one—in fact, each user's request has its own main thread.
  prefs: []
  type: TYPE_NORMAL
- en: '`SynchronizationContext` is the base class for an abstraction that defines
    a standard way to provide the execution of some code in the context of the *special*
    thread. This is no magic; in fact, the code that is being executed is defined
    in a lambda and posted in a queue. On the main thread, some code provided by the
    infrastructure dequeues the lambda and executes it in its context.'
  prefs: []
  type: TYPE_NORMAL
- en: This automatic marshaling is fundamental because, after executing any asynchronous
    method, such as downloading an image from the internet, you want to avoid calling
    the `Invoke` method needed to marshal the result back into the main thread, which
    is required in order to update the user interface with the returned data.
  prefs: []
  type: TYPE_NORMAL
- en: Every time you await some asynchronous operation, the generated code takes care
    to *capture* the current `SynchronizationContext` and make sure that the continuation
    is executed on that specific thread. Basically, you don't need to do anything
    because the infrastructure already does it for you.
  prefs: []
  type: TYPE_NORMAL
- en: 'Are we done? Not really, because there are times where this does not happen.
    From what we said, the three IDs in the following example should all be the same:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: This is not the case because it is a console application that doesn't set, by
    default, any synchronization context. The reason for this is in the Microsoft
    documentation for the `Console` class. You will see the *Thread Safety* section,
    at the end of the documentation page, stating *This type is thread safe*. In other
    words, there is no reason to go back to the original thread.
  prefs: []
  type: TYPE_NORMAL
- en: If you instead create a new Windows Forms application and call that code in
    a button-click handler, you will see that the ID is always the same, thanks to
    `SynchronizationContext`.
  prefs: []
  type: TYPE_NORMAL
- en: It is always important to understand what happens, in terms of threading, to
    your asynchronous code because there are times where marshalling the result back
    to the main one is not desirable because marshalling has a performance impact.
    For example, library developers must be very careful when writing asynchronous
    code because they can't know if their code will be executed in the presence or
    absence of a synchronization context.
  prefs: []
  type: TYPE_NORMAL
- en: 'A clear example is when the library developer is processing chunks of data
    coming from the network. Every chunk is retrieved by means of an asynchronous
    HTTP request and the number of chunks can be very high, as in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Unless the processing code is going to interact with the UI (or anything related
    to the main thread), disabling the synchronization context is definitely a performance
    gain and very easy to do:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: By applying the `ConfigureAwait` method to the asynchronous method, the result
    of the operation will not be posted back to the main thread, and the generated
    continuation will be executed on the secondary thread (whenever the asynchronous
    operation is scheduled on a different thread).
  prefs: []
  type: TYPE_NORMAL
- en: 'This modified behavior has two consequences:'
  prefs: []
  type: TYPE_NORMAL
- en: Posting the message in the main thread queue has a *performance impact*. For
    example, library developers may want to set `ConfigureAwait` to `false` when doing
    some internal work to improve the performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Whenever you should decide to execute an asynchronous method synchronously using
    the `Wait` method or the `Result` property, you may incur a *deadlock*. This can
    happen because the synchronization context posts back the execution to the main
    thread, which is busy. While this situation should be avoided by never using `Wait`
    and `Result`, an alternative approach is to make the call finish its execution
    on the secondary thread by setting `ConfigureAwait` to `false`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Please note that if you really want to continue the execution on the secondary
    thread, ensure that you apply `ConfigureAwait` to all the following calls. In
    fact, the first asynchronous call executed without `ConfigureAwait` will cause
    the execution to return to the main thread.
  prefs: []
  type: TYPE_NORMAL
- en: Since the code following `ConfigureAwait` is executed on a secondary thread,
    remember to manually marshal back to the main thread to avoid race conditions.
    For example, to update the UI, you must call the relevant *Windows Forms* or *WPF*
    `Invoke` method.
  prefs: []
  type: TYPE_NORMAL
- en: The task paradigm is a revolution in programming languages that could not exist
    without the help of the new language keyword and the compiler generation magic.
    This new feature had a great resonance in other languages as well. For example,
    ECMAScript 2017 adopted these concepts by providing both promises and async/await
    keyword support.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout this long chapter, we learned the importance of asynchronous programming
    and how the Task Library makes asynchronous code intuitive and easy to write,
    while still not forcing us to bother too much about the implicit complexity. Beyond
    acquiring a general understanding of these tools, it is now important to experiment
    and dig into each aspect to master those techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed the most important tools that any developer can
    use to take advantage of the multithreading and asynchronous programming techniques.
  prefs: []
  type: TYPE_NORMAL
- en: The building blocks are the fundamental abstractions that allow code to run
    in a different execution context, regardless of the OS they are currently running
    on. Those primitives must be used with wisdom, but that doesn't limit the developer's
    possibilities in any way compared to native languages and libraries.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to this, the task paradigm offers a natural approach when it comes
    to interacting with all those events whose nature is asynchronous. The `System.Threading.Tasks`
    namespace provides all the required abstractions to interact with asynchronous
    phenomena.
  prefs: []
  type: TYPE_NORMAL
- en: The library has been widely restructured and widened to support the task paradigm.
    And most importantly, the language offers the `async` and `await` keywords to
    break down the complexity and make the asynchronous world flow as if it was procedural
    code.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn about the concepts of files, file streams,
    and serialization.
  prefs: []
  type: TYPE_NORMAL
- en: Test what you learned
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you have a very CPU-intensive, long-lasting algorithm to run, which strategy
    out of manual thread creation, using the task library, or using the thread pool
    would you adopt?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Name a performant synchronization technique that can be used to write a file
    and increase an integer value in memory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What method should you use to pause the execution for 100 milliseconds, and
    why?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What should you do to wait for the results produced by multiple asynchronous
    operations?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How can you create a task to await a CLR event?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What should you return from a method that has `Task` in the signature but does
    not use any asynchronous method?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How can you create a long-running task?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A button-click handler is making asynchronous access to the internet to load
    some data. Should you use `Control.Invoke` to update the results on screen? Why?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the reasons for evaluating the use of the `ConfigureAwait` method on
    a `Task`?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can you update the UI directly after having used `ConfigureAwait(false)`?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A very powerful library that can be used to measure the performance of some
    code is Benchmark.NET ([https://benchmarkdotnet.org/articles/overview.html](https://benchmarkdotnet.org/articles/overview.html)),
    which is also used internally by Microsoft to make optimizations on the runtime
    and the core libraries.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If you want to build your own *awaitable* object, you cannot miss this article
    from the Microsoft team, describing how the underlying infrastructure works: [https://devblogs.microsoft.com/pfxteam/await-anything/](https://devblogs.microsoft.com/pfxteam/await-anything/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To dig into more details about the synchronization context and `ConfigureAwait`,
    you can read the following article: [https://devblogs.microsoft.com/dotnet/configureawait-faq/](https://devblogs.microsoft.com/dotnet/configureawait-faq/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
