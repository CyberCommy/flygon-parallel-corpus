- en: Aggregating and Summarizing Data into Useful Reports
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will learn how to aggregate and summarize data into useful
    reports. We will learn how to calculate averages with `map` and `reduce` functions,
    perform faster average computation, and use pivot tables with key-value pair data
    points.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Calculating averages with `map` and `reduce`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Faster average computations with aggregate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pivot tabling with key-value paired data points
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculating averages with map and reduce
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will be answering the following three main questions in this section:'
  prefs: []
  type: TYPE_NORMAL
- en: How do we calculate averages?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is a map?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is reduce?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can check the documentation at [https://spark.apache.org/docs/latest/api/python/pyspark.html?highlight=map#pyspark.RDD.map](https://spark.apache.org/docs/latest/api/python/pyspark.html?highlight=map#pyspark.RDD.map).
  prefs: []
  type: TYPE_NORMAL
- en: The `map` function takes two arguments, one of which is optional. The first
    argument to `map` is `f`, which is a function that gets applied to the RDD throughout
    by the `map` function. The second argument, or parameter, is the `preservesPartitioning`
    parameter, which is `False` by default.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we look at the documentation, it says that `map` simply returns a new RDD
    by applying a function to each element of this RDD, and obviously, this function
    refers to `f` that we feed into the `map` function itself. There''s a very simple
    example in the documentation, where it says if we parallelize an `rdd` method
    that contains a list of three characters, `b`, `a`, and `c`, and we map a function
    that creates a tuple of each element, then we''ll create a list of three-tuples,
    where the original character is placed in the first elements of the tuple, and
    the `1` integer is placed in the second as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This will give us the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The `reduce` function takes only one argument, which is `f`. `f` is a function to
    reduce a list into one number. From a technical point of view, the specified commutative
    and associative binary operator reduces the elements of this RDD.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take an example using the KDD data we have been using. We launch our
    Jupyter Notebook instance that links to a Spark instance, as we have done previously.
    We then create a `raw_data` variable by loading a `kddcup.data.gz` text file from
    the local disk as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The next thing to do is to split this file into `csv`, and then we will filter
    for rows where feature 41 includes the word `normal`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we use the `map` function to convert this data into an integer, and then,
    finally, we can use the `reduce` function to compute the `total_duration`, and
    then we can print the `total_duration` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We will then get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The next thing to do is to divide `total_duration` by the counts of the data
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'This will give us the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: And after a little computation, we would have created two counts using `map`
    and `reduce`. We have just learned how we can calculate averages with PySpark,
    and what the `map` and `reduce` functions are in PySpark.
  prefs: []
  type: TYPE_NORMAL
- en: Faster average computations with aggregate
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we saw how we can use `map` and `reduce` to calculate
    averages. Let's now look at faster average computations with the `aggregate` function.
    You can refer to the documentation mentioned in the previous section.
  prefs: []
  type: TYPE_NORMAL
- en: The `aggregate` is a function that takes three arguments, none of which are
    optional.
  prefs: []
  type: TYPE_NORMAL
- en: The first one is the `zeroValue` argument, where we put in the base case of
    the aggregated results.
  prefs: []
  type: TYPE_NORMAL
- en: The second argument is the sequential operator (`seqOp`), which allows you to
    stack and aggregate values on top of `zeroValue`. You can start with `zeroValue`,
    and the `seqOp` function that you feed into `aggregate` takes values from your
    RDD, and stacks or aggregates it on top of `zeroValue`.
  prefs: []
  type: TYPE_NORMAL
- en: The last argument is `combOp`, which stands for combination operation, where
    we simply take the `zeroValue` argument that is now aggregated through the `seqOp`
    argument, and combine it into one value so that we can use this to conclude the
    aggregation.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, here we are aggregating the elements of each partition and then the results
    for all the partitions using a combined function and a neutral zero value. Here,
    we have two things to note:'
  prefs: []
  type: TYPE_NORMAL
- en: The `op` function is allowed to modify `t1`, but it should not modify `t2`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The first function `seqOp` can return a different result type `U`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In this case, we all need one operation for merging a `T` into `U`, and one
    operation for merging the two Us.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s go to our Jupyter Notebook to check how this is done. `aggregate` allows
    us to calculate both the total duration and the count at the same time. We call
    the `duration_count` function. We then take `normal_data` and we aggregate it.
    Remember that there are three arguments to aggregate. The first one is the initial
    value; that is, the zero value, `(0,0)`. The second one is a sequential operation,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: We need to specify a `lambda` function with two arguments. The first argument
    is the current accumulator, or the aggregator, or what can also be called a database
    (`db`). Then, we have the second argument in our `lambda` function as `new_value`,
    or the current value we're processing in the RDD. We simply want to do the right
    thing to the database, so to say, where we know that our database looks like a
    tuple with the sum of duration on the first element and the count on the second
    element. Here, we know that our database looks like a tuple, where the sum of
    duration is the first element, and the count is the second element. Whenever we
    look at a new value, we need to add the new value to the current running total and
    add `1` to the current running counts.
  prefs: []
  type: TYPE_NORMAL
- en: The running total is the first element, `db[0]`. And we then simply need to
    add `1` to the second element `db[1]`, which is the count. That's the sequential
    operation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Every time we get a `new_value`, as shown in the previous code block, we simply
    add it to the running total. And, because we''ve added `new_value` to the running
    total, we need to increment the counts by `1`. Secondly, we need to put in the
    combinator operation. Now, we simply need to combine the respective elements of
    two separate databases, `db1` and `db2`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Since the duration counts is a tuple that collects our total duration on the
    first element, and counts how many durations we looked at in the second element,
    computing the average is very simple. We need to divide the first element by the
    second element as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'This will give us the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: You can see that it returns the same results as we saw in the previous section,
    which is great. In the next section, we are going to look at pivot tabling with
    key-value paired data points.
  prefs: []
  type: TYPE_NORMAL
- en: Pivot tabling with key-value paired data points
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Pivot tables are very simple and easy to use. What we are going to do is use
    big datasets, such as the KDD cup dataset, and group certain values by certain
    keys.
  prefs: []
  type: TYPE_NORMAL
- en: For example, we have a dataset of people and their favorite fruits. We want
    to know how many people have apple as their favorite fruit, so we will group the
    number of people, which is the value, against a key, which is the fruit. This
    is the simple concept of a pivot table.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use the `map` function to move the KDD datasets into a key-value pair
    paradigm. We map feature `41` of the dataset using a `lambda` function in the `kv` key
    value, and we append the value as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: We use feature `41` as the key, and the value is the data point, which is `x`.
    We can use the `take` function to take one of these transformed rows to see how
    it looks.
  prefs: []
  type: TYPE_NORMAL
- en: Let's now try something similar to the previous example. To figure out the total
    duration against each type of value that is present in feature `41`, we can use
    the `map` function again and simply take the `41` feature as our key. We can take the
    float of the first number in the data point as our value. We will use the `reduceByKey`
    function to reduce each duration by its key.
  prefs: []
  type: TYPE_NORMAL
- en: So, instead of just reducing all of the data points regardless of which key
    they belong to, `reduceByKey` reduces duration numbers depending on which key
    it is associated with. You can view the documentation at [https://spark.apache.org/docs/latest/api/python/pyspark.html?highlight=map#pyspark.RDD.reduceByKey](https://spark.apache.org/docs/latest/api/python/pyspark.html?highlight=map#pyspark.RDD.reduceByKey).
    `reduceByKey` merges the values for each key using an associative and commutative
    `reduce` function. It performs local merging on each mapper before sending the
    results to the reducer, which is similar to a combiner in MapReduce.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `reduceByKey` function simply takes one argument. We will be using the
    `lambda` function. We take two different durations and add them together, and
    PySpark is smart enough to apply this reduction function depending on a key, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting output is shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dc4b7020-d1a2-48ee-8e9f-6f152fefc69f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If we collect the key-value duration data, we can see that the duration is
    collected by the value that appears in feature `41`. If we are using pivot tables
    in Excel, there is a convenience function that is the `countByKey` function, which
    does the exact same thing, demonstrated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'This will give us the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0e2d8e34-d50e-447b-818e-d2139cdaa519.png)'
  prefs: []
  type: TYPE_IMG
- en: You can see that calling the `kv.countByKey()` function is the same as calling
    the `reduceByKey` function, preceded by a mapping from the key to the duration.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have learned how to calculate averages with `map` and `reduce`.
    We also learned faster average computations with `aggregate`. Finally, we learned
    that pivot tables allow us to aggregate data based on different values of features,
    and that, with pivot tables in PySpark, we can leverage handy functions, such
    as `reducedByKey` or `countByKey`.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn about MLlib, which involves machine learning,
    which is a very hot topic.
  prefs: []
  type: TYPE_NORMAL
