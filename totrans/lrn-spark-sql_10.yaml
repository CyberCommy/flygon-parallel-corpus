- en: Using Spark SQL in Deep Learning Applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deep learning has emerged as a superior solution to several difficult problems
    in machine learning over the past decade. We hear about deep learning being deployed
    across many different areas, including computer vision, speech recognition, natural
    language processing, audio recognition, social media applications, machine translation,
    and biology. Often, the results produced using deep learning approaches have been
    comparable to or better than those produced by human experts.
  prefs: []
  type: TYPE_NORMAL
- en: There have been several different types of deep learning models that have been
    applied to different problems. We will review the basic concepts of these models
    and present some code. This is an emerging area in Spark, so even though there
    are several different libraries available, many are in their early releases or
    evolving on a daily basis. We will provide a brief overview of some of these libraries,
    including some code examples using Spark 2.1.0, Scala, and BigDL. We chose BigDL
    because it is one the few libraries that run directly on top of Spark Core (similar
    to other Spark packages) and works with Spark SQL DataFrame API and ML pipelines
    using a Scala API.
  prefs: []
  type: TYPE_NORMAL
- en: 'More specifically, in this chapter, you will learn the following:'
  prefs: []
  type: TYPE_NORMAL
- en: What is deep learning?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the key concepts of various deep learning models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding deep learning in Spark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with BigDL and Spark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A neural network, or an **artificial neural network** (**ANN**), is a set of
    algorithms, or actual hardware, that is loosely modeled after the human brain.
    They are essentially an interconnected set of processing nodes that are designed
    to recognize patterns. They adapt to, or learn from, a set of training patterns
    such as images, sound, text, time series, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks are typically organized into layers that consist of interconnected
    nodes. These nodes communicate with each other by sending signals over the connections.
    Patterns are presented to the network via an input layer, which is then passed
    on to one or more hidden layers. Actual computations are executed in these hidden
    layers. The last hidden layer connects to an output layer that outputs the final
    answer.
  prefs: []
  type: TYPE_NORMAL
- en: The total input to a particular node is typically a function of the output from
    each of the connected nodes. The contribution from these inputs to a node can
    be excitatory or inhibitory, and ultimately helps determine whether and to what
    extent the signal progresses further through the network (via an activation function).
    Typically, sigmoid activation functions have been very popular. In some applications,
    a linear, semilinear, or the hyperbolic tan (`Tanh`) function have also been used.
    In cases where the output of a node is a stochastic function of the total input,
    the input determines the probability that a given node gets a high activation
    value.
  prefs: []
  type: TYPE_NORMAL
- en: The weights of the connections within the network are modified based on the
    learning rules; for example, when a neural network is initially presented with
    a pattern, it makes a guess as to what the weights might be. It then evaluates
    how far its answer is the actual one and makes appropriate adjustments to its
    connection weights.
  prefs: []
  type: TYPE_NORMAL
- en: 'For good introduction to the basics of neural networks, refer: "*A Basic Introduction
    To Neural Networks*" by Bolo, available at: [http://pages.cs.wisc.edu/~bolo/shipyard/neural/local.html](http://pages.cs.wisc.edu/~bolo/shipyard/neural/local.html).'
  prefs: []
  type: TYPE_NORMAL
- en: We will present more specific details of various types of neural networks in
    the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding deep learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deep learning is the application of ANNs to learn tasks. Deep learning methods
    are based on learning data representations, instead of task-specific algorithms.
    Though the learning can be supervised or unsupervised, the recent focus has been
    toward creating efficient systems that learn these representations from large-scale,
    unlabeled Datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure depicts a simple deep learning neural network with two
    hidden layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00282.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Deep learning typically comprises of multiple layers of processing units with
    the learning of feature representation occurring within each layer. These layers
    form a hierarchy of features and deep learning assumes that the hierarchy corresponds
    to the levels of abstraction. Hence, it exploits the idea of hierarchical explanatory
    factors, where the more abstract concepts at a higher level are learned from the
    lower-level ones. Varying the numbers of layers and layer sizes can provide different
    amounts of abstraction, as required by the use case.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding representation learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deep learning methods are representation-learning methods with multiple levels
    of abstraction. Here, the non-linear modules transform the raw input into a representation
    at a higher, slightly more abstract level. Ultimately, very complex functions
    can be learned by composing sufficient numbers of such layers.
  prefs: []
  type: TYPE_NORMAL
- en: For a review paper on deep learning, refer to *Deep Learning*, by Yann LeCun,
    Yoshua Bengio, and Geoffrey Hinton, which is available at [http://www.nature.com/nature/journal/v521/n7553/full/nature14539.html?foxtrotcallback=true](http://www.nature.com/nature/journal/v521/n7553/full/nature14539.html?foxtrotcallback=true).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we illustrate the process of learning representations and features in
    a traditional pattern recognition task:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00283.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Traditional machine learning techniques were limited in their ability to process
    natural data in its original or raw form. Building such machine-learning systems
    required deep domain expertise and substantial effort to identify (and keep updated)
    the features from which the learning subsystem, often a classifier, can detect
    or classify patterns in the input.
  prefs: []
  type: TYPE_NORMAL
- en: Many of these conventional machine learning applications used linear classifiers
    on top of handcrafted features. Such classifiers typically required a good feature
    extractor that produced representations that were selective to the aspects of
    the image. However, all this effort is not required if good features could be
    learned, automatically, using a general-purpose learning procedure. This particular
    aspect of deep learning represents one of the key advantages of deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'In contrast to the earlier machine learning techniques, the high-level process
    in deep learning is, typically, in which the end-to-end learning process involves
    features that are also learned from the data. This is illustrated here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00284.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In the next section, we will briefly discuss one of most commonly used functions,
    stochastic gradient descent, for adjusting the weights in a network.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding stochastic gradient descent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A deep learning system can consist of millions of adjustable weights, and millions
    of labeled examples are used to train the machine. In practice, **stochastic gradient
    descent** (**SGD**) optimization is used widely in many different situations.
    In SGD, the gradient describes the relationship between the network's error and
    a single weight, that is, how does the error vary as the weight is adjusted.
  prefs: []
  type: TYPE_NORMAL
- en: 'This optimization approach consists of:'
  prefs: []
  type: TYPE_NORMAL
- en: Presenting the input vector for a few examples
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Computing outputs and the errors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Computing average gradient for the examples
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adjusting weights, appropriately
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This process is repeated for many small sets of training examples. The process
    stops when the average of the objective function stops decreasing.
  prefs: []
  type: TYPE_NORMAL
- en: This simple procedure usually produces a good set of weights very efficiently
    compared to the more sophisticated optimization techniques. Additionally, the
    training process takes a much shorter time as well. After the training process
    is complete, the performance of the system is measured by running the trained
    model on a test Dataset. The test set contains new inputs that have not been seen
    before (during the training phase) by the machine.
  prefs: []
  type: TYPE_NORMAL
- en: In deep learning neural networks, the activation function is typically set at
    the layer level and applies to all the neurons or nodes in a particular layer.
    Additionally, the output layer of a multilayered deep learning neural network
    plays a specific role; for example, in supervised learning (with labeled input),
    it applies the most likely label based on the signals received from the previous
    layer. Each node on the output layer represents one label, and that node produces
    one of the two possible outcomes, a `0` or a `1`. While such neural networks produce
    a binary output, the input they receive is often continuous; for example, the
    input to a recommendation engine can include factors such as how much the customer
    spent in the previous month and the average number of customer visits per week
    over the past one month. The output layer has to process such signals into a probability
    measure for the given input.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing deep learning in Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will review some of the more popular deep learning libraries
    using Spark. These include CaffeOnSpark, DL4J, TensorFrames, and BigDL.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing CaffeOnSpark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: CaffeOnSpark was developed by Yahoo for large-scale distributed deep learning
    on Hadoop clusters. By combining the features from the deep learning framework
    Caffe with Apache Spark (and Apache Hadoop), CaffeOnSpark enables distributed
    deep learning on a cluster of GPU and CPU servers.
  prefs: []
  type: TYPE_NORMAL
- en: For more details on CaffeOnSpark, refer to [https://github.com/yahoo/CaffeOnSpark](https://github.com/yahoo/CaffeOnSpark).
  prefs: []
  type: TYPE_NORMAL
- en: CaffeOnSpark supports neural network model training, testing, and feature extraction.
    It is complementary to non-deep learning libraries, Spark MLlib and Spark SQL.
    CaffeOnSpark's Scala API provides Spark applications with an easy mechanism to
    invoke deep learning algorithms over distributed Datasets. Here, deep learning
    is typically conducted in the same cluster as the existing data processing pipelines
    to support feature engineering and traditional machine learning applications.
    Hence, CaffeOnSpark allows deep learning training and testing processes to be
    embedded into Spark applications.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing DL4J
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: DL4J supports training neural networks on a Spark cluster in order to accelerate
    network training. The current version of DL4J uses a process of parameter averaging
    on each cluster node in order to train the network. The training is complete when
    the master has a copy of the trained network.
  prefs: []
  type: TYPE_NORMAL
- en: For more details on DL4J, refer to [https://deeplearning4j.org/spark](https://deeplearning4j.org/spark).
  prefs: []
  type: TYPE_NORMAL
- en: Introducing TensorFrames
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Experimental TensorFlow binding for Scala and Apache Spark is currently available
    on GitHub. TensorFrames is essentially TensorFlow on Spark Dataframes that lets
    you manipulate Apache Spark's DataFrames with TensorFlow programs. The Scala support
    is currently more limited than Python--the Scala DSL features a subset of TensorFlow
    transforms.
  prefs: []
  type: TYPE_NORMAL
- en: For more details on TensorFrames, visit [https://github.com/databricks/tensorframes](https://github.com/databricks/tensorframes).
  prefs: []
  type: TYPE_NORMAL
- en: In Scala, the operations can be loaded from an existing graph defined in the
    `ProtocolBuffers` format or using a simple Scala DSL. However, given the overall
    popularity of TensorFlow, this library is gaining traction and is more popular
    with the Python community.
  prefs: []
  type: TYPE_NORMAL
- en: Working with BigDL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'BigDL is an open source distributed deep learning library for Apache Spark.
    It was initially developed and open sourced by Intel. With BigDL, the developers
    can write deep learning applications as standard Spark programs. These programs
    directly run on top of the existing Spark or Hadoop clusters, as illustrated in
    this figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00285.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'BigDL is modeled after Torch and it provides support for deep learning, including
    numeric computing (via Tensors) and [neural networks](https://github.com/intel-analytics/BigDL/tree/master/spark/dl/src/main/scala/com/intel/analytics/bigdl/nn).
    Additionally, the developers can load pretrained [Caffe](http://caffe.berkeleyvision.org/)
    or [Torch](http://torch.ch/) models into BigDL-Spark programs, as illustrated
    in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00286.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: To achieve high performance, BigDL uses [Intel MKL](https://software.intel.com/en-us/intel-mkl)
    and multithreaded programming in each Spark task.
  prefs: []
  type: TYPE_NORMAL
- en: For BigDL documentation, examples, and API guides, check out [https://bigdl-project.github.io/master/](https://bigdl-project.github.io/master/).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure shows how a BigDL program is executed at a high-level on
    a Spark cluster. With the help of a cluster manager and the driver program, Spark
    tasks are distributed across the Spark worker nodes or containers (executors):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00287.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: We will execute a few examples of deep neural networks available in the BigDL
    distribution in the later sections of this chapter. At this time, this is one
    of the few libraries that work with the Spark SQL DataFrame API and ML pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will highlight how Spark can be leveraged for tuning
    hyperparameters in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: Tuning hyperparameters of deep learning models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When building a neural network, there are many important hyperparameters to
    choose carefully. Consider the given examples:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Number of neurons in each layer: Very few neurons will reduce the expressive
    power of the network, but too many will substantially increase the running time
    and return noisy estimates'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Learning rate: If it is too high, the neural network will only focus on the
    last few samples seen and disregard all the experience accumulated before, and
    if it is too low, it will take too long to reach a good state'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The hyperparameter tuning process is "embarrassingly parallel" and can be distributed
    using Spark.
  prefs: []
  type: TYPE_NORMAL
- en: For more details, refer to *Deep Learning with Apache Spark and TensorFlow*,
    by Tim Hunter, at [https://databricks.com/blog/2016/01/25/deep-learning-with-apache-spark-and-tensorflow.html](https://databricks.com/blog/2016/01/25/deep-learning-with-apache-spark-and-tensorflow.html).
  prefs: []
  type: TYPE_NORMAL
- en: Introducing deep learning pipelines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There is an emerging library for supporting deep learning pipelines in Spark,
    which provides high-level APIs for scalable deep learning in Python with Apache
    Spark. Currently, TensorFlow and TensorFlow-backed Keras workflows are supported,
    with a focus on model inference/scoring and transfer learning on image data at
    scale.
  prefs: []
  type: TYPE_NORMAL
- en: To follow developments on deep learning pipelines in Spark, visit [https://github.com/databricks/spark-deep-learning](https://github.com/databricks/spark-deep-learning).
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, it provides tools for data scientists and machine learning experts
    to turn deep learning models into SQL UDFs that can be used by a much wider group
    of users. This is also a good way to produce a deep learning model.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will shift our focus to supervised learning.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Supervised learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The most common form of machine learning is supervised learning; for example,
    if we are building a system to classify a specific set of images, we first collect
    a large Dataset of images from the same categories. During training, the machine
    is shown an image, and it produces an output in the form of a vector of scores,
    one for each category. As a result of the training, we expect the desired category
    to have the highest score out of all the categories.
  prefs: []
  type: TYPE_NORMAL
- en: A particular type of deep network--the **convolutional neural network** (**ConvNet**/**CNN**)--is
    much easier to train and generalizes much better than fully-connected networks.
    In supervised learning scenarios, deep convolutional networks have significantly
    improved the results of processing images, video, speech, and audio data. Similarly,
    recurrent nets have shone the light on sequential data, such as text and speech.
    We will explore these types of neural networks in the subsequent sections.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding convolutional neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Convolutional neural networks are a special kind of multilayered neural networks
    that are designed to recognize visual patterns directly from pixel images with
    minimal preprocessing. They can recognize patterns having wide variability and
    can effectively deal with distortions and simple geometric transformations. CNNs
    are also trained using a version of the backpropagation algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: The architecture of a typical ConvNet is structured as a series of stages containing
    several stages of stacked convolution, and non-linearity and pooling layers, followed
    by additional convolutional and fully-connected layers. The non-linearity function
    is typically the **Rectified Linear Unit** (**ReLU**) function, and the role of
    the pooling layer is to semantically merge similar features into one. Thus, the
    pooling allows representations to vary very little when elements in the previous
    layer vary in position and appearance.
  prefs: []
  type: TYPE_NORMAL
- en: LeNet-5 is a convolutional network designed for handwritten and machine-printed
    character recognition. Here, we present an example of Lenet-5 available in the
    BigDL distribution.
  prefs: []
  type: TYPE_NORMAL
- en: The full source code for the example is available at [https://github.com/intel-analytics/BigDL/tree/master/spark/dl/src/main/scala/com/intel/analytics/bigdl/models/lenet](https://github.com/intel-analytics/BigDL/tree/master/spark/dl/src/main/scala/com/intel/analytics/bigdl/models/lenet).
  prefs: []
  type: TYPE_NORMAL
- en: Here, we will use Spark shell to execute the same code. Note that the values
    of the constants have all been taken from the source code available at the aforementioned
    site.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, execute the `bigdl` shell script to set the environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We then start the Spark shell with the appropriate BigDL JAR specified, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The Dataset for this example can be downloaded from [http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/).
  prefs: []
  type: TYPE_NORMAL
- en: 'The Spark shell session for this example is, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: In the next section, we will present an example of text classification.
  prefs: []
  type: TYPE_NORMAL
- en: Using neural networks for text classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Other applications gaining importance involve natural language understanding
    and speech recognition.
  prefs: []
  type: TYPE_NORMAL
- en: The example in this section is available as a part of the BigDL distribution
    and the full source code is available at [https://github.com/intel-analytics/BigDL/tree/master/spark/dl/src/main/scala/com/intel/analytics/bigdl/example/textclassification](https://github.com/intel-analytics/BigDL/tree/master/spark/dl/src/main/scala/com/intel/analytics/bigdl/example/textclassification).
  prefs: []
  type: TYPE_NORMAL
- en: It uses a pretrained GloVe embedding to convert words to vectors, and then uses
    it to train the text classification model on a twenty Newsgroup Dataset with twenty
    different categories. This model can achieve over 90% accuracy after only two
    epochs of training.
  prefs: []
  type: TYPE_NORMAL
- en: 'The key portions the code defining the CNN model and optimizer are presented
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The input Datasets are described, as follows, along with their download URLs:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Embedding**: 100-dimensional pretrained GloVe embeddings of 400 k words trained
    on a 2014 dump of English Wikipedia. Download pretrained GloVe word embeddings
    from [http://nlp.stanford.edu/data/glove.6B.zip](http://nlp.stanford.edu/data/glove.6B.zip).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Training data**: "20 Newsgroup dataset" containing 20 categories and with
    a total of 19,997 texts. Download 20 Newsgroup datasets as the training data from
    [http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.tar.gz](http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.tar.gz).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In our example, we have reduced the number of categories to eight to avoid
    `Out-of-Memory` exceptions on laptops with less than 16 GB RAM. Put these Datasets
    in `BASE_DIR`; the final directory structure should be as illustrated:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00288.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Use the following command to execute the text classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The sample output is given here for your reference:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: In the next section, we will explore the use of deep neural networks for language
    processing.
  prefs: []
  type: TYPE_NORMAL
- en: Using deep neural networks for language processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As discussed in [Chapter 9](part0166.html#4U9TC0-e9cbc07f866e437b8aa14e841622275c),
    *Developing Applications with Spark SQL*, the standard approach to statistical
    modeling of language is typically based on counting the frequency of the occurrences
    of n-grams. This usually requires very large training corpora in most real-world
    use cases. Additionally, n-grams treat each word as an independent unit, so they
    cannot generalize across semantically related sequences of words. In contrast,
    neural language models associate each word with a vector of real-value features
    and therefore semantically-related words end up close to each other in that vector
    space. Learning word vectors also works very well when the word sequences come
    from a large corpus of real text. These word vectors are composed of learned features
    that are automatically discovered by the neural network.
  prefs: []
  type: TYPE_NORMAL
- en: Vector representations of words learned from text are now very widely used in
    natural-language applications. In the next section, we will explore Recurrent
    Neural Networks and their application to a text classification task.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Recurrent Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Generally, for tasks involving sequential inputs, it is recommended to use **Recurrent
    Neural Networks** (**RNNs**). Such input is processed one element at a time, while
    maintaining a "state vector" (in hidden units). The state implicitly contains
    information about all the past elements in the sequence.
  prefs: []
  type: TYPE_NORMAL
- en: Typically, in conventional RNNs, it is difficult to store information for a
    long time. In order to remember the input for a long time, the network can be
    augmented with explicit memory. Also, this is the approach used in the **Long
    Short-Term Memory** (**LSTM**) networks; they use hidden units that can remember
    the input. LSTM networks have proved to be more effective than conventional RNNs.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will explore the Recurrent Neural Networks for modeling
    sequential data. The following figure illustrates a simple Recurrent Neural Network
    or an Elman network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00289.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: This is probably the simplest possible version of a Recurrent Neural Network
    that is easy to implement and train. The network has an input layer, a hidden
    layer (also called the context layer or state), and an output layer. The input
    to the network in time `t` is **Input(t)**, output is denoted as **Output(t)**,
    and **Context(t)** is state of the network (hidden layer). Input vector is formed
    by concatenating the vector representing the current word, and output from neurons
    in the context layer at time `t − 1`.
  prefs: []
  type: TYPE_NORMAL
- en: These networks are trained in several epochs, in which all the data from the
    training corpus is sequentially presented. To train the network, we can use the
    standard backpropagation algorithm with stochastic gradient descent. After each
    epoch, the network is tested on validation data. If the log-likelihood of validation
    data increases, the training continues in the new epoch. If no significant improvement
    is observed, the learning rate can be halved at the start of each new epoch. If
    there is no significant improvement as a result of changing the learning rate,
    the training is finished. Convergence of such networks is usually achieved after
    10-20 epochs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, the output layer represents a probability distribution of the next word
    when given the previous word and **Context(t − 1)**. Softmax ensures that the
    probability distribution is valid. At each training step, the error vector is
    computed, and the weights are updated with the standard backpropagation algorithm,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*error(t) = desired(t) − Output(t)*'
  prefs: []
  type: TYPE_NORMAL
- en: Here, desired is a vector using `1-of-N` coding, representing the word that
    should have been predicted in the particular context, and **Output(t)** is the
    actual output from the network.
  prefs: []
  type: TYPE_NORMAL
- en: To improve performance, we can merge all the words that occur less often than
    a given threshold value (in the training text) into a special rare token. Hence,
    all the rare words are thus treated equally, that is, the probability is distributed
    uniformly between them.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we execute a simple RNN example provided in the BigDL library. The network
    is a fully-connected RNN where the output is fed back into the input. The example
    model supports sequence-to-sequence processing and is an implementation of a simple
    Recurrent Neural Network for language modeling.
  prefs: []
  type: TYPE_NORMAL
- en: For the full source code of this example, refer to [https://github.com/intel-analytics/BigDL/tree/master/spark/dl/src/main/scala/com/intel/analytics/bigdl/models/rnn](https://github.com/intel-analytics/BigDL/tree/master/spark/dl/src/main/scala/com/intel/analytics/bigdl/models/rnn).
  prefs: []
  type: TYPE_NORMAL
- en: The input Dataset, Tiny Shakespeare Texts, can be downloaded from [https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt](https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt).
  prefs: []
  type: TYPE_NORMAL
- en: After downloading the text, place it into an appropriate directory. We split
    the input Dataset into separate `train.txt` and `val.txt` files. In our example,
    we select 80% of the input to be the training Dataset, and the remaining 20 percent
    to be the validation Dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Split the input Dataset by executing these commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The `SentenceSplitter` and `SentenceTokenizer` classes use the `Apache OpenNLP`
    library. The trained model files--`en-token.bin` and `en-sent.bin`--can be downloaded
    from [http://opennlp.sourceforge.net/models-1.5/](http://opennlp.sourceforge.net/models-1.5/).
  prefs: []
  type: TYPE_NORMAL
- en: 'The key parts of the code related to the model and the optimizer are listed
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The following command executes the training program. Modify the parameters
    specific to your environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The next extract is from the output generated during the training process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we use the saved model to run on the test Dataset, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Introducing autoencoders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An autoencoder neural network is an unsupervised learning algorithm that sets
    the target values to be equal to the input values. Hence, the autoencoder attempts
    to learn an approximation of an identity function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Learning an identity function does not seem to be a worthwhile exercise; however,
    by placing constraints on the network, such as limiting the number of hidden units,
    we can discover interesting structures about the data. The key components of an
    autoencoder are depicted in this figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00290.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The original input, the compressed representation, and the output layers for
    an autoencoder are also illustrated in the following figure. More specifically,
    this figure represents a situation where, for example, an input image has pixel-intensity
    values from a 10×10 image (100 pixels), and there are `50` hidden units in layer
    two. Here, the network is forced to learn a "compressed" representation of the
    input, in which it must attempt to "reconstruct" the 100-pixel input using `50`
    hidden units:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00291.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: For more details on autoencoders, refer to *Reducing the Dimensionality of Data
    with Neural Networks* by G. E. Hinton and R. R. Salakhutdinov, available at [https://www.cs.toronto.edu/~hinton/science.pdf](https://www.cs.toronto.edu/~hinton/science.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: Now, we present an example of an autoencoder from the BigDL distribution against
    the MNIST Dataset.
  prefs: []
  type: TYPE_NORMAL
- en: To train the autoencoder, you will need to download the MNIST Dataset from [http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/).
  prefs: []
  type: TYPE_NORMAL
- en: 'You will need to download the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, you have to unzip them to get the following files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'For our implementation, ReLU is used as the activation function and the mean
    square error is used as the loss function. Key parts of the model and the optimizer
    code used in this example are listed here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the command used to execute the autoencoder example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The output generated by the example is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we introduced deep learning in Spark. We discussed various
    types of deep neural networks and their application. We also explored a few code
    examples provided in the BigDL distribution. As this is a rapidly evolving area
    in Spark, presently, we expect these libraries to provide a lot more functionalities
    using Spark SQL and the DataFrame/Dataset APIs. Additionally, we also expect them
    to mature and become more stable over the coming months.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will shift our focus to tuning Spark SQL applications.
    We will cover key foundational aspects regarding serialization/deserialization
    using encoders and the logical and physical plans associated with query executions,
    and then present the details of the **cost-based optimization** (**CBO**) feature
    released in Spark 2.2\. Additionally, we will present some tips and tricks that
    developers can use to improve the performance of their applications.
  prefs: []
  type: TYPE_NORMAL
