- en: '*Chapter 15*: Performance Tuning and Optimization for KVM VMs'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第15章*：KVM VM性能调优和优化'
- en: When we're thinking about virtualization, there are always questions that keep
    coming up. Some of them might be simple enough, such as what are we going to get
    out of virtualization? Does it simplify things? Is it easier to back up? But there
    are also much more complex questions that start coming up once we've used virtualization
    for a while. How do we speed things up on a compute level? Is there a way to do
    more optimization? What can we tune additionally to get some more speed out of
    our storage or network? Can we introduce some configuration changes that will
    enable us to get more out of the existing infrastructure without investing a serious
    amount of money in it?
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们思考虚拟化时，总会有一些问题不断出现。其中一些可能很简单，比如我们从虚拟化中能得到什么？它是否简化了事情？备份是否更容易？但是一旦我们使用虚拟化一段时间后，也会出现更复杂的问题。我们如何在计算层面加速？有没有更多的优化方法？我们可以调整什么来从存储或网络中获得更快的速度？我们可以引入一些配置更改，使我们能够在不在其中投入大量资金的情况下从现有基础设施中获得更多？
- en: That's why performance tuning and optimization is so important to our virtualized
    environments. As we will find out in this chapter, there are loads of different
    parameters to consider – especially if we didn't design things properly from the
    very start, which is usually the case. So, we're going to cover the subject of
    design first, explain why it shouldn't be just a pure trial-and-error process,
    and then move on to disassembling that thought process through different devices
    and subsystems.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么性能调优和优化对我们的虚拟化环境如此重要。正如我们将在本章中发现的那样，有许多不同的参数需要考虑-特别是如果我们从一开始就没有正确设计事物，这通常是情况。因此，我们将首先涵盖设计的主题，解释为什么它不应该只是一个纯粹的试错过程，然后继续通过不同的设备和子系统来解构这种思维过程。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Tuning VM CPU and memory performance – NUMA
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整VM CPU和内存性能- NUMA
- en: Kernel same-page merging
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内核同页合并
- en: Virtio device tuning
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Virtio设备调优
- en: Block I/O tuning
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 块I/O调优
- en: Network I/O tuning
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络I/O调优
- en: It's all about design
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一切都关乎设计
- en: There are some fundamental patterns that we constantly repeat in many other
    aspects of our lives. We usually do so in IT, too. It's completely normal for
    us not to be good at something when we just start doing it. For example, when
    we start training in any kind of sport, we're usually not as good as we become
    after a couple of years of sticking with it. When we start musical training, we're
    usually much better at it after a couple of years of attending musical school.
    The same principle applies to IT – when we start doing IT, we're nowhere near
    as good at it as we become with time and – primarily – *experience*.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们生活的许多其他方面，我们不断重复一些基本模式。在IT中，我们通常也会这样做。当我们刚开始做某件事时，通常我们并不擅长。例如，当我们开始进行任何一种运动训练时，通常不如我们坚持几年后的水平。当我们开始音乐训练时，通常在参加音乐学校几年后我们会更擅长。同样的原则也适用于IT-当我们开始从事IT时，我们远不如随着时间和主要是经验的积累变得更加擅长。
- en: We as humans are really good at putting *intellectual defenses* in the way of
    our learning. We're really good at saying *I'm going to learn through my mistakes*
    – and we usually combine that with *leave me alone*.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们人类在学习过程中很擅长在智力防御方面设置障碍。我们很擅长说“我会通过我的错误学习”-而且我们通常会将其与“别打扰我”结合起来。
- en: The thing is – there's so much knowledge out there already, it would be silly
    not to use it. So many people already went through the same or similar process
    as we did; it would be a pointless exercise in futility *not* to use that experience
    to our advantage. Furthermore, why waste time on this whole *I'm going to learn
    through my mistakes* thing when we can learn much more from people with much more
    experience than us?
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 事实是-已经有这么多的知识存在，不去利用它就太愚蠢了。已经有这么多人经历了与我们相同或类似的过程；不利用这种经验来谋取我们的利益将是毫无意义的。此外，为什么要浪费时间在这个“我会通过我的错误学习”的事情上，当我们可以从比我们经验更丰富的人那里学到更多呢？
- en: When we start using virtualization, we usually start small. For example, we
    start by installing a hosted-virtualization solution, such as VMware Player, Oracle
    VirtualBox, or something like that. Then, as time goes by, we move on to a hypervisor
    with a couple of **Virtual Machines** (**VMs**). As the infrastructure around
    us grows, we start following linear patterns in trying to make infrastructure
    work *as it used to, when it was smaller*, which is a mistake. Nothing in IT is
    linear – growth, cost, the time spent on administration…absolutely nothing. It's
    actually rather simple to deconstruct that – as environments grow, there are more
    co-dependencies, which means that one thing influences another, which influences
    another, and so on. This endless matrix of influences is something that people
    often forget, especially in the design phase.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们开始使用虚拟化时，通常会从小处开始。例如，我们开始安装托管虚拟化解决方案，如VMware Player，Oracle VirtualBox或类似的解决方案。随着时间的推移，我们会转向具有一对虚拟机（VMs）的hypervisor。随着我们周围的基础设施增长，我们开始遵循线性模式，试图使基础设施的工作方式与以前小型时相同，这是一个错误。IT中没有任何线性的东西-增长、成本、管理所花费的时间...绝对没有。实际上，解构这一点非常简单-随着环境的增长，存在更多的相互依赖关系，这意味着一件事会影响另一件事，进而影响另一件事，依此类推。这种无尽的影响矩阵是人们经常忘记的东西，特别是在设计阶段。
- en: 'Important note:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示：
- en: 'It''s really simple: linear design will get you nowhere, and proper design
    is the basis of performance tuning, which leaves much less work to be done on
    performance tuning afterward.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这很简单：线性设计将使你一事无成，而正确的设计是性能调优的基础，这样在性能调优方面就要做的工作就少得多了。
- en: Earlier on in this book (in [*Chapter 2*](B14834_02_Final_ASB_ePub.xhtml#_idTextAnchor029),
    *KVM as a Virtualization Solution*), we mentioned **Non-Uniform Memory Access**
    (**NUMA**). Specifically, we mentioned that the NUMA configuration options are
    a *very important part of VM configuration, especially if you're designing an
    environment that hosts loads of virtualized servers.* Let's use a couple of examples
    to elaborate on this point further. These examples will give us a good basis to
    take a *mile-high view* of the biggest problem in performance tuning and optimization
    and describe how to use good design principles to get us out of many different
    types of trouble. We're going to use Microsoft-based solutions as examples on
    purpose – not because we're religious about using them, but because of a simple
    fact. We have a lot of widely available documentation that we can use to our advantage
    – design documents, best practices, shorter articles, and so on. So, let's use
    them.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的早期（在[第2章](B14834_02_Final_ASB_ePub.xhtml#_idTextAnchor029)中，*KVM作为虚拟化解决方案*），我们提到了**非一致性内存访问**（**NUMA**）。具体来说，我们提到了NUMA配置选项是VM配置的一个非常重要的部分，特别是如果你正在设计一个承载大量虚拟化服务器的环境。让我们用一些例子来进一步阐述这一点。这些例子将为我们提供一个很好的基础，以从一个“里程高度”的视角来看待性能调优和优化中最大的问题，并描述如何使用良好的设计原则来摆脱许多不同类型的麻烦。我们故意使用微软的解决方案作为例子
    - 不是因为我们对使用它们有宗教信仰，而是因为一个简单的事实。我们有很多广泛可用的文档，我们可以利用它们 - 设计文档，最佳实践，简短的文章等。所以，让我们使用它们。
- en: General hardware design
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通用硬件设计
- en: Let's say that you've just started to design your new virtualized environment.
    When you order servers today from your channel partners – whichever they are –
    you need to select a model from a big list. It doesn't really matter which brand
    – there are a lot of models on offer. You can go with `1U` (so-called *pizza box*)
    servers, which mostly have either one or two CPUs, depending on the model. Then,
    you can select a `2U` server, a `3U` server…the list gets exponentially bigger.
    Let's say that you selected a `2U` server with one CPU.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你刚开始设计你的新虚拟化环境。当你今天从你的渠道合作伙伴那里订购服务器时，无论他们是谁，你需要从一个很长的列表中选择一个型号。品牌并不重要 - 有很多型号供选择。你可以选择1U（所谓的“披萨盒”）服务器，大多数情况下有一个或两个CPU，具体取决于型号。然后，你可以选择2U服务器，3U服务器……列表呈指数级增长。假设你选择了一个带有一个CPU的2U服务器。
- en: In the next step, you select the amount of memory – let's say 96 GB or 128 GB.
    You place your order, and a couple of days or weeks later, your server gets delivered.
    You open it up, and you realize something – all of the RAM is connected to `CPU1`
    memory channels. You put that in your memory bank, forget about it, and move on
    to the next phase.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一步中，你选择内存的数量 - 比如96GB或128GB。你下订单，几天或几周后，你的服务器就送到了。你打开它，然后意识到一些事情 - 所有的RAM都连接到CPU1的内存通道。你把它放在你的内存库里，忘记它，然后继续下一个阶段。
- en: Then, the question becomes about the micro-management of some very pedestrian
    settings. The BIOS version of the server, the drivers on the hypervisor level,
    and the BIOS settings (power management, C-states, Turbo Boost, hyperthreading,
    various memory-related settings, not allowing cores to turn themselves off, and
    so on) can have a vast influence on the performance of our VMs running on a hypervisor.
    Therefore, it's definitely best practice to first check whether there are any
    newer BIOS/firmware versions for our hardware, and check the manufacturer and
    other relevant documentation to make sure that the BIOS settings are as optimized
    as possible. Then, and only then, we can start *checkboxing* some physical and
    deployment procedures – deploying our server in a rack, installing an OS and everything
    that we need, and start using it.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，问题变成了一些非常普通设置的微观管理。服务器的BIOS版本，hypervisor级别的驱动程序和BIOS设置（电源管理，C状态，Turbo Boost，超线程，各种与内存相关的设置，不允许核心关闭自己等）对我们在hypervisor上运行的VM的性能有着巨大的影响。因此，最佳实践肯定是首先检查我们的硬件是否有任何更新的BIOS/固件版本，并检查制造商和其他相关文档，以确保BIOS设置尽可能优化。然后，只有在这之后，我们才能开始勾选一些物理和部署程序
    - 在机架中部署我们的服务器，安装操作系统和我们需要的一切，然后开始使用它。
- en: Let's say that after a while, you realize that you need to do some upgrades
    and order some PCI Express cards – two single-port Fibre Channel 8 Gbit/s host-based
    adapters, two single-port 10 Gbit/s Ethernet cards, and two PCI Express NVMe SSDs.
    For example, by ordering these cards, you want to add some capabilities – to access
    Fibre Channel storage and to speed up your backup process and VM migrations by
    switching both of these functionalities from 1 Gbit/s to 10 Gbit/s networking.
    You place your order, and a couple of days or weeks later, your new PCI Express
    cards are delivered. You open them up, shut down your server, take it out of the
    rack, and install these cards. `2U` servers usually have space for two or even
    three PCI Express riser cards, which are effectively used for connecting additional
    PCI Express devices. Let's say that you use the first PCI Express riser to deploy
    the first two cards – the Fibre Channel controllers and 10 Gbit/s Ethernet cards.
    Then, noticing that you don't have enough PCI Express connectors to connect everything
    to the first PCI Express riser, you use the second PCI Express riser to install
    your two PCI Express NVMe SSDs. You screw everything down, close the server cover,
    put the server back in your rack, and power it back on. Then, you go back to your
    laptop and connect to your server in a vain attempt to format your PCI Express
    NVMe SSDs and use them for new VM storage. You realize that your server doesn't
    recognize these SSDs. You ask yourself – what's going on here? Do I have a bad
    server?
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.1 – A PCI Express riser for DL380p G8 – you have to insert your'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: PCI Express cards into its slots
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_15_01.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: Figure 15.1 – A PCI Express riser for DL380p G8 – you have to insert your PCI
    Express cards into its slots
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: 'You call up your sales rep, and tell them that you think the server is malfunctioning
    as it can''t recognize these new SSDs. Your sales rep connects you to the pre-sales
    tech; you hear a small chuckle from the other side and the following information:
    "Well, you see, you can''t do it that way. If you want to use the second PCI Express
    riser on your server, you have to have a CPU kit (CPU plus heatsink) in your second
    CPU socket, and memory for that second CPU, as well. Order these two things, put
    them in your server, and your PCI Express NVMe SSDs will work without any problems."'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: You end your phone conversation and are left with a question mark over your
    head – *what is going on here? Why do I need to have a second CPU and memory connected
    to its memory controllers to use some PCI Express cards?*
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: 'This is actually related to two things:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: You can't use the memory slots of an uninstalled CPU, as that memory needs a
    memory controller, which is inside the CPU.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can't use PCI Express on an uninstalled CPU, as the PCI Express lanes that
    connect PCI Express risers' cards to the CPU aren't necessarily provided by the
    chipset – the CPU can also be used for PCI Express lanes, and it often is, especially
    for the fastest connections, as you'll learn in a minute.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We know this is confusing; we can feel your pain as we've been there. Sadly,
    you'll have to stay with us for a little bit longer, as it gets even more confusing.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: In [*Chapter 4*](B14834_04_Final_ASB_ePub.xhtml#_idTextAnchor062), *Libvirt
    Networking*, we learned how to configure SR-IOV by using an Intel X540-AT2 network
    controller. We mentioned that we were using the HP ProLiant DL380p G8 server when
    configuring SR-IOV, so let's use that server for our example here, as well. If
    you take a look at specifications for that server, you'll notice that it uses
    an *Intel C600* chipset. If you then go to Intel's ARK website ([https://ark.intel.com](https://ark.intel.com))
    and search for information about C600, you'll notice that it has five different
    versions (C602, C602J, C604, C606, and C608), but the most curious part of it
    is the fact that all of them only support eight PCI Express 2.0 lanes. Keeping
    in mind that the server specifications clearly state that this server supports
    PCI Express 3.0, it gets really confusing. How can that be and what kind of trickery
    is being used here? Yes, PCI Express 3.0 cards can almost always work at PCI Express
    2.0 speeds, but it would be misguiding at best to flat-out say that *this server
    supports PCI Express 3.0*, and then discover that it supports it by delivering
    PCI Express 2.0 levels of performance (twice as slow per PCI Express lane).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第4章*](B14834_04_Final_ASB_ePub.xhtml#_idTextAnchor062)中，*Libvirt网络*，我们学习了如何通过使用Intel
    X540-AT2网络控制器来配置SR-IOV。我们提到在配置SR-IOV时我们使用了HP ProLiant DL380p G8服务器，所以让我们在这里也使用该服务器作为我们的示例。如果您查看该服务器的规格，您会注意到它使用了*Intel
    C600*芯片组。然后，如果您前往Intel的ARK网站（[https://ark.intel.com](https://ark.intel.com)）并搜索有关C600的信息，您会注意到它有五个不同的版本（C602、C602J、C604、C606和C608），但其中最耐人寻味的部分是所有这些版本都只支持8条PCI
    Express 2.0通道。考虑到服务器规格清楚地说明该服务器支持PCI Express 3.0，这变得非常令人困惑。这是怎么回事，这里使用了什么样的诡计？是的，PCI
    Express 3.0卡几乎总是可以以PCI Express 2.0的速度工作，但最好不要直接说*这台服务器支持PCI Express 3.0*，然后发现它通过提供PCI
    Express 2.0级别的性能（每个PCI Express通道的速度减慢一倍）来支持它。
- en: It's only when you go to the HP ProLiant DL380p G8 QuickSpecs document and find
    the specific part of that document (the *Expansions Slots* part, with descriptions
    of three different types of PCI Express risers that you can use) where all the
    information that we need is actually spelled out for us. Let's use all of the
    PCI Express riser details for reference and explanation. Basically, the primary
    riser has two PCI Express v3.0 slots that are provided by processor 1 (x16 plus
    x8), and the third slot (PCI Express 2.0 x8) is provided by the chipset. For the
    optional riser, it says that all of the slots are provided by the CPU (x16 plus
    x8 times two). There are actually some models that can have three PCI Express
    risers, and for that third riser, all of the PCI Express lanes (x16 times two)
    are also provided by processor 2.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 只有当您查看HP ProLiant DL380p G8 QuickSpecs文档并找到该文档的特定部分（*扩展槽*部分，其中描述了三种不同类型的PCI
    Express扩展槽）时，我们才能找到我们实际需要的所有信息。让我们使用所有PCI Express扩展槽的详细信息作为参考和解释。基本上，主要扩展槽由处理器1提供两个PCI
    Express v3.0槽（x16加x8），第三个槽（PCI Express 2.0 x8）由芯片组提供。对于可选扩展槽，它表示所有槽都由CPU提供（x16加x8乘以2）。实际上，有一些型号可以有三个PCI
    Express扩展槽，对于第三个扩展槽，所有PCI Express通道（x16乘以2）也由处理器2提供。
- en: This is all *very important*. It's a huge factor in performance bottlenecks
    for many scenarios, which is why we centered our example around the idea of two
    PCI Express NVMe SSDs. We wanted to go through the whole journey with you.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这一切都*非常重要*。对于许多情景来说，这是性能瓶颈的一个巨大因素，这也是为什么我们的示例围绕着两个PCI Express NVMe固态硬盘的想法展开的。我们希望与您一起完成整个旅程。
- en: So, at this point, we can have an educated discussion about what should be the
    de facto standard hardware design of our example server. If our intention is to
    use these PCI Express NVMe SSDs for local storage for our VMs, then most of us
    would treat that as a priority. That would mean that we'd absolutely want to connect
    these devices to the PCI Express 3.0 slot so that they aren't bottlenecked by
    PCI Express 2.0 speeds. If we have two CPUs, we're probably better off using the
    *first PCI Express slot* in both of our PCI Express risers for that specific purpose.
    The reasoning is simple – they're *PCI Express 3.0 compatible* and they're *provided
    by the CPU*. Again, that's *very important* – it means that they're *directly
    connected* to the CPU, without the *added latency* of going through the chipset.
    Because, at the end of the day, the CPU is the central hub for everything, and
    data going from VMs to SSDs and back will go through the CPU. From a design standpoint,
    we should absolutely use the fact that we know this to our advantage and connect
    our PCI Express NVMe SSDs *locally* to our CPUs.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，此时我们可以就我们示例服务器的实际标准硬件设计进行有根据的讨论。如果我们的意图是将这些PCI Express NVMe固态硬盘用作我们的VM的本地存储，那么大多数人会将这视为优先事项。这意味着我们绝对希望将这些设备连接到PCI
    Express 3.0槽，以避免受到PCI Express 2.0速度的限制。如果我们有两个CPU，我们可能最好在我们的PCI Express扩展槽的*第一个PCI
    Express槽*中使用这个特定目的。原因很简单-它们是*PCI Express 3.0兼容*，并且它们是*由CPU提供*。再次强调，这是*非常重要*的-这意味着它们是*直接连接*到CPU，而不需要通过芯片组的*额外延迟*。因为，归根结底，CPU是一切的中心枢纽，从VM到固态硬盘再返回的数据都将通过CPU。从设计的角度来看，我们应该绝对利用我们知道的这一点，并将我们的PCI
    Express NVMe固态硬盘*本地*连接到我们的CPU。
- en: The next step is related to Fibre Channel controllers and 10 Gbit/s Ethernet
    controllers. The vast load of 8 Gbit/s Fibre Channel controllers are PCI Express
    2.0 compatible. The same thing applies to 10 Gbit/s Ethernet adapters. So, it's
    again a matter of priority. If you're using Fibre Channel storage a lot from our
    example server, logic dictates that you'd want to put your new and shiny Fibre
    Channel controllers in the fastest possible place. That would be the second PCI
    Express slot in both of our PCI Express risers. Again, second PCI Express slots
    are both provided by CPUs – processor 1 and processor 2\. So now, we're just left
    with 10 Gbit/s Ethernet adapters. We said in our example scenario that we're going
    to be using these adapters for backup and VM migration. The backup won't suffer
    all that much if it's done via a network adapter that's on the chipset. VM migration
    might be a tad sensitive to that. So, you connect your first 10 Gbit/s Ethernet
    adapter to the third PCI Express slot on the primary riser (for backup, provided
    by the chipset). Then, you also connect your second 10 Gbit/s Ethernet adapter
    to the third PCI Express slot on the secondary riser (PCI Express lanes provided
    by processor 2).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步与光纤通道控制器和10 Gbit/s以太网控制器有关。大部分8 Gbit/s光纤通道控制器都兼容PCI Express 2.0。同样的情况也适用于10
    Gbit/s以太网适配器。因此，这又是一个优先考虑的问题。如果您从我们的示例服务器大量使用光纤通道存储，那么逻辑推断您会希望将新的光纤通道控制器放在尽可能快的位置。这将是我们的两个PCI
    Express扩展槽中的第二个PCI Express槽。同样，第二个PCI Express槽都由CPU提供-处理器1和处理器2。现在，我们只剩下10 Gbit/s以太网适配器。在我们的示例场景中，我们说我们将使用这些适配器进行备份和VM迁移。如果通过芯片组上的网络适配器进行备份，备份不会受到太大影响。VM迁移可能对此有些敏感。因此，您将第一个10
    Gbit/s以太网适配器连接到主扩展槽上的第三个PCI Express槽（用于备份，由芯片组提供）。然后，您还将第二个10 Gbit/s以太网适配器连接到次级扩展槽上的第三个PCI
    Express槽（由处理器2提供的PCI Express通道）。
- en: We've barely started on the subject of design with the hardware aspect of it,
    and already we have such a wealth of information to process. Let's now move on
    to the second phase of our design – which relates to VM design. Specifically,
    we're going to discuss how to create new VMs that are designed properly from scratch.
    However, if we're going to do that, we need to know which application this VM
    is going to be created for. For that matter, we're going to create a scenario.
    We're going to use a VM that we're creating to host a node in a Microsoft SQL
    database cluster on top of a VM running Windows Server 2019\. The VM will be installed
    on a KVM host, of course. This is a task given to us by a client. As we already
    did the general hardware design, we're going to focus on VM design now.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚开始讨论硬件方面的设计问题，现在我们已经有了如此丰富的信息要处理。现在让我们继续进行我们设计的第二阶段-与VM设计相关。具体来说，我们将讨论如何从头开始创建正确设计的新VM。但是，如果我们要这样做，我们需要知道这个VM将为哪个应用程序创建。为此，我们将创建一个场景。我们将使用正在创建的VM来托管运行Windows
    Server 2019的VM上的Microsoft SQL数据库集群中的一个节点。当然，该VM将安装在KVM主机上。这是客户交给我们的任务。由于我们已经完成了一般的硬件设计，现在我们将专注于VM设计。
- en: VM design
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VM设计
- en: Creating a VM is easy – we can just go to `virt-manager`, click a couple of
    times, and we're done. The same applies to oVirt, RedHat Enterprise Virtualization
    Manager, OpenStack, VMware, and Microsoft virtualization solutions… it's more
    or less the same everywhere. The problem is designing VMs properly. Specifically,
    the problem is creating a VM that's going to be pre-tuned to run an application
    on a very high level, which then only leaves a small number of configuration steps
    that we can take on the server or VM side to improve performance – the premise
    being that most of the optimization process later will be done on the OS or application
    level.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 创建VM很容易-我们只需转到`virt-manager`，点击几次，就可以了。oVirt、RedHat Enterprise Virtualization
    Manager、OpenStack、VMware和Microsoft虚拟化解决方案也是如此……几乎在任何地方都是一样的。问题在于正确设计VM。具体来说，问题在于创建一个将预先调整为在非常高的水平上运行应用程序的VM，然后只留下一小部分配置步骤，我们可以在服务器或VM端采取来提高性能-前提是后续的大部分优化过程将在操作系统或应用程序级别完成。
- en: So, people usually start creating a VM in one of two ways – either by creating
    a VM from scratch with *XYZ* amount of resources added to the VM, or by using
    a template, which – as we explained in [*Chapter 8*](B14834_08_Final_ASB_ePub.xhtml#_idTextAnchor143),
    *Creating and Modifying VM Disks, Templates, and Snapshots* – will save a lot
    of time. Whichever way we use, there's a certain amount of resources that will
    be configured for our VM. We then remember what we're going to use this VM for
    (SQL), so we increase the amount of CPUs to, for example, four, and the amount
    of memory to 16 GB. We put that VM in the local storage of our server, spool it
    up, and start deploying updates, configuring the network, and rebooting and generally
    preparing the VM for the final installation step, which is actually installing
    our application (SQL Server 2016) and some updates to go along with it. After
    we're done with that, we start creating our databases and move on to the next
    set of tasks that need to be done.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，人们通常以两种方式之一开始创建VM-要么从头开始创建VM并向VM添加*XYZ*数量的资源，要么使用模板，正如我们在[*第8章*](B14834_08_Final_ASB_ePub.xhtml#_idTextAnchor143)中解释的那样，*创建和修改VM磁盘、模板和快照*，这将节省大量时间。无论我们使用哪种方式，都会为我们的VM配置一定数量的资源。然后，我们记住我们将使用这个VM（SQL），所以我们将CPU的数量增加到，例如，四个，内存的数量增加到16
    GB。我们将该VM放在服务器的本地存储中，启动它，并开始部署更新，配置网络，重新启动，通常准备VM进行最终安装步骤，即实际安装我们的应用程序（SQL Server
    2016）和一些相关更新。完成后，我们开始创建我们的数据库，并继续进行需要完成的下一组任务。
- en: Let's take a look at this process from a design and tuning perspective next.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们从设计和调优的角度来看这个过程。
- en: Tuning the VM CPU and memory performance
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调整VM CPU和内存性能
- en: 'There are some pretty straightforward issues with the aforementioned process.
    Some are just engineering issues, while some are more procedural issues. Let''s
    discuss them for a second:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 上述过程中有一些非常直接的问题。有些只是工程问题，而有些则更多是程序问题。让我们讨论一下：
- en: There is no *one-size-fits-all* solution to almost anything in IT. Every VM
    of every single client has a different set of circumstances and is in a different
    environment that consists of different devices, servers, and so on. Don't try
    to speed up the process to *impress* someone, as it will most definitely become
    a problem later.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在IT领域几乎没有*一刀切*的解决方案。每个客户的每个虚拟机都有不同的情况，并处于不同的环境中，包括不同的设备、服务器等等。不要试图加快流程以*给人留下印象*，因为这肯定会在以后成为问题。
- en: When you're done with deployment, stop. Learn the practice of breathe in, breathe
    out, and stop for a second and think – or wait for an hour or even a day. Remember
    what you're designing a VM for.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当你完成部署后，停下来。学会深呼吸，停下来思考一秒钟，或者等一个小时甚至一天。记住你设计虚拟机的目的。
- en: Before allowing a VM to be used in production, check its configuration. The
    number of virtual CPUs, the memory, the storage placement, the network options,
    the drivers, software updates – everything.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在允许虚拟机投入生产使用之前，检查其配置。虚拟CPU的数量、内存、存储位置、网络选项、驱动程序、软件更新——一切都要检查。
- en: A lot of pre-configuration can be done before the installation phase or during
    the template phase, before you clone the VM. If it's an existing environment that
    you're migrating to a new one, *collect information about the old environment*.
    Find out what the database sizes are, what storage is being used, and how happy
    people are with the performance of their database server and the applications
    using them.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在安装阶段或模板阶段之前，可以进行大量的预配置。如果你正在将现有环境迁移到新环境，*收集有关旧环境的信息*。了解数据库的大小、使用的存储以及人们对其数据库服务器和使用它们的应用程序的性能满意程度。
- en: At the end of the whole process, learn to take a *mile-high perspective* on
    the IT-related work that you do. From a quality assurance standpoint, IT should
    be a highly structured, procedural type of work. If you've done something before,
    learn to document the things that you did while installing things and the changes
    that you made. Documentation – as it stands now – is one of the biggest Achilles'
    heels of IT. Writing documentation will make it easier for you to repeat the process
    in the future when faced with the same (less often) or a similar (much more often)
    scenario. Learn from the greats – just as an example, we would know much less
    about Beethoven, for example, if he didn't keep detailed notes of the things he
    did day in, day out. Yes, he was born in 1770 and this year will mark 250 years
    since he was born, and that was a long time ago, but that doesn't mean that 250-year-old
    routines are bad.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在整个过程结束时，学会对你所做的与IT相关的工作采取*高层视角*。从质量保证的角度来看，IT应该是一种高度结构化的、程序化的工作。如果你以前做过某事，学会记录你在安装过程中所做的事情和你所做的更改。文档——就目前而言——是IT的最大软肋之一。撰写文档将使你在未来面对相同（较少）或类似（更多）的情况时更容易重复这个过程。向伟人学习——举个例子，如果贝多芬没有详细记录他日复一日所做的事情，我们对他的了解就会少得多。是的，他生于1770年，今年将是他诞辰250周年，那是很久以前的事了，但这并不意味着250年前的惯例是不好的。
- en: So now, your VM is configured and in production, and a couple of days or weeks
    later, you get a call from the company and they ask why the performance is *not
    all that great*. Why isn't it working just like on a physical server?
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你的虚拟机已经配置并投入生产使用，几天或几周后，你会接到公司的电话，他们会问为什么性能*并不那么好*。为什么它的工作效果不像物理服务器一样？
- en: 'As a rule of thumb, when you''re looking for performance issues on Microsoft
    SQL, they can be roughly divided into four categories:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个经验法则，当你在寻找Microsoft SQL的性能问题时，它们大致可以分为四类：
- en: Your SQL database is memory-limited.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你的SQL数据库受内存限制。
- en: Your SQL database is storage-limited.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你的SQL数据库受存储限制。
- en: Your SQL database is just misconfigured.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你的SQL数据库配置错误。
- en: Your SQL database is CPU-limited.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你的SQL数据库受CPU限制。
- en: In our experience, the first and second category can easily account for 80–85%
    of SQL performance issues. The third would probably account for 10%, while the
    last one is rather rare, but it still happens. Keeping that in mind, from an infrastructure
    standpoint, when you're designing a database VM, you should always look into VM
    memory and storage configuration first, as they are by far the most common reasons.
    The problems just kind of accumulate and snowball from there. Specifically, some
    of the most common key reasons for sub-par SQL VM performance is the memory location,
    looking at it from a CPU perspective, and storage issues – latencies/IOPS and
    bandwidth being the problem. So, let's describe these one by one.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们的经验，第一和第二类问题很容易占据SQL性能问题的80-85%。第三类可能占10%，而最后一类相对较少，但仍然会发生。记住，从基础设施的角度来看，当你设计数据库虚拟机时，你应该首先查看虚拟机内存和存储配置，因为它们是最常见的原因。问题会逐渐积累并不断恶化。具体来说，导致SQL虚拟机性能不佳的一些最常见的关键原因是内存位置、从CPU角度看待它，以及存储问题——延迟/IOPS和带宽成为问题。所以，让我们逐一描述这些问题。
- en: The first issue that we need to tackle is related to – funnily enough – *geography*.
    It's very important for a database to have its memory content as close as possible
    to the CPU cores assigned to its VMs. This is what NUMA is all about. We can easily
    overcome this specific issue on KVM with a bit of configuration. Let's say that
    we chose that our VM uses four virtual CPUs. Our test server has Intel Xeon E5-2660v2
    processors, which have 10 physical cores each. Keeping in mind that our server
    has two of these Xeon processors, we have 20 cores at our disposal overall.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要解决的第一个问题与——有趣的是——*地理*有关。对于数据库来说，将其内存内容尽可能靠近分配给其虚拟机的CPU核心非常重要。这就是NUMA的意义所在。我们可以通过一些配置轻松地解决这个特定问题。假设我们选择我们的虚拟机使用四个虚拟CPU。我们的测试服务器配备了英特尔至强E5-2660v2处理器，每个处理器都有10个物理核心。考虑到我们的服务器有两个这样的至强处理器，我们总共有20个核心可供使用。
- en: 'We have two basic questions to answer:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有两个基本问题要回答：
- en: How do these four cores for our VM correlate to 20 physical cores below?
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的虚拟机的四个核心如何与下面的20个物理核心相关？
- en: How does that relate to the VM's memory and how can we optimize that?
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The answer to both of these questions is that it depends on *our* configuration.
    By default, our VM might use two cores from two physical processors each and spread
    itself in terms of memory across both of them or 3+1\. None of these configuration
    examples are good. What you want is to have all the virtual CPU cores on *one*
    physical processor, and you want those virtual CPU cores to use memory that's
    local to those four physical cores – directly connected to the underlying physical
    processor's memory controller. What we just described is the basic idea behind
    NUMA – to have nodes (consisting of CPU cores) that act as building compute blocks
    for your VMs with local memory.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: If at all possible, you want to reserve all the memory for that VM so that it
    doesn't swap somewhere outside of the VM. In KVM, that *outside of the VM* would
    be in the KVM host swap space. Having access to real RAM memory all of the time
    is a performance and SLA-related configuration option. If the VM uses a bit of
    underlying swap partition that acts as its memory, it will not have the same performance.
    Remember, swapping is usually done on some sort of local RAID array, an SD card,
    or a similar medium, which are many orders of magnitude slower in terms of bandwidth
    and latency compared to real RAM memory. If you want a high-level statement about
    this – avoid memory overcommitment on KVM hosts at all costs. The same goes for
    the CPU, and this is a commonly used best practice on any other kind of virtualization
    solution, not just on KVM.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, for critical resources, such as a database VM, it definitely makes
    sense to *pin* vCPUs to specific physical cores. That means that we can use specific
    physical cores to run a VM, and we should configure other VMs running on the same
    host *not* to use those cores. That way, we're *reserving* these CPU cores specifically
    for a single VM, thus configuring everything for maximum performance not to be
    influenced by other VMs running on the physical server.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: Yes, sometimes managers and company owners won't like you because of this best
    practice (as if you're to blame), as it requires proper planning and enough resources.
    But that's something that they have to live with – or not, whichever they prefer.
    Our job is to make the IT system run as best as it possibly can.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: VM design has its basic principles, such as the CPU and memory design, NUMA
    configuration, configuring devices, storage and network configuration, and so
    on. Let's go through all of these topics step by step, starting with an advanced
    CPU-based feature that can really help make our systems run as best as possible
    if used properly – CPU pinning.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: CPU pinning
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: CPU pinning is nothing but the process of setting the *affinity* between the
    vCPU and the physical CPU core of the host so that the vCPU will be executing
    on that physical CPU core only. We can use the `virsh vcpupin` command to bind
    a vCPU to a physical CPU core or to a subset of physical CPU cores.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a couple of best practices when doing vCPU pinning:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: If the number of guest vCPUs is more than the single NUMA node CPUs, don't go
    for the default pinning option.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the physical CPUs are spread across different NUMA nodes, it is always better
    to create multiple guests and pin the vCPUs of each guest to physical CPUs in
    the same NUMA node. This is because accessing different NUMA nodes, or running
    across multiple NUMA nodes, has a negative impact on performance, especially for
    memory-intensive applications.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s look at the steps of vCPU pinning:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: Execute `virsh nodeinfo` to gather details about the host CPU configuration:![Figure
    15.2 – Information about our KVM node
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B14834_15_02.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: Figure 15.2 – Information about our KVM node
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: The next step is to get the CPU topology by executing the `virsh capabilities`
    command and check the section tagged `<topology>`:![Figure 15.3 – The virsh capabilities
    output with all the visible physical CPU cores
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是通过执行`virsh capabilities`命令并检查标记为`<topology>`的部分来获取CPU拓扑结构：![图15.3 – 具有所有可见物理CPU核心的virsh
    capabilities输出
- en: '](img/B14834_15_03.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_15_03.jpg)'
- en: Figure 15.3 – The virsh capabilities output with all the visible physical CPU
    cores
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.3 – 具有所有可见物理CPU核心的virsh capabilities输出
- en: Once we have identified the topology of our host, the next step is to start
    pinning the vCPUs.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们确定了主机的拓扑结构，下一步就是开始固定vCPU。
- en: Let's first check the current affinity or pinning configuration with the guest
    named `SQLForNuma`, which has four vCPUs:![Figure 15.4 – Checking the default
    vcpupin settings
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们首先检查名为`SQLForNuma`的客户端的当前亲和力或固定配置，该客户端有四个vCPU：![图15.4 – 检查默认的vcpupin设置
- en: '](img/B14834_15_04.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_15_04.jpg)'
- en: Figure 15.4 – Checking the default vcpupin settings
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.4 – 检查默认的vcpupin设置
- en: Let's change that by using CPU pinning.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过使用CPU固定来改变这一点。
- en: Let's pin `vCPU0` to physical core 0, `vCPU1` to physical core 1, `vCPU2` to
    physical core 2, and `vCPU3` to physical core 3:![Figure 15.5 – Configuring CPU
    pinning
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们将`vCPU0`固定到物理核心0，`vCPU1`固定到物理核心1，`vCPU2`固定到物理核心2，`vCPU3`固定到物理核心3：![图15.5
    – 配置CPU固定
- en: '](img/B14834_15_05.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_15_05.jpg)'
- en: Figure 15.5 – Configuring CPU pinning
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.5 – 配置CPU固定
- en: By using `virsh vcpupin`, we changed a fixed virtual CPU allocation for this
    VM.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用`virsh vcpupin`，我们改变了此VM的固定虚拟CPU分配。
- en: 'Let''s use `virsh dumpxml` on this VM to check the configuration change:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们在此VM上使用`virsh dumpxml`来检查配置更改：
- en: '![Figure 15.6 – CPU pinning VM configuration changes'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '![图15.6 – CPU固定VM配置更改'
- en: '](img/B14834_15_06.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_15_06.jpg)'
- en: Figure 15.6 – CPU pinning VM configuration changes
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.6 – CPU固定VM配置更改
- en: Notice the CPU affinity listed in the `virsh` command and the `<cputune>` tag
    in the XML dump of the running guest. As the XML tag says, this comes under the
    CPU tuning section of the guest. It is also possible to configure a set of physical
    CPUs for a particular vCPU instead of a single physical CPU.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 注意`virsh`命令中列出的CPU亲和力以及运行客户端的XML转储中的`<cputune>`标记。正如XML标记所说，这属于客户端的CPU调整部分。还可以配置一组物理CPU用于特定vCPU，而不是单个物理CPU。
- en: There are a couple of things to remember. vCPU pinning can improve performance;
    however, this depends on the host configuration and the other settings on the
    system. Make sure you do enough tests and validate the settings.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 有几件事情要记住。 vCPU固定可以提高性能；但是，这取决于主机配置和系统上的其他设置。确保进行足够的测试并验证设置。
- en: 'You can also make use of `virsh vcpuinfo` to verify the pinning. The output
    of the `virsh vcpuinfo` command is as follows:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用`virsh vcpuinfo`来验证固定。`virsh vcpuinfo`命令的输出如下：
- en: '![Figure 15.7 – virsh vcpuinfo for our VM'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '![图15.7 – 用于我们的VM的virsh vcpuinfo'
- en: '](img/B14834_15_07.jpg)'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_15_07.jpg)'
- en: Figure 15.7 – virsh vcpuinfo for our VM
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.7 – 用于我们的VM的virsh vcpuinfo
- en: If we're doing this on a busy host, it will have consequences. Sometimes, we
    literally won't be able to start our SQL machine because of these settings. So,
    for the greater good (the SQL VM working instead of not wanting to start), we
    can change the memory mode configuration from `strict` to `interleave` or `preferred`,
    which will relax the insistence on using strictly local memory for this VM.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在繁忙的主机上进行此操作，将会产生后果。有时，我们可能无法启动我们的SQL机器，因为这些设置。因此，为了更大的利益（SQL VM能够工作而不是不想启动），我们可以将内存模式配置从`strict`更改为`interleave`或`preferred`，这将放宽对于为此VM严格使用本地内存的坚持。
- en: Let's now explore the memory tuning options as they are the next logical thing
    to discuss.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们探讨内存调整选项，因为这是下一个逻辑要讨论的事情。
- en: Working with memory
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 内存调整
- en: Memory is a precious resource for most environments, isn't it? Thus, the efficient
    use of memory should be achieved by tuning it. The first rule in optimizing KVM
    memory performance is not to allocate more resources to a guest during setup than
    it will use.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大多数环境来说，内存都是宝贵的资源，不是吗？因此，应通过调整来实现对内存的有效使用。优化KVM内存性能的第一条规则是在设置期间不要为客户端分配比其使用的资源更多的资源。
- en: 'We will discuss the following in greater detail:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将更详细地讨论以下内容：
- en: Memory allocation
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存分配
- en: Memory tuning
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存调整
- en: Memory backing
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存支持
- en: Let's start by explaining how to configure memory allocation for a virtual system
    or guest.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从解释如何为虚拟系统或客户端配置内存分配开始。
- en: Memory allocation
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 内存分配
- en: 'To make the allocation process simple, we will consider the `virt-manager`
    libvirt client again. Memory allocation can be done from the window shown in the
    following screenshot:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使分配过程简单，我们将再次考虑`virt-manager` libvirt客户端。内存分配可以从以下截图中显示的窗口中完成：
- en: '![Figure 15.8 – VM memory options'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '![图15.8 – VM内存选项'
- en: '](img/B14834_15_08.jpg)'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_15_08.jpg)'
- en: Figure 15.8 – VM memory options
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.8 – VM内存选项
- en: 'As you can see in the preceding screenshot, there are two main options: **Current
    allocation** and **Maximum allocation**:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在前面的截图中所看到的，有两个主要选项：**当前分配**和**最大分配**：
- en: '**Maximum allocation**: The runtime maximum memory allocation of the guest.
    This is the maximum memory that can be allocated to the guest when it''s running.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最大分配**：客户端的运行时最大内存分配。这是客户端在运行时可以分配的最大内存。'
- en: '**Current allocation**: How much memory a guest always uses. For memory ballooning
    reasons, we can have this value lower than the maximum.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**当前分配**：客户端始终使用的内存量。出于内存球形原因，我们可以将此值设置为低于最大值。'
- en: The `virsh` command can be used to tune these parameters. The relevant `virsh`
    command options are `setmem` and `setmaxmem`.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '`virsh`命令可用于调整这些参数。相关的`virsh`命令选项是`setmem`和`setmaxmem`。'
- en: Memory tuning
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 内存调整
- en: The memory tuning options are added under `<memtune>` of the guest configuration
    file.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 内存调整选项添加在客户端配置文件的`<memtune>`下。
- en: Additional memory tuning options can be found at [http://libvirt.org/formatdomain.html#elementsMemoryTuning](http://libvirt.org/formatdomain.html#elementsMemoryTuning).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 其他内存调整选项可以在[http://libvirt.org/formatdomain.html#elementsMemoryTuning](http://libvirt.org/formatdomain.html#elementsMemoryTuning)找到。
- en: 'The admin can configure the memory settings of a guest manually. If the `<memtune>`
    configuration is omitted, the default memory settings apply for a guest. The `virsh`
    command at play here is as follows:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 管理员可以手动配置客户机的内存设置。如果省略了`<memtune>`配置，那么默认的内存设置将适用于客户机。这里使用的`virsh`命令如下：
- en: '[PRE0]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'It can have any of the following values; this best practice is well documented
    in the man page:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 它可以具有以下任何值；这个最佳实践在man页面中有很好的记录：
- en: '[PRE1]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The default/current values that are set for the `memtune` parameter can be
    fetched as shown:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 可以获取为`memtune`参数设置的默认/当前值，如下所示：
- en: '![Figure 15.9 – Checking the memtune settings for the VM'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '![图15.9 - 检查VM的memtune设置'
- en: '](img/B14834_15_09.jpg)'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_15_09.jpg)'
- en: Figure 15.9 – Checking the memtune settings for the VM
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.9 - 检查VM的memtune设置
- en: When setting `hard_limit`, you should not set this value too low. This might
    lead to a situation in which a VM is terminated by the kernel. That's why determining
    the correct amount of resources for a VM (or any other process) is such a design
    problem. Sometimes, designing things properly seems like dark arts.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在设置`hard_limit`时，不应将此值设置得太低。这可能导致虚拟机被内核终止。这就是为什么确定虚拟机（或任何其他进程）的正确资源量是一个设计问题。有时，正确设计东西似乎就像黑暗艺术一样。
- en: 'To learn more about how to set these parameters, please see the help output
    for the `memtune` command in the following screenshot:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解如何设置这些参数，请参阅以下截图中`memtune`命令的帮助输出：
- en: '![Figure 15.10 – Checking virsh help memtune'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '![图15.10 - 检查virsh帮助memtune'
- en: '](img/B14834_15_10.jpg)'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_15_10.jpg)'
- en: Figure 15.10 – Checking virsh help memtune
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.10 - 检查virsh帮助memtune
- en: As we have covered memory allocation and tuning, the final option is memory
    backing.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们讨论了内存分配和调整之后，最后一个选项是内存后备。
- en: Memory backing
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 内存后备
- en: 'The following is the guest XML representation of memory backing:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是内存后备的客户机XML表示：
- en: '[PRE2]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'You may have noticed that there are three main options for memory backing:
    `locked`, `nosharepages`, and `hugepages`. Let''s go through them one by one,
    starting with `locked`.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能已经注意到内存后备有三个主要选项：`locked`、`nosharepages`和`hugepages`。让我们逐一介绍它们，从`locked`开始。
- en: locked
  id: totrans-135
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: locked
- en: 'In KVM virtualization, guest memory lies in the process address space of the
    `qemu-kvm` process in the KVM host. These guest memory pages can be swapped out
    by the Linux kernel at any time, based on the requirement that the host has, and
    this is where `locked` can help. If you set the memory backing option of the guest
    to `locked`, the host will not swap out memory pages that belong to the virtual
    system or guest. The virtual memory pages in the host system memory are locked
    when this option is enabled:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在KVM虚拟化中，客户机内存位于KVM主机中的`qemu-kvm`进程的进程地址空间中。这些客户机内存页面可以根据主机的需求随时被Linux内核交换出去，这就是`locked`可以帮助的地方。如果将客户机的内存后备选项设置为`locked`，主机将不会交换属于虚拟系统或客户机的内存页面。当启用此选项时，主机系统内存中的虚拟内存页面将被锁定：
- en: '[PRE3]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We need to use `<memtune>` to set `hard_limit`. The calculus is simple – whatever
    the amount of memory for the guest we need plus overhead.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要使用`<memtune>`来设置`hard_limit`。计算很简单 - 我们需要为客户机加上开销的内存量。
- en: nosharepages
  id: totrans-139
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: nosharepages
- en: 'The following is the XML representation of `nosharepages` from the guest configuration
    file:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是来自客户机配置文件的`nosharepages`的XML表示：
- en: '[PRE4]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: There are different mechanisms that can enable the sharing of memory when the
    memory pages are identical. Techniques such as `nosharepages` option instructs
    the hypervisor to disable shared pages for this guest – that is, setting this
    option will prevent the host from deduplicating memory between guests.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 有不同的机制可以在内存页面相同的情况下实现内存共享。诸如`nosharepages`选项之类的技术指示了虚拟化程序禁用此客户机的共享页面 - 也就是说，设置此选项将阻止主机在客户机之间进行内存去重。
- en: hugepages
  id: totrans-143
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: hugepages
- en: 'The third and final option is `hugepages`, which can be represented in XML
    format, as follows:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个也是最后一个选项是`hugepages`，可以用XML格式表示如下：
- en: '[PRE5]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: HugePages were introduced in the Linux kernel to improve the performance of
    memory management. Memory is managed in blocks known as pages. Different architectures
    (i386, ia64) support different page sizes. We don't necessarily have to use the
    default setting for x86 CPUs (4 KB memory pages), as we can use larger memory
    pages (2 MB to 1 GB), a feature that's called HugePages. A part of the CPU called
    the **Memory Management Unit** (**MMU**) manages these pages by using a list.
    The pages are referenced through page tables, and each page has a reference in
    the page table. When a system wants to handle a huge amount of memory, there are
    mainly two options. One of them involves increasing the number of page table entries
    in the hardware MMU. The second method increases the default page size. If we
    opt for the first method of increasing the page table entries, it is really expensive.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: HugePages是在Linux内核中引入的，以改善内存管理的性能。内存以称为页面的块进行管理。不同的架构（i386、ia64）支持不同的页面大小。对于x86
    CPU（4 KB内存页面）来说，我们不一定要使用默认设置，因为我们可以使用更大的内存页面（2 MB到1 GB），这个功能称为HugePages。CPU的一个部分称为**内存管理单元**（**MMU**）通过使用列表来管理这些页面。页面通过页表引用，并且每个页面在页表中都有一个引用。当系统想要处理大量内存时，主要有两种选项。其中一种涉及增加硬件MMU中的页表条目数。第二种方法是增加默认页面大小。如果我们选择增加页表条目的第一种方法，那么成本就会很高。
- en: The second and more efficient method when dealing with large amounts of memory
    is using HugePages or increased page sizes by using HugePages. The different amounts
    of memory that each and every server has means that there is a need for different
    page sizes. The default values are okay for most situations, while huge memory
    pages (for example, 1 GB) are more efficient if we have large amounts of memory
    (hundreds of gigabytes or even terabytes). This means less *administrative* work
    in terms of referencing memory pages and more time spent actually getting the
    content of these memory pages, which can lead to a significant performance boost.
    Most of the known Linux distributions can use HugePages to manage large memory
    amounts. A process can use HugePages memory support to improve performance by
    increasing the CPU cache hits against the **Translation LookAside Buffer** (**TLB**),
    as explained in [*Chapter 2*](B14834_02_Final_ASB_ePub.xhtml#_idTextAnchor029),
    *KVM as a Virtualization Solution*. You already know that guest systems are simply
    processes in a Linux system, thus the KVM guests are eligible to do the same.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we move on, we should also mention `MADV_HUGEPAGE` regions (to avoid
    the risk of consuming more memory resources), or enabled system-wide. There are
    three main options for configuring THP in a system: `always`, `madvise`, and `never`:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'From the preceding output, we can see that the current THP setting in our server
    is `madvise`. Other options can be enabled by using one of the following commands:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'In short, what these values mean is the following:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: '`always`: Always use THP.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`madvise`: Use HugePages only in `MADV_HUGEPAGE`.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`never`: Disable the feature.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The system settings for performance are automatically optimized by THP. We can
    have performance benefits by using memory as cache. It is possible to use static
    HugePages when THP is in place or in other words THP won't prevent it from using
    a static method. If we don't configure our KVM hypervisor to use static HugePages,
    it will use 4 Kb transparent HugePages. The advantages we get from using HugePages
    for a KVM guest's memory are that less memory is used for page tables and TLB
    misses are reduced; obviously, this increases performance. But keep in mind that
    when using HugePages for guest memory, you can no longer swap or balloon guest
    memory.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s have a quick look at how to use static HugePages in your KVM setup.
    First, let''s check the current system configuration – it''s clear that the HugePages
    size in this system is currently set at 2 MB:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.11 – Checking the HugePages settings'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_15_11.jpg)'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: Figure 15.11 – Checking the HugePages settings
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: We're primarily talking about all the attributes starting with HugePages, but
    it's worth mentioning what the `AnonHugePages` attribute is. The `AnonHugePages`
    attribute tells us the current THP usage on the system level.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s configure KVM to use a custom HugePages size:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: 'View the current explicit `hugepages` value by running the following command
    or fetch it from `sysfs`, as shown:'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We can also use the `sysctl -a |grep huge` command:![Figure 15.12 – The sysctl
    hugepages settings
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B14834_15_12.jpg)'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: Figure 15.12 – The sysctl hugepages settings
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: 'As the HugePage size is 2 MB, we can set hugepages in increments of 2 MB. To
    set the number of hugepages to 2,000, use the following command:'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The total memory assigned for hugepages cannot be used by applications that
    are not hugepage-aware – that is, if you over-allocate hugepages, normal operations
    of the host system can be affected. In our examples, 2048*2 MB would equal 4,096
    MB of memory, which we should have available when we do this configuration.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to tell the system that this type of configuration is actually OK and
    configure `/etc/security/limits.conf` to reflect that. Otherwise, the system might
    refuse to give us access to 2,048 hugepages times 2 MB of memory. We need to add
    two lines to that file:'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'To make it persistent, you can use the following:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Then, mount the `fs` hugepages, reconfigure the VM, and restart the host:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Reconfigure the HugePage-configured guest by adding the following settings
    in the VM configuration file:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'It''s time to shut down the VM and reboot the host. Inside the VM, do the following:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'On the host, do the following:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: After the host reboot and the restart of the VM, it will now start using the
    hugepages.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: The next topic is related to sharing memory content between multiple VMs, referred
    to as KSM. This technology is heavily used to *save* memory. At any given moment,
    when multiple VMs are powered on the virtualization host, there's a big statistical
    chance that those VMs have blocks of memory contents that are the same (they have
    the same contents). Then, there's no reason to store the same contents multiple
    times. Usually, we refer to KSM as a deduplication process being applied to memory.
    Let's learn how to use and configure KSM.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: Getting acquainted with KSM
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: KSM is a feature that allows the sharing of identical pages between the different
    processes running on a system. We might presume that the identical pages exist
    due to certain reasons—for example, if there are multiple processes spawned from
    the same binary or something similar. There is no rule such as this though. KSM
    scans these identical memory pages and consolidates a **Copy-on-Write** (**COW**)
    shared page. COW is nothing but a mechanism where when there is an attempt to
    change a memory region that is shared and common to more than one process, the
    process that requests the change gets a new copy and the changes are saved in
    it.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: Even though the consolidated COW shared page is accessible by all the processes,
    whenever a process tries to change the content (write to that page), the process
    gets a new copy with all of the changes. By now, you will have understood that,
    by using KSM, we can reduce physical memory consumption. In the KVM context, this
    can really add value, because guest systems are `qemu-kvm` processes in the system,
    and there is a huge possibility that all the VM processes will have a good amount
    of similar memory.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: For KSM to work, the process/application has to register its memory pages with
    KSM. In KVM-land, KSM allows guests to share identical memory pages, thus achieving
    an improvement in memory consumption. That might be some kind of application data,
    a library, or anything else that's used frequently. This shared page or memory
    is marked as `copy on write`. In short, KSM avoids memory duplication and it's
    really useful when similar guest OSes are present in a KVM environment.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: By using the theory of prediction, KSM can provide enhanced memory speed and
    utilization. Mostly, this common shared data is stored in cache or main memory,
    which causes fewer cache misses for the KVM guests. Also, KSM can reduce the overall
    guest memory footprint so that, in a way, it allows the user to do memory overcommitting
    in a KVM setup, thus supplying the greater utilization of available resources.
    However, we have to keep in mind that KSM requires more CPU resources to identify
    the duplicate pages and to perform tasks such as sharing/merging.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: 'Previously, we mentioned that the processes have to mark the *pages* to show
    that they are eligible candidates for KSM to operate. The marking can be done
    by a process based on the `MADV_MERGEABLE` flag, which we will discuss in the
    next section. You can explore the use of this flag in the `madvise` man page:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'So, the kernel has to be configured with KSM, as follows:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.13 – Checking the KSM settings'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_15_13.jpg)'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: Figure 15.13 – Checking the KSM settings
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: 'KSM gets deployed as a part of the `qemu-kvm` package. Information about the
    KSM service can be fetched from the `sysfs` filesystem, in the `/sys` directory.
    There are different files available in this location, reflecting the current KSM
    status. These are updated dynamically by the kernel, and it has a precise record
    of the KSM usage and statistics:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.14 – The KSM settings in sysfs'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_15_14.jpg)'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: Figure 15.14 – The KSM settings in sysfs
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: In an upcoming section, we will discuss the `ksmtuned` service and its configuration
    variables. As `ksmtuned` is a service to control KSM, its configuration variables
    are analogous to the files we see in the `sysfs` filesystem. For more details,
    you can check out [https://www.kernel.org/doc/html/latest/admin-guide/mm/ksm.html](https://www.kernel.org/doc/html/latest/admin-guide/mm/ksm.html).
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: 'It is also possible to tune these parameters with the `virsh` command. The
    `virsh node-memory-tune` command does this job for us. For example, the following
    command specifies the number of pages to scan before the shared memory service
    goes to sleep:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: As with any other service, the `ksmtuned` service also has logs stored in a
    log file, `/var/log/ksmtuned`. If we add `DEBUG=1` to `/etc/ksmtuned.conf`, we
    will have logging from any kind of KSM tuning actions. Refer to [https://www.kernel.org/doc/Documentation/vm/ksm.txt](https://www.kernel.org/doc/Documentation/vm/ksm.txt)
    for more details.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we start the KSM service, as shown next, you can watch the values change
    depending on the KSM service in action:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We can then check the status of the `ksm` service like this:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.15 – The ksm service command and the ps command output'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_15_15.jpg)'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: Figure 15.15 – The ksm service command and the ps command output
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the KSM service is started and we have multiple VMs running on our host,
    we can check the changes by querying `sysfs` by using the following command multiple
    times:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Let's explore the `ksmtuned` service in more detail. The `ksmtuned` service
    is designed so that it goes through a cycle of actions and adjusts KSM. This cycle
    of actions continues its work in a loop. Whenever a guest system is created or
    destroyed, libvirt will notify the `ksmtuned` service.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: 'The `/etc/ksmtuned.conf` file is the configuration file for the `ksmtuned`
    service. Here is a brief explanation of the configuration parameters available.
    You can see these configuration parameters match with the KSM files in `sysfs`:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: KSM is designed to improve performance and allow memory overcommits. It serves
    this purpose in most environments; however, KSM may introduce a performance overhead
    in some setups or environments – for example, if you have a few VMs that have
    similar memory content when you start them and loads of memory-intensive operations
    afterward. This will create issues as KSM will first work very hard to reduce
    the memory footprint, and then lose time to cover for all of the memory content
    differences between multiple VMs. Also, there is a concern that KSM may open a
    channel that could potentially be used to leak information across guests, as has
    been well documented in the past couple of years. If you have these concerns or
    if you see/experience KSM not helping to improve the performance of your workload,
    it can be disabled.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: 'To disable KSM, stop the `ksmtuned` and `ksm` services in your system by executing
    the following:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: We have gone through the different tuning options for CPU and memory. The next
    big subject that we need to cover is NUMA configuration, where both CPU and memory
    configuration become a part of a larger story or context.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: Tuning the CPU and memory with NUMA
  id: totrans-219
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we start tuning the CPU and memory for NUMA-capable systems, let's see
    what NUMA is and how it works.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: Think of NUMA as a system where you have more than one system bus, each serving
    a small set of processors and associated memory. Each group of processors has
    its own memory and possibly its own I/O channels. It may not be possible to stop
    or prevent running VM access across these groups. Each of these groups is known
    as a **NUMA node**.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 将NUMA视为一个系统，其中有多个系统总线，每个总线为一小组处理器和关联内存提供服务。每组处理器都有自己的内存，可能还有自己的I/O通道。可能无法阻止或阻止运行的VM跨越这些组。这些组中的每一个称为**NUMA节点**。
- en: In this concept, if a process/thread is running on a NUMA node, the memory on
    the same node is called local memory and memory residing on a different node is
    known as foreign/remote memory. This implementation is different from the **Symmetric
    Multiprocessor System** (**SMP**), where the access time for all of the memory
    is the same for all the CPUs, as memory access happens through a centralized bus.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个概念中，如果一个进程/线程在一个NUMA节点上运行，同一节点上的内存称为本地内存，而驻留在不同节点上的内存称为外部/远程内存。这种实现与**对称多处理系统**（**SMP**）不同，SMP中所有内存的访问时间对所有CPU都是相同的，因为内存访问是通过一个集中的总线进行的。
- en: 'An important subject in discussing NUMA is the NUMA ratio. The NUMA ratio is
    a measure of how quickly a CPU can access local memory compared to how quickly
    it can access remote/foreign memory. For example, if the NUMA ratio is 2.0, then
    it takes twice as long for the CPU to access remote memory. If the NUMA ratio
    is 1, that means that we''re using SMP. The bigger the ratio, the bigger the latency
    price (overhead) that a VM memory operation will have to pay before getting the
    necessary data (or saving it). Before we explore tuning in more depth, let''s
    discuss exploring the NUMA topology of a system. One of the easiest ways to show
    the current NUMA topology is via the `numactl` command:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 讨论NUMA的一个重要主题是NUMA比率。NUMA比率是衡量CPU访问本地内存相对于访问远程/外部内存的速度的指标。例如，如果NUMA比率为2.0，则CPU访问远程内存的时间是访问本地内存的两倍。如果NUMA比率为1，这意味着我们正在使用SMP。比率越大，VM内存操作在获取必要数据（或保存数据）之前必须支付的延迟成本（开销）就越大。在更深入地探讨调优之前，让我们讨论一下系统的NUMA拓扑。显示当前NUMA拓扑的最简单方法之一是通过`numactl`命令：
- en: '![Figure 15.16 – The numactl -H output'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '![图15.16 - numactl -H输出'
- en: '](img/B14834_15_16.jpg)'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_15_16.jpg)'
- en: Figure 15.16 – The numactl -H output
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.16 - numactl -H输出
- en: 'The preceding `numactl` output conveys that there are 10 CPUs in the system
    and they belong to a single NUMA node. It also lists the memory associated with
    each NUMA node and the node distance. When we discussed CPU pinning, we displayed
    the topology of the system using the `virsh` capabilities. To get a graphical
    view of the NUMA topology, you can make use of a command called `lstopo`, which
    is available with the `hwloc` package in CentOS-/Red Hat-based systems:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的`numactl`输出表明系统中有10个CPU，它们属于单个NUMA节点。它还列出了与每个NUMA节点关联的内存和节点距离。当我们讨论CPU固定时，我们使用`virsh`功能显示了系统的拓扑结构。要获得NUMA拓扑的图形视图，可以使用一个名为`lstopo`的命令，该命令在基于CentOS-/Red
    Hat的系统中与`hwloc`软件包一起提供：
- en: '![Figure 15.17 – The lstopo command to visualize the NUMA topology'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '![图15.17 - 使用lstopo命令可视化NUMA拓扑'
- en: '](img/B14834_15_17.jpg)'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_15_17.jpg)'
- en: Figure 15.17 – The lstopo command to visualize the NUMA topology
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.17 - 使用lstopo命令可视化NUMA拓扑
- en: This screenshot also shows the PCI devices associated with the NUMA nodes. For
    example, `ens*` (network interface) devices are attached to NUMA node 0\. Once
    we have the NUMA topology of the system and understand it, we can start tuning
    it, specially for the KVM virtualized setup.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 此截图还显示了与NUMA节点关联的PCI设备。例如，`ens*`（网络接口）设备连接到NUMA节点0。一旦我们了解了系统的NUMA拓扑，就可以开始调整它，特别是针对KVM虚拟化设置。
- en: NUMA memory allocation policies
  id: totrans-232
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: NUMA内存分配策略
- en: 'By modifying the VM XML configuration file, we can do NUMA tuning. Tuning NUMA
    introduces a new element tag called `numatune`:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 通过修改VM XML配置文件，我们可以进行NUMA调优。调优NUMA引入了一个名为`numatune`的新元素标签：
- en: '[PRE22]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'This is also configurable via the `virsh` command, as shown:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 这也可以通过`virsh`命令进行配置，如下所示：
- en: '![Figure 15.18 – Using virsh numatune to configure the NUMA settings'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '![图15.18 - 使用virsh numatune配置NUMA设置'
- en: '](img/B14834_15_18.jpg)'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_15_18.jpg)'
- en: Figure 15.18 – Using virsh numatune to configure the NUMA settings
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.18 - 使用virsh numatune配置NUMA设置
- en: 'The XML representation of this tag is as follows:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 此标签的XML表示如下：
- en: '[PRE23]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Even though the element called `numatune` is optional, it is provided to tune
    the performance of the NUMA host by controlling the NUMA policy for the domain
    process. The main sub-tags of this optional element are `memory` and `nodeset`.
    Some notes on these sub-tags are as follows:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管名为`numatune`的元素是可选的，但它是用来通过控制域进程的NUMA策略来调整NUMA主机性能的。此可选元素的主要子标签是`memory`和`nodeset`。有关这些子标签的一些说明如下：
- en: '`memory`: This element describes the memory allocation process on the NUMA
    node. There are three policies that govern memory allocation for NUMA nodes:'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`memory`：此元素描述了NUMA节点上的内存分配过程。有三种策略来管理NUMA节点的内存分配：'
- en: 'a) `Strict`: When a VM tries to allocate memory and that memory isn''t available,
    allocation will fail.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: a) `Strict`：当虚拟机尝试分配内存并且内存不可用时，分配将失败。
- en: 'b) `Interleave`: Nodeset-defined round-robin allocation across NUMA nodes.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: b) `Interleave`：在NUMA节点之间进行循环分配。
- en: 'c) `Preferred`: The VM tries to allocate memory from a preferred node. If that
    node doesn''t have enough memory, it can allocate memory from the remaining NUMA
    nodes.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: c) `Preferred`：虚拟机尝试从首选节点分配内存。如果该节点没有足够的内存，它可以从剩余的NUMA节点分配内存。
- en: '`nodeset`: Specifies a NUMA node list available on the server.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nodeset`：指定服务器上可用的NUMA节点列表。'
- en: 'One of the important attributes here is *placement*, as explained at the following
    URL – [https://libvirt.org/formatdomain.html](https://libvirt.org/formatdomain.html):'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的一个重要属性是*placement*，在以下URL中有解释 - [https://libvirt.org/formatdomain.html](https://libvirt.org/formatdomain.html)：
- en: '"Attribute placement can be used to indicate the memory placement mode for
    domain process, its value can be either "static" or "auto", defaults to placement
    of vCPU, or "static" if nodeset is specified. "auto" indicates the domain process
    will only allocate memory from the advisory nodeset returned from querying numad,
    and the value of attribute nodeset will be ignored if it''s specified. If placement
    of vCPU is ''auto'', and numatune is not specified, a default numatune with placement
    ''auto'' and mode ''strict'' will be added implicitly."'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '"属性放置可用于指示域进程的内存放置模式，其值可以是"static"或"auto"，默认为vCPU的放置，或者如果指定了nodeset，则为"static"。"auto"表示域进程将仅从查询numad返回的建议nodeset分配内存，如果指定了属性nodeset的值将被忽略。如果vCPU的放置是''auto''，并且未指定numatune，则将隐式添加一个默认的numatune，其放置为''auto''，模式为''strict''。"'
- en: We need to be careful with these declarations, as there are inheritance rules
    that apply. For example, the `<numatune>` and `<vcpu>` elements default to the
    same value if we specify the `<nodeset>` element. So, we can absolutely configure
    different CPU and memory tuning options, but also be aware of the fact that these
    options can be inherited.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要小心这些声明，因为有适用的继承规则。例如，如果我们指定了`<nodeset>`元素，`<numatune>`和`<vcpu>`元素默认为相同的值。因此，我们绝对可以配置不同的CPU和内存调优选项，但也要意识到这些选项可以被继承。
- en: There are some more things to consider when thinking about CPU pinning in the
    NUMA context. We discussed the basis of CPU pinning earlier in this chapter, as
    it gives us better, predictable performance for our VMs and can increase cache
    efficiency. Just as an example, let's say that we want to run a VM as fast as
    possible. It would be prudent to run it on the fastest storage available, which
    would be on a PCI Express bus on the CPU socket where we pinned the CPU cores.
    If we're not using an NVMe SSD local to that VM, we can use a storage controller
    to achieve the same thing. However, if the storage controller that we're using
    to access VM storage is physically connected to another CPU socket, that will
    lead to latency. For latency-sensitive applications, that will mean a big performance
    hit.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在考虑NUMA上下文中的CPU固定时，还有一些要考虑的事情。我们在本章的早些时候讨论了CPU固定的基础，因为它可以为我们的虚拟机提供更好、更可预测的性能，并且可以提高缓存效率。举个例子，假设我们想尽可能快地运行一个虚拟机。明智的做法是在固定CPU核心的CPU插槽上运行它，这样可以在最快的存储上运行，这将在PCI
    Express总线上。如果我们没有使用NVMe SSD本地运行虚拟机，我们可以使用存储控制器来实现相同的效果。但是，如果我们用来访问虚拟机存储的存储控制器物理连接到另一个CPU插槽，那将导致延迟。对于延迟敏感的应用程序，这将意味着性能大幅下降。
- en: However, we also need to be aware of the other extreme – if we do too much pinning,
    it can create other problems in the future. For example, if our servers are not
    architecturally the same (having the same amount of cores and memory), migrating
    VMs might become problematic. We can create a scenario where we're migrating a
    VM with CPU cores pinned to cores that don't exist on the target server of our
    migration process. So, we always need to be careful about what we do with the
    configuration of our environments so that we don't take it too far.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们也需要意识到另一个极端——如果我们进行过多的固定，将来可能会产生其他问题。例如，如果我们的服务器在架构上不同（具有相同数量的核心和内存），迁移虚拟机可能会变得棘手。我们可能会出现这样的情况，即迁移过程中将CPU核心固定到目标服务器上不存在的核心。因此，我们在配置环境时总是需要小心，以免走得太远。
- en: The next subject on our list is `emulatorpin`, which can be used to pin our
    `qemu-kvm` emulator to a specific CPU core so that it doesn't influence the performance
    of our VM cores. Let's learn how to configure that.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 我们列表上的下一个主题是`emulatorpin`，它可以用来将我们的`qemu-kvm`模拟器固定到特定的CPU核心，以便它不影响我们虚拟机核心的性能。让我们学习如何配置它。
- en: Understanding emulatorpin
  id: totrans-253
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解emulatorpin
- en: 'The `emulatorpin` option also falls into the CPU tuning category. The XML representation
    of this would be as follows:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '`emulatorpin`选项也属于CPU调优类别。其XML表示如下：'
- en: '[PRE24]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The `emulatorpin` element is optional and is used to pin the emulator (`qemu-kvm`)
    to a host physical CPU. This does not include the vCPU or IO threads from the
    VM. If this is omitted, the emulator is pinned to all the physical CPUs of the
    host system by default.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '`emulatorpin`元素是可选的，用于将模拟器（`qemu-kvm`）固定到主机物理CPU。这不包括来自VM的vCPU或IO线程。如果省略此项，模拟器将默认固定到主机系统的所有物理CPU。'
- en: 'Important note:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示：
- en: Please note that `<vcpupin>`, `<numatune>`, and `<emulatorpin>` should be configured
    together to achieve optimal, deterministic performance when you tune a NUMA-capable
    system.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，当您调整支持NUMA的系统时，应该一起配置`<vcpupin>`、`<numatune>`和`<emulatorpin>`，以实现最佳的确定性性能。
- en: 'Before we leave this section, there are a couple more things to cover: the
    guest system NUMA topology and hugepage memory backing with NUMA.'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们离开这一部分之前，还有一些事情需要涵盖：客户端系统NUMA拓扑和NUMA的大页内存支持。
- en: 'Guest NUMA topology can be specified using the `<numa>` element in the guest
    XML configuration; some call this virtual NUMA:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用`<numa>`元素在客户端XML配置中指定客户端NUMA拓扑结构；有些人称之为虚拟NUMA：
- en: '[PRE25]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The `cell id` element tells the VM which NUMA node to use, while the `cpus`
    element configures a specific core (or cores). The `memory` element assigns the
    amount of memory per node. Each NUMA node is indexed by number, starting from
    `0`.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '`cell id`元素告诉虚拟机使用哪个NUMA节点，而`cpus`元素配置特定的核心（或核心）。`memory`元素为每个节点分配内存量。每个NUMA节点都以数字索引，从`0`开始。'
- en: 'Previously, we discussed the `memorybacking` element, which can be specified
    to use hugepages in guest configurations. When NUMA is present in a setup, the
    `nodeset` attribute can be used to configure the specific hugepage size per NUMA
    node, which may come in handy as it ties a given guest''s NUMA nodes to certain
    hugepage sizes:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我们讨论了`memorybacking`元素，可以在客户端配置中指定使用大页。当NUMA存在于设置中时，`nodeset`属性可用于配置每个NUMA节点的特定大页大小，这可能会很有用，因为它将给定客户端的NUMA节点与某些大页大小联系起来：
- en: '[PRE26]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: This type of configuration can optimize the memory performance, as guest NUMA
    nodes can be moved to host NUMA nodes as required, while the guest can continue
    to use the hugepages allocated by the host.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: NUMA tuning also has to consider the NUMA node locality for PCI devices, especially
    when a PCI device is being passed through to the guest from the host. If the relevant
    PCI device is affiliated to a remote NUMA node, this can affect data transfer
    and thus hurt the performance.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: The easiest way to display the NUMA topology and PCI device affiliation is by
    using the `lstopo` command that we discussed earlier. The non-graphic form of
    the same command can also be used to discover this configuration. Please refer
    to the earlier sections.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: KSM and NUMA
  id: totrans-268
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We discussed KSM in enough detail in previous sections. KSM is NUMA-aware,
    and it can manage KSM processes happening on multiple NUMA nodes. If you remember,
    we encountered a `sysfs` entry called `merge_across_node` when we fetched KSM
    entries from `sysfs`. That''s the parameter that we can use to manage this process:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: If this parameter is set to `0`, KSM only merges memory pages from the same
    NUMA node. If it's set to `1` (as is the case here), it will merge *across* the
    NUMA nodes. That means that the VM CPUs that are running on the remote NUMA node
    will experience latency when accessing a KSM-merged page.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: Obviously, you know the guest XML entry (the `memorybacking` element) for asking
    the hypervisor to disable shared pages for the guest. If you don't remember, please
    refer back to the memory tuning section for details of this element. Even though
    we can configure NUMA manually, there is something called automatic NUMA balancing.
    We did mention it earlier, but let's see what this concept involves.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: Automatic NUMA balancing
  id: totrans-273
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The main aim of automatic NUMA balancing is to improve the performance of different
    applications running in a NUMA-aware system. The strategy behind its design is
    simple: if an application is using local memory to the NUMA node where vCPUs are
    running, it will have better performance. By using automatic NUMA balancing, KVM
    tries to shift vCPUs around so that they are local (as much as possible) to the
    memory addresses that the vCPUs are using. This is all done automatically by the
    kernel when automatic NUMA balancing is active. Automatic NUMA balancing will
    be enabled when booted on the hardware with NUMA properties. The main conditions
    or criteria are as follows:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: '`numactl --hardware`: Shows multiple nodes'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cat /sys/kernel/debug/sched_features`: Shows NUMA in the flags'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To illustrate the second point, see the following code block:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We can check whether it is enabled in the system via the following method:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Obviously, we can disable automatic NUMA balancing via the following:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The automatic NUMA balancing mechanism works based on the number of algorithms
    and data structures. The internals of this method are based on the following:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: NUMA hinting page faults
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NUMA page migration
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pseudo-interleaving
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fault statistics
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Task placement
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Task grouping
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One of the best practices or recommendations for a KVM guest is to limit its
    resource to the amount of resources on a single NUMA node. Put simply, this avoids
    the unnecessary splitting of VMs across NUMA nodes, which can degrade the performance.
    Let's start by checking the current NUMA configuration. There are multiple available
    options to do this. Let's start with the `numactl` command, NUMA daemon, and `numastat`,
    and then go back to using a well-known command, `virsh`.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: The numactl command
  id: totrans-291
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first option to confirm NUMA availability uses the `numactl` command, as
    shown:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.19 – The numactl hardware output'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_15_19.jpg)'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: Figure 15.19 – The numactl hardware output
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: 'This lists only one node. Even though this conveys the unavailability of NUMA,
    further clarification can be done by running the following command:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: This will *not* list NUMA flags if the system is not NUMA-aware.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: Generally, don't make VMs *wider* than what a single NUMA node can provide.
    Even if the NUMA is available, the vCPUs are bound to the NUMA node and not to
    a particular physical CPU.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: Understanding numad and numastat
  id: totrans-300
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `numad` man page states the following:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: numad is a daemon to control efficient use of CPU and memory on systems with
    NUMA topology.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: '`numad` is also known as the automatic `numad` man page states the following:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: '"numad is a user-level daemon that provides placement advice and process management
    for efficient use of CPUs and memory on systems with NUMA topology."'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: '`numad` is a system daemon that monitors the NUMA topology and resource usage.
    It will attempt to locate processes for efficient NUMA locality and affinity,
    dynamically adjusting to changing system conditions. `numad` also provides guidance
    to assist management applications with the initial manual binding of CPU and memory
    resources for their processes. Note that `numad` is primarily intended for server
    consolidation environments, where there might be multiple applications or multiple
    virtual guests running on the same server system. `numad` is most likely to have
    a positive effect when processes can be localized in a subset of the system''s
    NUMA nodes. If the entire system is dedicated to a large in-memory database application,
    for example, especially if memory accesses will likely remain unpredictable, `numad`
    will probably not improve performance.'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: 'To adjust and align the CPUs and memory resources automatically according to
    the NUMA topology, we need to run `numad`. To use `numad` as an executable, just
    run the following:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'You can check whether this is started as shown:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.20 – Checking whether numad is active'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_15_20.jpg)'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: Figure 15.20 – Checking whether numad is active
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the `numad` binary is executed, it will start the alignment, as shown
    in the following screenshot. In our system, we have the following VM running:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.21 – Listing running VMs'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_15_21.jpg)'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: Figure 15.21 – Listing running VMs
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: 'You can use the `numastat` command, covered in an upcoming section, to monitor
    the difference before and after running the `numad` service. It will run continuously
    by using the following command:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: We can always stop it, but that will not change the NUMA affinity state that
    was configured by `numad`. Now let's move on to `numastat`.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: 'The `numactl` package provides the `numactl` binary/command and the `numad`
    package provides the `numad` binary/command:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.22 – The numastat command output for the qemu-kvm process'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_15_22.jpg)'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: Figure 15.22 – The numastat command output for the qemu-kvm process
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: 'Important note:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: The numerous memory tuning options that we have used have to be thoroughly tested
    using different workloads before moving the VM to production.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: Before we jump on to the next topic, we'd just like to remind you of a point
    we made earlier in this chapter. Live-migrating a VM with pinned resources might
    be complicated, as you have to have some form of compatible resources (and their
    amount) on the target host. For example, the target host's NUMA topology doesn't
    have to be aligned with the source host's NUMA topology. You should consider this
    fact when you tune a KVM environment. Automatic NUMA balancing may help, to a
    certain extent, the need for manually pinning guest resources, though.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: Virtio device tuning
  id: totrans-326
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the virtualization world, a comparison is always made with bare-metal systems.
    Paravirtualized drivers enhance the performance of guests and try to retain near-bare-metal
    performance. It is recommended to use paravirtualized drivers for fully virtualized
    guests, especially when the guest is running with I/O-heavy tasks and applications.
    `lguest`. Virtio was introduced to achieve a common framework for hypervisors
    for IO virtualization.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: 'In short, when we use paravirtualized drivers, the VM OS knows that there''s
    a hypervisor beneath it, and therefore uses frontend drivers to access it. The
    frontend drivers are part of the guest system. When there are emulated devices
    and someone wants to implement backend drivers for these devices, hypervisors
    do this job. The frontend and backend drivers communicate through a virtio-based
    path. Virtio drivers are what KVM uses as paravirtualized device drivers. The
    basic architecture looks like this:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.23 – The Virtio architecture'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_15_23.jpg)'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: Figure 15.23 – The Virtio architecture
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: There are mainly two layers (virt queue and virtual ring) to support communication
    between the guest and the hypervisor.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: '**Virt queue** and **virtual ring** (**vring**) are the transport mechanism
    implementations in virtio. Virt queue (virtio) is the queue interface that attaches
    the frontend and backend drivers. Each virtio device has its own virt queues and
    requests from guest systems are put into these virt queues. Each virt queue has
    its own ring, called a vring, which is where the memory is mapped between QEMU
    and the guest. There are different virtio drivers available for use in a KVM guest.'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: 'The devices are emulated in QEMU, and the drivers are part of the Linux kernel,
    or an extra package for Windows guests. Some examples of device/driver pairs are
    as follows:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: '`virtio-net`: The virtio network device is a virtual Ethernet card. `virtio-net`
    provides the driver for this.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`virtio-blk`: The virtio block device is a simple virtual block device (that
    is, a disk). `virtio-blk` provides the block device driver for the virtual block
    device.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`virtio-balloon`: The virtio memory balloon device is a device for managing
    guest memory.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`virtio-scsi`: The virtio SCSI host device groups together one or more disks
    and allows communicating to them using the SCSI protocol.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`virtio-console`: The virtio console device is a simple device for data input
    and output between the guest and host userspace.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`virtio-rng`: The virtio entropy device supplies high-quality randomness for
    guest use, and so on.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In general, you should make use of these virtio devices in your KVM setup for
    better performance.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: Block I/O tuning
  id: totrans-342
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Going back to basics – a virtual disk of a VM can be either a block device or
    an image file. For better VM performance, a block device-based virtual disk is
    preferred over an image file that resides on a remote filesystem such as NFS,
    GlusterFS, and so on. However, we cannot ignore that the file backend helps the
    virt admin to better manage guest disks and it is immensely helpful in some scenarios.
    From our experience, we have noticed most users make use of disk image files,
    especially when performance is not much of a concern. Keep in mind that the total
    number of virtual disks that can be attached to a VM has a limit. At the same
    time, there is no restriction on mixing and using block devices and files and
    using them as storage disks for the same guest.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: A guest treats the virtual disk as its storage. When an application inside a
    guest OS writes data to the local storage of the guest system, it has to pass
    through a couple of layers. That said, this I/O request has to traverse through
    the filesystem on the storage and the I/O subsystem of the guest OS. After that,
    the `qemu-kvm` process passes it to the hypervisor from the guest OS. Once the
    I/O is within the realm of the hypervisor, it starts processing the I/O like any
    other applications running in the host OS. Here, you can see the number of layers
    that the I/O has to pass through to complete an I/O operation. Hence, the block
    device backend performs better than the image file backend.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are our observations on disk backends and file- or image-based
    virtual disks:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: A file image is part of the host filesystem and it creates an additional resource
    demand for I/O operations compared to the block device backend.
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using sparse image files helps to over allocate host storage but its usage will
    reduce the performance of the virtual disk.
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The improper partitioning of guest storage when using disk image files can cause
    unnecessary I/O operations. Here, we are mentioning the alignment of standard
    partition units.
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At the start of this chapter, we discussed virtio drivers, which give better
    performance. So, it's recommended that you use the virtio disk bus when configuring
    the disk, rather than the IDE bus. The `virtio_blk` driver uses the virtio API
    to provide high performance for storage I/O device, thus increasing storage performance,
    especially in large enterprise storage systems. We discussed the different storage
    formats available in [*Chapter 5*](B14834_05_Final_ASB_ePub.xhtml#_idTextAnchor079),
    *Libvirt Storage*; however, the main ones are the `raw` and `qcow` formats. The
    best performance will be achieved when you are using the `raw` format. There is
    obviously a performance overhead delivered by the format layer when using `qcow`.
    Because the format layer has to perform some operations at times, for example,
    if you want to grow a `qcow` image, it has to allocate the new cluster and so
    on. However, `qcow` would be an option if you want to make use of features such
    as snapshots. These extra facilities are provided with the image format, `qcow`.
    Some performance comparisons can be found at [http://www.Linux-kvm.org/page/Qcow2](http://www.Linux-kvm.org/page/Qcow2).
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: 'There are three options that can be considered for I/O tuning, which we discussed
    in [*Chapter 7*](B14834_07_Final_ASB_ePub.xhtml#_idTextAnchor125), *Virtual Machine
    – Installation, Configuration, and Life Cycle Management*:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: Cache mode
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I/O mode
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I/O tuning
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's briefly go through some XML settings so that we can implement them on
    our VMs.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: 'The cache option settings can reflect in the guest XML, as follows:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-356
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The XML representation of I/O mode configuration is similar to the following:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'In terms of I/O tuning, a couple of additional remarks:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: Limiting the disk I/O of each guest may be required, especially when multiple
    guests exist in our setup.
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If one guest is keeping the host system busy with the number of disk I/Os generated
    from it (noisy neighbor problem), that's not fair to the other guests.
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generally speaking, it is the system/virt administrator's responsibility to
    ensure all the running guests get enough resources to work on—in other words,
    the **Quality of Service** (**QOS**).
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: 'Even though the disk I/O is not the only resource that has to be considered
    to guarantee QoS, this has some importance. Tuning I/O can prevent a guest system
    from monopolizing shared resources and lowering the performance of other guests
    running on the same host. This is really a requirement, especially when the host
    system is serving a `virsh blkdeviotune` command. The different options that can
    be set using this command are displayed as follows:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.24 – Excerpt from the virsh blkdeviotune –help command'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_15_24.jpg)'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: Figure 15.24 – Excerpt from the virsh blkdeviotune –help command
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: Details about parameters such as `total-bytes-sec`, `read-bytes-sec`, `writebytes-sec`,
    `total-iops-sec`, and so on are easy to understand from the preceding command
    output. They are also documented in the `virsh` command man page.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, to throttle the `vdb` disk on a VM called `SQLForNuma` to 200
    I/O operations per second and 50 MB-per-second throughput, run this command:'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-369
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Next, we are going to look at network I/O tuning.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: Network I/O tuning
  id: totrans-371
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'What we''ve seen in most KVM environments is that all the network traffic from
    a guest will take a single network path. There won''t be any traffic segregation,
    which causes congestion in most KVM setups. As a first step for network tuning,
    we''d advise trying different networks or dedicated networks for management, backups,
    or live migration. But when you have more than one network interface for your
    traffic, please try to avoid multiple network interfaces for the same network
    or segment. If this is at all in play, apply some network tuning that is common
    for such setups; for example, use `arp_filter` to control ARP Flux. ARP Flux happens
    when a VM has more than one network interface and is using them actively to reply
    to ARP requests, so we should do the following:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-373
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: After that, you need to edit `/etc/sysctl.conf` to make this setting persistent.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: For more information on ARP Flux, please refer to [http://linux-ip.net/html/ether-arp.html#ether-arp-flux](http://linux-ip.net/html/ether-arp.html#ether-arp-flux).
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: 'Additional tuning can be done on the driver level; that said, by now we know
    that virtio drivers give better performance compared to emulated device APIs.
    So, obviously, using the `virtio_net` driver in guest systems should be taken
    into account. When we use the `virtio_net` driver, it has a backend driver in
    `qemu` that takes care of the communication initiated from the guest network.
    Even if this was performing better, some more enhancements in this area introduced
    a new driver called `vhost_net`, which provides in-kernel virtio devices for KVM.
    Even though vhost is a common framework that can be used by different drivers,
    the network driver, `vhost_net`, was one of the first drivers. The following diagram
    will make this clearer:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.25 – The vhost_net architecture'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_15_25.jpg)'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
- en: Figure 15.25 – The vhost_net architecture
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: As you may have noticed, the number of context switches is really reduced with
    the new path of communication. The good news is that there is no extra configuration
    required in guest systems to support vhost because there is no change to the frontend
    driver.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
- en: '`vhost_net` reduces copy operations, lowers latency and CPU usage, and thus
    yields better performance. First of all, the kernel module called `vhost_net`
    (refer to the screenshot in the next section) has to be loaded in the system.
    As this is a character device inside the host system, it creates a device file
    called `/dev/vhost-net` on the host.'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: How to turn it on
  id: totrans-382
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When QEMU is launched with `-netdev tap,vhost=on`, it will instantiate the
    `vhost-net` interface by using `ioctl()` calls. This initialization process binds
    `qemu` with a `vhost-net` instance, along with other operations such as feature
    negotiations and so on:'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.26 – Checking vhost kernel modules'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_15_26.jpg)'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
- en: Figure 15.26 – Checking vhost kernel modules
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the parameters available with the `vhost_net` module is `experimental_
    zcopytx`. What does it do? This parameter controls something called bridge zero
    copy transmit. Let''s see what this means (as stated on [http://www.google.com/patents/US20110126195](http://www.google.com/patents/US20110126195)):'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: '"A system for providing a zero copy transmission in virtualization environment
    includes a hypervisor that receives a guest operating system (OS) request pertaining
    to a data packet associated with a guest application, where the data packet resides
    in a buffer of the guest OS or a buffer of the guest application and has at least
    a partial header created during the networking stack processing. The hypervisor
    further sends, to a network device driver, a request to transfer the data packet
    over a network via a network device, where the request identifies the data packet
    residing in the buffer of the guest OS or the buffer of the guest application,
    and the hypervisor refrains from copying the data packet to a hypervisor buffer."'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
- en: 'If your environment uses large packet sizes, configuring this parameter may
    have a noticeable effect. The host CPU overhead is reduced by configuring this
    parameter when the guest communicates to the external network. This does not affect
    the performance in the following scenarios:'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
- en: Guest-to-guest communication
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guest-to-host communication
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Small packet workloads
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Also, the performance improvement can be obtained by enabling multi queue `virtio-net`.
    For additional information, check out [https://fedoraproject.org/wiki/Features/MQ_virtio_net](https://fedoraproject.org/wiki/Features/MQ_virtio_net).
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
- en: One of the bottlenecks when using `virtio-net` was its single RX and TX queue.
    Even though there are more vCPUs, the networking throughput was affected by this
    limitation. `virtio-net` is a single-queue type of queue, so multi-queue `virtio-net`
    was developed. Before this option was introduced, virtual NICs could not utilize
    the multi-queue support that is available in the Linux kernel.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
- en: 'This bottleneck is lifted by introducing multi-queue support in both frontend
    and backend drivers. This also helps guests scale with more vCPUs. To start a
    guest with two queues, you could specify the `queues` parameters to both `tap`
    and `virtio-net`, as follows:'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-396
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The equivalent guest XML is as follows:'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-398
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Here, `M` can be `1` to `8`, as the kernel supports up to eight queues for
    a multi-queue tap device. Once it''s configured for `qemu`, inside the guest,
    we need to enable multi-queue support with the `ethtool` command. Enable the multi-queue
    through `ethtool` (where the value of `K` is from `1` to `M`), as follows:'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-400
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'You can check the following link to see when multi-queue `virtio-net` provides
    the greatest performance benefit: [https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/virtualization_tuning_and_optimization_guide/sect-virtualization_tuning_optimization_guide-networking-techniques](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/virtualization_tuning_and_optimization_guide/sect-virtualization_tuning_optimization_guide-networking-techniques).'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
- en: Don't use the options mentioned on the aforementioned URL blindly – please test
    the impact on your setup, because the CPU consumption will be greater in this
    scenario even though the network throughput is impressive.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
- en: KVM guest time-keeping best practices
  id: totrans-403
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are different mechanisms for time-keeping. One of the best-known techniques
    is **Network Time Protocol** (**NTP**). By using NTP, we can synchronize clocks
    to great accuracy, even when using networks that have jitter (variable latency).
    One thing that needs to be considered in a virtualization environment is the maxim
    that the guest time should be in sync with the hypervisor/host, because it affects
    a lot of guest operations and can cause unpredictable results if they are not
    in sync.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
- en: There are different ways to achieve time sync, however; it depends on the setup
    you have. We have seen people using NTP, setting the system clock from the hardware
    clock using `hwclock –s`, and so on. The first thing that needs to be considered
    here is trying to make the KVM host time in sync and stable. You can use NTP-like
    protocols to achieve this. Once it's in place, the guest time has to be kept in
    sync. Even though there are different mechanisms for doing that, the best option
    would be using `kvm-clock`.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
- en: kvm-clock
  id: totrans-406
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`kvm-clock` is also known as a virtualization-aware (paravirtualized) clock
    device. When `kvm-clock` is in use, the guest asks the hypervisor about the current
    time, guaranteeing both stable and accurate timekeeping. The functionality is
    achieved by the guest registering a page and sharing the address with the hypervisor.
    This is a shared page between the guest and the hypervisor. The hypervisor keeps
    updating this page unless it is asked to stop. The guest can simply read this
    page whenever it wants time information. However, please note that the hypervisor
    should support `kvm-clock` for the guest to use it. For more details, you can
    check out [https://lkml.org/lkml/2010/4/15/355](https://lkml.org/lkml/2010/4/15/355).'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
- en: 'By default, most of the newer Linux distributions use `kvm_clock` are configured
    inside the guest via the following method:'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-409
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'You can also use `ntpd` or `chrony` as your clock sources on Linux, which requires
    minimal configuration. In your Linux VM, edit `/etc/ntpd.conf` or `/etc/chronyd.conf`
    and modify the *server* configuration lines to point to your NTP servers by IP
    address. Then, just enable and start the service that you''re using (we''re using
    `chrony` as an example here):'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-411
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: There's another, a bit newer, protocol that's being heavily pushed for time
    synchronization, which is called the `ntpd` or `chronyd`. It uses timestamping
    on the network interface, and external sources, and the computer's system clock
    for synchronization.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
- en: 'Installing all of the necessary pre-requisites is just a matter of one `yum`
    command to enable and start a service:'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-414
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: By default, the `ptp4l` service will use the `/etc/sysconfig/ptp4l` configuration
    file, which is usually bound to the first network interface. If you want to use
    some other network interface, the simplest thing to do would be to edit the configuration
    file, change the interface name, and restart the service via `systemctl`.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, from the perspective of VMs, we can help them time sync by doing a little
    bit of configuration. We can add the `ptp_kvm` module to the global KVM host configuration,
    which is going to make our PTP as a service available to `chronyd` as a clock
    source. This way, we don''t have to do a lot of additional configuration. So,
    just add `ptp_kvm` as a string to the default KVM configuration, as follows:'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-417
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'By doing this, a `ptp` device will be created in the `/dev` directory, which
    we can then use as a `chrony` time source. Add the following line to `/etc/chrony.conf`
    and restart `chronyd`:'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-419
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: By using an API call, all Linux VMs are capable of then getting their time from
    the physical host running them.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
- en: Now that we've covered a whole bunch of VM configuration options in terms of
    performance tuning and optimization, it's time to finally step away from all of
    these micro-steps and focus on the bigger picture. Everything that we've covered
    so far in terms of VM design (related to the CPU, memory, NUMA, virtio, block,
    network, and time configuration) is only as important as what we're using it for.
    Going back to our original scenario – a SQL VM – let's see how we're going to
    configure our VM properly in terms of the software that we're going to run on
    it.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
- en: Software-based design
  id: totrans-422
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Remember our initial scenario, involving a Windows Server 2019-based VM that
    should be a node in a Microsoft SQL Server cluster? We covered a lot of the settings
    in terms of tuning, but there's more to do – much more. We need to be asking some
    questions. The sooner we ask these questions, the better, as they're going to
    have a key influence on our design.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
- en: 'Some questions we may ask are as follows:'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
- en: Excuse me, dear customer, when you say *cluster*, what do you mean specifically,
    as there are different SQL Server clustering methodologies?
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Which SQL licenses do you have or are you planning to buy?
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do you need active-active, active-passive, a backup solution, or something else?
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is this a single-site or a multi-site cluster?
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Which SQL features do you need exactly?
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Which licenses do you have and how much are you willing to spend on them?
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is your application capable of working with a SQL cluster (for example, in a
    multi-site scenario)?
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What kind of storage system do you have?
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What amount of IOPS can your storage system provide?
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How are latencies on your storage?
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do you have a storage subsystem with different tiers?
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are the service levels of these tiers in terms of IOPS and latency?
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you have multiple storage tiers, can we create SQL VMs in accordance with
    the best practices – for example, place data files and log files on separate virtual
    disks?
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do you have enough disk capacity to meet your requirements?
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These are just licensing, clustering, and storage-related questions, and they
    are not going to go away. They need to be asked, without hesitation, and we need
    to get real answers before deploying things. We have just mentioned 14 questions,
    but there are actually many more.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, we need to think about other aspects of VM design. It would be
    prudent to ask some questions such as the following:'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
- en: How much memory can you give for SQL VMs?
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Which servers do you have, which processors are they using, and how much memory
    do you have per socket?
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Are you using any latest-gen technologies, such as persistent memory?
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do you have any information about the scale and/or amount of queries that you're
    designing this SQL infrastructure for?
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is money a big deciding factor in this project (as it will influence a number
    of design decisions as SQL is licensed per core)? There's also the question of
    Standard versus Enterprise pricing.
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This stack of questions actually points to one very, very important part of
    VM design, which is related to memory, memory locality, the relationship between
    CPU and memory, and also one of the most fundamental questions of database design
    – latency. A big part of that is related to correct VM storage design – the correct
    storage controller, storage system, cache settings, and so on, and VM compute
    design – which is all about NUMA. We''ve explained all of those settings in this
    chapter. So, to configure our SQL VM properly, here''s a list of the high-level
    steps that we should follow:'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
- en: Configure a VM with the correct NUMA settings and local memory. Start with four
    vCPUs for licensing reasons and then figure out whether you need more (such as
    if your VM becomes CPU-limited, which you will see from performance graphs and
    SQL-based performance monitoring tools).
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you want to reserve CPU capacity, make use of CPU pinning so that specific
    CPU cores on the physical server's CPU is always used for the SQL VM, and only
    that. Isolate other VMs to the *remaining* cores.
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reserve memory for the SQL VM so that it doesn't swap, as only using real RAM
    memory will guarantee smooth performance that's not influenced by noisy neighbors.
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configure KSM per VM if necessary and avoid using it on SQL VMs as it might
    introduce latency. In the design phase, make sure you buy as much RAM memory as
    possible so that memory doesn't become an issue as it will be a very costly issue
    in terms of performance if a server doesn't have enough of it. Don't *ever* overcommit
    memory.
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configure the VM with multiple virtual hard disks and put those hard disks in
    storage that can provide levels of service needed in terms of latency, overhead,
    and caching. Remember, an OS disk doesn't necessarily need write caching, but
    database and log disks will benefit from it.
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use separate physical connections from your hosts to your storage devices and
    tune storage to get as much performance out of it as possible. Don't oversubscribe
    – both on the links level (too many VMs going through the same infrastructure
    to the *same* storage device) and the datastore level (don't put one datastore
    on a storage device and store all VMs on it as it will negatively impact performance
    – isolate workloads, create multiple targets via multiple links, and use masking
    and zoning).
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configure multipathing, load balancing, and failover – to get as much performance
    out of your storage, yes, but also to have redundancy.
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Install the correct virtio drivers, use vhost drivers or SR-IOV if necessary,
    and minimize the overhead on every level.
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tune the VM guest OS – turn off unnecessary services, switch the power profile
    to `High Performance` (most Microsoft OSes have a default setting that puts the
    power profile into `Balanced` mode for some reason). Tune the BIOS settings and
    check the firmware and OS updates – everything – from top to bottom. Take notes,
    measure, benchmark, and use previous benchmarks as baselines when updating and
    changing the configuration so that you know which way you're going.
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When using iSCSI, configure jumbo frames as in most use cases, this will have
    a positive influence on the storage performance, and make sure that you check
    the storage device vendor's documentation for any best practices in that regard.
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The takeway of this chapter is the following – don't just blindly install an
    application just because a client asks you to install it. It will come to haunt
    you later on, and it will be much, much more difficult to resolve any kind of
    problems and complaints. Take your time and do it right. Prepare for the whole
    process by reading the documentation, as it's widely available.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-458
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we did some digging, going deep into the land of KVM performance
    tuning and optimization. We discussed many different techniques, varying from
    simple ones, such as CPU pinning, to much more complex ones, such as NUMA and
    proper NUMA configuration. Don't be put off by this, as learning design is a process,
    and designing things correctly is a craft that can always be improved with learning
    and experience. Think of it this way – when architects were designing the highest
    skyscrapers in the world, didn't they move the goalposts farther and farther with
    each new highest building?
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter – the final chapter of this book - we will discuss troubleshooting
    your environments. It's at least partially related to this chapter, as we will
    be troubleshooting some issues related to performance as well. Go through this
    chapter multiple times before switching to the troubleshooting chapter – it will
    be very, very beneficial for your overall learning process.
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  id: totrans-461
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What is CPU pinning?
  id: totrans-462
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What does KSM do?
  id: totrans-463
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do we enhance the performance of block devices?
  id: totrans-464
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do we tune the performance of network devices?
  id: totrans-465
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How can we synchronize clocks in virtualized environments?
  id: totrans-466
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do we configure NUMA?
  id: totrans-467
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do we configure NUMA and KSM to work together?
  id: totrans-468
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  id: totrans-469
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Please refer to the following links for more information:'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
- en: 'RedHat Enterprise Linux 7 – installing, configuring, and managing VMs on a
    RHEL physical machine: [https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/virtualization_deployment_and_administration_guide/index](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/virtualization_deployment_and_administration_guide/index)'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'vCPU pinning: [http://libvirt.org/formatdomain.html#elementsCPUTuning](http://libvirt.org/formatdomain.html#elementsCPUTuning)'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'KSM kernel documentation: [https://www.kernel.org/doc/Documentation/vm/ksm.txt](https://www.kernel.org/doc/Documentation/vm/ksm.txt)'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Placement: [http://libvirt.org/formatdomain.html#elementsNUMATuning](http://libvirt.org/formatdomain.html#elementsNUMATuning)'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Automatic NUMA balancing: [https://www.redhat.com/files/summit/2014/summit2014_riel_chegu_w_0340_automatic_numa_balancing.pdf](https://www.redhat.com/files/summit/2014/summit2014_riel_chegu_w_0340_automatic_numa_balancing.pdf)'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Virtio 1.1 specification: [http://docs.oasis-open.org/virtio/virtio/v1.1/virtio-v1.1.html](http://docs.oasis-open.org/virtio/virtio/v1.1/virtio-v1.1.html)'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ARP Flux: [http://Linux-ip.net/html/ether-arp.html#ether-arp-flux](http://Linux-ip.net/html/ether-arp.html#ether-arp-flux)'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'MQ virtio: [https://fedoraproject.org/wiki/Features/MQ_virtio_net](https://fedoraproject.org/wiki/Features/MQ_virtio_net)'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'libvirt NUMA tuning on RHEL 7: [https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/virtualization_tuning_and_optimization_guide/sect-virtualization_tuning_optimization_guide-numa-numa_and_libvirt](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/virtualization_tuning_and_optimization_guide/sect-virtualization_tuning_optimization_guide-numa-numa_and_libvirt)'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
