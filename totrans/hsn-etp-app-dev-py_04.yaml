- en: Dealing with Concurrency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we saw in the previous chapter, when working on any large-scale enterprise
    application, we deal with a lot of data. This data is processed in a synchronous
    manner and the results are sent only after the data processing for a particular
    process is complete. This kind of model is absolutely fine when the data being
    processed in individual requests is not large. But consider a situation where
    a lot of data needs to be processed before a response is generated. What happens
    then? The answer is, slow application response times.
  prefs: []
  type: TYPE_NORMAL
- en: We need a better solution. A solution that will allow us to process data in
    parallel, resulting in faster application responses. But how do we achieve this?
    The answer to the question is **concurrency ...**
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The code listings in this book can be found under `chapter04` directory at [https://github.com/PacktPublishing/Hands-On-Enterprise-Application-Development-with-Python.](https://github.com/PacktPublishing/Hands-On-Enterprise-Application-Development-with-Python)
  prefs: []
  type: TYPE_NORMAL
- en: 'The code samples can be cloned by running the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The code samples mentioned in the chapter require Python 3.6 and above to run.
    A virtual environment is a preferred option to keep the dependencies segregated
    from the system.
  prefs: []
  type: TYPE_NORMAL
- en: The need for concurrency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most of the time, when we are building fairly simple applications, we do not
    require concurrency. Simple, sequential programming works just fine, in which
    one step executes after the completion of another. But as application use cases
    become more and more complex, and there are an increased number of tasks that
    can easily be pushed into the background to improve the application's user experience,
    we end up revolving around the concept of concurrency.
  prefs: []
  type: TYPE_NORMAL
- en: Concurrency is a different beast in itself and makes the task of programming
    much more complex. But regardless of the added complexity, concurrency also brings
    a lot of features to improve the user experience of applications.
  prefs: []
  type: TYPE_NORMAL
- en: Before we dive into the question of why we ...
  prefs: []
  type: TYPE_NORMAL
- en: Concurrency in GUI applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The hardware we have become accustomed to using has became more and more powerful
    with each passing year. Today, even the CPUs inside our smartphones have quad-core
    or octa-core configurations. These configurations allow the running of multiple
    processes or threads in parallel. Not exploiting the power of concurrency would
    be a waste of the hardware improvements mentioned previously. Today, when we open
    applications on our smartphones, most of them have two or more threads running,
    though we are unaware of that most of the time.
  prefs: []
  type: TYPE_NORMAL
- en: Let's consider a fairly simple example of opening up a photo gallery application
    on our device. As soon as we open the photo gallery, an application process is
    started. This process is responsible for loading up the GUI of the application.
    The GUI runs in the main thread and allows us to interact with the application.
    Now, this application also spawns another background thread, which is responsible
    for traversing through the filesystem of the OS and loading up the thumbnails
    of the photos. This loading up of thumbnails from the filesystem can be a tedious
    task and may take some time, depending on how many thumbnails need to be loaded.
  prefs: []
  type: TYPE_NORMAL
- en: Though we do notice that the thumbnails are slowly loading, throughout this
    whole time, our application GUI remains responsive and we can interact with it,
    see the progress, and so on. All of this is made possible through the use of concurrent
    programming.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine if concurrency hadn't been used here. The application would have been
    loading the thumbnails in the main thread itself. This would have caused the GUI
    to become unresponsive until the main thread finished loading the thumbnails.
    Not only would this have been very unintuitive, it also would have caused a bad
    user experience, which we avoided with the help of concurrent programming.
  prefs: []
  type: TYPE_NORMAL
- en: Now we have a fair idea of how concurrent programming can prove to be of great
    use, let's see how it can help us with the design and development of enterprise
    applications and what can be achieved with it.
  prefs: []
  type: TYPE_NORMAL
- en: Concurrency in enterprise applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Enterprise applications are large and usually deal with a lot of user-initiated
    actions such as data retrieval, updates, and so on. Now, let's take a short example
    scenario for our BugZot application, where a user may submit a graphic attachment
    along with their bug report. This is actually quite a common process when filing
    a bug that may affect the application UI or that displays an error on the UI.
    Now, every user may submit an image, which may differ in quality and hence their
    sizes may vary. This may involve images that are very small in size and  images
    that may have very large sizes and high resolutions. As an application developer,
    you may know that storing an image with 100% quality can, at ...
  prefs: []
  type: TYPE_NORMAL
- en: Concurrent programming with Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Python provides a number of ways through which parallelism or concurrency can
    be achieved. All of these methods have their own pros and cons, and differ fundamentally
    in terms of how they are implemented, and a choice needs to be made about which
    method to use when, keeping the use case in mind.
  prefs: []
  type: TYPE_NORMAL
- en: One of the methods provided by Python for implementing concurrency is performed
    at the thread level by allowing the application to launch multiple threads, each
    executing a job. These threads provide an easy-to-use concurrency mechanism and
    execute inside a single Python interpreter process, and hence are lightweight.
  prefs: []
  type: TYPE_NORMAL
- en: Another mechanism for achieving parallelism is through the use of multiple processes
    in place of multiple threads. With this approach, every process performs a separate
    task inside its own separate Python interpreter process. This approach provides
    some workarounds to the problems that a multithreaded Python program may face
    in the presence of the **Global Interpreter Lock** (**GIL**), which we will discuss
    in later sections of the chapter, but may also add to the additional overhead
    of managing multiple processes and increased memory usage.
  prefs: []
  type: TYPE_NORMAL
- en: So, first let's take a look at how we can achieve concurrency with the use of
    threads and discuss the benefits and drawbacks they come packaged with.
  prefs: []
  type: TYPE_NORMAL
- en: Concurrency with multithreading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In most modern processor systems, the use of multithreading is commonplace.
    With CPUs coming with more than one core and technologies such as hyper-threading,
    which allows a single core to run multiple threads at the same time, application
    developers do not waste a single chance to exploit the advantages provided by
    these technologies.
  prefs: []
  type: TYPE_NORMAL
- en: Python as a programming language supports the implementation of multithreading
    through the use of a threading module that allows developers to exploit thread-level
    parallelism in the application.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following example showcases how a simple program can be built using the
    threading module in Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Thread synchronization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we explored in the previous section, although threads can be implemented
    quite easily in Python, they do come with their own gotchas, which need to be
    taken care of when trying to write an application that is being targeted for production
    use cases. If these gotchas are not taken care of at the time of application development,
    they will produce hard-to-debug behaviors, which concurrent programs are quite
    famous for.
  prefs: []
  type: TYPE_NORMAL
- en: So, let's try to find out how we can work around the problem we discussed in
    the previous section. If we think hard, we can categorize the problem as a problem
    with the synchronization of multiple threads. The optimal behavior for the application
    would be to synchronize the writes to the file in such a way that only one thread
    is able to write to the file at any given point in time. This would enforce that
    no thread can start a write operation until one of the already-executing threads
    has completed its writes.
  prefs: []
  type: TYPE_NORMAL
- en: To implement such synchronization, we can leverage the power of locking. Locks
    provide a simple way to implement synchronization. For example, a thread that
    is going to start its write operation will first acquire a lock. If lock acquisition
    is successful, the thread can then progress to perform its write operation. Now,
    if a context switch happens in between and another thread is about to start a
    write operation, it will block, since the lock has already been taken. This will
    prevent the thread from writing the data in between an already-running write operation.
  prefs: []
  type: TYPE_NORMAL
- en: In Python multithreading, we can implement locks through the use of the `threading.Lock`
    class. The class provides two methods that facilitate the acquisition and release
    of locks. The `acquire()` method is called by the thread when it wants to acquire
    a lock before executing an operation. Once the lock is acquired, the thread continues
    with the execution of the operation. As soon as the operations of the threads
    are finished, the thread calls the `release()` method to release the lock such
    that the lock can be acquired by another thread that may be waiting for it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see how we can use locks to synchronize the threaded operations in our
    JSON to YAML converter example. The following code sample showcases the use of
    locks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we first create a `lock` variable by creating an instance of
    the `threading.Lock` class. This instance is then passed to all our threads that
    need to be synchronized. When a thread has to do a write operation, it first proceeds
    by acquiring a lock and then starting the writes. Once these writes are completed,
    the thread releases the lock for acquisition by the other threads.
  prefs: []
  type: TYPE_NORMAL
- en: If a thread acquires a lock but forgets to release it, the program may get into
    a state of deadlock since no other thread will be able to proceed. Proper caution
    should be taken so that the acquired locks are released once the thread finishes
    its operations, to avoid deadlocks.
  prefs: []
  type: TYPE_NORMAL
- en: Re-entrant locks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Beyond the `threading.Lock` class, which provides the general locking mechanism
    for multithreading, where a lock can only be acquired once until it is released,
    Python also provides another locking mechanism that might be useful for programs
    that implement recursive operations. This lock, known as a re-entrant lock and
    implemented using the `threading.RLock` class, can be used by recursive functions.
    The class provides similar methods to those provided by the lock class: `acquire()`
    and `release()`, which are to acquire and release the taken locks, respectively.
    The only difference occurs when a recursive function calls `acquire()` multiple
    times across the call stack. When the same function calls the acquire method again
    and again, ...'
  prefs: []
  type: TYPE_NORMAL
- en: Condition variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s imagine that somehow, we had a way through which we could tell our `Thread-1`
    to wait until `Thread-2` has made some data available for consumption. This is
    exactly what condition variables allow us to do. They allow us to synchronize
    two threads that depend on a shared resource. To understand more about this, let''s
    take a look at the following code sample, which creates two threads, one that
    feeds in the email ID and another thread that is responsible for sending the emails:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: In this code example, we defined two classes, namely, `EmailQueue`, which plays
    the role of producer and populates the email queue with email addresses on which
    the email needs to be sent. Then there is another class, `EmailSender`, which
    plays the role of the consumer and consumes the email addresses from the email
    queue and sends a mail to them.
  prefs: []
  type: TYPE_NORMAL
- en: Now, inside the `__init__` method of `EmailQueue`, we take in a Python list
    that we will use as a queue as a parameter, a variable defining how many items
    the list should hold at most, and a condition variable.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we have a method, `add_recipient`, which appends a new email ID inside
    an internal data structure of the `EmailQueue` to hold the email addresses temporarily
    until they are added to the sending queue.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's move inside the `run()`method where the actual magic happens. First,
    we start an infinite loop to keep the thread in always running mode. Next, we
    acquire a lock by calling the `acquire()`method of the condition variable. We
    do this so as to prevent any kind of corruption of our data structures if the
    thread switches the context at an unexpected time.
  prefs: []
  type: TYPE_NORMAL
- en: Once we have acquired the lock, we then check whether our email queue is full
    or not. If it is full, we print a message and make a call to the `wait()`method
    of the condition variable. The call to the `wait()`method releases the lock acquired 
    by the condition variable and makes the thread enter a blocking state. This blocking
    state will be over only when a `notify()` method is called on the condition variable.
    Now, when the thread receives a signal through `notify()`, it continues its operations,
    in which it first checks whether it has some data in the internal queue. If it
    finds some data in the internal queue, then it populates the email queue with
    that data and calls the `notify()`method of the conditional variable to inform
    the `EmailSender` consumer thread. Now, let's take a look at the `EmailSender`
    class.
  prefs: []
  type: TYPE_NORMAL
- en: Without going through every single line here, let's keep our focus on the `run()`method
    of the `EmailSender` class. Since this thread needs to always be running, we first
    start an infinite loop to do that. Then, the next thing we do is, acquire a lock
    on the shared condition variable. Once we have acquired the lock, we are now ready
    to manipulate the shared `email_queue` data structure. So, the first thing our
    consumer does is, check whether the email queue is empty or not. If it finds the
    queue to be empty, our consumer will call the `wait()`method of the condition
    variable, effectively causing it to release the lock and go into a blocking state
    until there is some data inside the email queue. This causes the transfer of control
    to the `EmailQueue` class, which is responsible for populating the queue.
  prefs: []
  type: TYPE_NORMAL
- en: Now, once the email queue has some email IDs in it, the consumer will start
    sending the mails. Once it exhausts the queue, it signals the `EmailSender` class
    about that by calling the condition variables `notify` method. This will allow
    the `EmailSender` to continue its operation of populating the email queue.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at what happens when we try to execute the previous example
    program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: With this example, we now have an understanding of how condition variables can
    be used in Python to solve producer-consumer problems. With this knowledge in
    mind, now let's take a look at some of the issues that can may arise when performing
    multithreading in our applications.
  prefs: []
  type: TYPE_NORMAL
- en: Common pitfalls with multithreading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Multithreading provides a lot of benefits but also comes with some pitfalls.
    These pitfalls, if not avoided, can prove to be a painful experience when the
    application goes into production. These pitfalls usually result in unexpected
    behaviors that may take place only once in a while, or may occur on every execution
    of a particular module. The painful thing about this is it is really hard to debug
    these problems when they are caused by the execution of multiple threads, since
    it is quite hard to predict when a particular thread will execute. So, it makes
    it worthwhile to discuss why these common pitfalls occur and how they can be avoided
    during the development stage itself.
  prefs: []
  type: TYPE_NORMAL
- en: Some of the common reasons for ...
  prefs: []
  type: TYPE_NORMAL
- en: Race conditions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the context of multithreading, a race condition is a situation where two
    or more threads try to modify a shared data structure at the same time, but due
    to the way the threads are scheduled and executed, the shared data structure is
    modified in a way that leaves it in an inconsistent state.
  prefs: []
  type: TYPE_NORMAL
- en: 'Is this statement confusing? No worries, let''s try to understand it with an
    example:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider our previous example of the JSON to YAML converter problem. Now, let''s
    assume that we did not use locks when we were writing the converted YAML output
    to the file. Now consider this: we have two threads, named `writer-1` and `writer-2`
    which are responsible for writing to the common YAML file. Now, imagine both the
    `writer-1` and `writer-2`, threads have started their operations of writing to
    the file and, with the way the operating system scheduled the threads to execute,
    `writer-1` starts writing to the file. Now, while the `writer-1` thread was writing
    to the file, the operating system decided that the thread finished its quota of
    time and swaps that thread with the `writer-2` thread. Now, one thing to note
    here is that the `writer-1` thread had not completed writing all the data when
    it was swapped. Now, the `writer-2` thread starts executing and completes writing
    of data in the YAML file. Upon completion of the `writer-2` thread, the OS then
    starts executing the `writer-1` thread again which starts to write the remaining
    data again to the YAML file and then finishes.'
  prefs: []
  type: TYPE_NORMAL
- en: Now, when we open the YAML file, what we see is a file with data mingled up
    from two writer threads, and hence, leaves our file in an inconsistent state.
    A problem such as what happened between the `writer-1` and `writer-2` threads
    is known as a race condition.
  prefs: []
  type: TYPE_NORMAL
- en: Race conditions come under the category of problems that are very hard to debug,
    since the order in which the threads will execute depends on machine to machine
    and OS to OS. So, a problem that may occur on one deployment may not occur on
    another deployment.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, how do we avoid race conditions? Well, we already have the answers to the
    question and we have just recently used them. So, let''s take a look at some of
    the ways in which race conditions can be prevented:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Utilizing locks in critical regions**: Critical regions refer to those areas
    of code where a shared variable is being modified by a thread. To prevent race
    conditions from happening in critical regions, we can use locks. A lock essentially
    causes all the threads to block except the thread that holds the lock. All the
    other threads that need to modify the shared resource will execute only when the
    thread that is currently holding the lock releases it. Some of the categories
    of locks that can be used are mutex locks, which can only be held by a single
    thread at a time; re-entrant locks, which allow a recursive function to take multiple
    locks on the same shared resource; and condition objects, which can be used to
    synchronize execution in producer-consumer type environments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Utilizing thread-safe data structures**: One other way of preventing race
    conditions is by using thread-safe data structures. A thread-safe data structure
    is one that will automatically manage the modifications being made to it by multiple
    threads and will serialize their operations. One of the thread-safe shared data
    structures that is provided by Python is a queue. A queue can be used easily when
    the operation involves multiple threads.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, we have an idea about what race conditions are, how they happen, and how
    they can be avoided. With this in mind, let's take a look at one of the other
    pitfalls that can arise due to the way we prevent race conditions from happening.
  prefs: []
  type: TYPE_NORMAL
- en: Deadlocks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A deadlock is a situation when two or more threads are blocked forever because
    they depend on each other or a resource that never gets freed up. Let''s try to
    understand how a deadlock occurs by taking a simple example:'
  prefs: []
  type: TYPE_NORMAL
- en: Consider our previous example of  the JSON to YAML converter. Now, let's assume
    we had used locks in our threads such that when a thread starts to write to the
    file, it first takes a mutex lock on the file. Now, until this mutex lock is freed
    up by the thread, other thread cannot execute.
  prefs: []
  type: TYPE_NORMAL
- en: So, let's imagine the same situation with two threads, `writer-1` and `writer-2`,
    which are trying to write to the common output file. Now, when `writer-1` starts
    to execute, it first acquires a lock on the file and starts its operation. ...
  prefs: []
  type: TYPE_NORMAL
- en: The story of GIL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What if someone told you that, even though you have created a multithreaded
    program, only a single thread can execute at a time? This situation used to be
    true when systems consisted of a single core that could execute only one thread
    at a time, and the illusion of multiple running threads was created by the CPU
    switching between threads frequently.
  prefs: []
  type: TYPE_NORMAL
- en: But this situation is also true in one of the implementations of Python. The
    original implementation of Python, also known as CPython consists of a global
    mutex also known as a GIL, which allows only one thread to execute the Python
    bytecode at a time. This effectively limits the application to executing only
    one thread at a time.
  prefs: []
  type: TYPE_NORMAL
- en: The GIL was introduced in CPython because of the fact that the CPython interpreter
    wasn't thread-safe. The GIL proved to be an effective way to workaround the thread-safety
    issues by trading the properties of running multiple threads concurrently.
  prefs: []
  type: TYPE_NORMAL
- en: The existence of GIL has been a highly debated topic in the Python community
    and a lot of proposals have been made to eliminate it, but none of the proposals
    have made it to a production version of Python, for various reasons, which include
    the performance impact on single-threaded applications, breaking the backward
    compatibility of features that have grown to be dependent upon the presence of
    the GIL, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: So, what does the presence of GIL mean for your multithreaded application? Effectively,
    if your applications exploit multithreading to perform I/O workloads, then you
    might not be impacted on that much in terms of performance loss due to GIL, since
    most of the I/O happens outside the GIL, and hence multiple threads can be multiplexed. The
    impact of GIL will be felt only when the application uses multiple threads to
    perform CPU-intensive tasks that require heavy manipulation of application-specific
    data structures. Since all data structure manipulation involves the execution
    of Python bytecode, the GIL will severely limit the performance of a multithreaded
    application by not allowing more than one thread to execute concurrently.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, is there a workaround for the problem that GIL causes? The answer to this
    is yes, but which solution should be adopted depends completely on the use case
    of the application. The following options can prove to be of help for avoiding
    the GIL:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Switching the Python implementation:** If your application does not necessarily
    depend on the underlying Python implementation and a switch to another implementation
    can be made, then there are some Python implementations that do not come with
    GIL. Some of the implementations that do not have GIL in place are: Jython and
    IronPython, which can completely exploit multiprocessor systems to execute multithreaded
    applications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Utilizing multiprocessing:** Python has a lot of options when it comes to
    building programs with concurrency in mind. We explored multithreading, which
    is one of the options for implementing concurrency but is limited by the GIL.
    Another option for achieving concurrency is by using Python''s multiprocessing
    capabilities, which allow the launching of multiple processes to execute tasks
    in parallel. Since every process runs in its own instance of Python interpreter,
    the GIL doesn''t become an issue here and allows for the full exploitation of
    the multiprocessor systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With the knowledge of how GIL impacts multithreaded applications, let's now
    discuss how multiprocessing can help you to overcome the limitations of concurrency.
  prefs: []
  type: TYPE_NORMAL
- en: Concurrency with multiprocessing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Python language provides some quiet easy ways to achieve concurrency in
    applications. We saw this with the Python threading library and the same is true
    for the Python multiprocessing capabilities too.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to build concurrency in your program with the help of multiprocessing,
    it is quite easy to achieve, all thanks to the Python multiprocessing library
    and the APIs exposed by the library.
  prefs: []
  type: TYPE_NORMAL
- en: So, what do we mean when we say that we will implement concurrency by using
    multiprocessing. Let's try to answer this. Usually, when we talk about concurrency,
    there are two methods that can help us achieve it. One of those methods is running
    a single application instance and allowing it to use multiple threads. ...
  prefs: []
  type: TYPE_NORMAL
- en: Python multiprocessing module
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Python provides an easy way to implement a multiprocess program. This ease of
    implementation is facilitated by the Python multiprocessing module, which provides
    important classes, such as the `Process` class to start new processes; the `Queue`, and `Pipe` classes
    to facilitate communication between multiple processes; and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following example provides a quick overview of how to use Python''s multiprocessing
    library to create a URL loader that executes as a separate process to load a URL:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we created a simple program using the Python multiprocessing
    library, which loads a URL in the background and prints its information to `stdout`. The
    interesting bit here is understanding how easily we spawned a new process in our
    program. So, let's take a look. To achieve multiprocessing, we first import the `Process` class
    from Python's multiprocessing module. The next step is to create a function that
    takes the URL to load as a parameter and then loads that URL using Python's `urllib` module. Once
    the URL is loaded, we print the data from the URL to `stdout`.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we define the code that runs when the program starts executing. Here,
    we have first defined the URL we want to load with the `url` variable. The next
    bit is where we introduce the multiprocessing in our program by creating an object
    of the `Process` class. For this object, we provide the target parameter as the
    function we want to execute. This is similar to the target method we have grown
    accustomed to while using the Python `threading` library. The next parameter to
    the `Process` constructor is the `args` parameter, which takes in the arguments
    that need to be passed to the target function while calling it.
  prefs: []
  type: TYPE_NORMAL
- en: To spawn a new process, we make a call to the `start()`method of the `Process` object. This
    spawns a new process in which our target function starts executing and doing its
    magic. The last thing we do is to wait for this spawned process to exit by calling
    the `join()` method of the `Process` class.
  prefs: []
  type: TYPE_NORMAL
- en: This is as simple as it gets to create a multiprocess application in Python.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we know how to create a multiprocess application in Python, but how do
    we divide a particular set of tasks between multiple processes. Well, that''s
    quite easy. The following code sample modifies the entrypoint code from our previous
    example to exploit the power of the `Pool` class from the multiprocessing module
    to achieve this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we used the `Pool` class from the multiprocessing library to
    create a pool of four processes that will execute our code. Using the `map` method
    of the `Pool` class, we then map the input data to the executing function in a
    separate process to achieve concurrency.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we have multiple processes churning through our tasks. But what if we wanted
    to make these processes communicate with each other. For example, in the previous
    problem of URL loading, instead of printing the data on `stdout`, we wanted the
    process to return that data instead? The answer to this lies in the use of *pipe*, which
    provides a two-way mechanism for the processes to communicate with each other.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following example utilizes pipes to make the URL loader send the data loaded
    from the URL back to the parent process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we have used pipes to provide a two-way communication mechanism
    for the parent and child processes to talk to each other. When we make a call
    to the `pipe` constructor inside the `__main__` section of the code, the constructor
    returns a pair of connection objects. Each of these connection objects contains
    a `send()` and a `recv()` method facilitating communication between the ends. Data
    sent from the `child_pipe` using the `send()`method can be read by the `parent_pipe` using
    the `recv()`method of the `parent_pipe` and vice versa.
  prefs: []
  type: TYPE_NORMAL
- en: If two processes read or write from/to the same end of pipe at the same time,
    there is the potential for possible data corruption in the pipe. Although, if
    the processes are using two different ends or two different pipes, this does not
    become an issue. Only the data that can be pickled can be sent through the pipes.
    This is one of the limitations of the Python multiprocessing module.
  prefs: []
  type: TYPE_NORMAL
- en: Synchronizing processes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As much as synchronizing the actions of the threads was important, the synchronizing
    of actions inside the context of multiprocessing is also important. Since multiple
    processes may be accessing the same shared resource, their access to shared resource
    needs to be serialized. To help achieve this, we have the support of locks here
    too.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following example showcases how to use locks in the context of the multiprocessing
    module to synchronize the operations of multiple processes by fetching the HTML
    associated with the URLs and writing that HTML to a common local file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored how to achieve concurrency in Python applications
    and how it can be useful. During this exploration, we uncovered the capabilities
    of the Python multithreading module and how it can be used to spawn multiple threads
    to divide workloads on. We then moved on to understand how to synchronize the
    actions of those threads and learned about various issues that may crop up in
    a multithreaded application, if not taken care of. The chapter then moved on to
    explore the limitations that are imposed by the presence of the **global interpreter
    lock** (**GIL**)in some Python implementations and how it affects multithreaded
    workloads. To explore possible ways to overcome the restrictions imposed by the
    GIL, we moved on to understand the use of Python's multiprocessing module and
    how it can help us to leverage the full potential of a multiprocessor system by
    achieving parallelism powered by the use of multiple processes instead of multiple
    threads.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What are the different methods through which Python enables the building of
    concurrent applications?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What happens to an acquired lock if the thread that has acquired it terminates
    abruptly?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How can we terminate executing threads when the application receives a termination
    signal?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How can we share state between multiple processes?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is there a way through which we can create a pool of processes that can then
    be used to work on the incoming set of tasks in a task queue?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
