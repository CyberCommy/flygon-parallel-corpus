- en: Creating Artifact Report Recipes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Using HTML templates
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a paper trail
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with CSVs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualizing events with Excel
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Auditing your work
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Probably within the first few hours of starting your career in cyber security,
    you were already hunched over a screen, feverishly scanning a spreadsheet for
    clues. This sounds familiar because it is true and part of the daily process for
    most investigations. Spreadsheets are the bread and butter of cyber security.
    Within them are details of various processes and specific information extracted
    from valuable artifacts. In this cookbook, we will frequently output parsed artifact
    data into a spreadsheet due to its portability and ease of use. However, considering
    that at one time or another every cyber security professional has created a technical
    report for a nontechnical audience, a spreadsheet may not be the best option.
  prefs: []
  type: TYPE_NORMAL
- en: Why create reports at all? I think I've heard that muttered by stressed examiners
    before. Today, everything is built on information interchange and people want
    to know things as soon as you do. But that doesn't necessarily mean they want
    a technical spreadsheet and to figure it out themselves. Examiners must be able
    to effectively distill technical knowledge to laymen audiences in order to properly
    do their job. As good as an artifact may be, even if it is the proverbial smoking
    gun for a given case, it will likely require detailed explanation to nontechnical
    individuals for them to fully understand the meaning and ramifications. Give up;
    reports are here to stay and there's nothing that can be done about that.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, you will learn how to create a number of different types of
    reports and a script to automatically audit our investigation. We will create
    HTML, XLSX, and CSV reports to summarize data in a meaningful manner:'
  prefs: []
  type: TYPE_NORMAL
- en: Developing an HTML dashboard template
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parsing FTK Imager acquisition logs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a robust CSV writer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Plotting charts and data with Microsoft Excel
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating an audit trail of screenshots throughout an investigation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visit [www.packtpub.com/books/content/support](http://www.packtpub.com/books/content/support)
    to download the code bundle for this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Using HTML templates
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Recipe Difficulty: Easy'
  prefs: []
  type: TYPE_NORMAL
- en: 'Python Version: 2.7 or 3.5'
  prefs: []
  type: TYPE_NORMAL
- en: 'Operating System: Any'
  prefs: []
  type: TYPE_NORMAL
- en: HTML can be an effective medium for a report. There are a great number of snazzy
    templates out there that can make even technical reports look appealing. That's
    the first step towards hooking the audience. Or, at the very least, a preventative
    measure to forestall the audience from instantly nodding off. This recipe uses
    one such template and some test data to create a visually compelling example of
    acquisition details. We really have our work cut out for us here.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This recipe introduces HTML templating with the `jinja2` module. The `jinja2`
    library is a very powerful tool and has a number of different documented features.
    We will be using it in a rather simple scenario. All other libraries used in this
    script are present in Python''s standard library. We can use pip to install `jinja2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In addition to `jinja2`, we will also be using a slightly modified template,
    called light bootstrap dashboard. This slightly modified dashboard has been provided
    with the recipe's code bundle.
  prefs: []
  type: TYPE_NORMAL
- en: To learn more about the `jinja2` library, visit [http://jinja.pocoo.org/docs/2.9/](http://jinja.pocoo.org/docs/2.9/).
  prefs: []
  type: TYPE_NORMAL
- en: To download the light bootstrap dashboard, visit [https://www.creative-tim.com/product/light-bootstrap-dashboard](https://www.creative-tim.com/product/light-bootstrap-dashboard).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We deploy an HTML dashboard following these principles:'
  prefs: []
  type: TYPE_NORMAL
- en: Design HTML template global variables.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Process the test acquisition metadata.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Render the HTML templates with the inserted acquisition metadata.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a report in the desired output directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, we import the required libraries to handle argument parsing, creating
    counts of objects, and copying files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'This recipe''s command-line handler takes one positional argument, `OUTPUT_DIR`,
    which represents the desired output path for the HTML dashboard. After checking
    whether the directory exists, and creating it if it doesn''t, we call the `main()`
    function and pass the output directory to it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Defined at the top of the script are a number of global variables: `DASH`,
    `TABLE`, and `DEMO`. These variables represent the various HTML and JavaScript
    files we create as a product of the script. This is a book about Python, so we
    will not get into the details of how these files are structured and how they work.
    However, let''s look at an example to showcase how `jinja2` bridges the gap between
    these types of files and Python.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A portion of the global variable `DEMO` is captured in the following snippet.
    Note that the string block is passed to the `jinja2.Template()` method. This allows
    us to create an object for which we can use `jinja2` to interact with and dynamically
    insert data into the JavaScript file. Specifically, the following code block shows
    two locations where we can use `jinja2` to insert data. These are denoted by the
    double curly braces and the keywords we will refer to them by in the Python code
    - `pi_labels` and `pi_series`, respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s now turn our attention to the `main()` function. This function is really
    quite simple for reasons you will understand in the second recipe. This function
    creates a list of lists containing sample acquisition data, prints a status message
    to the console, and sends that data to the `process_data()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The purpose of the `process_data()` method is to get the sample acquisition
    data into an HTML or JavaScript format that we can drop in place within the `jinja2`
    templates. This dashboard is going to have two components: a series of charts
    visualizing the data and a table of the raw data. The following code block deals
    with the latter. We accomplish this by iterating through the acquisition list
    and adding each element of the table to the `html_table` string with the appropriate
    HTML tags:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Next, we use the `Counter()` method from the `collections` library to quickly
    generate a dictionary-like object of the number of occurrences of each item in
    the sample data. For example, the first `Counter` object, `device_types`, creates
    a dictionary-like object where each key is a different device type (for example,
    mobile, external, and computer) and the value represents the number of occurrences
    of each key. This allows us to quickly summarize data across the data set and
    cuts down on the legwork required before we can plot this information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have created the `Counter` objects, we again iterate through each acquisition
    to perform a more manual summation of acquisition date information. This `date_dict`
    object maintains keys for all the acquisition data and adds the size of all acquisitions
    made on that day as the key''s value. We specifically split on a space to isolate
    just the date value from the date-time string (for example, `08/15/2017`). If
    the specific date is already in the dictionary, we add the acquisition size directly
    to the key. Otherwise, we create the key and assign its value to the acquisition
    size. Once we have created the various summarizing objects, we call the `output_html()`
    method to populate the HTML dashboard with this information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The `output_html()` method starts by printing a status message to the console
    and storing the current working directory to a variable. We append the folder
    path to light-bootstrap-dashboard and use `shutil.copytree()` to copy the bootstrap
    files to the output directory. Following that, we create three file paths representing
    the output locations and names of the three `jinja2` templates:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s start by looking at the two HTML files, as these are relatively simple.
    After opening file objects for the two HTML files, we use the `jinja2.render()`
    method and use keyword arguments to refer to the placeholders in the curly brackets
    from the `Template` objects. With the file rendered with the Python data, we write
    the data to the file. Simple, right? The JavaScript file, thankfully, is not much
    more difficult:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'While syntactically similar to the previous code block, when we render the
    data this time, we feed the data to the `return_labels()` and `return_series()`
    methods. These methods take the key and values from the `Counter` objects and
    format them appropriately to work with the JavaScript file. You may have also
    noticed a call to the `calculate_size()` method in the previous code block called
    on the `dates` dictionary. Let''s explore these three supporting functions now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The `calculate_size()` method simply uses the built-in `sum()` method to return
    each date key''s total size collected. The `return_labels()` and `return_series()`
    methods use string methods to format the data appropriately. Essentially, the
    JavaScript file expects the labels to be within single quotes, which is accomplished
    with the `format()` method, and both labels and series must be comma-delimited:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'When we run this script, we receive a copy of the report in the specified output
    directory along with the required assets for loading and rendering the page. We
    can zip up this folder and provide it to team members, as it is designed to be
    portable. Viewing this dashboard shows us the first page with the chart information:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00015.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'And the second page as the table of acquisition information:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00016.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This script can be further improved. We have provided a couple of recommendations
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: Add support for additional types of reports to better highlight the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Include the ability to export the tables and charts for printing and sharing
    through additional javascript
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a paper trail
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Recipe Difficulty: Medium'
  prefs: []
  type: TYPE_NORMAL
- en: 'Python Version: 2.7 or 3.5'
  prefs: []
  type: TYPE_NORMAL
- en: 'Operating System: Any'
  prefs: []
  type: TYPE_NORMAL
- en: Most imaging utilities create audit logs recording the details of the acquisition
    media and other available metadata. Admit it; unless something goes horribly wrong,
    these logs are mostly untouched if the evidence verifies. Let's change that and
    leverage the newly created HTML dashboard from the previous recipe and make better
    use of this acquisition data.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All libraries used in this script are present in Python's standard library or
    functions imported from the prior script.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We parse acquisition logs with these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Identify and validate FTK logs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Parse the log to extract relevant fields.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a dashboard with the acquisition data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, we import the required libraries to handle argument parsing, parsing
    dates, and the `html_dashboard` script we created in the previous recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'This recipe''s command-line handler takes two positional arguments, `INPUT_DIR`
    and `OUTPUT_DIR`, which represent the path to the directory containing acquisition
    logs and the desired output path, respectively. After creating the output directory,
    if necessary, and validating that the input directory exists, we call the `main()`
    method and pass these two variables to it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'In the `main()` function, we use the `os.listdir()` function to get a directory
    listing of the input directory and identify only those files with a `.txt` file
    extension. This is important, as FTK Imager creates acquisition logs with the
    `.txt` extension. This helps us avoid some files that should not be processed
    by the extension alone. We will, however, take it one step further. After we create
    a list of the possible FTK logs, we create a placeholder list, `ftk_data`, to
    store the processed acquisition data. Next, we iterate through each potential
    log and set up a dictionary with the desired keys we will extract. To further
    eliminate false positives, we call the `validate_ftk()` method, which returns
    either a `True` or `False` Boolean value depending on the results of its inspection.
    Let''s take a quick look at how it works:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Thankfully, each FTK Imager log contains the words `"Created by AccessData"`
    on the first line. We can rely on this to be the case to verify that the log is
    likely a valid FTK Imager log. With the input `log_file` path, we open the file
    object and read the first line using the `readline()` method. With the first line
    extracted, we check whether the phrase is present and return `True` if it is or
    `False` otherwise:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Back in the `main()` method, after having validated the FTK Imager log, we open
    the file, set a few variables to `None`, and begin iterating through each line
    in the file. Based on the dependable layout of these logs, we can use specific
    keywords to identify whether the current line is one we are interested in. For
    example, if the line contains the phrase `"Evidence Number:"`, we can be sure
    that this line contains the evidence number value. And in fact, we split the phrase
    and take the value to the right of the colon and associate it with the dictionary
    `e_numb` key. This type of logic can be applied to most of the desired values,
    with a few exceptions.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the acquisition time, we must use the `datetime.strptime()` method to convert
    the string into an actual `datetime` object. We must do this to store it in the
    format that the HTML dashboard is expecting. We use the `strftime()` method on
    the `datetime` object and associate it with the `date` key in the dictionary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The bytes per sector and sector count are handled a little differently from
    the rest. Due to the fact that the HTML dashboard script is expecting to receive
    the data size (in GB), we need to extract these values and calculate the acquired
    media size. To do this, once identified, we convert each value into an integer
    and assign it to the two local variables that were originally `None`. Once we
    finish iterating through all lines, we check whether these variables are no longer
    `None`, and if they are not, we send them to the `calculate_size()` method. This
    method performs the necessary calculation and stores the media size within the
    dictionary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the file has been processed, the dictionary with the extracted acquisition
    data is appended to the `ftk_data` list. After all the logs have been processed,
    we call the `html_dashboard.process_data()` method and supply it with the acquisition
    data and output directory. The `process_data()` function is, of course, the exact
    same as the previous recipe. Therefore, you know that this acquisition data replaces
    the sample acquisition data of the previous recipe and populates the HTML dashboard
    with real data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'When we run this tool, we can see the acquisition log information, as shown
    in the following two screenshots:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00017.jpeg)![](../images/00018.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This script can be further improved. Here''s a recommendation:'
  prefs: []
  type: TYPE_NORMAL
- en: Create additional scripts to support logs from other acquisition tools, such
    as **Guymager**, **Cellebrite**, **MacQuisition**, and so on
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with CSVs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Recipe Difficulty: Easy'
  prefs: []
  type: TYPE_NORMAL
- en: 'Python Version: 2.7 or 3.5'
  prefs: []
  type: TYPE_NORMAL
- en: 'Operating System: Any'
  prefs: []
  type: TYPE_NORMAL
- en: Everyone has reviewed data in a CSV spreadsheet at some point. They are pervasive
    and a common output format for most applications. Writing CSVs with Python is
    one of the easiest methods to create a report of processed data. In this recipe,
    we will demonstrate how you can use the `csv` and `unicodecsv` libraries to create
    quick reports with Python.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Part of this recipe uses the `unicodecsv` module. This module replaces the
    built-in Python 2 `csv` module and adds Unicode support. Python 3''s `csv` module
    does not have this limitation and can be used without the support of any additional
    library. All other libraries used in this script are present in Python''s standard
    library. The `unicodecsv` library can be installed with `pip`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: To learn more about the `unicodecsv` library, visit [https://github.com/jdunck/python-unicodecsv](https://github.com/jdunck/python-unicodecsv).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We follow these steps to create CSV spreadsheets:'
  prefs: []
  type: TYPE_NORMAL
- en: Identify the version of Python that invoked the script.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Output a list of lists and a list of dictionaries using Python 2 and Python
    3 conventions to spreadsheets in the current working directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, we import the required libraries to write spreadsheets. Later on in
    this recipe, we also import the `unicodecsv` module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'This recipe does not use `argparse` as a command-line handler. Instead, we
    directly call the desired functions based on the version of Python. We can determine
    the version of Python running with the `sys.version_info` attribute. If the user
    is using Python 2.X, we call both the `csv_writer_py2()` and `unicode_csv_dict_writer_py2()`
    methods. Both of these methods take four arguments, where the last argument is
    optional: these are the data to write, a list of headers, the desired output directory,
    and, optionally, the name of the output CSV spreadsheet. Alternatively, if Python
    3.X is being used, we call the `csv_writer_py3()` method. While similar, CSV writing
    is handled a little differently between the two versions of Python, and the `unicodecsv`
    module is applicable only to Python 2:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'This recipe has two global variables that represent sample data types. The
    first of these, `TEST_DATA_LIST`, is a nested list structure containing strings
    and integers. The second, `TEST_DATA_DICT`, is another representation of this
    data but stored as a list of dictionaries. Let''s look at how the various functions
    write this sample data to the output CSV file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The `csv_writer_py2()` method first checks whether the name input was provided.
    If it is still the default value of `None`, we simply assign the output name ourselves.
    Next, after printing a status message to the console, we open a `File` object
    in the `"wb"` mode in the desired output directory. Note that it is important
    to open CSV files in the `"wb"` mode in Python 2 to prevent intervening gaps between
    rows in the resulting spreadsheet. Once we have the `File` object, we use the
    `csv.writer()` method to convert this into a `writer` object. With this, we can
    use the `writerow()` and `writerows()` methods to write a single list of data
    and a nested list structure, respectively. Now, let''s look at how `unicodecsv`
    works with lists of dictionaries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The `unicodecsv` module is a drop in for the built-in `csv` module and can be
    used interchangeably. The difference, and it's a big one, is that `unicodecsv`
    automatically handles Unicode strings in a way that the built-in `csv` module
    in Python 2 does not. This was addressed in Python 3.
  prefs: []
  type: TYPE_NORMAL
- en: First, we attempt to import the `unicodecsv` module and print a status message
    to the console if the import fails before exiting the script. If we are able to
    import the library, we check whether the name input was supplied and create a
    name if it wasn't, before opening a `File` object. With this `File` object, we
    use the `unicodecsv.DictWriter` class and supply it with the list of headers.
    This object, by default, expects the keys present in the supplied `fieldnames`
    list to represent all of the keys in each dictionary. If this behavior is not
    desired or if this is not the case, it can be ignored by setting the extrasaction
    keyword argument to the string `ignore`. Doing so will result in all additional
    dictionary keys not specified in the `fieldnames` list being ignored and not added
    to the CSV spreadsheet.
  prefs: []
  type: TYPE_NORMAL
- en: 'After the `DictWriter` object is set up, we use the `writerheader()` method
    to write the field names and `writerows()` to, this time, write the list of dictionaries
    to the CSV file. Another important thing to note is that the columns will be in
    the order of the elements in the supplied `fieldnames` list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Lastly, the `csv_writer_py3()` method operates in mostly the same fashion.
    However, note the difference in how the `File` object is created. Rather than
    opening a file in the `"wb"` mode, with Python 3, we open the file in the `"w"`
    mode and set the newline keyword argument to an empty string. After doing that,
    the rest of the operations proceed in the same manner as previously described:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'When we run this code, we can look at either of the two newly generated CSV
    files and see the same information, as in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00019.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This script can be further improved. Here''s a recommendation:'
  prefs: []
  type: TYPE_NORMAL
- en: Create more robust CSV writers with additional feature sets and options. The
    idea here is that you could supply data of different types and have a method to
    handle them equivalently.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualizing events with Excel
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Recipe Difficulty: Easy'
  prefs: []
  type: TYPE_NORMAL
- en: 'Python Version: 2.7 or 3.5'
  prefs: []
  type: TYPE_NORMAL
- en: 'Operating System: Any'
  prefs: []
  type: TYPE_NORMAL
- en: Let's take it one step further from the previous recipe with Excel. Excel is
    a very robust spreadsheet application and we can do a lot with it. We will use
    Excel to create a table and plot graphs of the data.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are a number of different Python libraries with varying support for Excel
    and its many features. In this recipe, we use the `xlsxwriter` module to create
    a table and graph of the data. This module can be used for much more than that.
    This module can be installed by `pip` using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: To learn more about the `xlsxwriter` library, visit [https://xlsxwriter.readthedocs.io/](https://xlsxwriter.readthedocs.io/).
  prefs: []
  type: TYPE_NORMAL
- en: We also use a custom `utilcsv` module that we wrote based on the previous recipe
    to handle interactions with CSVs. All other libraries used in this script are
    present in Python's standard library.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We create an Excel spreadsheet via the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a workbook and worksheet objects.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a table of spreadsheet data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a chart of the event log data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, we import the required libraries to handle argument parsing, creating
    counts of objects, parsing dates, writing XLSX spreadsheets, and our custom `utilcsv`
    module, which handles CSV reading and writing in this recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'This recipe''s command-line handler takes one positional argument: `OUTPUT_DIR`.
    This represents the desired output path for the `XLSX` file. Before calling the
    `main()` method, we check whether the output directory exists and create it if
    it does not:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The `main()` function is really quite simple; its job is to print a status
    message to the console, use the `csv_reader()` method, which is a slightly modified
    function from the previous recipe, and then write the resulting data to the output
    directory with the `xlsx_writer()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The `xlsx_writer()` starts by printing a status message and creating the `workbook`
    object in the output directory. Next, we create two `worksheet` objects for the
    dashboard and data worksheets. The dashboard worksheet will contain a graph summarizing
    the raw data on the data worksheet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'We use the `add_format()` method on the `workbook` object to create customized
    formats for the spreadsheet. These formats are dictionaries with key-value pairs
    configuring the format. Most of these keys are self-explanatory based on the key
    name. A description of the various format options and features can be found at
    [http://xlsxwriter.readthedocs.io/format.html](http://xlsxwriter.readthedocs.io/format.html):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: With the formats set, we can enumerate through the list of lists and write each
    using the `write()` method. This method takes a few inputs; the first and second
    arguments are the row and column followed by the value to write. Note that in
    addition to the `write()` method, we also use the `write_number()` and `write_datetime()`
    methods. These preserve the data type within the XLSX spreadsheet. Specifically,
    with the `write_datetime()` method, we supply it with the `date_format` variable
    to appropriately format the date object. After looping through all of the data,
    we have successfully stored the data within the spreadsheet and retained its value
    types. However, we can do much more than that with an XLSX spreadsheet.
  prefs: []
  type: TYPE_NORMAL
- en: 'We use the `add_table()` method to create a table of the data we just wrote.
    To accomplish this, we must supply the function using the Excel notation to denote
    the top-left and bottom-right columns of the table. Beyond that, we can also provide
    a dictionary of objects to further configure the table. In this case, the dictionary
    only contains the header names for each column of the table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: With the data worksheet complete, let's now turn our focus on the dashboard
    worksheet. We will create a graph on this dashboard, breaking down the event IDs
    by frequency. First, we calculate this frequency using a `Counter` object, as
    shown in the HTML dashboard recipe. Next, we set a title for this page by merging
    a number of columns and setting the title text and format.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once that is complete, we iterate through the frequency of event IDs `Counter`
    object and write them to the worksheet. We write them starting at row `100` to
    make sure the data is out of the way and not at the forefront. Once this data
    is written, we convert it into a table using the same method discussed previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can plot this chart we keep talking about. We use the `add_chart()`
    method and specify the type as a bar chart. Next, we use the `set_title()` and
    `set_size()` methods to properly configure this graph. All that is left is to
    use the `add_series()` method to add the data to the chart. This method takes
    a dictionary with a category and values key. In a bar chart, the categories values
    represent the *x* axis and the values represent the *y* axis. Note the use of
    Excel notation to designate the range of cells that make up the categories and
    values keys. Once the data has been selected, we use the `insert_chart()` method
    on the `worksheet` object to display it before closing the `workbook` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'When we run this script, we can review the data in an XLSX spreadsheet and
    the chart we created summarizing Event IDs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00020.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Auditing your work
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Recipe Difficulty: Easy'
  prefs: []
  type: TYPE_NORMAL
- en: 'Python Version: 2.7 or 3.5'
  prefs: []
  type: TYPE_NORMAL
- en: 'Operating System: Any'
  prefs: []
  type: TYPE_NORMAL
- en: Keeping detailed investigative notes is a key to any investigation. Without
    this, it can be difficult to put all of the pieces together or accurately recall
    findings. Sometimes, it can be helpful to have a screenshot or a series of them
    to remind you of the various steps you took during your review.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to create a recipe with cross-platform support, we have elected to
    use the `pyscreenshot` module. This module relies on a few dependencies, specifically
    the **Python Imaging Library** (**PIL**), and one or more backends. The backend
    used here is the WX GUI library. All three of these modules can be installed with
    `pip`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: To learn more about the pyscreenshot library, visit [https://pypi.python.org/pypi/pyscreenshot](https://pypi.python.org/pypi/pyscreenshot).
  prefs: []
  type: TYPE_NORMAL
- en: All other libraries used in this script are present in Python's standard library.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We use the following methodology to accomplish our objective:'
  prefs: []
  type: TYPE_NORMAL
- en: Process user-supplied arguments.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Take screenshots based on user-supplied inputs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Save screenshots to the specified output folder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, we import the required libraries to handle argument parsing, sleeping
    the script, and taking screenshots:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'This recipe''s command-line handler takes two positional arguments, `OUTPUT_DIR`
    and `INTERVAL`, which represent the desired output path and the interval between
    screenshots, respectively. The optional `total` argument can be used to impose
    an upper limit on the number of screenshots that should be taken. Note that we
    specify the type for both `INTERVAL` and `total` arguments as integers. After
    validating that the output directory exists, we pass these inputs to the `main()`
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The `main()` function creates an infinite `while` loop and starts incrementing
    a counter by one for each screenshot taken. Following that, the script sleeps
    for the provided interval before using the `pyscreenshot.grab()` method to capture
    a screenshot. With the screenshot captured, we create the output filename and
    use the screenshot object''s `save()` method to save it to the output location.
    That''s really it. We print a status message notifying the user about this and
    then check whether the `total` argument was provided and whether the counter is
    equal to it. If it is, the `while` loop is exited, but otherwise, it continues
    forever. As a word of caution/wisdom, if you choose not to provide a `total` limit,
    make sure to stop the script manually once you have completed your review. Otherwise,
    you may come back to an ominous blue screen and full hard drive:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'With the screenshotting script running every five seconds and storing the pictures
    in the folder of our choice, we can see the following output, as captured in the
    following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00021.gif)'
  prefs: []
  type: TYPE_IMG
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This script can be further improved. We have provided a couple of recommendations
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: Add video recording support to the script
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add the functionality to automatically create archives of the screenshots with
    the date as the archive name
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
