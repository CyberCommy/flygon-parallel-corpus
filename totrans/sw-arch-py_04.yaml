- en: Chapter 4. Good Performance is Rewarding!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Performance is one of the cornerstones of modern-day software applications.
    Every day we interact with high performing computing systems in many different
    ways, as part of our work and our leisure.
  prefs: []
  type: TYPE_NORMAL
- en: When you book an airline ticket from one of the travel sites on the web, you
    are interacting with a high performance system that carries out hundreds s of
    such transactions at a given time. When you transfer money to someone or pay your
    credit card bill online via an Internet banking transaction, you are interacting
    with a high performance and high throughput transactional system. Similarly when
    you play online games on your mobile phone and interact with other players, again
    there is a network of servers built for high concurrency and low latency that
    is receiving input from you and thousands of other players, performing computations
    at the back and sending data to you – all with reasonable and quiet efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: Modern day web applications that serve millions of users concurrently became
    possible with the advent of high-speed Internet and huge drops in the price/performance
    ratio of hardware. Performance is still a key quality attribute of modern day
    software architecture and writing high performing and scalable software still
    continues to be something of a difficult art. You may write an application which
    ticks all the boxes of functionality and other quality attributes, but if it fails
    its performance tests, then it cannot be moved to production.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter and the next, we focus on two aspects of writing software with
    high throughput – namely performance and scalability. In this chapter, the focus
    is on performance, the various aspects of it, how to measure it, the performance
    of various data structures, and when to choose what – with the focus on Python.
  prefs: []
  type: TYPE_NORMAL
- en: 'The topics we will be discussing in this chapter roughly fall under the following
    sections:'
  prefs: []
  type: TYPE_NORMAL
- en: Defining performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Software performance engineering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Types of performance testing tool
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Performance complexity and the Big-O notation:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Measuring performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finding performance complexity using graphs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Improving performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Profiling:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deterministic profiling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cProfile` and `profile`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Third-party profilers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Other tools:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Objgraph
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pympler
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Programming for performance – data structures:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lists
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dictionaries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tuples
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'High performance containers – the collections module:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`deque`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`defaultdict`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`OrderedDict`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Counter`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ChainMap`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`namedtuple`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Probabilistic data structure – bloom filters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is performance?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The performance of a software system can be broadly defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '*"The degree to which the system is able to meet its throughput and/or latency
    requirements in terms of the number of transactions per second or time taken for
    a single transaction."*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: We've already taken an overview of measuring performance in the introductory
    chapter. Performance can be measured either in terms of response time/latency
    or in terms of throughput. The former is the time it takes for the application
    to complete a request/response loop on average. The latter is the rate at which
    the system processes its input in terms of the number of requests or transactions
    successfully completed per minute.
  prefs: []
  type: TYPE_NORMAL
- en: The performance of a system is a function of its software and of its hardware
    capabilities. A badly written piece of software could still be made to perform
    better by scaling the hardware – for example, the amount of RAM.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly a piece of software can be made to work better on existing hardware
    by increasing its performance – for example, by rewriting routines or functions
    to be more efficient in terms of time or memory or by modifying the architecture.
  prefs: []
  type: TYPE_NORMAL
- en: However, the right type of performance engineering is the one where the software
    is tuned for the hardware in an optimal fashion so that software scales linearly
    or better with respect to the available hardware.
  prefs: []
  type: TYPE_NORMAL
- en: Software performance engineering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Software performance engineering includes all the activities of software engineering
    and analysis applied during the **Software** **Development Life Cycle** (**SDLC**)
    and directed towards meeting performance requirements.
  prefs: []
  type: TYPE_NORMAL
- en: In conventional software engineering, performance testing and feedback are done
    usually towards the end of the SDLC. This approach is purely measurement-based
    and waits for the system to be developed before applying tests and diagnostics
    and tuning the system based on the results.
  prefs: []
  type: TYPE_NORMAL
- en: Another more formal model named **Software Performance Engineering** (**SPE**)
    itself, develops performance models early in the SDLC and uses results from the
    models to modify the software design and architecture to meet performance requirements
    in multiple iterations.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this approach, both performance as a non-functional requirement and software
    development meeting its functional requirement go hand in hand. There is a specific
    **Performance** **Engineering Life Cycle** (**PELC**) that parallels the steps
    in the SDLC. At every step, starting from the design and architecture all the
    way to deployment, feedback between both the life cycles is used to iteratively
    improve the software quality:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Software performance engineering](../Images/image00410.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: SPE - Performance Engineering Life Cycle mirroring Software Development Life
    Cycle
  prefs: []
  type: TYPE_NORMAL
- en: In both approaches, performance testing and diagnostics are important, followed
    by tuning the design/architecture or the code based on the results obtained. Hence
    performance testing and measurement tools play an important role in this step.
  prefs: []
  type: TYPE_NORMAL
- en: Performance testing and measurement tools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: These tools fall under two broad categories – namely, the ones used for performance
    testing and diagnostics, and the ones used for performance metrics gathering and
    instrumentation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Performance testing and diagnostic tools can be classified further as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Stress testing tools**: These tools are used to supply workload to the system
    under test, simulating peak workloads in production. These tools can be configured
    to send a continuous stream of input to the application to simulate high stress
    or to periodically send a burst of very high traffic – much exceeding even peak
    stress – to test the robustness of the system. These tools are also called **load
    generators**. Examples of common stress testing tools used for web application
    testing include **httpperf**, **ApacheBench**, **LoadRunner**, **Apache JMeter**,
    and **Locust**. Another class of tools involves those that actually record real
    user traffic and then replay it via the network to simulate real user load. For
    example, the popular network packet capturing and monitoring tool, **Wireshark**
    and its console cousin program, `tcpdump`, can be used to do this. We won''t be
    discussing these tools in this chapter as they are general-purpose and examples
    of usage for them can be found in abundance on the Web.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Monitoring tools**: These tools work with the application code to generate
    performance metrics such as the time and memory taken for functions to execute,
    the number of function calls made per request-response loop, the average and peak
    times spent on each function, and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Instrumentation tools**: Instrumentation tools trace metrics, such as the
    time and memory required for each computing step, and also track events, such
    as exceptions in code, covering such details as the module/function/line number
    where the exception occurred, the timestamp of the event, and the environment
    of the application (environment variables, application configuration parameters,
    user information, system information, and so on). Often external instrumentation
    tools are used in modern web application programming systems to capture and analyze
    such data in detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Code or application profiling tools**: These tools generates statistics about
    functions, their frequency of duration of calls, and the time spent on each function
    call. This is a kind of dynamic program analysis. It allows the programmer to
    find critical sections of code where the most time is spent, allowing him/her
    to optimize those sections. Optimization without profiling is not advised as the
    programmer may end up optimizing the wrong code, thereby not surfacing the intended
    benefits up to the application.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Most programming languages come with their own set of instrumentation and profiling
    tools. In Python, a set of tools in the standard library (such as the `profile`
    and `cProfile` modules) do this – this is supplemented by a rich ecosystem of
    third-party tools. We will discuss these tools in the coming sections.
  prefs: []
  type: TYPE_NORMAL
- en: Performance complexity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It would be helpful to spend some time discussing what we mean by the performance
    complexity of code before we jump into code examples in Python and discuss tools
    to measure and optimize performance.
  prefs: []
  type: TYPE_NORMAL
- en: The performance complexity of a routine or function is defined in terms of how
    they respond to changes in the input size typically in terms of the time spent
    in executing the code.
  prefs: []
  type: TYPE_NORMAL
- en: This is usually represented by the so-called Big-O notation which belongs to
    a family of notations called the **Bachmann–Landau notation or asymptotic** notation.
  prefs: []
  type: TYPE_NORMAL
- en: The letter O is used as the rate of growth of a function with respect to input
    size - also called the **order** of the function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Commonly used Big-O notations or function orders are shown in the following
    table in order of increasing complexity:'
  prefs: []
  type: TYPE_NORMAL
- en: '| # | Order | Complexity | Example |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | *O(1)* | Constant | Looking for a key in a constant look-up table such
    as a HashMap or dictionary in Python |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | *O(log (n))* | Logarithmic | Searching for an item in a sorted array
    with a binary search. All operations on a heapq in Python |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | *O(n)* | Linear | Searching an item in an array (list in Python) by traversing
    it |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | *O(n*k)* | Linear | Worst-case complexity of Radix sort |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | *O(n * log (n))* | n log-star n | Worst-case complexity in a mergesort
    or heapsort algorithm |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | *O(n²)* | Quadratic | Simple sorting algorithms such as bubblesort, insertion
    sort, and selection sort. Worst-case complexity on some sorting algorithms such
    as quicksort, shellsort, and so on |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | *O(2^n)* | Exponential | Trying to break a password of size n using brute
    force, solving the travelling salesman problem using dynamic programming |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | *O(n!)* | Factorial | Generating all partitions of a set |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Common Big-O notations for function orders with respect to input size
    "n"'
  prefs: []
  type: TYPE_NORMAL
- en: When implementing a routine or algorithm accepting an input of a certain size
    *n*, the programmer ideally should aim for implementing it in an order that falls
    in the first five. Anything which is of the order of *O(n) or O(n* log(n))* or
    lesser indicates reasonable to good performance.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithms with an order of *O(n²)* can usually be optimized to work at a lower
    order. We will see some examples of this in the sections in the following diagram.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows how each of these orders grow with respect to *n*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Performance complexity](../Images/image00411.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Graph of growth rate of each order of complexity (y-axis) w.r.t input size (x-axis)
  prefs: []
  type: TYPE_NORMAL
- en: Measuring performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we've had an overview of what performance complexity is and also of
    performance testing and measurement tools, let us take an actual look at the various
    ways of measuring performance complexity with Python.
  prefs: []
  type: TYPE_NORMAL
- en: One of the simplest time measurements can be done by using the `time` command
    of a POSIX/Linux system.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is done by using the following command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'For example, here is a screenshot of the time it takes to fetch a very popular
    page from the Web:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Measuring performance](../Images/image00412.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Output of the time command on fetching a web page from the Internet via wget
  prefs: []
  type: TYPE_NORMAL
- en: 'See that it shows three classes of time output, namely `real`, `user`, and
    `sys`. It is important to know the distinction between these three so let us look
    at them briefly:'
  prefs: []
  type: TYPE_NORMAL
- en: '`real`: Real time is the actual wall clock time that elapsed for the operation.
    This is the time of the operation from start to finish. It will include any time
    the process sleeps or spends blocked – such as time taken for I/O to complete.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`User`: User time is the amount of actual CPU time spent within the process
    in user mode (outside the kernel). Any sleep time or time spent in waiting such
    as I/O doesn''t add to the user time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Sys`: System time is the amount of CPU time spent on executing system calls
    within the kernel for the program. This counts only those functions that execute
    in kernel space such as privileged system calls. It doesn''t count any system
    calls that execute in user space (which is counted in `User`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The total CPU time spent by a process is `user` + `sys` time. The real or wall
    clock time is the time mostly measured by simple time counters.
  prefs: []
  type: TYPE_NORMAL
- en: Measuring time using a context manager
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In Python, it is not very difficult to write a simple function that serves as
    a context manager for blocks of code whose execution time you want to measure.
  prefs: []
  type: TYPE_NORMAL
- en: But first we need a program whose performance we can measure.
  prefs: []
  type: TYPE_NORMAL
- en: 'Take a look at the following steps to learn how to use a context manager for
    measuring time:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us write a program that calculates the common elements between two sequences
    as a test program. Here is the code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Let us write a simple context-manager timer to time this code. For timing we
    will use `perf_counter` of the `time` module, which gives the time to the most
    precise resolution for short durations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Let us time the function for some simple input data. For this a `test` function
    is useful that generates random data, given an input size:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output of the `timer` method on the `test` function on the Python
    interactive interpreter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'In fact both test data generation and testing can be combined in the same function
    to make it easy to test and generate data for a range of input sizes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let us measure the time taken for different ranges of input sizes in the
    Python interactive console:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Oops, the time spent for `1000` items is less than that for `800`! How''s that
    possible? Let''s try again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Now the time spent for `800` items seems to be lesser than that for `400` and
    `500`. And time spent for `1000` items has increased to more than twice what it
    was before.
  prefs: []
  type: TYPE_NORMAL
- en: The reason is that our input data is random, which means it will sometimes have
    a lot of common items – which takes more time – and sometimes have much less.
    Hence on subsequent calls the time taken can show a range of values.
  prefs: []
  type: TYPE_NORMAL
- en: In other words, our timing function is useful to get a rough picture, but not
    very useful when it comes to getting the true statistical measure of time taken
    for program execution, which is more important.
  prefs: []
  type: TYPE_NORMAL
- en: For this we need to run the timer many times and take an average. This is somewhat
    similar to the **amortized** analysis of algorithms, which takes into account
    both the lower end and upper end of the time taken for executing algorithms and
    gives the programmer a realistic estimate of the average time spent.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Python comes with such a module, which helps to perform such timing analysis,
    in its standard library, namely the `timeit` module. Let us look at this module
    in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Timing code using the timeit module
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `timeit` module in the Python standard library allows the programmer to
    measure the time taken to execute small code snippets. The code snippets can be
    a Python statement, an expression, or a function.
  prefs: []
  type: TYPE_NORMAL
- en: The simplest way to use the `timeit` module is to execute it as a module in
    the Python command line.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, here is timing data for some simple Python inline code measuring
    the performance of a list comprehension calculating squares of numbers in a range:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The result shows the time taken for execution of the code snippet. When run
    on the command line, the `timeit` module automatically determines the number of
    cycles to run the code and also calculates the average time spent in a single
    execution.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The results show that the statement we are executing is linear or O(n) as a
    range of size 100 takes 5.5 usec and that of 1,000 takes 56.5 usec or about 10
    times its time. A usec – or microsecond - is 1 millionth of a second or 1*10-6
    seconds.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is how to use the `timeit` module on the Python interpreter in a similar
    manner:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Observe that when used in this way, the programmer has to pass the correct number
    of iterations as the `number` argument and, to average, has to divide by the same
    number. The multiplication by `1000000` is to convert the time to microseconds
    (usec).
  prefs: []
  type: TYPE_NORMAL
- en: The `timeit` module uses a `Timer` class behind the scenes. The class can be
    made use of directly as well as for finer control.
  prefs: []
  type: TYPE_NORMAL
- en: When using this class, `timeit` becomes a method of the instance of the class
    to which the number of cycles is passed as an argument.
  prefs: []
  type: TYPE_NORMAL
- en: The `Timer` class constructor also accepts an optional `setup` argument, which
    sets up the code for the `Timer` class. This can contain statements for importing
    the module that contains the function, setting up globals, and so on. It accepts
    multiple statements separated by semi-colons.
  prefs: []
  type: TYPE_NORMAL
- en: Measuring the performance of our code using timeit
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let us rewrite our `test` function to test the common items between two sequences.
    Now that we are going to use the `timeit` module, we can remove the context manager
    timer from the code. We will also hard-code the call to `common_items` in the
    function.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We also need to create the random input outside the test function since otherwise
    the time taken for it will add to the test function's time and corrupt our results.
  prefs: []
  type: TYPE_NORMAL
- en: Hence we need to move the variables out as globals in the module and write a
    `setup` function, which will generate the data for us as a first step.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our rewritten `test` function looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The `setup` function with the global variables looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Let's assume the module containing both the `test` and `common_items` functions
    is named `common_items.py`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The timer test can now be run as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: So the time taken for a range of `100` numbers is around 117 usec (0.12 microseconds)
    on average.
  prefs: []
  type: TYPE_NORMAL
- en: 'Executing it now for a few other ranges of input sizes gives the following
    output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: So the maximum time taken for this test run is 12.4 microseconds for an input
    size of `1000` items.
  prefs: []
  type: TYPE_NORMAL
- en: Finding out time complexity – graphs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Is it possible to find out from these results what the time performance complexity
    of our function is? Let us try plotting it in a graph and see the results.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `matplotlib` library is very useful in plotting graphs in Python for any
    type of input data. We just need the following simple piece of code for this to
    work:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code gives you the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Take a look at the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Finding out time complexity – graphs](../Images/image00413.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Plot of the input range versus time taken for the common_items function
  prefs: []
  type: TYPE_NORMAL
- en: This is clearly not linear, yet of course not quadratic (in comparison with
    the figure on Big-O notations). Let us try and plot a graph of O(n*log(n)) superimposed
    on the current plot to see if there's a match.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we now need two series of `ydata`, we need another slightly modified
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code gives you the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'You get the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Finding out time complexity – graphs](../Images/image00414.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Plot of time complexity of common_items superimposed on the plot of y = x*log(x)
  prefs: []
  type: TYPE_NORMAL
- en: The superimposed plot shows that the function is a close match for the n*log(n)
    order, if not exactly the same. So our current implementation's complexity seems
    to be roughly O(n*log(n)).
  prefs: []
  type: TYPE_NORMAL
- en: Now that we've done the performance analysis, let us see if we can rewrite our
    routine to perform better.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the current code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The routine first does a pass over an outer `for` loop (of size `n`) and does
    a check in a sequence (also of size `n`) for the item. Now the second search is
    also of time complexity `n` on average.
  prefs: []
  type: TYPE_NORMAL
- en: However, some items would be found immediately and some items would take linear
    time (k) where 1 <k < n. On average, the distribution would be somewhere in between,
    which is why the code has an average complexity approximating O(n*log(n)).
  prefs: []
  type: TYPE_NORMAL
- en: A quick analysis will tell you that the inner search can be avoided by converting
    the outer sequence to a dictionary, setting values to 1\. The inner search will
    be replaced with a loop on the second sequence that increments values by 1.
  prefs: []
  type: TYPE_NORMAL
- en: In the end, all common items will have a value greater than 1 in the new dictionary.
  prefs: []
  type: TYPE_NORMAL
- en: 'The new code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'With this change, the timer gives the following updated results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Let us plot this and superimpose it on an O(n) graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s take a look at the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Finding out time complexity – graphs](../Images/image00415.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Plot of time taken by common_items function (v2) against y = x graph
  prefs: []
  type: TYPE_NORMAL
- en: The upper green line is the reference **y** = **x** graph and the lower blue
    line is the plot of the time taken by our new function. It is pretty obvious that
    the time complexity is now linear or O(n).
  prefs: []
  type: TYPE_NORMAL
- en: However, there seems to be a constant factor here as the slopes of two lines
    are different. From a quick calculation one can compute this factor as roughly
    `0.35`.
  prefs: []
  type: TYPE_NORMAL
- en: 'After applying this change, you will get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '![Finding out time complexity – graphs](../Images/image00416.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Plot of time taken by common_items function (v2) against y = 0.35*x graph
  prefs: []
  type: TYPE_NORMAL
- en: You can see that the plots pretty much superimpose on each other. So our function
    is now performing at O(c*n) where c ~= 0.35.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another implementation of the `common_items` function is to convert both sequences
    to sets and return their intersection. It would be an interesting exercise for
    the reader to make this change, time it, and plot the graphs to determine the
    time complexity.
  prefs: []
  type: TYPE_NORMAL
- en: Measuring CPU time with timeit
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `Timer` module by default uses the `perf_counter` function of the `time`
    module as the default `timer` function. As mentioned earlier, this function returns
    the wall clock time spent to the maximum precision for small time durations, hence
    it will include any sleep time, time spent for I/O, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'This can be made clear by adding a little sleep time to our test function as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code will give you the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The time jumped by as much as 300 times since we are sleeping `0.01` seconds
    (10 milliseconds) upon every invocation, so the actual time spent on the code
    is now determined almost completely by the sleep time as the result shows `10545.260819926625`
    microseconds (or about 10 milliseconds).
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes you may have such sleep times and other blocking/wait times but you
    want to measure only the actual CPU time taken by the function. To use this, the
    `Timer` object can be created using the `process_time` function of the time module
    as the `timer` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'This can be done by passing in a `timer` argument when you create the `Timer`
    object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: If you now increase the sleep time by a factor of, say, 10, the testing time
    increases by that factor, but the return value of the timer remains the same.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, here is the result when sleeping for 1 second. The output comes
    after about 100 seconds (since we are iterating `100` times), but notice that
    the return value (time spent per invocation) doesn''t change:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Let us move on to profiling next.
  prefs: []
  type: TYPE_NORMAL
- en: Profiling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will discuss profilers and take a deep look at the modules
    in the Python standard library, which provides support for deterministic profiling.
    We will also look at third-party libraries that provide support for profiling
    such as `line_profiler` and `memory_profiler`.
  prefs: []
  type: TYPE_NORMAL
- en: Deterministic profiling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deterministic profiling means that all function calls, function returns, and
    exception events are monitored, and precise timings are made for the intervals
    between these events. Another type of profiling, namely **statistical profiling**,
    randomly samples the instruction pointer and deduces where time is being spent
    – but this may not be very accurate.
  prefs: []
  type: TYPE_NORMAL
- en: Python, being an interpreted language, already has a certain overhead in terms
    of metadata kept by the interpreter. Most deterministic profiling tools makes
    use of this information and hence only add very little extra processing overhead
    for most applications. Hence deterministic profiling in Python is not a very expensive
    operation.
  prefs: []
  type: TYPE_NORMAL
- en: Profiling with cProfile and profile
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `profile` and `cProfile` modules provide support for deterministic profiling
    in the Python standard library. The `profile` module is purely written in Python.
    The `cProfile` module is a C extension that mimics the interface of the `profile`
    module but adds lesser overhead to it when compared to profile.
  prefs: []
  type: TYPE_NORMAL
- en: Both modules report statistics that are converted into reportable results using
    the `pstats` module.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the following code, which is a prime number iterator, in order
    to show our examples using the `profile` modules:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The prime number iterator generates the first `n` prime numbers given the value
    of `n`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'To profile this code, we just need to pass the code to be executed as a string
    to the `run` method of the profile or cProfile module. In the following examples,
    we will be using the `cProfile` module:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Profiling with cProfile and profile](../Images/image00417.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Profiling output of the prime iterator function for the first 100 primes
  prefs: []
  type: TYPE_NORMAL
- en: 'See how the profiler reports its output. The output is ordered into six columns
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`ncalls`: The number of calls per function'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tottime`: The total time spent in the call'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`percall`: The `percall` time (quotient of `tottime`/`ncalls`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cumtime`: The cumulative time in this function plus any child function'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`percall`: Another `percall` column (the quotient of `cumtime`/number of primitive
    calls)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`filename: lineno(function)`: The filename and line number of the function
    call'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this case, our function took `4` microseconds to complete with most of that
    time (`3` microseconds) being spent inside the `is_prime` method, which also dominates
    the number of calls at 271.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the outputs of the profiler at `n = 1000` and `10000` respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Profiling with cProfile and profile](../Images/image00418.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Profiling output of the prime iterator function for the first 1,000 primes
  prefs: []
  type: TYPE_NORMAL
- en: 'Take a look at the following additional output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Profiling with cProfile and profile](../Images/image00419.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Profiling output of the Prime iterator function for first 10,000 primes
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, at `n`=`1000` it took about `0.043` seconds (43 microseconds)
    and at `n`=`10000` it took `0.458` seconds (458 microseconds). Our `Prime` iterator
    seems to be performing at an order close to O(n).
  prefs: []
  type: TYPE_NORMAL
- en: As usual, most of that time is spent in `is_primes`. Is there a way to reduce
    that time?
  prefs: []
  type: TYPE_NORMAL
- en: At this point, let us analyze the code.
  prefs: []
  type: TYPE_NORMAL
- en: Prime number iterator class – performance tweaks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A quick analysis of the code tells us that inside `is_prime` we are dividing
    the value by every number in the range from `3` to the successor of the square
    root of the value.
  prefs: []
  type: TYPE_NORMAL
- en: This contains many even numbers as well – we are doing unnecessary computation,
    which we can avoid by dividing only by the odd numbers.
  prefs: []
  type: TYPE_NORMAL
- en: 'The modified `is_prime` method is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: With this, the profile for `n`=`1000` and `n`=`10000` looks as follows.
  prefs: []
  type: TYPE_NORMAL
- en: The following is the output of the profiler for `n = 1000`.
  prefs: []
  type: TYPE_NORMAL
- en: '![Prime number iterator class – performance tweaks](../Images/image00420.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Profiling output of the Prime iterator function for the first 1,000 primes with
    tweaked code
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the output of the profiler for `n` = `10000`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Prime number iterator class – performance tweaks](../Images/image00421.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Profiling output of the Prime iterator function for first 10,000 primes with
    tweaked code
  prefs: []
  type: TYPE_NORMAL
- en: You can see that, at `1000`, the time has dropped a bit (43 microseconds to
    38 microseconds) but at `10000`, there is nearly a 50% drop from 458 microseconds
    to 232 microseconds. At this point, the function is performing better than O(n).
  prefs: []
  type: TYPE_NORMAL
- en: Profiling – collecting and reporting statistics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The way we used cProfile in the example earlier, it ran and reported the statistics
    directly. Another way to use the module is to pass a `filename` argument to which
    it writes the statistics, which can later be loaded and interpreted by the `pstats`
    module.
  prefs: []
  type: TYPE_NORMAL
- en: 'We modify the code as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: By doing this, the stats, instead of getting printed out, are saved to the file
    named `prime.stats`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is how to parse the statistics using the `pstats` module and print the
    results ordered by the number of calls:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Profiling – collecting and reporting statistics](../Images/image00422.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Parsing and printing saved profile results using the pstats module
  prefs: []
  type: TYPE_NORMAL
- en: The `pstats` module allows sorting the profile results by a number of headers
    such as total time (`tottime`), number of primitive calls (`pcalls`), cumulative
    time (`cumtime`), and so on. You can see from the output of pstats again that
    most of the processing in terms of number of calls are being spent in the is_prime
    method, as we are sorting the output by 'ncalls' or the number of function calls.
  prefs: []
  type: TYPE_NORMAL
- en: The `Stats` class of the `pstats` module returns a reference to itself after
    every operation. This is a very useful aspect of some Python classes and allows
    us to write compact one line code by chaining method calls.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another useful method of the `Stats` object is to find out the callee/caller
    relationship. This can be done by using the `print_callers` method instead of
    `print_stats`. Here is the output from our current statistics:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Profiling – collecting and reporting statistics](../Images/image00423.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Printing callee/caller relationships ordered by primitive calls using pstats
    module
  prefs: []
  type: TYPE_NORMAL
- en: Third-party profilers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Python ecosystem comes with a plethora of third-party modules for solving
    most problems. This is true in the case of profilers as well. In this section,
    we will take a quick look at a few popular third-party profiler applications contributed
    by developers in the Python community.
  prefs: []
  type: TYPE_NORMAL
- en: Line profiler
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Line profiler is a profiler application developed by Robert Kern for performing
    line by line profiling of Python applications. It is written in Cython, an optimizing
    static compiler for Python that reduces the overhead of profiling.
  prefs: []
  type: TYPE_NORMAL
- en: 'Line profiler can be installed via `pip` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: As opposed to the profiling modules in Python, which profile functions, line
    profiler is able to profile code line by line, thus providing more granular statistics.
  prefs: []
  type: TYPE_NORMAL
- en: Line profiler comes with a script called `kernprof.py` that makes it easy to
    profile code using line profiler. One needs only to decorate the functions that
    need to be profiled with the `@profile` decorator when using `kernprof`.
  prefs: []
  type: TYPE_NORMAL
- en: For example, we realized that most of the time in our prime number iterator
    was being spent in the `is_prime` method. However, line profiler allows us to
    go into more detail and find which lines of those functions take the most time.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do this, just decorate the method with the `@profile` decorator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Since `kernprof` accepts a script as an argument, we need to add some code
    to invoke the prime number iterator. To do that, we can append the following at
    the end of the `primes.py` module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, run it with line profiler as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: By passing `-v` to the `kernprof` script, we tell it to display the profile
    results in addition to saving them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Line profiler](../Images/image00424.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Line profiler results from profiling the is_prime method using n = 1000
  prefs: []
  type: TYPE_NORMAL
- en: 'Line profiler tells us that the majority of the time – close to 90% of the
    total time spent in the method – is spent in the first two lines: the for loop
    and the reminder check.'
  prefs: []
  type: TYPE_NORMAL
- en: This tells us that, if ever we want to optimize this method, we need to concentrate
    on these two aspects.
  prefs: []
  type: TYPE_NORMAL
- en: Memory profiler
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Memory profiler is a profiler similar to line profiler in that it profiles Python
    code line by line. However, instead of profiling the time taken in each line of
    code, it profiles lines by memory consumption.
  prefs: []
  type: TYPE_NORMAL
- en: 'Memory profiler can be installed the same way as line profiler:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Once installed, memory for lines can be printed by decorating the function with
    the `@profile` decorator in a similar way to line profiler.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a simple example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Here''s how to run this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Memory profiler](../Images/image00425.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Memory profiler profiling a list comprehension of squares of the first 1,000
    numbers
  prefs: []
  type: TYPE_NORMAL
- en: 'Memory profiler shows memory increments line by line. In this case, there is
    almost no increment for the line containing the number of squares (the list comprehension)
    as the numbers are rather small. The total memory usage remains what it was at
    the beginning: about 32 MB.'
  prefs: []
  type: TYPE_NORMAL
- en: 'What happens if we change the value of `n` to a million? This can be done by
    rewriting the last line of the code as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '![Memory profiler](../Images/image00426.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Memory profiler profiling a list comprehension of squares of the first 1 million
    numbers
  prefs: []
  type: TYPE_NORMAL
- en: Now you can see that there is a clear memory increment of about 39 MB for the
    list comprehension calculating the squares, with a total final memory usage of
    about 70 MB.
  prefs: []
  type: TYPE_NORMAL
- en: To demonstrating the real usefulness of memory profiler, let us look at another
    example.
  prefs: []
  type: TYPE_NORMAL
- en: This involves finding the strings from a sequence that are subsequences of any
    of the strings present in another sequence, generally containing larger strings.
  prefs: []
  type: TYPE_NORMAL
- en: Substring (subsequence) problem
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let us say you have a sequence containing the following strings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'And say there is another sequence as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'The problem is to find the strings in `seq2` that are substrings – as is found
    anywhere contiguously in any of the strings in `seq1`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, the answer is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'This can be solved using a brute-force search – checking for each string one
    by one in each of the parent strings as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: However, a quick analysis will tell you that the time complexity of this function
    scales rather badly as the size of the sequences increase. Since every step needs
    iteration through two sequences and then a search in each string in the first
    sequence, the average performance would be O(n1*n2), where n1, n2 are the sizes
    of the sequences respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the results of some tests of this function with input sizes (both
    sequences of the same size) of random strings varying from length 2 to 10:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Input size | Time taken |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 100 | 450 usec |'
  prefs: []
  type: TYPE_TB
- en: '| 1000 | 52 microseconds |'
  prefs: []
  type: TYPE_TB
- en: '| 10000 | 5.4 seconds |'
  prefs: []
  type: TYPE_TB
- en: The results indicate the performance is almost exactly O(n²).
  prefs: []
  type: TYPE_NORMAL
- en: 'Is there a way to rewrite the function to be more performance-efficient? This
    approach is captured in the following `sub_string` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: In this approach, we pre-compute all the substrings of a size range from the
    strings in `seq1` and store it in a dictionary. Then it is a matter of going through
    the strings in `seq2` and checking if they are in this dictionary and if so adding
    them to a list.
  prefs: []
  type: TYPE_NORMAL
- en: To optimize the calculation, we only compute strings whose size is in the range
    of the minimum and maximum length of the strings in `seq2`.
  prefs: []
  type: TYPE_NORMAL
- en: As with almost all solutions to performance issues, this one trades space for
    time. By pre-computing all the substrings, we are expending more space in memory
    but this eases the computation time.
  prefs: []
  type: TYPE_NORMAL
- en: 'The test code looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Here are the timing results of this function using the `timeit` module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Here are the summarized results for this test:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Input size | Time taken |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 100 | 1.08 microseconds |'
  prefs: []
  type: TYPE_TB
- en: '| 1000 | 11.97 microseconds |'
  prefs: []
  type: TYPE_TB
- en: '| 10000 | 0.12 microseconds |'
  prefs: []
  type: TYPE_TB
- en: '| 100000 | 1.26 seconds |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Input size versus time taken for sub-sequence solution via brute force'
  prefs: []
  type: TYPE_NORMAL
- en: A quick calculation tells us that the algorithm is now performing at O(n). Pretty
    good!
  prefs: []
  type: TYPE_NORMAL
- en: But this is at the expense of memory in terms of the pre-computed strings. We
    can get an estimate of this by invoking memory profiler.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the decorated function for doing this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'The test function would now be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Let's test this for the sequence of sizes 1,000 and 10,000 respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the result for an input size of 1,000:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Substring (subsequence) problem](../Images/image00427.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Memory profiler results for testing sub-strings of sequences of size 1,000
  prefs: []
  type: TYPE_NORMAL
- en: 'And here is the result for an input size of 10,000:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Substring (subsequence) problem](../Images/image00428.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Memory profiler results for testing sub-strings of sequences of size 10,000
  prefs: []
  type: TYPE_NORMAL
- en: For the sequence of size of 1,000, the memory usage increased by a paltry 1.4
    MB. For the sequence of size 10,000 it increased by 6.2 MB. Clearly these are
    not very significant numbers.
  prefs: []
  type: TYPE_NORMAL
- en: So the test with memory profiler makes it clear that our algorithm, while being
    efficient on time performance, is also memory-efficient.
  prefs: []
  type: TYPE_NORMAL
- en: Other tools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will discuss a few more tools that will aid the programmer
    in debugging memory leaks and also enable him to visualize his objects and their
    relations.
  prefs: []
  type: TYPE_NORMAL
- en: Objgraph
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Objgraph (**object graph**) is a Python object visualization tool that makes
    use of the `graphviz` package to draw object reference graphs.
  prefs: []
  type: TYPE_NORMAL
- en: It is not a profiling or instrumentation tool but can be used along with such
    tools to visualize object trees and references in complex programs while hunting
    for elusive memory leaks. It allows you to find out references to objects to figure
    out what references are keeping an object alive.
  prefs: []
  type: TYPE_NORMAL
- en: 'As with almost everything in the Python world, it is installable via `pip`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: However objgraph is really useful only if it can generate graphs. Hence we need
    to install the `graphviz` package and the `xdot` tool.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a Debian/Ubuntu system, you will install this as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s look at a simple example of using `objgraph` to find out hidden references:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: We have a class named `MyRefClass` with a single instances `ref` that is referred
    to by 100 instances of the class `C` created in a `for` loop. These are references
    that may cause memory leaks. Let us see how `objgraph` allows us to identify them.
  prefs: []
  type: TYPE_NORMAL
- en: 'When this piece of code is executed, it stops at the debugger (`pdb`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The left side of the image has been cropped to show only the relevant part.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next is the diagram generated by objgraph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Objgraph](../Images/image00429.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Objgraph back references visualization for the object ref'
  prefs: []
  type: TYPE_NORMAL
- en: The red box in the preceding diagram says **99 more references**, which means
    that it is showing one instance of class **C** and informing us there are 99 more
    like it – totaling to 100 instances of C, refer to the single object **ref**.
  prefs: []
  type: TYPE_NORMAL
- en: In a complex program where we are unable to track object references that cause
    memory leaks, such reference graphs can be put to good use by the programmer.
  prefs: []
  type: TYPE_NORMAL
- en: Pympler
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Pympler is a tool that can be used to monitor and measure the memory usage
    of objects in a Python application. It works on both Python 2.x and 3.x. It can
    be installed using `pip` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: The documentation of pympler is rather lacking. However, it's well-known use
    is to track objects and print their actual memory usage via its `asizeof` module.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is our `sub_string` function modified to print the memory usage
    of the sequences dictionary (where it stores all the generated substrings):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'When running this for a sequence size of 10,000:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: The memory size of `5870408` bytes (or around 5.6 MB) is in line with what memory
    profiler reported (around 6 MB)
  prefs: []
  type: TYPE_NORMAL
- en: Pympler also comes with a package called `muppy` which allows to keep track
    of all objects in a program. This can be summarized with the `summary` package
    to print out the summary of memory usage of all objects (classified according
    to their types) in an application.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a report of our `sub_string` module run with n =10,000\. To do this,
    the execution part has to be modified as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'The following shows the output that `pympler` summarizes at the end of the
    program:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Pympler](../Images/image00430.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Summary of memory usage classified by object type by pympler
  prefs: []
  type: TYPE_NORMAL
- en: Programming for performance – data structures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We've looked at the definition of performance, measuring performance complexity,
    and the different tools for measuring program performance. We've also gained insights
    by profiling code for statistics, memory usage, and the like.
  prefs: []
  type: TYPE_NORMAL
- en: We also saw a couple of examples of program optimization to improve the time
    performance of the code.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will take a look at common Python data structures and discuss
    what their best and worst performance scenarios are and also discuss some situations
    of where they are an ideal fit and where they may not be the best choice.
  prefs: []
  type: TYPE_NORMAL
- en: Mutable containers – lists, dictionaries, and sets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Lists, dictionaries, and sets are the most popular and useful mutable containers
    in Python.
  prefs: []
  type: TYPE_NORMAL
- en: Lists are appropriate for object access via a known index. Dictionaries provide
    a near constant time look-up for objects with known keys. Sets are useful to keep
    groups of items while dropping duplicates and finding their difference, intersection,
    union, and so on in near linear time.
  prefs: []
  type: TYPE_NORMAL
- en: Let us look at each of these in turn.
  prefs: []
  type: TYPE_NORMAL
- en: Lists
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Lists provide a near constant time O(1) order for the following operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '`get(index)` via the `[]` operator'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `append(item)` via the `.append` method
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'However, lists perform badly `(O(n))` in the following cases:'
  prefs: []
  type: TYPE_NORMAL
- en: Seeking an item via the `in` operator
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inserting at an index via the `.insert` method
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A list is ideal in the following cases:'
  prefs: []
  type: TYPE_NORMAL
- en: If you need a mutable store to keep different types or classes of items (heterogeneous).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If your search of objects involves getting the item by a known index.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you don't have a lot of lookups via searching the list (**item in list**).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If any of your elements are non-hashable. Dictionaries and sets require their
    entries to be hashable. So in this case, you almost default to using a list.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you have a huge list – of, say, more than 100,000 items – and you keep finding
    that you search it for elements via the `in` operator, you should replace it with
    a dictionary.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, if you find that you keep inserting to a list instead of appending
    to it most of the time, you can think of replacing the list with `deque` from
    the `collections` module.
  prefs: []
  type: TYPE_NORMAL
- en: Dictionaries
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Dictionaries provide a constant time order for:'
  prefs: []
  type: TYPE_NORMAL
- en: Setting an item via a key
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting an item via a key
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deleting an item via a key
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'However, dictionaries take slightly more memory than lists for the same data.
    A dictionary is useful in the following situations:'
  prefs: []
  type: TYPE_NORMAL
- en: You don't care about the insertion order of the elements
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You don't have duplicate elements in terms of keys
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A dictionary is also ideal where you load a lot of data uniquely indexed by
    keys from a source (database or disk) in the beginning of the application and
    need quick access to them – in other words, a lot of random reads as against fewer
    writes or updates.
  prefs: []
  type: TYPE_NORMAL
- en: Sets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The usage scenario of sets lies somewhere between lists and dictionaries. Sets
    are in implementation closer to dictionaries in Python – since they are unordered,
    don't support duplicate elements, and provide near O(1) time access to items via
    keys. They are kind of similar to lists in that they support the pop operation
    (even if they don't allow index access!).
  prefs: []
  type: TYPE_NORMAL
- en: Sets are usually used in Python as intermediate data structures for processing
    other containers – for operations such as dropping duplicates, finding common
    items across two containers, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Since the order of set operations is exactly same as that of a dictionary, you
    can use them for most cases where a dictionary needs to be used, except that no
    value is associated to the key.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples include:'
  prefs: []
  type: TYPE_NORMAL
- en: Keeping heterogeneous, unordered data from another collection while dropping
    duplicates
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Processing intermediate data in an application for a specific purpose – such
    as finding common elements, combining unique elements across multiple containers,
    dropping duplicates, and so on
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Immutable containers – tuples
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Tuples are an immutable version of lists in Python. Since they are unchangeable
    after creation, they don't support any of the methods of list modification such
    as insert, append, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Tuples have the same time complexity as when using the index and search (via
    **item in tuple**) as lists. However, they take much less memory overhead when
    compared to lists; the interpreter optimizes them more as they are immutable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hence tuples can be used whenever there are use cases for reading, returning,
    or creating a container of data that is not going to be changed but requires iteration.
    Some examples are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Row-wise data loaded from a data store that is going to have only read access.
    For example, results from a DB query, processed rows from reading a CSV file,
    and so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Constant set of values that needs iteration over and over again. For example,
    a list of configuration parameters loaded from a configuration file.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When returning more than one value from a function. In this case, unless one
    explicitly returns a list, Python always returns a tuple by default.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When a mutable container needs to be a dictionary key. For example, when a list
    or set needs to be associated to a value as a dictionary key, the quick way is
    to convert it to a tuple.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: High performance containers – the collections module
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The collection module supplies high performance alternatives to the built-in
    default container types in Python, namely `list`, `set`, `dict`, and `tuple`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will briefly look at the following container types in the collections module:'
  prefs: []
  type: TYPE_NORMAL
- en: '`deque`: Alternative to a list container supporting fast insertions and pops
    at either ends'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`defaultdict`: Sub-class of `dict` that provides factory functions for types
    to provide missing values'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`OrderedDict`: Sub-class of `dict` that remembers the order of insertion of
    keys'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Counter`: Dict sub-class for keeping count and statistics of hashable types'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Chainmap`: Class with a dictionary-like interface for keeping track of multiple
    mappings'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`namedtuple`: Type for creating tuple-like classes with named fields'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: deque
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A deque or *double ended queue* is like a list but supports nearly constant
    (O(1)) time appends and pops from either side as opposed to a list, which has
    an O(n) cost for pops and inserts at the left.
  prefs: []
  type: TYPE_NORMAL
- en: 'Deques also support operations such as rotation for moving `k` elements from
    back to front and reverse with an average performance of O(k). This is often slightly
    faster than the similar operation in lists, which involves slicing and appending:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: By a simple `timeit` measurement, you should find that deques have a slight
    performance edge over lists (about 10-15%), in the above example.
  prefs: []
  type: TYPE_NORMAL
- en: defaultdict
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Default dicts are dict sub-classes that use type factories to provide default
    values to dictionary keys.
  prefs: []
  type: TYPE_NORMAL
- en: A common problem one encounters in Python when looping over a list of items
    and trying to increment a dictionary count is that there may not be any existing
    entry for the item.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, if one is trying to count the number of occurrences of a word
    in a piece of text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: We are forced to write code like the preceding or a variation of it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another example is when grouping objects according to a key using a specific
    condition, for example, trying to group all strings with the same length to a
    dictionary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: A `defaultdict` container solves these problems elegantly by defining a type
    factory to supply the default argument for any key that is not yet present in
    the dictionary. The default factory type supports any of the default types and
    defaults to `None`.
  prefs: []
  type: TYPE_NORMAL
- en: 'For each type, its empty value is the default value. This means:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'The word-count code can then be rewritten as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, for the code which groups strings by their length we can write this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: OrderedDict
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: OrderedDict is a sub-class of dict that remembers the order of the insertion
    of entries. It kind of behaves as a dictionary and list hybrid. It behaves like
    a mapping type but also has list-like behavior in remembering the insertion order
    plus supporting methods such as `popitem` to remove the last or first entry.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: You can compare and contrast how the dictionary changes the order around and
    how the `OrdredDict` container keeps the original order.
  prefs: []
  type: TYPE_NORMAL
- en: This allows a few recipes using the `OrderedDict` container.
  prefs: []
  type: TYPE_NORMAL
- en: Dropping duplicates from a container without losing the order
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Let us modify the cities list to include duplicates:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: See how the duplicates are dropped but the order is preserved.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a Least Recently Used (LRU) cache dictionary
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: An LRU cache gives preference to entries that are recently used (accessed) and
    drops those entries that are least used. This is a common caching algorithm used
    in HTTP caching servers such as Squid and in places where one needs to keep a
    limited size container that keeps recently accessed items preferentially over
    others.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here we make use of the behavior of `OrderedDict`: when an existing key is
    removed and re-added, it is added at the end (the right side):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: Here is a demonstration.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: Since a key `mumbai` was set first and never set again, it became the leftmost
    one and got dropped off.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Notice how the next candidate to drop off is `bangalore`, followed by `chennai`.
    This is because `chennai` was set once more after `bangalore` was set.
  prefs: []
  type: TYPE_NORMAL
- en: Counter
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A counter is a subclass of a dictionary to keep a count of hashable objects.
    Elements are stored as dictionary keys and their counts get stored as the values.
    The `Counter` class is a parallel for multisets in languages such as C++ or Bag
    in languages like Smalltalk.
  prefs: []
  type: TYPE_NORMAL
- en: A counter is a natural choice for keeping the frequency of items encountered
    when processing any container. For example, a counter can be used to keep the
    frequency of words when parsing text or the frequency of characters when parsing
    words.
  prefs: []
  type: TYPE_NORMAL
- en: For example, both of the following code snippets perform the same operation
    but the counter one is less verbose and compact.
  prefs: []
  type: TYPE_NORMAL
- en: They both return the most common 10 words from the text of the famous Sherlock
    Holmes Novel, namely the "The Hound of Baskerville" from its gutenberg version
    online.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the `defaultdict` container in the following code:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the `Counter` class in the following code:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: ChainMap
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A `ChainMap` is a dictionary-like class that groups multiple dictionaries or
    similar mapping data structures together to create a single view that is updateable.
  prefs: []
  type: TYPE_NORMAL
- en: All of the usual dictionary methods are supported. Lookups search successive
    maps until a key is found.
  prefs: []
  type: TYPE_NORMAL
- en: The `ChainMap` class is a more recent addition to Python, having been added
    in Python 3.3.
  prefs: []
  type: TYPE_NORMAL
- en: When you have a scenario where you keep updating keys from a source dictionary
    to a target dictionary over and over again, a `ChainMap` class can work in your
    favor in terms of performance, especially if the number of updates is large.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some practical uses of a `ChainMap`:'
  prefs: []
  type: TYPE_NORMAL
- en: A programmer can keep the `GET` and `POST` arguments of a web framework in separate
    dictionaries and keep the configuration updated via a single `ChainMap`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keeping multilayered configuration overrides in applications.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Iterating over multiple dictionaries as a view when there are no overlapping
    keys.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A `ChainMap` class keeps the previous mappings in its maps attribute. However,
    when you update a dictionary with mappings from another dictionary, the original
    dictionary state is lost. Here is a simple demonstration:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: namedtuple
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A namedtuple is like a class with fixed fields. Fields are accessible via attribute
    lookups like a normal class but are also indexable. The entire namedtuple is also
    iterable like a container. In other words, a namedtuple behaves like a class and
    a tuple combined in one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s create an instance of Employee:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'We can iterate over the fields of the instance, as if it is an iterator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'Once created, the `namedtuple` instance, like a tuple, is read-only:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'To update values, the `_replace` method can be used. It returns a new instance
    with the specified keyword arguments replaced with new values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'A namedtuple is much more memory-efficient when compared to a class which has
    the same fields. Hence a namedtuple is very useful in the following scenarios:'
  prefs: []
  type: TYPE_NORMAL
- en: A large amount of data needs to be loaded as read-only with keys and values
    from a store. Examples are loading columns and values via a DB query or loading
    data from a large CSV file.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When a lot of instances of a class need to be created but not many write or
    set operations need to be done on the attributes. Instead of creating class instances,
    `namedtuple` instances can be created to save on memory.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `_make` method can be used to load an existing iterable that supplies fields
    in the same order to return a `namedtuple` instance. For example, if there is
    an `employees.csv` file with the columns name, age, gender, title, and department
    in that order, we can load them all into a container of `namedtuples` using the
    following command line:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: Probabilistic data structures – bloom filters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before we conclude our discussion on the container data types in Python, let
    us take a look at an important probabilistic data structure named **Bloom Filter**.
    Bloom filter implementations in Python behave like containers, but they are probabilistic
    in nature.
  prefs: []
  type: TYPE_NORMAL
- en: A bloom filter is a sparse data structure that allows us to test for the presence
    of an element in the set. However, we can only positively be sure of whether an
    element is not there in the set – that is, we can assert only for true negatives.
    When a bloom filter tells us an element is there in the set, it might be there
    – in other words, there is a non-zero probability that the element may actually
    be missing.
  prefs: []
  type: TYPE_NORMAL
- en: Bloom filters are usually implemented as bit vectors. They work in a similar
    way to a Python dictionary in that they use hash functions. However, unlike dictionaries,
    bloom filters don't store the actual elements themselves. Also elements, once
    added, cannot be removed from a bloom filter.
  prefs: []
  type: TYPE_NORMAL
- en: Bloom filters are used when the amount of source data implies an unconventionally
    large amount of memory if we store all of it without hash collisions.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Python, the `pybloom` package provides a simple bloom filter implementation
    (however, at the time of writing, it doesn''t support Python 3.x, so the examples
    here are shown in Python 2.7.x):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'Let us write a program to read and index words from the text of `The Hound
    of Baskervilles`, which was the example we used in the discussion of the Counter
    data structure, but this time using a bloom filter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'Executing this, we get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The words `holmes`, `watson`, `hound`, and `moor` are some of the most common
    in the story of *The Hound of Basekervilles*, so it is reassuring that the bloom
    filter finds these words. On the other hand, the word `queen` never appears in
    the text so the bloom filter is correct on that fact (true negative). The length
    of the words in the text is 62,154, out of which only 9,403 got indexed in the
    filter.
  prefs: []
  type: TYPE_NORMAL
- en: Let us try and measure the memory usage of the bloom filter as opposed to the
    Counter. For that we will rely on memory profiler.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this test, we will rewrite the code using the `Counter` class as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'And the one using the bloom filter as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output from running the memory profiler for the first one:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Probabilistic data structures – bloom filters](../Images/image00431.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Memory usage by the Counter object when parsing the text of The Hound of the
    Basekervilles
  prefs: []
  type: TYPE_NORMAL
- en: 'The following result is for the second one:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Probabilistic data structures – bloom filters](../Images/image00432.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Memory usage by the Bloom filter for parsing text of The Hound of the Basekervilles
  prefs: []
  type: TYPE_NORMAL
- en: The final memory usage is roughly the same at about 50 MB each. In the case
    of the Counter, nearly no memory is used when the Counter class is created but
    close to 0.7 MB is used when words are added to the counter.
  prefs: []
  type: TYPE_NORMAL
- en: However, there is a distinct difference in the memory growth pattern between
    both these data structures.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of the bloom filter, an initial memory of 0.16 MB is allotted to
    it upon creation. The addition of the words seems to add nearly no memory to the
    filter and hence to the program.
  prefs: []
  type: TYPE_NORMAL
- en: 'So when should we use a bloom filter as opposed to, say, a dictionary or set
    in Python? Here are some general principles and real-world usage scenarios:'
  prefs: []
  type: TYPE_NORMAL
- en: When you are fine with not storing the actual element itself but only interested
    in the presence (or absence) of the element. In other words, where your application
    use case relies more on checking the absence of data than its presence.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When the size of your input data is so large that storing each and every item
    in a deterministic data structure (as a dictionary or hashtable) in memory is
    not feasible. A bloom filter takes much less data in memory as opposed to a deterministic
    data structure.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When you are fine with a certain well-defined error rate of *false positives*
    with your dataset – let us say this is 5% out of 1 million pieces of data – you
    can configure a bloom filter for this specific error rate and get a data hit rate
    that will satisfy your requirements.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Some real-world examples of using bloom filters are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Security testing**: Storing data for malicious URLs in browsers, for example'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bio-informatics**: Testing the presence of a certain pattern (a k-mer) in
    a genome'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To avoid storing URLs with just one hit in a distributed web caching infrastructure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter was all about performance. At the start of the chapter, we discussed
    performance and SPE. We looked at the two categories of performance testing and
    diagnostic tools – namely, stress testing tools and profiling/instrumentation
    tools.
  prefs: []
  type: TYPE_NORMAL
- en: We then discussed what performance complexity really means in terms of the Big-O
    notation and discussed briefly the common time orders of functions. We looked
    at the time taken by functions to execute and learned the three classes of time
    usage – namely `real`, `user`, and `sys` in POSIX systems.
  prefs: []
  type: TYPE_NORMAL
- en: We moved on to measuring performance and time in the next section – starting
    with a simple context manager timer and moving on to more accurate measurements
    using the `timeit` module. We measured the time taken for certain algorithms for
    a range of input sizes. By plotting the time taken against the input size and
    superimposing it on the standard time complexity graphs, we were able to get a
    visual understanding of the performance complexity of functions. We optimized
    the common item problem from its O(n*log(n)) performance to O(n) and the plotted
    graphs of time usage confirmed this.
  prefs: []
  type: TYPE_NORMAL
- en: We then started our discussion on profiling code and saw some examples of profiling
    using the `cProfile` module. The example we chose was a prime number iterator
    returning the first `n` primes performing at O(n). Using the profiled data, we
    optimized the code a bit, making it perform better than O(n). We briefly discussed
    the `pstats` module and used its `Stats` class to read profile data and produce
    custom reports ordered by a number of available data fields. We discussed two
    other third-party profilers – the `liner_profiler` and the `memory_profiler`,
    which profile code line by line – and discussed the problem of finding sub-sequences
    among two sequences of strings, writing an optimized version of them, and measuring
    its time and memory usage using these profilers.
  prefs: []
  type: TYPE_NORMAL
- en: Among other tools, we discussed objgraph and pympler – the former as a visualization
    tool to find relations and references between objects, helping to explore memory
    leaks, and the latter as a tool to monitor and report the memory usage of objects
    in the code and provide summaries.
  prefs: []
  type: TYPE_NORMAL
- en: In the last section on Python containers, we looked at the best and worst use
    case scenarios of standard Python containers – such as list, dict, set, and tuple.
    We then studied high performance container classes in the collections module –
    `deque`, `defaultdict`, `OrderedDict`, `Counter`, `Chainmap`, and `namedtuple`,
    with examples and recipes for each. Specifically, we saw how to create an LRU
    cache very naturally using `OrderedDict`.
  prefs: []
  type: TYPE_NORMAL
- en: Towards the end of the chapter, we discussed a special data structure called
    the bloom filter, which is very useful as a probabilistic data structure to report
    true negatives with certainty and true positives within a pre-defined error rate.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will discuss a close cousin of performance, scalability,
    where we will look at the techniques of writing scalable applications and the
    details of writing scalable and concurrent programs in Python.
  prefs: []
  type: TYPE_NORMAL
