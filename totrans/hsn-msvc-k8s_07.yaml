- en: Talking to the World - APIs and Load Balancers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we're finally going to open Delinkcious to the world and let
    users interact with it from outside the cluster.  This is important because Delinkcious
    users can't access the internal services running inside the cluster. We're going
    to significantly expand the capabilities of Delinkcious by adding a Python-based
    API gateway service and expose it to the world (including social login). We'll add
    a gRPC-based news service that users can hit to get news about other users they
    follow. Finally, we will add a message queue that lets services communicate in
    a loosely coupled manner.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Getting familiar with Kubernetes services
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: East-west versus north-south communication
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding ingress and load balancing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Providing and consuming a public REST API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Providing and consuming an internal gRPC API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sending and receiving events via a message queue
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparing for service meshes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will add a Python service to Delinkcious. There is no need
    to install anything new. We will build a Docker image for the Python service later.
  prefs: []
  type: TYPE_NORMAL
- en: The code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can find the updated Delinkcious application here: [https://github.com/the-gigi/delinkcious/releases/tag/v0.5](https://github.com/the-gigi/delinkcious/releases/tag/v0.5)'
  prefs: []
  type: TYPE_NORMAL
- en: Getting familiar with Kubernetes services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Pods (one or more containers bundled together) are the units of work in Kubernetes.
    Deployments make sure that there are enough pods running. However, individual
    pods are ephemeral. Kubernetes services are where the action is and how you can
    expose your pods as a coherent service to other services in the cluster or even
    externally to the world. A Kubernetes service provides a stable identity and typically
    maps 1:1 to an application service (which may be a microservice or a traditional
    fat service). Let''s look at all the services:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: You've seen how the Delinkcious microservices are deployed using Kubernetes
    services and how they can discover and call each other through the environment
    variables Kubernetes provides. Kubernetes also provides DNS-based discovery.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each service can be accessed inside the cluster via the DNS name:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: I prefer to use environment variables because it allows me to run the services
    outside of Kubernetes for testing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is how to find the IP address of the `social-graph-manager` service using
    both environment variables and DNS:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Kubernetes associates a service with its backing pods by specifying a label
    selector. For example, as shown in the following code, `news-service` is backed
    by pods that have the `svc: link` and `app: manager` labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, Kubernetes manages the IP addresses of all the pods that match the label
    selector using an `endpoints` resource, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The `endpoints` resource always keeps an up-to-date list of the IP addresses
    and ports of all the backing pods of a service. When pods are added, removed,
    or recreated with another IP address and port, the `endpoints` resource is updated.
    Now, let's see what types of services are available in Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Service types in Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Kubernetes services always have a type. It''s important to understand when
    to use each type of service. Let''s go over the various service types and the
    differences between them:'
  prefs: []
  type: TYPE_NORMAL
- en: '**ClusterIP (default)**: The ClusterIP type means that the service is only
    accessible inside the cluster. This is the default, and it''s perfect for microservices
    to communicate with each other. For testing purposes, you can expose such services
    using `kube-proxy` or `port-forwarding`. It is also a good way to view the Kubernetes
    dashboard or other UIs of internal services, such as Argo CD in Delinkcious.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you don't specify a type of ClusterIP, set the `ClusterIP` to `None`.
  prefs: []
  type: TYPE_NORMAL
- en: '**NodePort**: A service that''s of the NodePort type is exposed to the world
    through a dedicated port on all the nodes. You can access the service through
    `<Node IP>:<NodePort>`. The NodePort will be selected from a range you can control
    via `--service-node-port-range` to the Kubernetes API server if you run it yourself
    (by default, this is 30000-32767).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can also explicitly specify NodePort in your service definition. If you
    have a lot of services exposed via node ports that you specify, you'll have to
    manage those ports carefully to avoid conflicts. When a request comes into any
    node though the dedicated NodePort, the kubelet will take care of forwarding it
    to a node that has one of the backing pods on it (you can find it via endpoints).
  prefs: []
  type: TYPE_NORMAL
- en: '**LoadBalancer**: This type of service is most common when your Kubernetes
    cluster runs on a cloud platform that provides load balancer support. Although
    there are Kubernetes-aware load balancers for on-premise clusters too, the external
    load balancer will be in charge of accepting external requests and routing them
    through the service to the backing pods. There are often cloud provider-specific
    intricacies such as special annotations or having to create dual services to handle
    internal and external requests. We will use the LoadBalancer type to expose Delinkcious
    to the world of minikube, which provides a load balancer emulation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ExternalName**: These services just resolve requests to the service to an
    externally provided DNS name. This is useful if your services need to talk to
    external services not running in the cluster, but you still want to be able to
    find them as if they were Kubernetes services. This may be useful if you plan
    to migrate those external services to the cluster at some point.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we understand what services are all about, let's discuss the differences
    between cross-service communication inside the cluster and exposing services outside
    the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: East-west versus north-south communication
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: East-west communication is when services/pods/containers communicate with each
    other inside the cluster. As you may recall, Kubernetes exposes all the services
    inside the cluster via both DNS and environment variables. This solves the service
    discovery problem inside the cluster. It is up to you to impose further restrictions
    via network policies or other mechanisms. For example, in [Chapter 5](0d340a5c-b2da-41ab-a50d-56bd985c10f2.xhtml),* Configuring
    Microservices with Kubernetes*, we established mutual authentication between the
    link service and the social graph service.
  prefs: []
  type: TYPE_NORMAL
- en: 'North-south communication is about exposing services to the world. In theory,
    you could expose just your services via NodePort, but this approach is beset by
    numerous problems, including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: You have to deal with secure/encrypted transport yourself
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can't control which pods will actually service requests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You have to either let Kubernetes choose random ports for your services or manage
    port conflicts carefully
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Only one service can be exposed via each port (for example, the coveted port
    `80` can't be reused)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The production approved methods for exposing your services are used via an ingress
    controller and/or a load balancer.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding ingress and load balancing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The ingress concept in Kubernetes is about controlling access to your services
    and potentially providing additional features, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: SSL termination
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Authentication
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Routing to multiple services
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is an ingress resource that defines routing rules for other relevant information,
    and there is also an ingress controller that reads all the ingress resources defined
    in the cluster (across all namespaces). The ingress resource receives all the
    requests and routes to the target services that distribute them to the backing
    pods. The ingress controller serves as a cluster-wide software load balancer and
    router. Often, there will be a hardware load balancer that sits in front of the
    cluster and sends all traffic to the ingress controller.
  prefs: []
  type: TYPE_NORMAL
- en: Let's go ahead and put all of these concepts together and expose Delinkcious
    to the world by adding a public API gateway.
  prefs: []
  type: TYPE_NORMAL
- en: Providing and consuming a public REST API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will build a whole new service in Python (API gateway) to
    demonstrate that Kubernetes is really language-agnostic. Then, we will add user
    authentication via OAuth2 and expose the API gateway service externally.
  prefs: []
  type: TYPE_NORMAL
- en: Building a Python-based API gateway service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The API gateway service is designed to receive all requests from outside the
    cluster and route them to the proper services. Here is the directory''s structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'This is a little different from the Go services. The code is under the `api_gateway_service`
    directory, which is also a Python package. The Kubernetes resources are under
    the `k8s` subdirectory, and there is a `tests` subdirectory too. In the top directory,
    the `run.py` file is the entry point, as defined in the `Dockerfile`. The `main()`
    function in `run.py` invokes the `app.run()` method of the app that''s imported
    from the `api.py` module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The `api.py` module is responsible for creating the app, hooking up the routing,
    and implementing social login.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing social login
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `api-gateway` service utilizes several Python packages to assist in implementing
    social login via GitHub. Later, we will cover the user flow, but first, we will
    take a look at the code that implements it. The `login()` method is reaching out
    to GitHub and requesting authorization to the current user, who must be logged
    in to GitHub and give authorization to Delinkcious.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `logout()` method just removed the access token from the current session. The
    `authorized()` method is getting called by GitHub as a redirect after a successful
    login attempt and provides an access token that is displayed for the user in their
    browser. This access token must be passed as a header to all future requests to
    the API gateway:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'When a user is passing a valid access token, Delinkcious can retrieve their
    name and email from GitHub. If the access token is missing or invalid, the request
    will be rejected with a 401 access denied error. This happens in the `_get_user()`
    function in `resources.py`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The GitHub object is created and initialized in the `create_app()` function
    of the `api.py` module. First, it imports a few third-party libraries, that is,
    `Flask`, `OAuth`, and `Api` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, it initializes the `Flask` app with a GitHub `Oauth` provider:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, it sets the routing map and stores the initialized `app` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Routing traffic to internal microservices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The main job of the API gateway service is to implement the API gateway pattern
    we discussed in [Chapter 2](d4214218-a4e9-4df8-813c-e00df71da935.xhtml), *Getting
    Started with Microservices*. For example, here is how it routes the get links
    requests to the proper method of the link microservice.
  prefs: []
  type: TYPE_NORMAL
- en: The `Link` class is derived from the `Resource` base class. It gets the host
    and port from the environment and constructs the base URL.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `get()` method is called when a GET request for the `links` endpoint comes
    in. It extracts the username from the GitHub token in the `_get_user()` function
    and parses the query part of the request URL for the other parameter. Then, it
    makes its own request to the link manager service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Utilizing base Docker images to reduce build time
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When we built Go microservices for Delinkcious, we used the scratch image as
    the base and just copied the Go binary. The images are super lightweight, at less
    than 10 MB. However, the API gateway is almost 500 MB, even when using `python:alpine`,
    which is much lighter than the standard Debian-based Python image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'In addition, the API gateway needs to build some bindings to native libraries.
    Installing the C/C++ toolchain and then building the native libraries takes a
    long time (more than 15 minutes). Docker shines here with reusable layers and
    base images. We can put all the heavyweight stuff into a separate base image at
    `svc/shared/docker/python_flask_grpc/Dockerfile`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The `requirements.txt` file contains the dependencies for `Flask` applications
    that execute social login and need to consume a gRPC service (more on this later):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'With all of this in place, we can build the base image, and then the API gateway
    Dockerfile can be based on it. The following is the super-simple build script
    at `svc/shared/docker/python_flask_grpc/build.sh` that builds the base image and
    pushes it to DockerHub:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s take a look at the Dockerfile for the API gateway service at `svc/api_gateway_service/Dockerfile`.
    It is based on our base image. Then, it copies the `api_gate_service` directory,
    exposes the `5000` port, and executes the `run.py` script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The benefit is that as long as the heavy base image doesn't change, then making
    changes to the actual API service gateway code will result in lightning fast Docker
    image builds. We're talking a few seconds compared to 15 minutes. At this point,
    we have a nice and quick build-test-debug-deploy for the API gateway service.
    Now is a good time to add ingress to the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Adding ingress
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'On Minikube, you must enable the ingress add-on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: On other Kubernetes clusters, you may want to install your own favorite ingress
    controller (such as Contour, Traefik, or Ambassador).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code is for the ingress manifest for the API gateway service.
    By using this pattern, our entire cluster will have a single ingress that funnels
    every request to our API gateway service, which will route it to the proper internal
    service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The single ingress service is simple and effective. On most cloud platforms,
    you pay per ingress resource, since a load balancer is created for each ingress
    resource. You can scale the number of API gateway instances easily since it is
    totally stateless.
  prefs: []
  type: TYPE_NORMAL
- en: Minikube does a lot of magic under the covers with networking, simulating load
    balancers, and tunneling traffic. I don't recommend using Minikube to test ingress
    to the cluster. Instead, we will use a service of the LoadBalancer type and access
    it through the Minikube cluster IP.
  prefs: []
  type: TYPE_NORMAL
- en: Verifying that the API gateway is available outside the cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Delinkcious uses GitHub as a social login provider. You must have a GitHub account
    to follow along.
  prefs: []
  type: TYPE_NORMAL
- en: 'The user flow is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Find the Delinkcious URL (on Minikube, this will change frequently).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Log in and get an access token.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Hit the Delinkcious API gateway from outside the cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let's dive in and go over this in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Finding the Delinkcious URL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In a production cluster, you''ll have a well-known DNS name configured and
    a load balancer hooked up to that name. With Minikube, we can get the API gateway
    service URL using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'It''s convenient to store it in an environment variable for interactive use
    with commands, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Getting an access token
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here are the steps for getting an access token:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have the API gateway URL, we can browse to the login endpoint,
    that is, `http://192.168.99.138:31658/login`. If you''re signed into your GitHub
    account, you''ll see the following dialog box:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/806ac3a0-d918-4aef-9c87-de521ba70753.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, if this is the first time your logging in to Delinkcious, GitHub will
    ask you to authorize Delinkcious to get access to your email and name:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/975e7472-5cc3-4b1c-ad44-32550e819dae.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If you approve of this, then you''ll be redirected to a page that will show
    you a lot of information about your GitHub profile, but, most importantly, provide
    you with an access token, as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/d2ca79b6-c9ee-4133-b2cc-621775611cb3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s store the access token in an environment variable, too:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have all the information we need to access Delinkcious from the
    outside, let's take it for a test drive.
  prefs: []
  type: TYPE_NORMAL
- en: Hitting the Delinkcious API gateway from outside the cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We''ll use HTTPie to hit the API gateway endpoint at `${DELINKCIOUS_URL}/v1.0/links`.
    To authenticate, we must provide the access token as a header, that is, `"Access-Token:
    ${DELINKCIOUS_TOKEN}"`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Starting with a clean slate, let''s verify that there are no links whatsoever:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Alright – so far, so good. Let''s add a couple of links by sending a POST request
    to the `/v1.0/links` endpoint. Here is the first link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'And here is the second link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'No errors. That''s great. By getting the links again, we can see the new links
    we just added:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: We have successfully established an end-to-end flow, including user authentication,
    thus hitting a Python API gateway service that talks to a Go microservice via
    its internal HTTP REST API and stores information in a relational DB. Now, let's
    up the ante and add yet another service.
  prefs: []
  type: TYPE_NORMAL
- en: This time, it will be a Go microservice that uses a gRPC transport.
  prefs: []
  type: TYPE_NORMAL
- en: Providing and consuming an internal gRPC API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The service we will implement in this section is called the news service. Its
    job is to keep track of link events, such as link added or link updated, and return
    new events to users.
  prefs: []
  type: TYPE_NORMAL
- en: Defining the NewsManager interface
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This interface exposes a single `GetNews()` method. Users may invoke it and
    receive a list of link events from users they follow. Here is the Go interface
    and related structs. It doesn''t get much simpler: a single method with a request
    struct with `username` and `token` fields, as well as a result struct. The resulting
    struct contains a list of `Event` structs with the following information: `EventType`,
    `Username`, `Url`, and `Timestamp`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Implementing the news manager package
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The implementation of the core logic service is in `pkg/news_manager`. Let's
    take a look at the `new_manager.go` file. The `NewsManager` struct has an `InMemoryNewsStore`
    called `eventStore` that implements the `GetNews()` method for the `NewsManager`
    interface. It delegates the work of actually fetching the news to the store.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, it is aware of pagination and takes care of converting the token from
    a string into an integer to match the store preferences:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The store is very basic and just keeps a map between usernames and all their
    events, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The store implements its own `GetNews()` method (a different signature from
    the `interface` method). It just returns the requested slice for the target user
    based on the start index and the maximum page size:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'It also has a method for adding new events:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Now that we've implemented the core logic of storing and providing news to users,
    let's look at how to expose this functionality as a gRPC service.
  prefs: []
  type: TYPE_NORMAL
- en: Exposing NewsManager as a gRPC service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before diving into the gRPC implementation of the news service, let''s see
    what all the fuss is about. The gRPC is a collection of a wire protocol, payload
    format, conceptual framework, and code generation facilities for interconnecting
    services and applications. It originated in Google (hence the g in gRPC) and is
    a highly performant and mature RPC framework. It has many things going for it,
    such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Cross-platform
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wide spread adoption by industry
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Idiomatic client libraries for all relevant programming languages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extremely efficient wire protocols
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Google protocol buffers for strongly typed contracts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: HTTP/2 support enables bi-directional streaming
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Highly extensible (customize your own authentication, authorization, load balancing,
    and health checking)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Excellent documentation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The bottom line is that for internal microservices, it is superior in almost
    every way to HTTP-based REST APIs.
  prefs: []
  type: TYPE_NORMAL
- en: For Delinkcious, it's a great fit because Go-kit, which we selected as our microservice
    framework, has great support for gRPC.
  prefs: []
  type: TYPE_NORMAL
- en: Defining the gRPC service contract
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: gRPC requires that you define a contract for your service in a special DSL inspired
    by protocol buffers. It is pretty intuitive and lets gRPC generate a lot of boilerplate
    code for you. I chose to locate the contract and the generated code in a separate
    top-level directory called **pb** (common short name for **protocol buffers**)
    because different parts of the generated code will be used by services and consumers.
    In these cases, it is often best to put the shared code in a separate location
    and not arbitrarily throw it into the service or the client.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the `pb/new-service/pb/news.proto` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: We don't need to go over the syntax and meaning of each and every line. The
    short version is that requests and responses are always messages. Service-level
    errors need to be embedded in the response message. Other errors, such as network
    or invalid payloads, will be reported separately. One interesting tidbit is that,
    in addition to primitive data types and embedded messages, you can use other high-level
    types, such as the `google.protobuf.Timestamp` data type. This elevates the abstraction
    level significantly and brings the benefits of strong typing for things such as
    dates and timestamps that you always have to serialize and deserialize yourself
    when working with JSON over HTTP/REST.
  prefs: []
  type: TYPE_NORMAL
- en: The service definition is cool, but we need some actual code to connect the
    dots. Let's see how gRPC can help with this task.
  prefs: []
  type: TYPE_NORMAL
- en: Generating service stubs and client libraries with gRPC
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The gRPC model is used to generate both service stubs and client libraries using
    a tool called `protoc`. We need to generate both Go code for the news service
    itself and Python code for the API gateway that consumes it.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can generate `news.pb.go` by running the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'You can generate `news_pb2.py` and `news_pb2_grpc.py` by running the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: At this point, both the Go client code and Python client code can be used to
    call the news service from Go code or from Python code.
  prefs: []
  type: TYPE_NORMAL
- en: Using Go-kit to build the NewsManager service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here is the implementation of the service itself in `news_service.go`. It looks
    very similar to an HTTP service. Let''s dissect the important sections. First,
    it imports some libraries, including the generated gRPC code in `pb/news-service-pb`,
    `pkg/news_manager`, and a general gRPC library called `google.golang.org/grpc`.
    At the beginning of the `Run()` function, it gets the `service` port to listen
    from the environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we need to create a standard TCP listener on the target port:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Furthermore, we have to connect to a NATS message queue service. We''ll discuss
    this in detail in the next section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Here comes the main initialization code. It instantiates a new news manager,
    creates a new gRPC server, creates a news manager object, and registers the news
    manager with the gRPC server. The `pb.RegisterNewsManager()` method was generated
    by gRPC from the `news.proto` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, the gRPC server starts listening on the TCP listener:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Implementing the gRPC transport
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The last piece of the puzzle is implementing the gRPC transport in the `transport.go`
    file. It is similar, conceptually, to the HTTP transport, but there are a few
    details that are different. Let's break it down so it's clear how all the pieces
    fit together.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, all the relevant packages are imported, including the gRPC transport
    from go-kit. Note that in `news_service.go`, there is no mention of go-kit anywhere.
    You can definitely implement a gRPC service directly in Go with the general gRPC
    libraries. However, here, go-kit will help make this much easier via its service
    and endpoints concepts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The `newEvent()` function is a helper that adopts `om.Event` from our abstract
    object model to the gRPC-generated event object. The most important part is translating
    the event type and the timestamp:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Decoding the request and encoding the response is pretty trivial – there''s
    no need to serialize or deserialize any JSON code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Creating the endpoint is similar to the HTTP transport you''ve seen with other
    services. It invokes the actual service implementation and then translates the
    response and handles errors, if there are any:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'The handler implements the gRPC news interface from the code generated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'The `newNewsServer()` function ties everything together. It returns a gRPC
    handler wrapped in a Go-kit handler that hooks up the endpoint, the request decoder,
    and the response encoder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: This may seem very confusing, with all the layers and nested functions, but
    the bottom line is that you have to write very little glue code (and can generate
    it, which is ideal) and end up with a very clean, safe (strongly typed), and efficient
    gRPC service.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a gRPC news service that can serve the news, let's see how
    we can feed it the news.
  prefs: []
  type: TYPE_NORMAL
- en: Sending and receiving events via a message queue
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The news service needs to store link events for each user. The link service
    knows when links are added, updated, or deleted by different users. One approach
    to solve this problem is to add another API to the news service and have the link
    service invoke this API and notify the news service for each relevant event. However,
    this approach creates a tight coupling between the link service and the news service.
    The link service doesn''t really care about the news service since it doesn''t
    need anything from it. Instead, let''s go for a loosely-coupled solution. The
    link service will just send events to a general-purpose message queue service.
    Then, independently, the news service will subscribe to receive messages from
    that messages queue. There are several benefits to this approach, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: No need for more complicated service code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fits perfectly with the interaction model of event notification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Easy to add additional listeners to the same events without changing the code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The terms that I used here, that is, *message*, *event*, and *notification*,
    are interchangeable. The idea is that a source has some information to share with
    the world in a fire-and-forget way.
  prefs: []
  type: TYPE_NORMAL
- en: It doesn't need to know who is interested in the information (this could be
    nobody or multiple listeners) and whether it was processed successfully. Delinkcious
    uses the NATS messaging system for loosely coupled communication between services.
  prefs: []
  type: TYPE_NORMAL
- en: What is NATS?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'NATS ([https://nats.io/](https://nats.io/)) is an open source message queue
    service. It is a **Cloud Native Computing Foundation** (**CNCF**) project that''s
    implemented in Go and is considered one of the top contenders when you need a
    message queue in Kubernetes. NATS supports multiple models of message passing,
    such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Publish-subscribe
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Request-reply
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Queueing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'NATS is very versatile and can be used for many use cases. It can also run
    in a highly available cluster. For Delinkcious, we will use the publish-subscribe
    model. The following diagram illustrates the pub-sub message passing model. A
    publisher publishes a message and all the subscribers receive the same message:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/f623f1c6-9276-40f2-b18b-989601014789.png)'
  prefs: []
  type: TYPE_IMG
- en: Let's deploy NATS in our cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying NATS in the cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, let''s install the NATS operator ([https://github.com/nats-io/nats-operator](https://github.com/nats-io/nats-operator)).
    The NATS operator helps you to manage NATS clusters in Kubernetes. Here are the
    commands to install it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'The NATS operator provides a NatsCluster **Custom Resource Definition** (**CRD**)
    that we will use to deploy NATS in our Kubernetes cluster. Don''t get confused
    by this NATS cluster within the Kubernetes cluster relationship. This is really
    nice since we can deploy the NATS cluster just like built-in Kubernetes resources.
    Here is the YAML manifest that''s available in `svc/shared/k8s/nats_cluster.yaml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s deploy it using `kubectl` and verify that it was deployed properly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: This looks good. The `nats-cluster` service listening on port `4222` is the
    NATS server. The other service is a management service. Let's send some events
    to the NATS server.
  prefs: []
  type: TYPE_NORMAL
- en: Sending link events with NATS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As you may recall, we defined a `LinkManagerEvents` interface in our object
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'The `LinkManager` package receives this event link in its `NewLinkManager()`
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Later, when a link is added, updated, or deleted, `LinkManager` will call the
    corresponding `OnLinkXXX()` method. For example, when `AddLink()` is called, the
    `OnLinkAdded()` method is called on the sink for each follower:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'This is great, but how are these events going to get to the NATS server? That''s
    where the link service comes into the picture. When instantiating the `LinkManager`
    object, it will pass a dedicated event sender object as the sink that implements
    `LinkManagerEvents`. Whenever it receives an event such as `OnLinkAdded()` or
    `OnLinkUpdated()`, it publishes the event to the NATS server on the `link-events`
    subject. It ignores the `OnLinkDeleted()` event for now. This object lives in
    `pkg/link_manager_events package/sender.go`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the implementation of the `OnLinkAdded()`, `OnLinkUpdated()`, and `OnLinkDeleted()` methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'The `NewEventSender()` factory function accepts the URL of the NATS service
    it will send the events to and returns a `LinkManagerEvents` interface that can
    serve as a sink for `LinkManager`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, all the link service has to do is figure out the URL for the NATS server.
    Since the NATS server runs as a Kubernetes service, its hostname and port are
    available through environment variables, just like the Delinkcious microservices.
    The following is the relevant code from the `Run()` function of the link service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: At this point, whenever a new link is added or updated for a user, `LinkManager`
    will invoke the `OnLinkAdded()` or `OnLinkUpdated()` method for each of the followers,
    which will result in that event being sent to the NATS server on the `link-events`
    topic, where all the subscribers will receive it and can handle it. The next step
    is for the news service to subscribe to these events.
  prefs: []
  type: TYPE_NORMAL
- en: Subscribing to link events with NATS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The news service uses the `Listen()` function from `pkg/link_manager_events/listener.go`.
    It accepts the NATS server URL and an event sink that implements the `LinkManagerEvents`
    interface. It connects to the NATS server and then subscribes to the `link-events`
    subject. This is the same subject that the event sender is sending those events
    to:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s look at the `nats.go` file that defines the `link-events` subject,
    as well as the `connect()` function that''s used by both the event sender and
    the `Listen()` function. The connect function uses the `go-nats` client to establish
    a connection and then wraps it with a JSON encoder, which allows it to send and
    receive Go structs that get serialized automatically. This is pretty neat:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'The news service calls the `Listen()` function in its `NewNewsManager()` factory
    function. First, it instantiates the news manager object that implements `LinkManagerEvents`.
    Then, `if` composes a NATS server URL if a NATS hostname was provided and calls
    the `Listen()` function, thereby passing the news manager object as the sink:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: The next step is to do something with the incoming events.
  prefs: []
  type: TYPE_NORMAL
- en: Handling link events
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The news manager was subscribed to link events by the `NewNewsManager()` function,
    and the result is that those events will arrive as calls on `OnLinkAdded()` and
    `OnlinkUpdated()` (delete link events are ignored). The news manager creates an
    `Event` object that''s defined in the abstract object model, populates it with `EventType`,
    `Username`, `Url`, and `Timestamp`, and then calls the event store''s `AddEvent()`
    function. Here is the `OnLinkAdded()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the `OnLinkUpdated()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s see what the store does in its `AddEvent()` method. It''s pretty simple:
    the subscribed user is located in the `userEvents` map. If they don''t exist yet,
    then an empty entry is created and the new event added. If the target user calls
    `GetNews()`, they''ll receive the events that have been collected for them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'That concludes our coverage of the news service and its interactions with the
    link manager via the NATS service. This is an application of the **command query
    responsibility segregation** (**CQRS**) pattern we discussed in [Chapter 2](d4214218-a4e9-4df8-813c-e00df71da935.xhtml),
    *Getting Started with Microservices*. Here is what the Delinkcious system looks
    like now:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/47cae881-3b57-49f8-84b6-7fd50313a172.png)'
  prefs: []
  type: TYPE_IMG
- en: Now that we understand how events are handled in Delinkcious, let's take a quick
    look at service meshes.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding service meshes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A service mesh is another layer of management that's running in your cluster.
    We will look into service meshes and Istio in particular in [Chapter 13](b39834c8-859c-42a5-846a-e48b76dfd6cc.xhtml),
    *Service Mesh – Working with Istio*. At this point, I just want to mention that
    a service mesh often takes the role of the ingress controller too.
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the primary reasons to use a service mesh for ingress is that the built-in
    ingress resource, being very generic, is limited and suffers from multiple issues,
    such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: No good way to validate the rules
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ingress resources can conflict with one other
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with specific ingress controllers is often complicated and requires
    custom annotations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we accomplished many tasks and connected all the dots. In particular,
    we implemented two microservices design patterns (API gateway and CQRS), added
    a whole new service implemented in Python (including a split Docker base image),
    added a gRPC service, added an open source message queue system (NATS) to our
    cluster and integrated it with pub-sub message passing, and, finally, opened up
    our cluster to the world and demonstrated end-to-end interaction by adding and
    fetching links from Delinkcious.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, Delinkcious can be considered Alpha-grade software. It's functional,
    but not even close to production ready. In the next chapter, we will start making
    Delinkcious more robust by taking care of the most valuable commodity of any software
    system – the data. Kubernetes provides many facilities for managing data and stateful
    services that we will put to good use.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can refer to the following sources for more information regarding what
    was covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '**The Kubernetes service**: [https://kubernetes.io/docs/concepts/services-networking/service/](https://kubernetes.io/docs/concepts/services-networking/service/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Exposing your app as a service**: [https://kubernetes.io/docs/tutorials/kubernetes-basics/expose/expose-intro/](https://kubernetes.io/docs/tutorials/kubernetes-basics/expose/expose-intro/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Building Oauth apps**: [https://developer.github.com/apps/building-oauth-apps/](https://developer.github.com/apps/building-oauth-apps/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**High-performance gRPC**: [https://grpc.io/](https://grpc.io/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://www.devx.com/architect/high-performance-services-with-grpc.html](http://www.devx.com/architect/high-performance-services-with-grpc.html)'
  prefs: []
  type: TYPE_NORMAL
- en: '**NATS message broker**: [https://nats.io/](https://nats.io/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
