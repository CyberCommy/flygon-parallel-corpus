- en: Machine Learning with MLlib
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover how to build machine learning models with PySpark''s
    MLlib module. Even though it is now being deprecated and most of the models are
    now being moved to the ML module, if you store your data in RDDs, you can use
    MLlib to do machine learning. You will learn the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Loading the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transforming the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Standardizing the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating an RDD for training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predicting hours of work for census respondents
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Forecasting the income level of census respondents
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a clustering model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Computing performance statistics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loading the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to build a machine learning model, we need data. Thus, before we start,
    we need to read some data. In this recipe, and throughout this chapter, we will
    be using the 1994 census income data.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To execute this recipe, you need to have a working Spark environment. If you
    do not have one, you might want to go back to [Chapter 1](part0026.html#OPEK0-dc04965c02e747b9b9a057725c821827), *Installing
    and Configuring Spark* and follow the recipes you will find there.
  prefs: []
  type: TYPE_NORMAL
- en: The dataset was sourced from [http://archive.ics.uci.edu/ml/datasets/Census+Income](http://archive.ics.uci.edu/ml/datasets/Census+Income).
  prefs: []
  type: TYPE_NORMAL
- en: The dataset is located in the `data` folder in the GitHub repository for the
    book.
  prefs: []
  type: TYPE_NORMAL
- en: All the code that you will need in this chapter can be found in the GitHub repository
    we set up for the book: [http://bit.ly/2ArlBck](http://bit.ly/2ArlBck); go to
    `Chapter05` and open the `5\. Machine Learning with MLlib.ipynb` notebook.
  prefs: []
  type: TYPE_NORMAL
- en: No other prerequisites are required.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will read the data into a DataFrame so it is easier for us to work with.
    Later on, we will convert it into an RDD of labeled points. To read the data,
    execute the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First, we specify the path to our dataset. In our case, as with all the other
    datasets we use in this book, `census_income.csv` is located in the `data` folder,
    accessible from the parent folder.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we use the `.read` property of `SparkSession`, which returns the `DataFrameReader`
    object. The first parameter to the `.csv(...)` method specifies the path to the
    data. Our dataset has the column names in the first row, so we use the `header`
    option to instruct the reader to use the first row for column names. The `inferSchema`
    parameter instructs the `DataFrameReader` to automatically detect the datatype
    of each column.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s check whether the datatype inference is correct:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code produces the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00107.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, the datatype of certain columns was detected properly; without
    the `inferSchema` parameter, all the columns would default to strings.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'However, there''s a small problem with our dataset: most of the string columns
    have either leading or trailing white spaces. Here''s how you can correct this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We loop through all the columns in the `census` DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: The `.dtypes` property of a DataFrame is a list of tuples where the first element
    is the column name and the second element is the datatype.
  prefs: []
  type: TYPE_NORMAL
- en: 'If the type of the column is equal to string, we apply two functions: `.ltrim(...)`,
    which removes any leading whitespaces in a string, and `.rtrim(...)`, which removes
    any trailing whitespaces. The `.withColumn(...)` method does not append any new
    columns as we reuse the same name for the column: `col`.'
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Jumping straight into modeling the data is a misstep almost every new data scientist
    makes; we get too eager to get to the reward stage, so we forget about the fact
    that most of the time is actually spent doing the boring stuff of cleaning up
    our data and getting familiar with it. In this recipe, we will explore the census
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To execute this recipe, you need to have a working Spark environment. You should
    have already gone through the previousrecipe where we loaded the census data into
    a DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: No other prerequisites are required.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, we list all the columns we want to keep:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we select the numerical and categorical features as we will be exploring
    these separately:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First, we extract all the columns with their corresponding datatypes.
  prefs: []
  type: TYPE_NORMAL
- en: We have already discussed the `.dtypes` property of DataFrame stores in the
    previous recipe.
  prefs: []
  type: TYPE_NORMAL
- en: We will only keep `label`, which is the column that holds an identifier regarding
    whether a person makes more than $50,000 or not, and a handful of other numeric
    columns. In addition, we carry over all the string features.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we create a DataFrame with only the selected columns and extract all the
    numeric and categorical columns; we store these in the `cols_num` and `cols_cat`
    lists, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Numerical features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s explore the numerical features. Just like in [Chapter 4](part0186.html#5HC8K0-dc04965c02e747b9b9a057725c821827), *Preparing
    Data for Modeling*, for the numerical variables, we will calculate some basic
    descriptive statistics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: First, we further subset our `census_subset` to include only the numerical columns.
    Next, we extract the underlying RDD. Since every element of this RDD is a row,
    we first need to create a list so we can work with it; we achieve that using the
    `.map(...)` method.
  prefs: []
  type: TYPE_NORMAL
- en: For documentation on the `Row` class, check out [http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Row](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Row).
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have our RDD ready, we simply call the `.colStats(...)` method from
    the statistics module of MLlib. `.colStats(...)` accepts an RDD of numeric values;
    these can be either lists or vectors (either dense or sparse, see the documentation
    on `pyspark.mllib.linalg.Vectors` at [http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.linalg.Vectors](http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.linalg.Vectors)).
    A `MultivariateStatisticalSummary` trait is returned, which contains data such
    as count, max, mean, min, norms L1 and L2, number of nonzero observations, and
    the variance.
  prefs: []
  type: TYPE_NORMAL
- en: If you are familiar with C++ or Java, traits can be viewed as virtual classes
    (C++) or interfaces (Java). You can read more about traits at [https://docs.scala-lang.org/tour/traits.html](https://docs.scala-lang.org/tour/traits.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'In our example, we only select the min, mean, max, and variance. Here''s what
    we get back:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00108.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: So, the average age is about 39 years old. However, we definitely have an outlier
    in our dataset of 90 years old. In terms of capital gain or loss, the census respondents
    seem to be making more money than losing. On average, the respondents worked 40
    hours per week but we had someone working close to 100-hour weeks.
  prefs: []
  type: TYPE_NORMAL
- en: Categorical features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For the categorical data, we cannot calculate simple descriptive statistics.
    Thus, we are going to calculate frequencies for each distinct value in each categorical
    column. Here''s a code snippet that will achieve this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'First, we repeat what we have just done for the numerical columns but for the
    categorical ones: we subset `census_subset` to only the categorical columns and
    the label, access the underlying RDD, and transform each row into a list. We''re
    going to store the results in the `results_cat` dictionary. We loop through all
    the categorical columns and aggregate the data using the `.groupBy(...)` transformation.
    Finally, we create a list of tuples where the first element is the value (`el[0]`)
    and the second element is the frequency (`len(el[1])`).'
  prefs: []
  type: TYPE_NORMAL
- en: The `.groupBy(...)` transformation outputs a list where the first element is
    the value and the second is a `pyspark.resultIterable.ResultIterable` object that
    is effectively a list of all elements from the RDD that contains the value.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have our data aggregated, let''s see what we deal with:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00109.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The preceding list is abbreviated for brevity. Check (or run the code in) the `5\.
    Machine Learning with MLlib.ipynb` notebook present in our GitHub repository.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, we are dealing with an imbalanced sample: it is heavily skewed
    toward males and mostly white people. Also, in 1994 there were not many people
    earning more than $50,000, only about a quarter.'
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Another important metric you might want to check is the correlations between
    numerical variables. Calculating correlations with MLlib is very easy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The `.corr(...)` action returns a NumPy array or arrays, or, in other words,
    a matrix where each element is a Pearson (by default) or Spearman correlation
    coefficient.
  prefs: []
  type: TYPE_NORMAL
- en: 'To print it out, we just loop through all the elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We only print the upper triangular portion of the matrix without the diagonal.
    Using the enumerate allows us to print out the column names since the correlations
    NumPy matrix does not list them. Here''s what we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00110.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, there is not much correlation between our numerical variables.
    This is actually a good thing, as we can use all of them in our model since we
    will not suffer from much multicollinearity.
  prefs: []
  type: TYPE_NORMAL
- en: If you do not know what multicollinearity, is check out this lecture: [https://onlinecourses.science.psu.edu/stat501/node/343](https://onlinecourses.science.psu.edu/stat501/node/343).
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You might also want to check out this tutorial from Berkeley University: [http://ampcamp.berkeley.edu/big-data-mini-course/data-exploration-using-spark.html](http://ampcamp.berkeley.edu/big-data-mini-course/data-exploration-using-spark.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to build a successful statistical or machine learning model, we need
    to follow a simple (but hard!) rule: make it as simple as possible (so it generalizes
    the phenomenon being modeled well) but not too simple (so it loses its main ability
    to predict). A visual example of how this manifests is as follows (from [http://bit.ly/2GpRybB](http://bit.ly/2GpRybB)):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00111.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The middle chart shows a good fit: the model line follows the true function
    well. The model line on the left chart oversimplifies the phenomenon and has literally
    no predictive power (apart from a handful of points)—a perfect example of underfitting.
    The model line on the right follows the training data almost perfectly but if
    new data was presented, it would most likely misrepresent it—a concept known as
    overfitting, that is, it does not generalize well. As you can see from these three
    charts, the complexity of the model needs to be just right so it models the phenomenon
    well.'
  prefs: []
  type: TYPE_NORMAL
- en: Some machine learning models have a tendency to overtrain. For example, any
    models that try to find a mapping (a function) between the input data and the
    independent variable (or a label) have a tendency to overfit; these include parametric
    regression models, such as linear or generalized regression models, as well as
    recently (again!) popular neural networks (or deep learning models). On the other
    hand, some decision tree-based models (such as random forests) are less prone
    to overfitting even with more complex models.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, how do we get the model just right? There are four rules of thumb:'
  prefs: []
  type: TYPE_NORMAL
- en: Select your features wisely
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do not overtrain, or select a model that is less prone to overfitting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Run multiple model estimations with randomly selected data from your dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tune hyperparameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this recipe, we will focus on the first point, the remaining points will
    be covered in some of the recipes found in this and the two next chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To execute this recipe, you need to have a working Spark environment. You would
    have already gone through the *Loading the data* recipe where we loaded the census
    data into a DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: No other prerequisites are required.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to find the best features for the problem at hand, we first need to
    understand what problem we are dealing with, as different methods will be used
    for selecting features in regression problems or for classifiers:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Regression**: In regression, your target (or ground truth) is a *continuous*
    variable (such as number of work hours per week). You have two methods to select
    your best features:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pearson''s correlation**: We covered this one in the previous recipe. As
    noted there, the correlation can only be calculated between two numerical (continuous)
    features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Analysis of variance (ANOVA)**: It is a tool to explain (or test) the distribution
    of observations conditional on some categories. Thus, it can be used to select
    the most discriminatory (categorical) features of the continuous dependent variable.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Classification**: In classification, your target (or label) is a discrete
    variable of two (binomial) or many (multinomial) levels. There are also two methods
    that help to select the best features:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Linear discriminant analysis (LDA)**: This helps to find a linear combination
    of continuous features that best explains the variance of the categorical label'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***χ²* test**: A test that tests the independence between two categorical variables'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark, for now, allows us to test (or select) the best features between comparable
    variables; it only implements the correlations (the `pyspark.mllib.stat.Statistics.corr(...)`
    we covered earlier) and the χ² test (the `pyspark.mllib.stat.Statistics.chiSqTest(...)`
    or the `pyspark.mllib.feature.ChiSqSelector(...)` methods).
  prefs: []
  type: TYPE_NORMAL
- en: 'In this recipe, we will use `.chiSqTest(...)` to test the independence between
    our label (that is, an indicator that someone is earning more than $50,000) and
    the occupation of the census responder. Here''s a snippet that does this for us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First, we import the linear algebra portion of MLlib; we will be using some
    matrix representations later.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we build a pivot table where we group by the `occupation` feature and
    pivot by the `label` column (either `<=50K` or `>50K`). Each occurrence is counted
    and this results in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00112.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, we flatten the output by accessing the underlying RDD and selecting only
    the counts with the map transformation: `.map(lambda row: (row[1:]))`. The `.flatMap(...)`
    transformation creates a long list of all the values we need. We collect all the
    data on the driver so we can later create `DenseMatrix`.'
  prefs: []
  type: TYPE_NORMAL
- en: You should be cautious about using the `.collect(...)` action since it brings
    all the data to the driver. As you can see, we are only bringing the heavily aggregated
    representation of our dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have all our numbers on the driver, we can create their matrix representation;
    we will have a matrix of 15 rows and 2 columns. First, we check how many distinct
    occupation values there are by checking the count of the `census_occupation` elements.
    Next, we call the `DenseMatrix(...)` constructor to create our matrix. The first
    parameter specifies the number of rows, the second one the number of columns.
    The third parameter specifies the data, and the final one indicates whether the
    data is transposed or not. The dense representation looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00113.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'And in a more readable format (as a NumPy matrix), it looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00114.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, we simply call the `.chiSqTest(...)` and pass our matrix as its only parameter.
    What is left is to check `pValue` and whether `nullHypothesis` was rejected or
    not:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00115.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: So, as you can see, `pValue` is `0.0`, so we can reject the null hypothesis
    that states the distribution of occupation between those that earn more than $50,000
    versus those that earn less than $50,000 is the same. Thus, we can conclude, as
    Spark tells us, that the occurrence of the outcomes is statistically independent,
    that is, occupation should be a strong indicator for someone who earns more than
    $50,000.
  prefs: []
  type: TYPE_NORMAL
- en: See also...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are many statistical tests that help to establish whether two populations
    (or samples) are similar or not, or whether they follow certain distributions.
    For a good overview, we suggest the following document: [http://www.statstutor.ac.uk/resources/uploaded/tutorsquickguidetostatistics.pdf](http://www.statstutor.ac.uk/resources/uploaded/tutorsquickguidetostatistics.pdf).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transforming the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Machine learning** (**ML**) is a field of study that aims at using machines
    (computers) to understand world phenomena and predict their behavior. In order
    to build an ML model, all our data needs to be numeric. Since almost all of our
    features are categorical, we need to transform our features. In this recipe, we
    will learn how to use a hashing trick and dummy encoding.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To execute this recipe, you need to have a working Spark environment. You would
    have already gone through the *Loading the data *recipe where we loaded the census
    data into a DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: No other prerequisites are required.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will be reducing the dimensionality of our dataset roughly by half, so first
    we need to extract the total number of distinct values in each column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, for each feature, we will use the `.HashingTF(...)` method to encode
    our data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, we loop through all the categoricals and append a tuple of the column
    name (the `col`) and the count of distinct values found in that column. The latter
    is achieved by selecting the column of interest, running the `.distinct()` transformation,
    and counting the resulting number of values. `len_ftrs` is now a list of tuples.
    By calling the `dict(...)` method, Python will create a dictionary that will take
    the first element of the tuple as a key and the second element as the corresponding
    value. The resulting dictionary looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00116.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now that we know the total number of distinct values in each feature, we can
    use the hashing trick. First, we import the feature component of the MLlib as
    that is where the `.HashingTF(...)` is located. Next, we subset the census DataFrame
    to only the columns we want to keep. We then use the `.map(...)` transformation
    on the underlying RDD: for each element, we enumerate all the columns and if the
    index of the column is greater than or equal to five, we create a new instance
    of `.HashingTF(...)`, which we then use to transform the value and convert it
    into an NumPy array. The only thing you need to specify for the `.HashingTF(...)`
    method is the output number of elements; in our case, we roughly halve the number
    of the number of distinct values so we will have some hashing collisions, but
    that is fine.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For your reference, our `cols_to_keep` looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00117.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'After doing the preceding to our current dataset, `final_data`, it looks as
    follows; note the format might look a bit odd but we will soon be getting it ready
    for creating the training RDD:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00118.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The only thing that''s left is to handle our label; as you can see, it is still
    a categorical variable. However, since it only takes two values, we can encode
    it as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The `labelEncode(...)` method takes the label and checks whether it is `'>50k'`
    or not; if yes, we get a Boolean true, otherwise we get false. We can represent
    the Boolean data as integers by simply wrapping it inside Python's `int(...)` method.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we again use `.map(...)`, where we pass the first element of our `row`—the
    label—to the `labelEncode(...)` method. We then loop through all the remaining
    lists and combine them together. That portion of the code might look a bit peculiar
    at first, but it is actually fairly easy to understand. We loop through all the
    remaining elements (the `row[1:]`) and since each element is a list (hence we
    name it `sublist`), we create another loop (the `for item in sublist` portion)
    to extract the individual items. The resulting RDD looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00119.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: See also...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Check out this link for a nice overview of how to deal with categorical features
    in Python: [http://pbpython.com/categorical-encoding.html](http://pbpython.com/categorical-encoding.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Standardizing the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Data standardization (or normalization) is important for a number of reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: Some algorithms converge faster on standardized (or normalized) data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If your input variables are on vastly different scales, the interpretability
    of coefficients might be hard or conclusions drawn might be wrong
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For some models, the optimal solution might be wrong if you do not standardize
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this recipe, we will show you how to standardize the data so if your modeling
    project requires standardized data, you will know how to do it.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To execute this recipe, you need to have a working Spark environment. You would
    have already gone through the previous recipe where we encoded the census data.
  prefs: []
  type: TYPE_NORMAL
- en: No other prerequisites are required.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'MLlib offers a method to do most of this work for us. Even though the following
    code might be confusing at first, we will walk through it step by step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, we create the `StandardScaler(...)` object. The two parameters set to
    `True`—the former stands for mean, the latter stands for standard deviation—indicate
    that we want the model to standardize our features using Z-score: ![](img/00120.jpeg),
    where ![](img/00121.jpeg) is the *i*^(th) observation of the *f* feature, μ^(*f*)
    is the mean of all the observations in the *f* feature, and σ^(*f*) is the standard
    deviation of all the observations in the *f* feature.'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we `.fit(...)` the data using `StandardScaler(...)`. Note that we do not
    standardize the first feature as it is actually our label. Finally, we `.transform(...)`
    our dataset so we get the scaled features.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, since we do not scale our label, we need to somehow bring it back
    to our scaled dataset. So first, from `final_data`, we extract the label (using
    the `.map(lamba row: row[0])` transformation). However, we would not be able to
    join it with the `final_data_scaled` as it is since there is no key to join on. Note,
    we essentially want to join in a row-by-row fashion. So, we use the `.zipWithIndex()`
    method, which gives us a tuple in return, with the first element being the data
    and the second element being the row number. Since we want to join on the row
    number, we need to bring it to the first position in the tuple since that is how
    the `.join(...)` works for RDDs; we achieve this with the second `.map(...)` operation.'
  prefs: []
  type: TYPE_NORMAL
- en: In RDDs, the `.join(...)` operation cannot specify the key explicitly; both
    RDDs need to be two-element tuples, where the first element is the key and the
    second element is the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the join is complete, we simply extract the joined data by using the `.map(lambda
    row: row[1])` transformation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s how our data looks now:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00122.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can also peek into `sModel` to see what means and standard deviations were
    used to transform our data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00123.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Creating an RDD for training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we can train an ML model, we need to create an RDD where each element
    is a labeled point. In this recipe, we will use the `final_data` RDD we created
    in the previous recipe to prepare our RDD for training.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To execute this recipe, you need to have a working Spark environment. You would
    have already gone through the previous recipe when we standardized the encoded
    census data.
  prefs: []
  type: TYPE_NORMAL
- en: No other prerequisites are required.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many of the MLlib models require an RDD of labeled points to train. The next
    code snippets will create such an RDD for us to build classification and regression
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here''s the snippet to create the classification RDD of labeled points that
    we will be using to predict whether someone is making more than $50,000:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here''s the snippet to create the regression RDD of labeled points that we
    will be using to predict the number of hours people work:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we create the RDDs, we have to import the `pyspark.mllib.regression`
    submodule, as that is where we can access the `LabeledPoint` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Next, we simply loop through all the elements of the `final_data` RDD and create
    a labeled point for each element using the `.map(...)` transformation.
  prefs: []
  type: TYPE_NORMAL
- en: The first parameter of `LabeledPoint(...)` is the label. If you look at the
    the two code snippets, the only difference between them is what we consider labels
    and features.
  prefs: []
  type: TYPE_NORMAL
- en: As a reminder, a classification problem aims to find the probability of an observation
    belonging to a specific class; thus, the label is normally a categorical or, in
    other words, discrete. On the other hand, the regression problem aims to predict
    a value given an observation; thus, the label is normally numerical, or continuous
    if you will.
  prefs: []
  type: TYPE_NORMAL
- en: So, in the `final_data_income` case, we are using the binary indicator for whether
    the census respondent earns more (a value of 1) or less (the label equal to 0)
    than $50,000, whereas in the `final_data_hours`, we use the `hours-per-week` feature
    (see the *Loading the data* recipe), which, in our case, is the fifth piece of
    each of the elements of the `final_data` RDD. Note for this label we need to scale
    it back, so we need to multiply by the standard deviation and add the mean.
  prefs: []
  type: TYPE_NORMAL
- en: We assume here that you are working through the `5\. Machine Learning with MLlib.ipynb`
    notebook and have the `sModel` object already created. If you do not, please go
    back to the previous recipe and follow the steps outlined there.
  prefs: []
  type: TYPE_NORMAL
- en: The second parameter of the `LabeledPoint(...)` is a vector of all the features.
    You can pass either a NumPy array, list, `scipy.sparse` column matrix, or `pyspark.mllib.linalg.SparseVector`
    or `pyspark.mllib.linalg.DenseVector`; in our case, we encoded our features into `DenseVector`
    as we have already encoded all our features using the hashing trick.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We could use the full dataset to train our models, but we would then run into
    another problem: how do we evaluate how good our model is? Therefore, any data
    scientist normally performs a split of the data into two subsets: training and
    testing.'
  prefs: []
  type: TYPE_NORMAL
- en: See the *See also* section of this recipe for why this often isn't good enough,
    and you should actually be splitting the data into training, testing, and validation
    datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are two code snippets that show how easily this can be done in PySpark:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the second:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: By simply calling the `.randomSplit(...)` method of an RDD, we can quickly divide
    our RDDs into training and testing subsets. The only required parameter for the
    `.randomSplit(...)` method is a list where each element specifies the proportion
    of the dataset to randomly select. Note, these proportions need to sum up to 1.
  prefs: []
  type: TYPE_NORMAL
- en: We could have passed a list of three elements if we wanted to get the training,
    testing, and validation subsets.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Why you should be splitting into three datasets, and not two, is nicely explained
    here: [http://bit.ly/2GFyvtY](http://bit.ly/2GFyvtY)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predicting hours of work for census respondents
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will build a simple linear regression model that will aim
    to predict the number of hours each of the census respondents works per week.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To execute this recipe, you need to have a working Spark environment. You would
    have already gone through the previous recipe where we created training and testing
    datasets for estimating regression models.
  prefs: []
  type: TYPE_NORMAL
- en: No other prerequisites are required.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Training models with MLlib is pretty straightforward. See the following code
    snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you can see, we first create the `LinearRegressionWithSGD` object and call
    its `.train(...)` method.
  prefs: []
  type: TYPE_NORMAL
- en: For a very good overview of different derivatives of stochastic gradient descent,
    check this out: [http://ruder.io/optimizing-gradient-descent/](http://ruder.io/optimizing-gradient-descent/).
  prefs: []
  type: TYPE_NORMAL
- en: 'The first, and the only, required parameter we pass to the method is an RDD
    of labeled points that we created earlier. There is a host of parameters, though,
    that you can specify:'
  prefs: []
  type: TYPE_NORMAL
- en: Number of iterations; the default is `100`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Step is the parameter used in SGD; the default is `1.0`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`miniBatchFraction` specifies the proportion of data to be used in each SGD
    iteration; the default is `1.0`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `initialWeights` parameter allows us to initialize the coefficients to some
    specific values; it has no defaults and the algorithm will start with the weights
    equal to `0.0`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The regularizer type parameter, `regType`, allows us to specify the type of
    the regularizer used: `''l1''` for L1 regularization and `''l2''` for L2 regularization;
    the default is `None`, no regularization'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `regParam` parameter specifies the regularizer parameter; the default is
    `0.0`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model can also fit the intercept but it is not set by default; the default
    is false
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Before training, the model by default can validate data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can also specify `convergenceTol`; the default is `0.001`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s now see how well our model predicts working hours:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'First, from our full testing dataset, we select 10 observations (so we can
    print them on the screen). Next, we extract the true value from the testing dataset,
    whereas for the prediction we simply call the `.predict(...)` method of the `workhours_model_lm`
    model and pass the `.features` vector. Here is what we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00124.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, our model does not do very well, so further refining would be
    necessary. This, however, goes beyond the scope of this chapter and the book itself.
  prefs: []
  type: TYPE_NORMAL
- en: Forecasting the income levels of census respondents
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this recipe, we will show you how to solve a classification problem with
    MLlib by building two models: the ubiquitous logistic regression and a slightly
    more sophisticated model, the **SVM** ( **Support Vector Machine**).'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To execute this recipe, you need to have a working Spark environment. You would
    have already gone through the *Creating an RDD for training* recipe where we created
    training and testing datasets for estimating classification models.
  prefs: []
  type: TYPE_NORMAL
- en: No other prerequisites are required.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Just like with the linear regression, building a logistic regression starts
    with creating a `LogisticRegressionWithSGD` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As with the `LinearRegressionWithSGD` model, the only required parameter is
    the RDD with labeled points. Also, you can specify the same set of parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: The number of iterations; the default is `100`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The step is the parameter used in SGD; the default is ``1.0``
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`miniBatchFraction` specifies the proportion of data to be used in each SGD
    iteration; the default is `1.0`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `initialWeights` parameter allows us to initialize the coefficients to some
    specific values; it has no defaults and the algorithm will start with the weights
    equal to `0.0`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The regularizer type parameter, `regType`, allows us to specify the type of
    the regularizer used: `l1` for L1 regularization and `l2` for L2 regularization;
    the default is `None`, no regularization'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `regParam` parameter specifies the regularizer parameter; the default is
    `0.0`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model can also fit the intercept but it is not set by default; the default
    is false
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Before training, the model by default can validate data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can also specify `convergenceTol`; the default is `0.001`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `LogisticRegressionModel(...)` object that is returned upon finalizing the
    training allows us to utilize the model. By passing a vector of features to the
    `.predict(...)` method, we can predict the class the observations will most likely
    be associated with.
  prefs: []
  type: TYPE_NORMAL
- en: Any classification model produces a set of probabilities and logistic regression
    is not an exception. In the binary case, we can specify a threshold that, once
    breached, would indicate that the observation would be assigned with the class
    equal to 1 rather than 0; this threshold is normally set to `0.5`. `LogisticRegressionModel(...)`
    assumes `0.5` by default, but you can change it by calling the `.setThreshold(...)`
    method and passing a desired threshold value that is between 0 and 1 (not inclusive).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see how our model performs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'As with the linear regression example, we first extract 10 records from our
    test dataset so we can fit them on the screen. Next, we extract the desired label
    and call the `income_model_lr` model of `.predict(...)` the class. Here''s what
    we get back:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00125.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: So, out of 10 records, we got 9 right. Not bad.
  prefs: []
  type: TYPE_NORMAL
- en: In the *Computing performance statistics* recipe, we will learn how to use the
    full testing dataset to more formally evaluate our models.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Logistic regression is normally the benchmark used to asses the relative performance
    of other classification models, that is, whether they are performing better or
    worse. The drawback of logistic regression, however, is that it cannot handle
    cases where two classes cannot be separated by a line. SVMs do not have these
    kinds of problem, as their kernel can be expressed in quite flexible ways:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: In this example, just like with the `LogisticRegressionWithSGD` model, we can
    specify a host of parameters (we will not be repeating them here). However, the
    `miniBatchFraction` parameter instructs the SVM model to only use half of the
    data in each iteration; this helps preventing overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: 'The results for the 10 observations from the `small_sample_income` RDD are
    calculated the same way as with the logistic regression model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The model produces the same results as the logistic regression model, so we
    will not be repeating them here. However, in the *Computing performance statistics*
    recipe, we will see how these differ.
  prefs: []
  type: TYPE_NORMAL
- en: Building a clustering models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Often, it is hard to get our hands on data that is labeled. Also, sometimes
    you might want to find underlying patterns in your dataset. In this recipe, we
    will learn how to build the popular k-means clustering model in Spark.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To execute this recipe, you need to have a working Spark environment. You should
    have already gone through the *Standardizing the data* recipe where we standardized
    the encoded census data.
  prefs: []
  type: TYPE_NORMAL
- en: No other prerequisites are required.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Just like with classification or regression models, building clustering models
    is pretty straightforward in Spark. Here''s the code that aims to find patterns
    in the census data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, we need to import the clustering submodule of MLlib. Just like before,
    we first create the clustering estimator object, `KMeans`. The `.train(...)` method
    requires two parameters: the RDD we want to use to find the clusters in, and the
    number of clusters we expect. We also chose to randomly initialize the centroids
    of the clusters by specifying `initializationMode`; the default for this one is
    `k-means||`. Other parameters include:'
  prefs: []
  type: TYPE_NORMAL
- en: '`maxIterations` specifies after how many iterations the estimation should stop;
    the default is `100`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`initializationSteps` is only useful if the default initialization mode is
    used; the default for this parameter is `2`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`epsilon` is a stopping criteria—if all the centers of the centroids move (in
    terms of the Euclidean distance) less than this, the iterations stop; the default
    is `0.0001`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`initialModel` allows you to specify the centers previously estimated in the
    form of `KMeansModel`; the default is `None`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once the model is estimated, we can use it to predict the clusters and see
    how good our model actually is. However, at the moment, Spark does not provide
    the means to evaluate clustering models. Thus, we will use the metrics provided
    by scikit-learn:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The clustering metrics are located in the `.metrics` submodule of scikit-learn.
    We are using two of the metrics available: homogeneity and completeness. Homogeneity
    measures whether all the points in a cluster come from the same class whereas
    the completeness score estimates whether, for a given class, all the points end
    up in the same cluster; a value of 1 for either of the scores means a perfect
    model.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see what we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00126.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Well, our clustering model did not do so well: the homogeneity score of 15%
    means that the remaining 85% of observations were misclustered, and we only clustered
    ∼12% properly of all those that belong to the same class.'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For more on the evaluation of clustering models, you might want to check out [https://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-clustering-1.html](https://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-clustering-1.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Computing performance statistics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous recipes, we have already seen some values predicted by our classification
    and regression models and how far or how close they were from/to the original
    values. In this recipe, we will learn how to fully calculate the performance statistics
    for these models.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to execute this recipe, you need to have a working Spark environment
    and you should have gone through the *Predicting hours of work for census respondents*
    and *Forecasting income levels of census respondents* recipes presented earlier
    in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: No other prerequisites are required.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Getting the performance metrics for regression and classification in Spark
    is extremely simple:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First, we load the evaluation module; doing this exposes the `.RegressionMetrics(...)`
    and the `.BinaryClassificationMetrics(...)` methods, which we can use.
  prefs: []
  type: TYPE_NORMAL
- en: Regression metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`true_pred_reg` is an RDD of tuples where the first element is the prediction
    from our linear regression model and the second element is the expected value
    (the number of hours worked per week). Here''s how we create it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The `metrics_lm` object contains a variety of metrics: `explainedVariance`,
    `meanAbsouteError`, `meanSquaredError`, `r2`, and `rootMeanSquaredError`. Here,
    we will only print out a couple of them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s see what we got for the linear regression model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00127.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Not unexpectedly, the model performs really poorly, given what we have already
    seen. Do not be too surprised by the negative R-squared; it can turn negative,
    that is, a nonsensical value for R-squared, if the predictions of the model are
    nonsensical.
  prefs: []
  type: TYPE_NORMAL
- en: Classification metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will evaluate the two models we built earlier; here is the logistic regression:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'And here is the SVM:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: The two metrics—the area under the **Precision-Recall** (**PR**) and the area
    under the **Receiver Operating Characteristics** (**ROC**) curve—allow us to compare
    the two models.
  prefs: []
  type: TYPE_NORMAL
- en: Check out this interesting discussion about the two metrics on stack exchange: [https://stats.stackexchange.com/questions/7207/roc-vs-precision-and-recall-curves](https://stats.stackexchange.com/questions/7207/roc-vs-precision-and-recall-curves).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see what we got. For the logistic regression, we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00128.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'And for the SVM, we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00129.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'It comes a bit as a surprise that the SVM performed a bit worse than the logistic
    regression. Let''s see the confusion matrix to see where these two models differ.
    For the logistic regression, we achieve this with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'And we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00130.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'For the SVM, the code looks pretty much the same, with the exception of the
    input RDD:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'With the preceding, we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00131.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, the logistic regression is more accurate in predicting both
    the positive and negative cases, thus achieving fewer of the misclassified (false
    positives and false negatives) observations. However, the differences are not
    that dramatic.
  prefs: []
  type: TYPE_NORMAL
- en: 'To calculate the overall error rate, we can use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'For the SVM, the preceding code looks the same, with an exception of using `true_pred_class_svm`
    instead of `true_pred_class_lr`. The preceding produces the following. For the
    logistic regression, we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00132.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'For the SVM, the results look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00133.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The error is slightly higher for the SVM, but still a fairly reasonable model.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you want to learn more about various performance metrics, we suggest you
    visit the following URL: [https://machinelearningmastery.com/metrics-evaluate-machine-learning-algorithms-python/](https://machinelearningmastery.com/metrics-evaluate-machine-learning-algorithms-python/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
