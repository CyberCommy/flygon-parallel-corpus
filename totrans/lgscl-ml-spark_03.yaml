- en: Chapter 3. Understanding the Problem by Understanding the Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter will cover in details of the DataFrame, Datasets, and **Resilient
    Distributed Dataset** (**RDD**) APIs for working with structured data targeting
    to provide a basic understanding of machine learning problems with the available
    data. At the end of the chapter you will be able to apply basic to complex data
    manipulation with ease. Some comparisons will be made available with basic abstractions
    in Spark using RDD, DataFrame, and Dataset based data manipulation to show both
    gains in terms of programming and performance. In addition, we will guide you
    on the right track so that you will be able to use Spark to persist an RDD or
    data objects in memory, allowing it to be reused efficiently across the parallel
    operations in the later stage. In a nutshell, the following topics will be covered
    throughout this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing and preparing your data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Resilient Distributed Dataset (RDD) basics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dataset basics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dataset from string and typed class
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark and data scientists, workflow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deeper into Spark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyzing and preparing your data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In practice, several factors affect the success of **machine learning** (**ML**)
    applications on a given task. Therefore, the representation and quality of the
    experimental dataset is first and foremost considered as the first class entities.
    It is always advisable to have better data. For example, irrelevant and redundant
    data, data features with null values or noisy data result in unreliable source
    of information. The bad properties in datasets make the knowledge discovery process
    during the machine learning model training phase more tedious and time inefficient.
  prefs: []
  type: TYPE_NORMAL
- en: As a result, the data preprocessing will contribute a considerable amount of
    computational time across the total ML workflow steps. As we stated in the previous
    chapter, unless you know your available data, it would be difficult to understand
    the problem itself. Moreover, knowing the data will help you to formulate your
    problem. In parallel, and more importantly, before trying to apply an ML algorithm
    to a problem, first you have to identify if the problem is really a machine learning
    problem and whether an ML algorithm could directly be applied to solve the problem.
    The next step that you need to take is to know the machine learning classes. More
    technically, you need to know if an identified problem falls under classification,
    clustering, rule retraction, or regression classes.
  prefs: []
  type: TYPE_NORMAL
- en: For the sake of simplicity, we assume you have a machine learning problem. Now
    you need to do some data pre-processing that includes some steps like data cleaning,
    normalization, transformation, feature extraction, and selection. The product
    of a data pre-processing workflow step is the final training set that is typically
    used to build/train the ML model.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous chapter, we also argued that a machine learning algorithm learns
    from the data and activities during the model building and feed backing. It is
    critical that you feed your algorithm with the right data for the problem you
    want to solve. Even if you have good data (or well-structured data to be more
    precise), you need to make sure that the data is in an appropriate scale, with
    a well-known format to be parsed by the programming languages and, most importantly,
    if the most meaningful features are also included.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, you will learn how to prepare your data so that your machine-learning
    algorithm becomes spontaneous towards best performance. The overall data processing
    is a huge topic; however, we will try to cover essential techniques to make some
    large scale machine learning applications in *[Chapter 6](part0049_split_000.html#1ENBI2-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 6.  Building Scalable Machine Learning Pipelines")*, *Building Scalable
    Machine Learning Pipelines*.
  prefs: []
  type: TYPE_NORMAL
- en: Data preparation process
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you are more focused and disciplined during the data handling and preparation
    steps, you are likely to get more consistent and better results in the first place.
    However, the data preparation is a tedious process consisting of several steps.
    Nevertheless, the process for getting data ready for a machine learning algorithm
    can be summarized in three steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Data selection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data pre-processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data transformation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data selection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This step will focus on selecting the subset of all available datasets that
    you will be using and working with within your machine learning application development
    and deployment. There is always a strong urge to include all the available data
    in machine learning application development since more data will provide more
    features. In other words, by holding the well-known aphorism, *more is better*.
    However, essentially, this might not be true in all cases. You need to consider
    what data you need to have before you actually answer the question. The ultimate
    goal is to provide a solution of a particular hypothesis. You might be doing some
    assumptions about the data as well in the first place. Although it is difficult,
    if you are a domain expert of that problem, you can make some assumption to know
    at least some insights before applying your ML algorithms. However, be careful
    to record those assumptions so that you can test them at a later stage when required.
    We will present some common question to help you out in thinking through the data
    selection process:'
  prefs: []
  type: TYPE_NORMAL
- en: The first question would be, *what is the extent of the data you have available?*
    For example, the extent could be the throughout time, database tables, connected
    system files, and so on. Therefore, the better practice is to ensure that you
    have a clear understanding and low-level structure of everything that you can
    use, or holding informally the available resources (while of course including
    the available data and computational resources).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second question is a little bit weird! *What data are not yet available
    but important to solve the problem?* In this case, you might have to wait for
    the data to be available or alternatively you can at least generate or simulate
    these types of data using some generator or software.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The third question might be: *what data don't you need to address the problem?*
    That means again the redundancies so excluding these redundant or unwanted data
    is almost always easier than including it altogether. You might be wondering whether
    or not to note down the data you excluded and why? We think it should be yes since
    you might need some trivial data in the later stages.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Moreover, in practice in this case small problems or games, toy competition
    data will already have been selected for you; therefore, you don't need to be
    worried at all!
  prefs: []
  type: TYPE_NORMAL
- en: Data pre–processing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'After you have selected the data you will be working with, you need to consider
    how you could use the data and the proper utilization required. This pre-processing
    step will address some steps or techniques for getting the selected data into
    a form that you can work and apply during your model building and validation steps.
    The three most common data pre-processing steps that are used are formatting,
    cleaning, and sampling the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Formatting**: The selected data may not be in a good shape so might not be
    suitable for you to work with directly. Very often, your data might be in a raw
    data format (a flat file format such as a text format or a less used proprietary
    format) and if you are lucky enough then data might be in a relational database.
    If this is the case, then it would better be to apply some conversion steps (that
    is, converting a relational database to its format for example, since using Spark
    you cannot make any conversion). As already stated, the beauty of Spark is its
    support for diverse file formats. Therefore, we will be able to take advantage
    in the following sections.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cleaning**: Very often the data you will be using comes with many unwanted
    records or sometimes with missing entries against a record. This cleaning process
    deals with the removal or fixing of missing data. There may be always some trivial
    data objects that are insignificant or incomplete and addressing them should be
    the first priority. Consequently, these instances may need to be removed, ignored
    or deleted from the datasets to get rid of this problem. Additionally, if the
    privacy or security is a concern because of the presence of the sensitive information
    against some attributes, those attributes need to be anonymized or removed from
    the data entirely (if appropriate).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sampling**: The third step would be the sampling over the top of the formatted
    and cleaned datasets. Sampling is often required since there might be a time when the
    available data size is large or a number of records are huge. However, we argue
    to use the data as much as possible. Another reason is that more data can result
    in a longer execution time during the whole machine learning process. If this
    is the case, this also increases the running times of the algorithms and requires
    a more powerful computational infrastructure. Therefore, you can take a smaller
    representative sample of the selected data that may be much faster for exploring
    and prototyping the machine learning solution before considering the whole dataset.
    It is obvious that whatever the machine learning tools you apply for your machine
    learning application development and commercialization, data will influence the
    pre-processing you will be required to perform.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data transformation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'After selecting appropriate data sources and pre-processing those data, the
    final step is to transform the processed data. Your specific ML algorithm and
    knowledge of the problem domain will be influenced in this step. Three common
    data transformations techniques are scaling attributes, decompositions and attribute
    aggregations. This step is also commonly referred to as feature engineering that
    will be discussed in more details in the next chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Scaling**: The pre-processed data may contain attributes with a mixture of
    scales for various quantities and units, for example dollars, kilograms, and sales
    volume. However, the machine-learning methods have the data attributes within
    the same scale such as between 0 and 1 for the smallest and largest value for
    a given feature. Therefore, consider any feature scaling you may need to perform
    the proper scaling of the processed data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Decomposition**: The data might have some features that represent a complex
    concept that provides a more powerful response from the machine learning algorithms
    when you split the datasets into the fundamental parts. For example, consider
    a day that is composed of 24 hours, 1,440 minutes, and 86,400 seconds that in
    turn could be split out further. Probably some specific hours or only the hours
    in a day are relevant to the problem which to be investigated and resolved. Therefore,
    consider an appropriate feature extraction and selection to perform the proper
    decomposition of the processed data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Aggregation**: Often segregated or scattered features may be trivial on their.
    However, those features can be aggregated into a single feature that would be
    more meaningful to the problem you are trying to solve. For example, several data
    instances can be presented in an online shopping website for each time a customer
    logged on the site. These data objects could be aggregated into a count for the
    number of logins by discarding additional instances. Therefore, consider appropriate
    feature aggregation to process the data properly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apache Spark has its distributed data structures includes RDD, DataFrame, and
    Datasets by which you can perform the data pre-processing efficiently. These data
    structures have different advantages and performance for processing the data.
    In the next sections, we will describe those data structures individually and
    also show examples of how to process the large Dataset using them.
  prefs: []
  type: TYPE_NORMAL
- en: Resilient Distributed Dataset basics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 1](part0014_split_000.html#DB7S2-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 1. Introduction to Data Analytics with Spark"), *Introduction to Data
    Analytics with Spark*, we have described the Resilient Distributed Datasets in
    brief including the data transformation and action as well as the caching mechanism.
    We also stated that RDDs are basically an immutable collection of records that
    can only be created by operations such as map, filter, group by, and so on. In
    this chapter, we are going to use this native data structure of Spark for data
    manipulation and data pre-processing for a practical machine learning application
    commonly referred to as spam-filtering. Spark provides another two higher label
    APIs such as DataFrame and Datasets for data manipulation.
  prefs: []
  type: TYPE_NORMAL
- en: We will, however, show all the APIs including RDD here because you might need
    this API to handle more complex data manipulation. We have referred to some commonly
    used definitions regarding Spark actions and operations from the Spark programming
    guide.
  prefs: []
  type: TYPE_NORMAL
- en: As we already discussed some basics of RDD operations using action and transformations.
    The RDDs can be created by both stable storages such as the **Hadoop Distributed
    File System** (**HDFS**) and by transformations on existing RDDs. Spark periodically
    logs those transformations while creating RDDs over a set of transformations,
    rather than actual data, therefore, technically the original RDD and the Datasets
    do not get changed.
  prefs: []
  type: TYPE_NORMAL
- en: A transformed Dataset can be created from an existing one; however, the reverse
    is not possible in Spark. After finishing a computation on the Dataset, an action
    returns a value to the driver program. For example, according to the Spark programming
    guidelines, the map is a transformation that passes each Dataset element using
    a function and returns a brand new RDD that represents and holds the results.
    In contrast, reduce is also an action that aggregates all the elements of an RDD
    by using a function and returns a brand new RDD too as the final result to the
    driver program.
  prefs: []
  type: TYPE_NORMAL
- en: More technically, suppose we have a text file that contains a sequence of number
    separated by commas (that is, a CSV file). Now after reading the same you will
    have an RDD and consequently, you might want to count the frequencies of each
    number. For doing this, you need to convert the RDD into key value pairs, where
    the key is the number and the value will be the frequency of each number.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, you might need to collect the result in the driver program
    by doing some operations. In the next few sections, we will provide more details
    on some useful topics such as transformations and actions by showing some examples
    based on a practical machine learning problem.
  prefs: []
  type: TYPE_NORMAL
- en: Reading the Datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For reading Datasets from different data sources like a local filesystem, HDFS,
    Cassandra, HBase, and more, Spark provides different APIs that are easy to use.
    It supports the different representation of data including text file, sequence
    file, Hadoop input format, CSV, TSV, TXT, MD, JSON, and other data formats. The
    input API or methods support running on compressed files, directories and wildcards.
    For example, *Table 1* shows the list of reading formats. The `textFile()` method
    reads different file formats such as `.txt` and `.gz` from the directory `/my/directory:`
  prefs: []
  type: TYPE_NORMAL
- en: '| textFile("/my/directory"),textFile("/my/directory/*.txt"),textFile("/my/directory/*.gz").
    |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Reading files formats'
  prefs: []
  type: TYPE_NORMAL
- en: Reading from files
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You might need to read a Dataset from local or HDFS. The following code show
    the different methods for creating RDDs from a given Dataset stored on your local
    machine or in HDFS.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, before reading and writing with Spark, we need to create the Spark
    entry point by means of a Spark session that can be instantiated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Here, the Spark SQL warehouse is set to as `E:/Exp/` path. You should set your
    path accordingly based on OS types you are on. Well, now we have our Spark session
    as variable `spark`, let's see how to use it with ease for reading from a text
    file.
  prefs: []
  type: TYPE_NORMAL
- en: Reading from a text file
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'It uses `textFile()` methods of `SparkContext()` and returns an RDD of a string
    containing a collection of lines. In [Chapter 1](part0014_split_000.html#DB7S2-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 1. Introduction to Data Analytics with Spark"), *Introduction to Data
    Analytics with Spark*, we explained what SparkContext is. Nevertheless, Spark
    Context is the entry point of a Spark application. Suppose we have a Dataset called
    `1.txt` containing some tweets data as unstructured texts. You can download the
    data from the Packt materials and store under the `project_path/input/test/` directory,
    defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Here we have created RDDs of a string that is stored with the variable `distFile`
    in two partitions. However, to work with Java, the RDDs have to be converted into
    JavaRDD. Let''s do it by calling the `toJavaRDD()` method as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Reading multiple text files from a directory
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'It will return RDD as (filename and content) pairs. Suppose we have multiple
    files stored to read in the directory `csvFiles/` is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Please note, when the data objects in an RDD do not hold in the main memory
    or HDD, we need to perform a partition on the RDD to increase the parallelism.
  prefs: []
  type: TYPE_NORMAL
- en: Reading from existing collections
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The second source for creating an RDD is from the collections of your driver
    program such as list a containing integers. Before going deeper into this, let''s
    initialize Spark in an alternative way as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Here Java Spark context is available as a variable `sc`. This time, we have
    created the Spark context so we will be able to create the Java RDDs of string
    without using the `toJavaRDD()` method.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, you can do it by using the parallelized method of Spark Context as shown
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Reading list of integers**: It returns a parallelized RDD of integers:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '**Reading list of pairs**: It returns a parallelized `pairRDD` of the list
    of pairs (integer, string):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Pre–processing with RDD
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To continue the discussion about the data pre-processing that we started in
    the previous section, we will show an example of a machine learning problem and
    how to pre-process the Dataset using RDD in this section.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are considering the **Spam filter** application that is a popular example
    of s supervised learning problem. The problem is to predict and identify the spam
    messages from the incoming e-mails (please refer to *Table 2*). As usual, to train
    the model, you have to train a model by using the historical data (the historical
    e-mail that you have received over a couple of days, hours or months an even year).
    The final output of pre-processing tasks is to make the feature vectors or extract
    the features including its labels or classes. Typically, you might be doing the
    following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Stop word removal**: The text file might contain some word that is useless
    or redundant for the feature vector such as *and*, *the*, and *of*, since these
    are very common in all forms of English sentences. Another reason is they are
    not very meaningful in deciding spam or ham status or they may contain trivial
    significance. These words, therefore, need to be filtered from the e-mail Dataset
    before moving the next step.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lemmatization**: Some words possessing the same meaning but with different
    endings, need to be readjusted in order to make them consistent across the data
    set and if they all carry the same form will be easier to make them transform
    into feature vectors. For example, *attached*, *attachment*, and *attach* could
    all be represented and later on interpreted as e-mail *attachments*. The `SMSSpamCollection`
    Dataset was downloaded from the UCI ML repositories at [https://archive.ics.uci.edu/ml/machine-learning-databases/00228/](https://archive.ics.uci.edu/ml/machine-learning-databases/00228/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Please note, all the words in the email body are usually converted to lowercase
    for simplicity in this phase. Now, let''s take a look at the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| ham: What you doing? how are you?ham: Ok lar... Joking wif u oni.ham: dun
    say so early hor... U c already then say.ham: MY NO. IN LUTON 0125698789 RING
    ME IF UR AROUND! H*spam: FreeMsg: Txt: CALL to No: 86888 & claim your reward of
    3 hours talk time to use from your phone now! ubscribe6GBP/ mnth inc 3hrs 16 stop?txtStopham:
    Siva is in hostel aha.ham: Cos i was out shopping wif darren jus now n i called
    him 2 ask wat present he wan lor. Then he started guessing who i was wif n he
    finally guessed darren lor.spam: Sunshine Quiz! Win a super Sony DVD recorder
    if you can name the capital of Australia? Text MQUIZ to 82277\. B |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Test file for training set containing ham and spam messages'
  prefs: []
  type: TYPE_NORMAL
- en: '**Removal of non-words**: Numbers and punctuation have to be removed too. However,
    we will not show here all the possible transformation for pre-processing data
    due to page limitation and brevity, but we will try to show some basic transformations
    and actions for pre-processing segment of the Dataset that contains some labels
    data as spam or ham presented in *Table 2*. Where the Dataset or e-mails are labelled
    as ham or spam followed by the message or e-mails. Ham means non-spam and spams
    are identified as junk emails messages.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The overall pre-processing using RDD can be described using the following steps. The
    first step we need is to prepare the feature vectors using RDD operations and
    transformations. The remaining steps are given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Reading Dataset**: The following code is for reading Dataset that creates
    a `linesRDD` of strings from the `SMSSpamCollection` Dataset. Please download
    this Dataset from the Packt materials and store it in your disk or HDFS in  `Project+path/input/`
    directory. A detailed description of this Dataset will be provided later on:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: However, the `linesRDD` contains both the spam as well as the ham messages.
    Therefore, we need to separate the spam and ham message from the files.
  prefs: []
  type: TYPE_NORMAL
- en: '**Filter out the spam messages**: To filter the data from existing RDDs, Spark
    provides a method called `filter()`, which returns a new Dataset containing only
    the selected elements. In the following code you can see we have passed `new Function()`
    as a parameter that takes two arguments of the type String and Boolean of the`filter()`
    method. Basically, Spark APIs heavily rely on passing functions to the driver
    program for running on the cluster. There are two ways to create a function that
    includes:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing the function interfaces either creating anonymous inner class or
    named one and passing an instance of it to Spark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using lambda expressions (you will have to have Java 8 installed to take advantage
    of lambda expressions though)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following code segments that we have used explain the concept of anonymous
    class as a parameter that contains a  `call()` method that returns `true` if the
    line contains the word `spam`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Filter out the ham messages: similarly, we can filter out the ham messages
    as shown here:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '**Split the words from the lines**: To extract the features and labels from
    each line, we need to split those using space or tab characters. After that, we
    can have the lines without the spam or ham words.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following code segments show the separation of spam and ham features from
    the lines. We have used the `map` transformation that returns a new RDD formed
    by passing each line of the existing RDD through a function `call`. Here the `call`
    method always returns a single item. You will find a difference with `flatMap`
    in the later section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Output: `ham.collect()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Pre–processing with RDD](img/00052.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: A snapshot of the spam RDD'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Output: `ham.collect()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Pre–processing with RDD](img/00066.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: A snapshot of the ham RDD'
  prefs: []
  type: TYPE_NORMAL
- en: '**Split the words from the lines of spam RDD**: After we get the feature lines
    against the spam and ham RDDs separately, we have to split the words for makinga
     feature vector in the future. The following codes do this split with a space
    by returning the wordlist RDD. The call method returns a list of words for each
    line:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '**Make label and feature pair RDD**: Now we have two RDDs for spam and ham.
    We want to label them by 1.0 or 0.0 for spam and ham words or features respectively.
    For ease of use, we can again create a new RDD-containing tuple of a label and
    features or wordlist for each line. In the following code we have used `Tuple2`
    for making a pair. You can also use `JavaPairRDD` for making a pair of labels
    and features:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '**Union of the two RDDs**: Now we have two labels for the Dataset in *Table
    2*, the feature pair RDD of spam and ham. Now to make the training Dataset, we
    can join these two RDDs into one. Spark has `union()`method for doing this that
    returns a new RDD, containing the union of the Dataset and the argument or another
    Dataset:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Counting all the lines from the preceding operations, are called a transformation.
    This returns a new Dataset from existing one in the worker nodes in each case.
    If you want to bring the return in the driver program or print the results that
    would be called an action operation. Spark supports several built-in methods as
    actions. The `count()` method counts the number of elements in the Dataset:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '**Printing the RDD**: The `collect()` and `take()` are also action method that
    are used to print or collect the Dataset as an array in the driver program, where,
    `take()` takes an argument of, say *n* that returns the first n elements of that
    Dataset. The following code segments print the first 10 elements or tuples out
    of the train set:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '**Save the result in the local filesystem**: Sometimes you might need to save
    the RDD in the filesystem as text. You can use the following code for saving your
    RDDs straight away:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Getting insight from the SMSSpamCollection dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following source code shows the basic ham and spam statistics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code generates the following spam and ham counts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: This means that out of 5,578 emails `747` e-mails are spam and `4,831` e-mails
    are labelled as ham or non-spam. In other words, the spam and ham ratio is 13.40%
    and 86.6%.
  prefs: []
  type: TYPE_NORMAL
- en: Working with the key/value pair
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This subsection describes the key/value pair that is frequently needed in the
    data analytics, especially in the text processing.
  prefs: []
  type: TYPE_NORMAL
- en: mapToPair()
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This method will return a Dataset of (K, V) pair where K is key and V is value.
    For example, if you have an RDD with a list of integer then you want to count
    the number of duplicate entries in the list then the first task is to map each
    number to 1\. After that you can do the reduce operation on it. The code produces
    the output and cache as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: More about transformation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, you can see more about transformation including some differences
    between similar types of methods. Mainly `map` and `flatMap`, `groupByKey`, `reduceByKey`
    and `aggregateByKey`, `sortByKey` and `sortBy` will be discussed in this section.
    However, interested readers can refer to the Spark programming guidelines for
    RDD operation in [2].
  prefs: []
  type: TYPE_NORMAL
- en: map and flatMap
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `flatMap` is similar to the map we have showed in the preceding examples,
    but each input item or each time calling the `call()` method of the anonymous
    class can be mapped to zero or more output items. So ,the `call()` function returns
    a Sequence rather than a single item like a map. For example, for input of following
    RDD, the output should be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: For the previous example, you could not do the map operation because the `call()`
    method of map return only one object rathers than a sequence of the objects.
  prefs: []
  type: TYPE_NORMAL
- en: groupByKey, reduceByKey, and aggregateByKey
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In order to perform some operation while pre-processing your Dataset you might
    need to do some aggregation such as sum and average based on key values. Spark
    provides some methods for doing these kinds of operations. Let''s say, you have
    the following pairs of RDD and you want to group the values based on the keys
    and do some aggregations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The aggregations you want to do can be done by the three methods of Spark including
    `groupByKey`, `reduceByKey`, and `aggregateByKey`. But they differ in term of
    performance, efficiency and flexibility to do an operation such as counting, computing
    summary statistics, finding unique elements from a data set and so on. The `groupByKey`
    method returns a Dataset of (k, `Iterable<v>`) pairs where k is the key and `Iterable<v>`
    is the sequence of values of the key k. The output of previous Dataset using this
    method is given as follows and it shows the collection values of each key:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '![groupByKey, reduceByKey, and aggregateByKey](img/00080.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Pairs using groupBykey'
  prefs: []
  type: TYPE_NORMAL
- en: In order to make the sum of values of each unique key, `groupByKey` is inefficient
    in terms of performance because it does not perform the map side in combination.
    You have to make more transformation to do this summation explicitly. So, it increases
    the network I/O and shuffle size. Better performance can be gained by `reduceByKey`
    or `aggregateByKey` because they perform the map side combination.
  prefs: []
  type: TYPE_NORMAL
- en: The methods return the Dataset with the result of each key aggregation such
    as summations of the values of each key. The following code show the operation
    of those methods which return the Dataset of (k,v) pairs where values (v) of the
    keys are aggregated by the given functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `reduceByKey` takes one function that reduces the values of each key while
    the `aggregateByKey` takes two functions where the first function is for specifying
    how the aggregation will take place inside each partition and the second function
    is for specifying how the aggregation will take place between the partitions:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Code: `reduceByKey()`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Code: `aggregateByKey()`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'For both cases, the output will be as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Output: `counts.collect()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![groupByKey, reduceByKey, and aggregateByKey](img/00088.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: RDD using count'
  prefs: []
  type: TYPE_NORMAL
- en: sortByKey and sortBy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Sorting is a common operation in data pre-processing. Spark provides two methods
    that transform one Dataset to another sorted paired Dataset that includes `sortByKey`
    and `sortBy`. For instance, we have a Dataset as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The `sortByKey()` method performs on (k,v) pairs and returns (k,v) pairs sorted
    by keys in ascending or descending order. You can also customize the sorting by
    providing a comparator as parameters. The following code shows the sorting by
    key of the preceding Dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Code: `sortByKey()`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '![sortByKey and sortBy](img/00019.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Pairs using sortByKey'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `sortBy()` method takes a function as a parameter where you can specify
    the sorting method either by key or by value. The following code shows the sorting
    by values of the preceding Dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Code: `sortBy()`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Output: `sortedRDD.collect()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![sortByKey and sortBy](img/00023.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Pairs using sortBy'
  prefs: []
  type: TYPE_NORMAL
- en: Dataset basics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As discussed in [Chapter 1](part0014_split_000.html#DB7S2-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 1. Introduction to Data Analytics with Spark"), *Introduction to Data
    Analytics with Spark* that in Spark 2.0.0 release, the DataFrame remains the primary
    computation abstraction for the Scala, Python and R, however, while using Java
    the same will be replaced with Dataset. Consequently, Dataset of type Row will
    be used throughout this book.
  prefs: []
  type: TYPE_NORMAL
- en: The Dataset is a distributed collection of data is structured the Rows. This
    is this is one of the more convenient ways for interacting with Spark SQL module.
  prefs: []
  type: TYPE_NORMAL
- en: In other words, it can be considered as an equivalent entity to a tabular data
    like a **Relational Database** (**RDB**) format.. The Like the other data abstractions
    like DataFrame and RDD, the Dataset can also be created from various data sources
    like structured data files (TSV, CSV, JSON, and TXT), Hive tables, secondary storages,
    external databases, or existing RDDs and DataFrames. However, upon the Spark 2.0.0
    release, the Java based computation does not support the DataFrame but you are
    developing your applications using Python, Scala or R, still you will be able
    making use of the DataFrames.
  prefs: []
  type: TYPE_NORMAL
- en: In the next few sections, you will find the operations and actions using Dataset
    and how to create a Dataset from different sources.
  prefs: []
  type: TYPE_NORMAL
- en: Reading datasets to create the Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As mentioned above, the Dataset is a component of Spark SQL module introduced
    from the Spark 1.5.0 release. Therefore, all the entry point of all functionally
    starts from the initialization of Spark `SQLContext` . Basically, Spark SQL is
    used for executing SQL queries written either as a basic SQL syntax or HiveQL.
  prefs: []
  type: TYPE_NORMAL
- en: 'A Dataset object will be returning when running SQL within another programming
    language. The following code segment will initialize the `SQLContext` within Spark
    Context. On the other hand, you might require having the `HiveContext` initialized
    for reading a data set from the Hive. You can also create a different context
    like `HiveContext` which provides a superset of basic functionalities of `SQLContext`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Reading from the files
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For example, you have a JSON file as shown here. Now you want to read this
    file using SQL context which basically returns a DataFrame which you can perform
    all the basic SQL operations and other DSL operations of Spark:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Reading from the Hive
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following code connects with Hive context where one table is created and
    people JSON file is loaded into hive create. The output of the DataFrame will
    be the same as above:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Pre-processing with Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the previous section, we have described the pre-processing with RDD for
    a practical machine learning application. Now we will do the same example using
    **DataFrame** (**DF**) API. You will find it very easy to manipulate the `SMSSpamCollection`
    Dataset (see at [http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/](http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/)).
    We will show the same example by tokenizing the spam and ham messages for preparing
    a training set:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Reading a Dataset:** You can read that Dataset using the Spark session variable
    `spark`that we have to initialize before using it. After reading the file as Dataset
    the output will be a tabular format of a single column. The default name of this
    column is `value`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Pre-processing with Dataset](img/00101.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: A snapshot of the SMS spam dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '**Create Row RDD from existing Dataset**: From the preceding output you can
    see one column containing all the lines together. In order to make two columns
    such as label and features, we have to split it. Since Dataset is immutable you
    cannot modify the existing columns or Dataset. So you have to create new Dataset
    using the existing Dataset. Here the code converts the Dataset to RDD that is
    the collection of Row dataset. The row is an interface, which represents one row
    of output from a relational operator. You can create a new Row using `RowFactory`
    class of Spark:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '**Create new row RDD from an existing row RDD**: After having the Row RDD you
    can perform normal map operation which is all contains Row Dataset but having
    two values. The following code split the each row and returns a new one:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '**Create Dataset from Row RDD**: Now you have Row RDD, which contains two values
    for each Row. For creating a DF, you have to define the column names or schemas
    and its data types. There are two methods to define including inferring the schema
    using reflection and programmatically specify the schema. The methods are as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The 1st method basically uses the POJO classes and fields names will be the
    schema
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The 2nd method create list of StruchFields by defining the datatypes and create
    the structype. For this example, we have used the 2nd method for creating DF from
    existing row RDD as shown here:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '![Pre-processing with Dataset](img/00040.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Schema of the collection'
  prefs: []
  type: TYPE_NORMAL
- en: '**Adding a new column**: Now that we have the DF of two columns. But we want
    to add new columns which convert the `labledSting` to `labedDouble` and `featureString`
    to `featureTokens`. You can do it similarly as previous code. After adding to
    new fields create new schema. Then create new DF after having normal map transformation
    in existing DF. The following code gives output of new DF having four columns:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '![Pre-processing with Dataset](img/00058.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: The dataset after adding a new column'
  prefs: []
  type: TYPE_NORMAL
- en: '**Some Dataset operations**: For data manipulation DF provides domain specific
    language in Java, Scala and others. You can do select, counting, filter, `groupBy`
    and so on operations into a DF. The following codes show some operations on the
    above DF:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '![Pre-processing with Dataset](img/00053.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: Dataset showing the label and features'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '![Pre-processing with Dataset](img/00009.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: Dataset showing that the label has been converted into double value'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '![Pre-processing with Dataset](img/00001.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: showing the Dataset statistics after manipulations'
  prefs: []
  type: TYPE_NORMAL
- en: More about Dataset manipulation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section will describe how to use SQL queries on DF and different way to
    create Datasets across the datasets. Mainly running the SQL queries on DataFrame
    and the creating DataFrame from the JavaBean will be discussed in this section.
    However, interested readers can refer Spark programing guidelines for SQL operation
    in [3].
  prefs: []
  type: TYPE_NORMAL
- en: Running SQL queries on Dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `SQLContext` of Spark has `sql` method enables applications to run SQL
    queries. This method returns a DataFrame as a result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[`FilternewColumnsAddedDF.createOrReplaceTempView`(`SMSSpamCollection`)]:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Running SQL queries on Dataset](img/00077.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: using SQL query to retrieve same result as Figure 11'
  prefs: []
  type: TYPE_NORMAL
- en: 'Count:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Running SQL queries on Dataset](img/00031.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14: Showing the Dataset statistics'
  prefs: []
  type: TYPE_NORMAL
- en: Creating Dataset from the Java Bean
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can create Dataset from a Java Bean; where you don''t need to define the
    schemas programmatically. For example, you can see **Plain Old Java Object** (**POJO**)
    named as Bean in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Create DF:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'The following output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Creating Dataset from the Java Bean](img/00064.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15: Corresponding feature and label string'
  prefs: []
  type: TYPE_NORMAL
- en: Dataset from string and typed class
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As already mentioned that the Dataset is a typed and immutable collection of
    objects. Datasets are basically mapped to a relational schema. With the Dataset
    abstraction, a new concept has been brought in Spark called an encoder. The encoder
    helps in entity conversion for example conversion between the JVM objects and
    the corresponding tabular representation. You will find this API quite similar
    to RDDs transformations such as `map, mapToPair, flatMap` or `filter.`
  prefs: []
  type: TYPE_NORMAL
- en: We will show the spam filter example using Datasets API in the following section.
    It reads the text file using and returns a Dataset as a tabular format. Then perform
    map transformation like RDDs for making (label, tokens) columns with adding an
    additional encoder parameter. Here, we have used the bean encoder with `SMSSpamTokenizedBean`
    class.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this sub-section, we will show how to create Dataset from string and typed
    class `SMSSpamTokenizedBean`. Let''s create the Spark session at first place as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Now create a new Dataset of type String from the `smm` filtering Dataset that
    means `Dataset<String>` and show the result as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Dataset from string and typed class](img/00010.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16: Showing the snapshot of the spam filtering dataset using Dataset'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s create a second Dataset from the typed class `SMSSpamTokenizedBean`
    by mapping the Dataset of string we created immediate before as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s print the Dataset along with its schema as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'The following output is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Dataset from string and typed class](img/00089.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 17: Showing the token and label and the lower side the schema'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now if you would like to convert this typed Dataset as type Row then you can
    use the `toDF()` method and to further create a temporary view out of the new
    `Dataset<Row>` you can use the `createOrReplaceTempView()` method with ease as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, might want to view the same Dataset by calling show `method()` as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Dataset from string and typed class](img/00128.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18: Corresponding labels and tokens. Labels are converted into double
    value'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s explore the typed class `SMSSpamTokenizedBean`. The class works
    as a Java tokenized bean class for the labeling the texts. More technically, the
    class takes the input then it sets the labels and after that gets the labels.
    Secondly, it also sets and gets the token for spam filtering. Including the setter
    and methods, here is the class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: Comparison between RDD, DataFrame and Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are some objectives to bring Dataset as a new Data Structure of Spark.
    Although RDD API is very flexible, it is sometimes harder to optimize the processing.
    On the other hand, the DataFrame API is very easier to optimize but it lacks some
    of the nice features of RDD. So, the goal of the Datasets is to allow the users
    to easily express transformations on objects and also providing the advantages
    (performance and robustness) of the Spark SQL execution engine.
  prefs: []
  type: TYPE_NORMAL
- en: The Dataset can perform many operations such as sorting or shuffling without
    de-serializing of an object. For doing this it requires an explicit Encoder that
    is used to serialize the object into a binary format. It is capable of mapping
    the schema of a given object (Bean) to the Spark SQL type system. On the other
    hand, RDDs are based on run-time reflection based serialisation and the operations
    that change the types of object of a Dataset also need an encoder for the new
    type.
  prefs: []
  type: TYPE_NORMAL
- en: Spark and data scientists workflow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As already stated that, a common task for a data scientist is to select the
    data, data pre-processing (formatting, cleaning and sampling) and data transformation
    (scaling, decomposition and aggregation) the raw data into a format that can be
    passed into machine learning models to build the models. As the size of the experimental
    datasets increases, the traditional single-node databases will not be feasible
    to handle these kinds of datasets, therefore, you need to switch a big data processing
    computing like Spark. Fortunately, we have the Spark to be an excellent option
    as a scalable distributed computing system to coup with your datasets.
  prefs: []
  type: TYPE_NORMAL
- en: '![Spark and data scientists workflow](img/00090.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure-19: Data scientist''s workflow for using the Spark'
  prefs: []
  type: TYPE_NORMAL
- en: Now let's move to the exact point, as a data scientist at first you will have
    to read the Dataset available in diverse formats. Then reading the Datasets will
    provide you with the concept of RDDs, DataFrames and Datasets that we already
    describe. You can cache the Dataset into the main memory; you can transform the
    read data sets from the DataFrame, SQL or as Datasets. And finally, you will perform
    an action to dump your data to the disks, computing nodes or clusters. The step
    what we describe here essentially forms a workflow that you will follow for the
    basic data processing using Spark that showed in *Figure 1*.
  prefs: []
  type: TYPE_NORMAL
- en: Deeper into Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will show you the advanced features of Spark including the
    use of shared variables (both the broadcast variables and accumulators) and their
    underlying concept will be discussed. However, we will discuss the data partition
    in later chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Shared variables
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The concept of shared variables in the context programming is not new. The
    variables that are required to use by many functions, and methods in parallel
    are called shared variables. Spark has some mechanism to use or implement the
    shared variables. In spark, the functions are passed to a spark operation like
    a map or reduce is executed on remote cluster nodes. The codes or functions work
    as a separate copy of variables on the nodes and no updates of the results are
    propagated back to the driver program. However, Spark provides two types of shared
    variables for two common usage patterns: broadcast variables and accumulators.'
  prefs: []
  type: TYPE_NORMAL
- en: Broadcast variables
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Broadcast variables provide the facility to persist a read-only to be variable
    cached on local machine rather than sending a copy to the computing nodes or driver
    program. Providing the copy of large input Dataset to every node in an efficient
    manner of spark. It also reduces the communication cost because Spark uses an
    efficient broadcast. Broadcast variables can be created from a variable `v` by
    calling `SparkContext.broadcast(v)`. The following code shows this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Accumulators
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Accumulators are another shared variable can be used to implement counters
    (as in MapReduce) or sums. Spark provides the supports for accumulators to be
    of numeric types only. However, you can also add support for new data types using
    existing techniques [1]. It is created from an initial value say `val` by calling:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code shows the uses of accumulator for adding the elements of
    an array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: There are many types and method in the Spark APIs needed to be known. However,
    more and details discussion is out of the scope of this book.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Interested readers should refer Spark and related materials on the following
    web pages:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Spark programming guide: [http://spark.apache.org/docs/latest/programming-guide.html](http://spark.apache.org/docs/latest/programming-guide.html).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Spark RDD operation: [http://spark.apache.org/docs/latest/programming-guide.html#rdd-operations](http://spark.apache.org/docs/latest/programming-guide.html#rdd-operations).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Spark SQL operation: [http://spark.apache.org/docs/latest/sql-programming-guide.html](http://spark.apache.org/docs/latest/sql-programming-guide.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this Chapter, we applied the basic data manipulations with RDDs, Dataset
    and DataFrame APIs. We also learn how to do some complex data manipulation through
    these APIs. We tried to focus on data manipulations, to understand a practical
    machine learning problem Spam-filtering. In addition to these, we showed how to
    read the data from different sources. Analyzing and preparing your data to understand
    the spam filtering as an example.
  prefs: []
  type: TYPE_NORMAL
- en: However, we did not develop any complete machine learning application, since
    our target was just to show you the basic data manipulation on the experimental
    Datasets. We intended to develop complete ML application in [Chapter 6](part0049_split_000.html#1ENBI2-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 6.  Building Scalable Machine Learning Pipelines"), *Building Scalable
    Machine Learning Pipelines*.
  prefs: []
  type: TYPE_NORMAL
- en: Which features should be used to create a predictive model is not only a vital
    question but also a difficult question that may require deep knowledge of the
    problem domain to be answered. It is possible to automatically select those features
    in data that are most useful or most relevant for the problem someone is working
    on. Considering these questions, the next chapter covers the feature engineering
    in detail, explaining the reasons why to apply it along with some best practices
    in feature engineering. Some topics which are still unclear will be clearer in
    the next chapter.
  prefs: []
  type: TYPE_NORMAL
