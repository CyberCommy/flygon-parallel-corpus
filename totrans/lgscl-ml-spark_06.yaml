- en: Chapter 6.  Building Scalable Machine Learning Pipelines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The ultimate goal of machine learning is to make a machine that can automatically
    build models from data without requiring tedious and time-consuming human involvement
    and interaction. Therefore, this chapter guides the readers through creating some
    practical and widely used machine learning pipelines and applications using Spark
    MLlib and Spark ML. Both APIs will be described in detail, and a baseline use
    case will also be covered for both. Then we will focus on scaling up the ML application
    so that it can cope with increasing data loads. After reading all the sections
    in this chapter, readers will be able to differentiate between both APIs and select
    the one which best fits their requirements. In a nutshell, the following topics
    will be covered throughout this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Spark machine learning pipeline APIs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cancer-diagnosis pipeline with Spark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cancer-prognosis pipeline with Spark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Market basket analysis with Spark Core
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OCR pipeline with Spark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Topic modeling using Spark MLlib and ML
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Credit-risk-analysis pipeline with Spark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scaling the ML pipelines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tips and performance considerations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark machine learning pipeline APIs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: MLlib's goal is to make practical machine learning (ML) scalable and easy. Spark
    introduces the pipeline API for the easy creation and tuning of practical ML pipelines.
    As discussed in [Chapter 4](part0038_split_000.html#147LC2-5afe140a04e845e0842b44be7971e11a
    "Chapter 4. Extracting Knowledge through Feature Engineering"), *Extracting Knowledge
    through Feature Engineering*, a practical ML pipeline involves a sequence of data
    collection, pre-processing, feature extraction, feature selection, model fitting,
    validation, and model evaluation stages. For example, classifying the text documents
    might involve text segmentation and cleaning, extracting features, and training
    a classification model with cross-validation toward tuning. Most ML libraries
    are not designed for distributed computation, or they do not provide native support
    for pipeline creation and tuning.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset abstraction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As described in [Chapter 1](part0014_split_000.html#DB7S2-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 1. Introduction to Data Analytics with Spark"), *Introduction to Data
    Analytics with Spark*, when running SQL from within another programming language,
    the results return as a DataFrame. A DataFrame is a distributed collection of
    data organized into named columns. A Dataset, on the other hand, is an interface
    that tries to provide the benefits of RDDs out of the Spark SQL.
  prefs: []
  type: TYPE_NORMAL
- en: A Dataset can be constructed from JVM objects, which can be used both in Scala
    and Java. In the Spark pipeline design, a dataset is represented by Spark SQL's
    Dataset. An ML pipeline involves a number of the sequence of Dataset transformations
    and models. Each transformation takes an input dataset and outputs the transformed
    dataset, which becomes the input to the next stage.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consequently, the data import and export are the start and end point of an
    ML pipeline. To make these easier, Spark MLlib and Spark ML provide import and
    export utilities of a Dataset, DataFrame, RDD, and model, for several application-specific
    types, including:'
  prefs: []
  type: TYPE_NORMAL
- en: LabeledPoint for classification and regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LabeledDocument for cross-validation and **Latent Dirichlet Allocation** (**LDA**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rating and ranking for collaborative filtering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, real datasets usually contain numerous types, such as user ID, item
    IDs, labels, timestamps, and raw records.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, the current utilities of Spark implementation cannot easily handle
    datasets consisting of these types, especially time-series datasets. If you recall
    the section *Machine learning pipeline - an overview*, in [Chapter 4](part0038_split_000.html#147LC2-5afe140a04e845e0842b44be7971e11a
    "Chapter 4. Extracting Knowledge through Feature Engineering"), *Extracting Feature
    through Feature Engineering*, feature transformation usually forms the majority
    of a practical ML pipeline. A feature transformation can be viewed as appending
    or dropping a new column created from existing columns.
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Figure 1*, *Text processing for machine learning model*, you will see that
    the text tokenizer breaks a document into a bag of words. After that, the TF-IDF
    algorithm converts a bag of words into a feature vector. During the transformations,
    the labels need to be preserved for the model-fitting stage:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Dataset abstraction](img/00034.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Text processing for machine learning model (DS indicates data sources)'
  prefs: []
  type: TYPE_NORMAL
- en: If you recall *Figure 5* and *Figure 6* from [Chapter 4](part0038_split_000.html#147LC2-5afe140a04e845e0842b44be7971e11a
    "Chapter 4. Extracting Knowledge through Feature Engineering"), *Extracting Feature
    through Feature Engineering*, ID, text, and words are conceded during the transformations
    steps. They are useful in making predictions and model inspection. However, they
    are actually unnecessary for model fitting to state. According to a Databricks
    blog on ML Pipeline at [https://databricks.com/blog/2015/01/07/ml-pipelines-a-new-high-level-api-for-mllib.html](https://databricks.com/blog/2015/01/07/ml-pipelines-a-new-high-level-api-for-mllib.html),
    it doesn't provide much information if the prediction dataset only contains the
    predicted labels.
  prefs: []
  type: TYPE_NORMAL
- en: Consequently, if you want to inspect the prediction metrics, such as the accuracy,
    precision, recall, weighted true positives, and weighted false positives, it is
    quite useful to look at the predicted labels along with the raw input text and
    tokenized words. The same recommendation also applies to other machine learning
    applications using Spark ML and Spark MLlib, too.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, an easy conversion between RDDs, Dataset, and DataFrames has been
    made possible for in-memory, disk, or external data sources such as Hive and Avro.
    Although creating new columns from existing columns is easy with user-defined
    functions, the manifestation of Dataset is a lazy operation.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, the Dataset supports only some standard data types. However, to
    increase the usability and for making a better fit for the machine learning model,
    Spark has also added the support for the Vector type as a user-defined type that
    supports both dense and sparse feature vectors under the `mllib.linalg.DenseVector`
    and `mllib.linalg.Vector`.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Complete DataFrame, Dataset, and RDD examples in Java, Scala, and Python can
    be found under the `examples/src/main/` folder under the Spark distribution. Interested
    readers can refer to Spark SQL's user guide at [http://spark.apache.org/docs/latest/sql-programming-guide.html](http://spark.apache.org/docs/latest/sql-programming-guide.html)
    to learn more about DataFrame, Dataset, and the operations they support.
  prefs: []
  type: TYPE_NORMAL
- en: Pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Spark provides the pipeline API under Spark ML. As previously stated, a pipeline
    is comprised of a sequence of stages consisting of transformers and estimators.
    There are two basic types of pipeline stages, called Transformer and Estimator.
  prefs: []
  type: TYPE_NORMAL
- en: A transformer takes a dataset as an input and produces an augmented dataset
    as the output so that the output can be fed to the next step. For example, **Tokenizer**
    and **H**ashingTF**** are two transformers. Tokenizer transforms a dataset with
    text into a dataset with tokenized words. A HashingTF, on the other hand, produces
    the term frequencies. The concept of tokenization and HashingTF is commonly used
    in text mining and text analytics.
  prefs: []
  type: TYPE_NORMAL
- en: On the contrary, an estimator must be the first on the input dataset to produce
    a model. In this case, the model itself will be used as the transformer for transforming
    the input dataset into the augmented output dataset. For example, a **Logistic
    Regression** or linear regression can be used as an estimator after fitting the
    training dataset with corresponding labels and features.
  prefs: []
  type: TYPE_NORMAL
- en: 'After that, it produces a logistic or linear regression model. It implies that
    developing a pipeline is easy and simple. Well, all you need is to declare the
    required stages, then configure the related stage''s parameters; finally, chain
    them in a pipeline object, as shown in *Figure 2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Pipeline](img/00068.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Spark ML pipeline model using logistic regression estimator (DS indicates
    data store and the steps inside the dashed line only happen during pipeline fitting)'
  prefs: []
  type: TYPE_NORMAL
- en: If you look at *Figure 2*, the fitted model consists of a tokenizer, a hashingTF
    feature extractor, and a fitted logistic regression model. The fitted pipeline
    model acts as a transformer that can be used for prediction, model validation,
    model inspection, and finally, model deployment. However, increasing the performance
    in terms of prediction accuracy, the model itself needs to be tuned. We will discuss
    more about how to tune a machine learning model in [Chapter 7](part0059_split_000.html#1O8H62-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 7. Tuning Machine Learning Models"), *Tuning Machine Learning Model*.
  prefs: []
  type: TYPE_NORMAL
- en: To show the pipelining technique more practically, the following section shows
    how to create a practical pipeline for cancer diagnosis using Spark ML and MLlib.
  prefs: []
  type: TYPE_NORMAL
- en: Cancer-diagnosis pipeline with Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will look at how to develop a cancer-diagnosis pipeline
    with Spark ML and MLlib. A real dataset will be used to predict the probability
    of breast cancer, which is almost curable since the culprit genes for this cancer
    type have already been identified successfully. However, we would like to argue
    about this cancer type since in third world countries in Africa and Asia it is
    still a lethal disease.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We suggest the readers keep an open mind about the outcome or the status of
    this disease, as we will just show how the Spark ML API can be used to predict
    cancer by integrating and combining datasets from the Wisconsin Breast Cancer
    (original), **Wisconsin Diagnosis Breast Cancer** (**WDBC**), and **Wisconsin
    Prognosis Breast Cancer** (**WPBC**) datasets from the following website: [http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml).'
  prefs: []
  type: TYPE_NORMAL
- en: Breast-cancer-diagnosis pipeline with Spark
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this subsection, we will develop a step-by-step cancer diagnosis pipeline.
    The steps include a background study of breast cancer, dataset collection, data
    exploration, problem formalization, and Spark-based implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Background study
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: According to Salama et al. (*Breast Cancer Diagnosis on Three Different Datasets
    Using Multi-Classifiers, International Journal of Computer and Information Technology*
    (*2277 - 0764*) *Volume 01- Issue 01, September 2012*), breast cancer comes in
    fourth position after thyroid cancer, melanoma, and lymphoma, in women between
    20 and 29 years.
  prefs: []
  type: TYPE_NORMAL
- en: Breast cancer develops from breast tissue that mutates due to several factors
    including sex, obesity, alcohol, family history, lack of physical exercise, and
    so on. Furthermore, according to statistics by **The Centre for Diseases Control
    and Prevention** (**TCDCP**) ([https://www.cdc.gov/cancer/breast/statistics/](https://www.cdc.gov/cancer/breast/statistics/)),
    in 2013, a total of 230,815 women and 2,109 men were diagnosed with breast cancer
    across the USA. Unfortunately, 40,860 women and 464 men died from it.
  prefs: []
  type: TYPE_NORMAL
- en: Research has found that about 5-10% cases are due to some genetic inheritance
    from parents, including BRCA1 and BRCA2 gene mutations and so on. An early diagnosis
    could help to save thousands of breast cancer sufferers around the globe. Although
    the culprit genes have been identified, chemotherapy has not proven very effective.
    Gene silencing is becoming popular, but more research is required.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned previously, the learning tasks in machine learning depend heavily
    on classification, regression, and clustering techniques. Moreover, traditional
    data-mining techniques are being applied along with these machine learning techniques,
    which are the most essential and important task. Therefore, by integrating with
    Spark, these applied techniques are gaining wide acceptance and adoption in the
    area of biomedical data analytics. Furthermore, numerous experiments are being
    performed on biomedical datasets using multiclass and multilevel classifiers and
    feature-selection techniques toward cancer diagnosis and prognosis.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset collection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**The Cancer Genome Atlas** (**TCGA**), **Catalogue of Somatic Mutations in
    Cancer** (**COSMIC**), **International Cancer Genome Consortium** (**ICGC**) is
    the most widely used cancer and tumor-related dataset for research purposes. These
    data sources have been curated from world-renowned institutes such as MIT, Harvard,
    Oxford, and others. However, the datasets that are available are unstructured,
    complex, and multidimensional. Therefore, we cannot use them directly to show
    how to apply large-scale machine learning techniques to them. The reason is that
    these datasets require lots of pre-processing and cleaning, which requires lots
    of pages.'
  prefs: []
  type: TYPE_NORMAL
- en: After practising this application, we believe readers will be able to apply
    the same technique for any kind of biomedical dataset for cancer diagnosis. Due
    to the page limitation, we should use simpler datasets that are structured and
    manually curated for machine learning application development and of course, many
    of them show good classification accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: For example, the Wisconsin Breast Cancer datasets from the UCI Machine Learning
    Repository available at [http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)
    contains data that was donated by researchers at the University of Wisconsin and
    includes measurements from digitized images of a fine-needle aspiration of a breast
    mass. The values represent characteristics of the cell nuclei present in the digital
    image described in the following subsection.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To read more about the Wisconsin breast cancer data, refer to the authors''
    publication: *Nuclear feature extraction for breast tumor diagnosis. IS&T/SPIE
    1993 International Symposium on Electronic Imaging: Science and Technology, volume
    1905, pp 861-870 by W.N. Street, W.H. Wolberg, and O.L. Mangasarian, 1993*.'
  prefs: []
  type: TYPE_NORMAL
- en: Dataset description and preparation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As shown in the **Wisconsin Breast Cancer Dataset** (**WDBC**) manual available
    at [https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.names](https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.names),
    the Clump thickness benign cells tend to be grouped in monolayers, while cancerous
    cells are often grouped in multilayers. Therefore, all the features and fields
    mentioned in the manual are important and before applying the machine learning
    technique since these features will help to identify if a particular cell is cancerous
    or not.
  prefs: []
  type: TYPE_NORMAL
- en: The breast cancer data includes 569 samples of cancer biopsies, each with 32
    features. One feature is the identification number of the patient, another is
    the cancer diagnosis, labeled as benign or malignant, and the remainder are numeric-valued
    is called bio-assay that was identified in the molecular laboratory works. The
    diagnosis is coded as either M to indicate malignant or B to indicate benign with
    regard to the cancer diagnosis.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Class distribution is as follows: Benign: 357 (62.74%) and Malignant: 212
    (37.25%). The training and test dataset will be prepared following the dataset
    description given here. The 30 numeric measurements include the mean, standard
    error, and worst, which is the mean of the three largest values. Field 3 is the
    mean radius, 13 is the Radius SE, and 23 is the Worst Radius. The 10 real-valued
    features are computed for each cell nucleus by means of different characteristics
    of the digitized cell nuclei described in *Table 1, 10 real-valued features and
    their descriptions*:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **No.** | **Value** | **Explanation** |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | Radius | Mean of distances from center to points on the perimeter |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | Texture | Standard deviation of gray-scale values |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | Perimeter | The perimeter of the cell nucleus |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | Area | Area of the cell nucleus covering the perimeter |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | Smoothness | Local variation in radius lengths |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | Compactness | Calculated as follows: (Perimeter)^2 / area - 1.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | Concavity | Severity of concave portions of the contour |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | Concave points | Number of concave portions of the contour |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | Symmetry | Indicates if the cell structure is symmetrical |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | Fractal dimension | Calculated as: coastline approximation - 1 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: 10 real-valued features and their descriptions'
  prefs: []
  type: TYPE_NORMAL
- en: 'All feature values are recorded with four significant digits and there are
    no missing or NULL values. Therefore, we don''t need to perform any data cleaning.
    However, from the previous description, it''s really difficult for someone to
    get any good knowledge of the data. For example, you are unlikely to know how
    each field relates to benign or malignant masses unless you are an oncologist.
    These patterns will be revealed as we continue the machine learning process. A
    sample snapshot of the dataset is shown in *Figure 3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Dataset description and preparation](img/00115.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Snapshot of the data (partial)'
  prefs: []
  type: TYPE_NORMAL
- en: Problem formalization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Figure 4*, *The breast cancer diagnosis and prognosis pipeline model*, describes
    the proposed breast cancer diagnosis model. The model consists of two phases,
    namely, the training and testing phases:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The training phase includes four steps: data collection, pre-processing, feature
    extraction, and feature selection'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The testing phase includes the same four steps as the training phase with the
    addition of the classification step
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the data-collection step, first the pre-processing is done to check if there
    is an unwanted value or any values are missing. We have already mentioned that
    there are no missing values. However, it is always good practice to check, since
    even the unwanted value of a special character could halt the whole training process.
    After that, the feature engineering step is done through the feature extraction
    and selection process for determining the correct input vector for the subsequent
    logistic or linear regression classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Problem formalization](img/00105.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: The breast cancer diagnosis and prognosis pipeline model'
  prefs: []
  type: TYPE_NORMAL
- en: This helps to make a decision regarding the class associated to the pattern
    vectors. Based on either feature selection or feature extraction, the dimensionality
    reduction technique is accomplished. However, please note that we will not use
    any formal dimensionality reduction algorithms to develop this application. For
    more on dimensionality reduction, you can refer to the *Dimensionality reduction*
    section in [Chapter 4](part0038_split_000.html#147LC2-5afe140a04e845e0842b44be7971e11a
    "Chapter 4. Extracting Knowledge through Feature Engineering"), *Extracting Knowledge
    through Feature Engineering*.
  prefs: []
  type: TYPE_NORMAL
- en: In the classification step, a logistic regression classifier is applied to get
    the best result for the diagnosis and prognosis of the tumor.
  prefs: []
  type: TYPE_NORMAL
- en: Developing a cancer-diagnosis pipeline with Spark ML
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As mentioned previously, the details of the attributes found in the WDBC dataset
    at [https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.names](https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.names)
    include patient ID, diagnosis (M = malignant, B = benign), and 10 real-valued
    features are computed for each cell nucleus, as described in *Table 1*, *10 real-valued
    features and their description*.
  prefs: []
  type: TYPE_NORMAL
- en: These features are computed from a digitized image of a **fine needle aspiration**
    (**FNA**) of a breast mass, since we have enough knowing about the dataset. In
    this subsection, we will look at how to develop a breast cancer diagnosis machine
    learning pipeline step-by-step including taking the input of the dataset to prediction
    in the 10 steps described in *Figure 4*, as a data workflow.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1: Import the necessary packages/libraries/APIs**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the code to import the packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 2: Initialize Spark session**'
  prefs: []
  type: TYPE_NORMAL
- en: 'A Spark session can be initialized with the help of the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Here we set the application name as `BreastCancerDetectionDiagnosis`, and the
    master URL as `local``.` The Spark Context is the entry point of the program.
    Please set these parameters accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 3: Take the breast cancer data as input and prepare JavaRDD out of the
    data**'
  prefs: []
  type: TYPE_NORMAL
- en: Here is the code to prepare `JavaRDD:`
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'To learn more about the data, please refer to *Figure 3*: *Snapshot of the
    data (partial*.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 4: Create LabeledPoint RDDs for regression**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create `LabeledPoint` RDDs for diagnosis (B = benign and M= Malignant):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 5: Create the Dataset of Row from the linesRDD and show the top features**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the code illustrated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The following figure shows the top features and their corresponding labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Developing a cancer-diagnosis pipeline with Spark ML](img/00161.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Top features and their corresponding labels'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 6: Split the Dataset to prepare the training and test sets**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here we split the original data frame into training and test set as 60% and
    40%, respectively. Here, `12345L` is the seed value. This value signifies that
    the split will be the same every time, so that the ML model produces the same
    result in each iteration. We follow the same conversion in each chapter for preparing
    the test and training set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: To see a quick snapshot of these two sets just write `trainingData.show()` and
    `testData.show()` for training and test sets, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 7: Create a Logistic Regression classifier**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a logistic regression classifier by specifying the max iteration and
    regression parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Logistic regression typically takes three parameters: the number of max iteration,
    the regression parameter, and the elastic-net regularization. See the following
    lines to get a clearer idea:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The preceding statements create a logistic regression model `lr` with max iteration
    `100`, regression parameter `0.01`, and elastic net parameter `0.4`.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 8: Create and train the pipeline model**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the code illustrated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Here we have created a pipeline whose stages are defined by the logistic regression
    stage, which is also an estimator we have just created. Note that you could try
    creating the Tokenizer and HashingTF stages if you are dealing with a text dataset.
  prefs: []
  type: TYPE_NORMAL
- en: However, in this cancer dataset, all of our values are numeric. Therefore, we
    don't create such stages to be chained to the pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 9: Create a Dataset, transform the model and prediction**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a Dataset of type Row and transform the model to do the prediction based
    on the test dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 10: Show the prediction with prediction precision**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![Developing a cancer-diagnosis pipeline with Spark ML](img/00037.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Prediction with prediction precision'
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 7* shows the prediction Dataset for the test set. The print method
    shown essentially generates output, much like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Developing a cancer-diagnosis pipeline with Spark ML](img/00155.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Sample output toward the prediction. The first value is the feature,
    the second is the label, and the final value is the prediction value'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s calculate the precision score. We do this by multiplying the counter
    by 100 and then dividing the value against how many predictions were done, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Therefore, the precision is 100%, which is fantastic. However, if you are still
    unsatisfied or have any confusion, the following chapter will demonstrate how
    you can still tune several parameters so that the prediction accuracy increases,
    as there might have been many false-negative predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, the result might vary on your platform due to the random-split
    nature and dataset processing on your side.
  prefs: []
  type: TYPE_NORMAL
- en: Cancer-prognosis pipeline with Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we showed how to develop a cancer diagnosis pipeline
    for predicting cancer based on two labels (Benign and Malignant). In this section,
    we will look at how to develop a cancer prognosis pipeline with Spark ML and MLlib
    APIs. The **Wisconsin Prognosis Breast Cancer** (**WPBC**) datasets will be used
    to predict the probability of breast cancer toward the prognosis for recurrent
    and non-recurrent tumor cells. Again, the dataset was downloaded from [https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Prognostic)](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Prognostic)).
    To understand the problem formalization, please refer to *Figure 1* once again
    as we will follow almost the same stages during the cancer-prognosis pipeline
    development.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset exploration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The details of the attributes found in the WPBC dataset in [https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wpbc.names](https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wpbc.names)
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: ID number
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Outcome (R = recurrent, N = non-recurrent)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Time (recurrence time if field 2 => R, disease-free time if field 2 => N)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '3 to 33: Ten real-valued features are computed for each cell nucleus: Radius,
    Texture, Perimeter, Area, Smoothness, Compactness, Concavity, Concave points,
    Symmetry, and Fractal dimension. Thirty-four is Tumor size and thirty-five is
    the Lymph node status, as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tumor size: Diameter of the excised tumor in centimeters'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lymph node status: The number of positive axillary lymph nodes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you compare *Figure 3* and *Figure 9*, you will see that the diagnosis and
    prognosis have the same features, yet the prognosis has two additional features
    (mentioned previously as 34 and 35). Note that these are observed at the time
    of surgery from the year 1988 to 1995 and out of the 198 instances, 151 are non-recurring
    (N) and 47 are recurring (R), as shown in *Figure 8*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, a real cancer diagnosis and prognosis dataset today contains many
    other features and fields in a structured or unstructured way:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Dataset exploration](img/00079.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Snapshot of the data (partial)'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For more detailed discussion and meaningful insights, interested readers can
    refer to the following research paper: *The Wisconsin Breast Cancer Problem: Diagnosis
    and DFS time prognosis using probabilistic and generalized regression neural classifiers
    Oncology Reports, special issue Computational Analysis and Decision Support Systems
    in Oncology, last quarter 2005 by Ioannis A. et al. found in the following link:*
    [http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.65.2463&rep=rep1&type=pdf](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.65.2463&rep=rep1&type=pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: Breast-cancer-prognosis pipeline with Spark ML/MLlib
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this subsection, we will look at how to develop a breast cancer prognosis
    machine learning pipeline step-by-step, including taking the input of the dataset
    to prediction in 10 different steps that are described in *Figure 1*, as a data
    workflow.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Readers are advised to download the dataset and the project files, along with
    the `pom.xml` file for the Maven project configuration, from the Packt materials.
    We have advised how to make the code work in previous chapters, for example, [Chapter
    1](part0014_split_000.html#DB7S2-0b803698e2de424b8aa3c56ad52b005d "Chapter 1. Introduction
    to Data Analytics with Spark"), *Introduction to Data Analytics with Spark*.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1: Import necessary packages/libraries/APIs**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 2: Initialize the necessary Spark environment**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Here we set the application name as `BreastCancerDetectionPrognosis`, the master
    URL as `local[*]`. The Spark Context is the entry point of the program. Please
    set these parameters accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 3: Take the breast cancer data as input and prepare JavaRDD out of the
    data**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To learn more about the data, please refer to *Figure 5* and its description
    and the Dataset exploration subsection.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 4: Create LabeledPoint RDDs**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create `LabeledPoint` RDDs for the prognosis for N = recurrent and R= non-recurrent,
    respectively, using the following code segments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 5: Create the Dataset from the lines RDD and show the top features**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The top features and their corresponding labels are shown in *Figure 9*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Breast-cancer-prognosis pipeline with Spark ML/MLlib](img/00012.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Top features and their corresponding labels'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 6: Split the Dataset to prepare the training and test sets**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here we split the dataset to test and the training set as 60% and 40%, respectively.
    Please adjust these based on your requirements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: To see a quick snapshot of these two sets, just write `trainingData.show()`
    and `testData.show()`, for training and test sets, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 7: Create a Logistic Regression classifier**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a logistic regression classifier by specifying the max iteration and
    regression parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 8: Create a pipeline and train the pipeline model**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Here, similarly to the diagnosis pipeline, we have created the prognosis pipeline
    whose stages are defined by only the logistic regression, which is again an estimator,
    and of course a stage.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 9: Create a Dataset and transform the model**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a Dataset and do the transformation to make a prediction based on the
    test dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 10: Show the prediction with prediction precision**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '![Breast-cancer-prognosis pipeline with Spark ML/MLlib](img/00092.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: Prediction with prediction precision'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'This code segment will produce an output similar to that shown in *Figure 7*,
    with different features, labels, and predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Therefore, the precision is almost 100%, which is fantastic. However, depending
    upon the data preparation, you might receive different results.
  prefs: []
  type: TYPE_NORMAL
- en: If you have any confusion, the following chapter demonstrates how to tune parameters
    so that the prediction accuracy increases, as they might have many false-negative
    predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In their book titled *Machine Learning with R, Packt Publishing, 2015*, Brett
    Lantz at el. argue that it's possible to eliminate the false negatives completely
    by classifying every mass as malignant, benign, recurrent, or non-recurrent. Obviously,
    this is not a realistic strategy. Still, it illustrates the fact that prediction
    involves striking a balance between the false-positive rate and the false-negative
    rate.
  prefs: []
  type: TYPE_NORMAL
- en: If you are still unsatisfied, we will be tuning several parameters in [Chapter
    7](part0059_split_000.html#1O8H62-0b803698e2de424b8aa3c56ad52b005d "Chapter 7. Tuning
    Machine Learning Models"), *Tuning Machine Learning Models*, so that the prediction
    accuracy increases toward more sophisticated methods for measuring predictive
    accuracy that can be used to identify places where the error rate can be optimized
    depending on the costs of each type of error.
  prefs: []
  type: TYPE_NORMAL
- en: Market basket analysis with Spark Core
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will look at how to develop a large-scale machine learning
    pipeline in terms of market basket analysis. Other than using the Spark ML and
    MLlib, we will demonstrate how to use Spark Core to develop such an application.
  prefs: []
  type: TYPE_NORMAL
- en: Background
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In an early paper, *An Efficient Market Basket Analysis Technique with Improved
    MapReduce Framework on Hadoop: An E-commerce Perspective* (available at [http://onlinepresent.org/proceedings/vol6_2012/8.pdf](http://onlinepresent.org/proceedings/vol6_2012/8.pdf)),
    the authors have argued that the **market basket analysis** (**MBA**) technique
    is of substantial importance to everyday business decision, since customers''
    purchase rules can be extracted from the association rules by discovering what
    items they are buying frequently and together. Consequently, purchase rules can
    be revealed for frequent shoppers based on these association rules.'
  prefs: []
  type: TYPE_NORMAL
- en: You might still be wondering why we need market basket analysis, why it is important,
    and why it is computationally expensive. Well, if you could identify highly specific
    association rules like, for example, if a customer prefers mango or orange jam
    along with their milk or butter, you need to have large-scale transactional data
    to be analyzed and processed. Moreover, some massive chain retailers or supermarkets,
    for example, E-mart (UK), HomePlus (Korea), Aldi (Germany), or Dunnes Stores (Ireland)
    use databases of many millions, or even billions, of transactions in order to
    find the associations among particular items with regard to brand, color, origin,
    or even flavor, to increase the probability of sales and profit.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will look at an efficient approach for large-scale market
    basket analysis with Spark libraries. After reading and practising this, you will
    be able to show how the Spark framework lifts the existing single-node pipeline
    to a pipeline usable on a multi-node data-mining cluster. The result is that our
    proposed association-rules mining algorithm can be reused in parallel with the
    same benefits.
  prefs: []
  type: TYPE_NORMAL
- en: We use the acronym SAMBA, for Spark-based Market Basket Analysis, *min_sup*
    for minimum support, and *min_conf* for minimum confidence. We also use the terms
    frequent patterns and frequent itemset interchangeably.
  prefs: []
  type: TYPE_NORMAL
- en: Motivations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Traditional main memory or disk-based computing and RDBMS are not capable of
    handling ever-increasing large transactional data. Furthermore, as discussed in
    [Chapter 1](part0014_split_000.html#DB7S2-0b803698e2de424b8aa3c56ad52b005d "Chapter 1. Introduction
    to Data Analytics with Spark"), *Introduction to Data Analytics with Spark*, MapReduce
    has several issues with the I/O operation, algorithmic complexity, low-latency,
    and fully disk-based operation. Therefore, finding the null transactions and later
    eliminating them from the future scheme is the initial part of this approach.
  prefs: []
  type: TYPE_NORMAL
- en: It is quite possible to find all the null transactions by identifying those
    transactions that do not appear against at least one frequent 1-itemset. As already
    mentioned, Spark caches the intermediate data into memory and provides an abstraction
    of **Resilient Distributed Datasets** (**RDDs**), which can be used to overcome
    these issues by making a huge difference, achieving tremendous success in the
    last three years for handling large-scale data in distributed computing systems.
    These successes are promising and motivating examples to explore this research
    work to applying Spark in market basket analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Please download the grocery dataset for the market basket analysis from [https://github.com/stedy/Machine-Learning-with-R-datasets/blob/master/groceries.csv](https://github.com/stedy/Machine-Learning-with-R-datasets/blob/master/groceries.csv).
    The first five rows of the raw `grocery.csv` data are as follows in *Figure 11*.
    These lines indicate 10 separate grocery-store transactions. The first transaction
    includes four items: citrus fruit, semi-finished bread, margarine, and ready soups.
    In comparison, the third transaction includes only one item, whole milk:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Exploring the dataset](img/00008.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: A snapshot of the groceries dataset'
  prefs: []
  type: TYPE_NORMAL
- en: Problem statements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We believe we have enough motivations and reasons for why we need to analyze
    the market basket using transactional or retail datasets. Now, let us discuss
    some background studies, which are needed to apply our Spark-based market basket
    analysis technique.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose you have a set of distinct items *I = {i1, i2...in}* and *n* is the
    number of distinct items. A transactional database *T = {t1, t2...tN}* is a set
    of *N* transactions and *|N|* is the number of total transactions. A set *X* ![Problem
    statements](img/00067.jpeg) is called a pattern or itemset. We assume that input
    is given as a sequence of transactions, where items are separated by a comma,
    as shown in *Table 1*.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the sake of simplicity to describe the background study, the same transactions
    are presented with a single character in *Table 2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Transaction 1Transaction 2Transaction 3Transaction 4... | crackers, ice-cream,
    coke, orange,beef, pizza, coke, breadbaguette, soda, shampoo, crackers, pepsiburger,
    cream cheese, diapers, milk... |'
  prefs: []
  type: TYPE_TB
- en: Table 1\. Sample transactions made by a customer
  prefs: []
  type: TYPE_NORMAL
- en: '| **TID** | **Itemset (Sequence of items)** |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | A, B, C, F |'
  prefs: []
  type: TYPE_TB
- en: '| 20 | C, D, E |'
  prefs: []
  type: TYPE_TB
- en: '| 30 | A, C, E, D |'
  prefs: []
  type: TYPE_TB
- en: '| 40 | A |'
  prefs: []
  type: TYPE_TB
- en: '| 50 | D, E, G |'
  prefs: []
  type: TYPE_TB
- en: '| 60 | B, D |'
  prefs: []
  type: TYPE_TB
- en: '| 70 | B |'
  prefs: []
  type: TYPE_TB
- en: '| 80 | A, E, C |'
  prefs: []
  type: TYPE_TB
- en: '| 90 | A, C, D |'
  prefs: []
  type: TYPE_TB
- en: '| 100 | B, E, D |'
  prefs: []
  type: TYPE_TB
- en: Table 2\. A transactional database
  prefs: []
  type: TYPE_NORMAL
- en: 'If ![Problem statements](img/00117.jpeg) , it is said that *X* occurs in *t*
    or *t* contains *X*. The support count is the frequency of occurrence of an itemset
    in all transactions, which can be described as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Problem statements](img/00093.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In other words, if the *support*![Problem statements](img/00033.jpeg) , we say
    that *X* is a frequent itemset. For example, in *Table 2*, the occurrences of
    itemsets *CD*, *DE,* and *CDE* are *3*, *3*, and *2*, respectively, and if the
    *min_sup* is *2*, all of these are frequent itemsets.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, association rules are statements of form ![Problem statements](img/00151.jpeg)
    or more formally:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Problem statements](img/00110.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Therefore, we can say that an association rule is a pattern that states when
    *X* occurs, then *Y* occurs with a certain probability. Confidence for the association
    rule defined in equation 1 can be expressed as how often items in *Y* appear in
    transactions that also contain *X*, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Problem statements](img/00032.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now we need to introduce a new parameter, called `lift`, which as a metric
    is a measure of how much more likely one item is to be purchased relative to its
    typical purchase rate, given that you know another item has been purchased. This
    is defined by the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Problem statements](img/00018.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In a nutshell, given a transactional database, now the problem of market basket
    analysis is to find the complete set of a customer's purchase rules by means of
    association rules from the frequent itemsets whose support and confidence are
    no less than the *min_sup* and *min_conf* threshold, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Large-scale market basket analysis using Spark
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As shown in *Figure 12*, we assume that transactional databases are stored in
    a distributed way in a cluster of DB servers. A DB server is a computing node
    with large storage and main memory. Therefore, it can store large datasets, so
    it can compute any task assigned to it. The Driver PC is also a computing node,
    which mainly works as a client and controls the overall process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Obviously, it needs to have a large memory for processing and holding the Spark
    codes to send across the computing nodes. The codes consist of a DB server ID,
    minimum support, minimum confidence, and mining algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Large-scale market basket analysis using Spark](img/00078.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: Workflow of the SAMBA algorithm using Spark'
  prefs: []
  type: TYPE_NORMAL
- en: From the patterns, frequent patterns are generated using reduce phase 1, which
    satisfies the constraints *min_sup*. The map phase is applied on the computed
    frequent patterns to generate the sub-patterns that eventually help to generate
    the association rules. From the sub-patterns, reduce phase 2 is applied to generate
    the association rules that satisfy the constraints *min_conf*.
  prefs: []
  type: TYPE_NORMAL
- en: The incorporation of two Map and Reduce phases is possible because of the Spark
    ecosystem toward the Spark core and associated APIs. The final results are the
    complete set of association rules with their respective support count and confidence.
  prefs: []
  type: TYPE_NORMAL
- en: These store keepers have the full form to place their items, based on the association
    between items, to increase sales to frequent and non-frequent shoppers. Due to
    space constraints, we cannot show a step-by-step example for the sample transactional
    database presented in *Table 2*.
  prefs: []
  type: TYPE_NORMAL
- en: However, we believe that the workflow and the pseudo codes will suffice to understand
    the total scenario. A DB server takes the input of codes sent from the Driver
    PC and starts the computation. From an environment variable Spark session, we
    create some initial data reference or RDD objects. Then, the initial RDD objects
    are transformed to create more and brand new RDD objects in the DB server. At
    first, it reads the dataset as a plain text (or other supported format) and null
    transactions using narrow/wide transformations (that is, `flatMap`, `mapToPair`,
    and `reduceByKey`).
  prefs: []
  type: TYPE_NORMAL
- en: 'Thereby, the filter join RDD operation provides a data segment without null
    transactions. Then the RDD objects are materialized to dump the RDD into the DB
    server''s storage as filtered datasets. Spark''s inter RDD join operation allows
    for the combining of the contents of multiple RDDs within a single data node.
    In summary, we follow the steps given here before getting the filtered dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: Set the system property of the distributed processing model and cluster manager
    (that is, Mesos) as true. This value can be saved on your application development
    as standard Spark code.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set SparkConf, AppName, Master URL, Spark local IP, Spark driver host IP, Spark
    executor memory, and Spark driver memory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create `JavaSparkContext` using the `SparkConf`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create `JavaRDD` and read the dataset as plain text, as transactions, and perform
    necessary partitioning.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform a `flatMap` operation over the RDD to split the transactions as items.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform the `mapToPair` operation to ease finding the key/value pairs of the
    items.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform the filter operation to remove all the null transactions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When we have the filtered databases, we materialize an action inter-RDD join
    operation to save the dataset on a DB server or partition if it does not have
    enough storage for a single machine, or cache if there's not enough memory.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 12* shows the complete workflow of getting association rules as the
    final results using Spark''s APIs. On the other hand, Figure 13 shows the pseudo-code
    of the algorithm, namely, **Spark-Based Market Basket Analysis** (**SAMBA**).
    There are actually two Map and Reduce operations associated, as outlined here:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Map/Reduce phase 1**: Mappers read the transactions from the HDFS servers
    and convert the transactions to patterns. Reducers, on the other hand, find the
    frequent patterns.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Map/Reduce phase 2**: Mappers convert the frequent patterns into sub-patterns.
    On the other hand, a reducer generates the association rules based on the given
    constraints (`min_conf` and `lift`):![Large-scale market basket analysis using
    Spark](img/00140.jpeg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Figure 13: The SAMBA algorithm'
  prefs: []
  type: TYPE_NORMAL
- en: After that, the SAMBA algorithm reads the **filtered database** (**FTDB**) and
    applies map phase 1 to generate all the possible combinations of the patterns.
    Then the `mapToPair()` methods them as patterns with their respective supports.
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm solution using Spark Core
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here we will look at how to do the market basket analysis using Spark Core.
    Please note, we will not use the Spark ML or MLlib since although MLlib provides
    a technique of calculating the association rules, however, it does not show how
    to calculate some other parameters such as calculating confidence, support, and
    lift that are very needed for a complete analysis of groceries dataset. Therefore,
    we will show a complete example, step-by-step, from data exploration to association
    rules generation.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1: Import the necessary packages and APIs**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the code to import packages and APIs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 2: Create the entry point by specifying the Spark session**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The entry point can be created with the help of the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 3: Create the Java RDD for the transactions**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Java RDD for the transactions can be created with the help of the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 4: Create a method for creating the list**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a method named `toList` from the created transactions RDDs, which will
    add all the items in the transactions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 5: Remove infrequent items and null transactions**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a method named `removeOneItemAndNullTransactions` to remove infrequent
    items and null transactions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 6: Flat mapping and 1-itemsets creation (map phase 1)**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Do the `flatmap` and create 1-itemsets. Finally, save the patterns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note that the last saving of the patterns RDD is for optional reference purposes
    and so that you can see the contents of the RDDs.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a screenshot of the 1-itemsets:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The algorithm solution using Spark Core](img/00152.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14: 1-itemsets'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 7: Combine and reduce frequent patterns (reduce phase 1)**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Combine and reduce all the frequent patterns, and save them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is a snapshot of the frequent patterns with their respective
    support (frequency in *Figure 15)*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The algorithm solution using Spark Core](img/00073.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15: Frequent patterns with their respective support (frequency)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 8: Generate all the candidate frequent patterns (map phase 2)**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Generate all the candidate frequent patterns or sub-patterns by removing the
    1-itemsets from the frequent patterns, and finally, save the candidate patterns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is a snapshot of the sub-patterns:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The algorithm solution using Spark Core](img/00054.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16: Sub-patterns for the items'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 9: Combine all the sub-patterns**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Combine all the sub-patterns and save them on disk or persist on memory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is a screenshot of the candidate patterns (sub-patterns) in combined
    form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The algorithm solution using Spark Core](img/00020.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 17: Candidate patterns (sub-patterns) in combined form'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 10: Generate association rules**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Generate all the association rules from the sub-patterns (reduce phase 2) by
    specifying the `confidence` and `lift`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: The following is the output of the association rules including their confidence
    and lift. For more details on support, confidence, and lift, refer to the problem
    statement section.
  prefs: []
  type: TYPE_NORMAL
- en: '[Antecedent=>Consequent], Support, Confidence, Lift:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The algorithm solution using Spark Core](img/00094.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18: Association rules including their confidence and lift'
  prefs: []
  type: TYPE_NORMAL
- en: Tuning and setting the correct parameters in SAMBA
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Note that if you attempt to use the default parameter settings as support =
    0.1 and confidence = 0.6, you might end up with null rules, or technically, no
    rules, to be generated. You might be wondering why. Actually, the default support
    of 0.1 means that in order to generate an association rule, an item must have
    appeared in at least *0.1 * 9385 = 938.5* transactions or 938.5 times (for the
    dataset we are using, |N| = 9385).
  prefs: []
  type: TYPE_NORMAL
- en: However, in this regard, in their book titled *Machine Learning with R, Packt
    Publishing, 2015*, Brett Lantz at el. argue that there is one way to tackle this
    issue while setting support. They suggest considering the minimum number of transactions
    needed before you would consider a pattern's interestingness. Moreover, for example,
    you could also argue that if an item is purchased twice a day (which is approximately
    60 times a month), then it may be non-trivial to consider that transaction.
  prefs: []
  type: TYPE_NORMAL
- en: From this perspective, it is possible to estimate how to set the value of support
    needed to find only rules matching at least that many transactions. Consequently,
    you can set the value of minimum support as 0.006, because 60 out of 9,835 equals
    0.006; we'll try setting the support there first.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, setting the minimum confidence also requires a tricky balance
    and in this regard, again, we would like to refer you to the book by Brett Lantz
    et al. titled *Machine Learning with R, Packt Publishing, 2015*. If confidence
    is too low, obviously we might be incredulous with a pretty large of unreliable
    rules false positive results.
  prefs: []
  type: TYPE_NORMAL
- en: As a result, the optimum value of the minimum confidence threshold depends heavily
    on the goals of your analysis. Consequently, if you start with conservative values,
    you can always reduce them to broaden the search if you aren't finding actionable
    intelligence. If you set the minimum confidence threshold at 0.25, it means that
    in order to be included in the results, the rule has to be correct at least 25
    percent of the time. This will eliminate the most unreliable rules while allowing
    some room for us to modify behavior with targeted promotions of the product.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's talk about the third parameter, the `lift`. Before suggesting how
    to set the value of the `lift`, let's see a practical example of how it might
    affect the generation of the association rules in the first place. For the third
    time, we refer to the book by Brett Lantz et al. titled *Machine Learning with
    R, Packt Publishing, 2015*.
  prefs: []
  type: TYPE_NORMAL
- en: For example, suppose at a supermarket store, many people often purchase milk
    and bread together. Therefore, naturally, you would expect to find many transactions
    that contain both milk and bread. However, if `lift` (milk => bread) is greater
    than 1, this implies that the two items are found together more often than one
    would expect by chance. As a consequence, a large `lift` value is, therefore,
    a strong indicator that a rule is important, and reflects a true connection between
    the items in the transactions.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, we need to set the values of these parameters carefully, by considering
    the preceding examples. However, as a standalone model the algorithms might take
    hours to finish. So, run the application with enough time. Alternatively, reduce
    the long transaction to reduce the time overhead.
  prefs: []
  type: TYPE_NORMAL
- en: OCR pipeline with Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Image processing and computer vision are two classical but still-emerging research
    areas that often make proper utilization of many types of machine learning algorithms.
    There are several use cases where the relationships of linking the patterns of
    image pixels to higher concepts are extremely complex and hard to define, and
    of course, computationally extensive, too.
  prefs: []
  type: TYPE_NORMAL
- en: From a practical point of view, it's relatively easier for a human being to
    recognize if an object is a face, a dog, or letters or characters. However, defining
    these patterns under certain circumstances is difficult. Additionally, image-related
    datasets are often noisy.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will develop a model similar to those used at the core of
    the **Optical Character Recognition** (**OCR**) used as document scanners. This
    kind of software helps to process paper-based documents by converting printed
    or handwritten text into an electronic form to be saved in a database.
  prefs: []
  type: TYPE_NORMAL
- en: When OCR software first processes a document, it divides the paper, or any object,
    into a matrix such that each cell in the grid contains a single glyph (also known
    as different graphical shapes), which is just an elaborate way of referring to
    a letter, symbol, or number, or any contextual information from the paper or the
    object.
  prefs: []
  type: TYPE_NORMAL
- en: To demonstrate the OCR pipeline, we will assume that the document contains only
    alpha characters in English that match glyphs to one of the 26 letters, A to Z.
    We will use the OCR letter dataset from the UCI Machine Learning Data Repository
    ([http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml) ). The dataset
    was donated by W. Frey and D. J. Slate et al. To explore the dataset, we have
    found that the dataset contains 20,000 examples of 26 English alphabet capital
    letters printed using 20 different randomly reshaped and distorted black-and-white
    fonts as glyphs of different shapes.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For more information about these data, refer to *Letter recognition using Holland-style
    adaptive classifiers, Machine Learning, Vol. 6, pp. 161-182, by W. Frey and D.J.
    Slate (1991)*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The image shown in *Figure 19* was published by Frey and Slate and provides
    an example of some of the printed glyphs. Distorted in this way, the letters are
    challenging for a computer to identify, yet are easily recognized by a human being.
    The statistical attributes for the top 20 rows are shown in *Figure 20*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![OCR pipeline with Spark](img/00119.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 19: Some of the printed glyphs [courtesy of the article titled Letter
    recognition using Holland-style adaptive classifiers, Machine Learning, Vol. 6,
    pp. 161-182, by W. Frey and D.J. Slate (1991)]'
  prefs: []
  type: TYPE_NORMAL
- en: Exploring and preparing the data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: According to the documentation provided by Frey and Slate, when the glyphs are
    scanned using an OCR reader to the computer they are automatically converted into
    pixels. Consequently, the 16 statistical attributes mentioned are recorded to
    the computer too.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the concentration of black pixels across the various areas of the
    box where the character is indicated should provide a way to differentiate among
    the 26 letters of the alphabet using an OCR or a machine learning algorithm to
    be trained.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To follow along with this example, download the `letterdata.data` file from
    the Packt Publishing website and save it to your project directory working on
    one or the other directory.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before reading the data from the Spark working directory, we confirm that we
    have received the data with the 16 features that define each example of the letter
    class. As expected, the letter has 26 levels, as shown in *Figure 20*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Exploring and preparing the data](img/00138.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 20: A snapshot of the dataset shown as Data Frame'
  prefs: []
  type: TYPE_NORMAL
- en: Recall that SVM, Naive Baseyan-based classifier, or any other classifier algorithms,
    along with their associated learners, require all the features to be numeric.
    Moreover, each feature is scaled to a fairly small interval.
  prefs: []
  type: TYPE_NORMAL
- en: Also, SVM works well on dense vectorized features and consequently, will perform
    poorly against the sparse vectorized features. In our case, every feature is an
    integer. Therefore, we do not need to convert any values into numbers. On the
    other hand, some of the ranges for these integer variables appear fairly wide.
  prefs: []
  type: TYPE_NORMAL
- en: In practical cases, it might require that we normalize the data against all
    few features points.
  prefs: []
  type: TYPE_NORMAL
- en: OCR pipeline with Spark ML and Spark MLlib
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Because of its accuracy and robustness, let's see whether the SVM is up to the
    task. As you can see in *Figure 17*, we have a multiclass OCR dataset (with 26
    classes, to be more precise); therefore, we need to have a multiclass classification
    algorithm, for example, the logistic regression model, since the current implementation
    of liner SVM in Spark does not support the multi-class classification.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Please refer to the following URL for more details: [http://spark.apache.org/docs/latest/mllib-linear-methods.html#linear-support-vector-machines-svms](http://spark.apache.org/docs/latest/mllib-linear-methods.html#linear-support-vector-machines-svms).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1: Import necessary packages/libraries/APIs**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the code to import the necessary packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 2: Initialize necessary Spark environment**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the code to initialize a Spark environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Here we set the application name as `OCRPrediction`, and the master URL as `local`.
    The Spark session is the entry point of the program. Please set these parameters
    accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 3: Read the data file and create a corresponding Dataset and show the
    first 20 rows**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the code to read the data file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: For the first 20 rows, please refer to *Figure 5*. As we can see, there are
    26 characters presented as single characters that need to be predicted; therefore,
    we need to assign each character a random double value to align the value to the
    other features. Therefore, in the next step, that is what we'll do.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 4: Create a dictionary for assigning each character a double value randomly**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code is to create a dictionary for assigning each character a
    double value randomly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'And here is the mapping output generated from the preceding code segment:'
  prefs: []
  type: TYPE_NORMAL
- en: '![OCR pipeline with Spark ML and Spark MLlib](img/00048.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 21: Mapping assignments'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 5: Creating the labeled point and the feature vector**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create the labeled points and feature vectors for the features combined from
    the 16 features (that is, 16 columns). Also, save them as Java RDD and dump or
    cache on disk or memory, and show the sample output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'If you look carefully at the preceding code segments, we have created an array
    named features for 16 features altogether and created a dense vector representation,
    since the dense vector representation is a more compact representation where the
    contents can be shown as in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![OCR pipeline with Spark ML and Spark MLlib](img/00041.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 22: The Java RDD for the corresponding label and features as vectors'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 6: Generating the training and test set**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the code for generating the test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'If you wish to see the snaps of the training or test datasets, you should dump
    or cache them. The following is a sample code for that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We have randomly generated the training and the test set for the model to be
    trained and tested. In our case, it was 70% and 30%, respectively, and 11L as
    the long seed. Readjust the values based on your dataset. Note that if you add
    a seed to a Random, you get the same results each time you run your code and these
    come as primes up to 1062348.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 7: Train the model**'
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, we have a multiclass dataset with 26 classes; therefore, we
    need to have a multiclass classification algorithm, for example, the logistic
    regression model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code segment builds a model using the training datasets by specifying
    the number of classes (that is, `26`) and the feature scaling as `Boolean true`.
    As you can see, we have used the RDD version of the training datasets using `training.rdd()`,
    since the training datasets are in normal vector format.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Spark has the support of the multiclass logistic regression algorithm that supports
    the **Limited-Memory-Broyden-Fletcher-Goldfarb-Shanno** (**LBFGS**) algorithm.
    In numerical optimization, the **Broyden-Fletcher-Goldfarb-Shanno** (**BFGS**)
    algorithm is an iterative method for solving unconstrained nonlinear optimization
    problems.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 8: Compute the raw scores on the test dataset**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the code to compute the raw scores:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: If you look at the preceding code carefully, you will see that we are actually
    calculating the predicted features out of the model we created in *Step 7* by
    making them Java RDD.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 9: Predict the outcome for label 8.0 (that is, I) and get the evaluation
    metrics**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code illustrates how to predict the outcome:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '![OCR pipeline with Spark ML and Spark MLlib](img/00050.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 23: Performance metrics for precision and recall'
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, the precision is 75%, which is obviously not satisfactory. However,
    if you are still unsatisfied, the following chapter will look at how to tune parameters
    so that the prediction accuracy increases.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To get an idea about how to calculate the precision, recall, true positive
    rate, and true negative rate, please refer to the Wikipedia page at [https://en.wikipedia.org/wiki/Sensitivity_and_specificity](https://en.wikipedia.org/wiki/Sensitivity_and_specificity),
    which discusses the sensitivity and the specificity elaborately. You can also
    refer to *Powers, David M W (2011). Evaluation: From Precision, Recall and F-Measure
    to ROC, Informedness, Markedness & Correlation(PDF). Journal of Machine Learning
    Technologies 2 (1): 37-63*.'
  prefs: []
  type: TYPE_NORMAL
- en: Topic modeling using Spark MLlib and ML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The topic modeling technique is widely used in the task of mining text from
    a large collection of documents. These topics can then be used to summarize and
    organize documents that include the topic terms and their relative weights. Since
    the release of Spark 1.3, MLlib supports the LDA, which is one of the most successfully
    used topic-modeling techniques in the area of text mining and **Natural Language
    Processing** (**NLP**). Moreover, LDA is also the first MLlib algorithm to adopt
    Spark GraphX.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To get more information about how the theory behind the LDA works, please refer
    to *David M. Blei, Andrew Y. Ng and Michael I. Jordan, Latent Dirichlet Allocation,
    Journal of Machine Learning Research 3 (2003) 993-1022*.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 24* shows the output of the topic distribution from randomly generated
    tweet text, to be discussed further in [Chapter 9](part0073_split_000.html#25JP22-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 9.  Advanced Machine Learning with Streaming and Graph Data"), *Advanced
    Machine Learning with Streaming and Graph Data*. Moreover, we will provide more
    justification for why we used LDA other than other topic-modeling algorithms in
    [Chapter 9](part0073_split_000.html#25JP22-0b803698e2de424b8aa3c56ad52b005d "Chapter 9. 
    Advanced Machine Learning with Streaming and Graph Data"), *Advanced Machine Learning
    with Streaming and Graph Data*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Topic modeling using Spark MLlib and ML](img/00159.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 24: The topic distribution and how it looks like'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will look at an example of topic modeling using the LDA
    algorithm of Spark MLlib with unstructured raw tweets datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Topic modeling with Spark MLlib
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this subsection, we represent a semi-automated technique of topic modeling
    using Spark. The following steps show the topic modeling from data reading to
    printing the topics, along with their term-weights. Using other options as defaults,
    we train LDA on the dataset downloaded from the GitHub URL at [https://github.com/minghui/Twitter-LDA/tree/master/data/Data4Model/test](https://github.com/minghui/Twitter-LDA/tree/master/data/Data4Model/test).
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1: Load required packages and APIs**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the code to load the required packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 2: Create a Spark session**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the code to create a Spark session:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 3: Reading and seeing the content of the datasets**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code illustrates how to read and see the content of the datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Please note that the use of the character `***` indicates to read all the text
    files in the input/text directory in the project path. If we print the first 20
    rows, just use the following code and you will see the following text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '![Topic modeling with Spark MLlib](img/00006.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 25: First 20 rows of the texts'
  prefs: []
  type: TYPE_NORMAL
- en: From the preceding screenshot, it is clear that the text files we are using
    are nothing but very unstructured texts containing the column name label. They
    therefore need to be pre-processed using the feature transformation using the
    regular expression tokenizer before we can use them for our purposes.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 4: Feature Transformers using RegexTokenizer**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the code for `RegexTokenizer`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'If you look at the preceding code segment carefully, you will see that we have
    specified our input column name as `value` and our output column name as `labelText`
    and the pattern. Now create another data frame using the regular expression tokenizer
    we just tokenized using the following code segment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s see what the new data frame `labelTextDataFrame` contains using
    the following statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: '![Topic modeling with Spark MLlib](img/00095.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 26: A new column with characters converted into the corresponding lowercase
    characters'
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding screenshot (*Figure 26*) shows that the tokenizer created a new
    column and that mostly, the uppercase words or characters have been converted
    into the corresponding lowercase characters. Since topic modeling cares about
    the term-weight and frequency of each input word, we need to separate the words
    from the label texts, which is done by using the following code segments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s create another data frame and see the results of the transformation
    using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: '![Topic modeling with Spark MLlib](img/00055.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 27: Label texts as separate words separated by a comma'
  prefs: []
  type: TYPE_NORMAL
- en: From the preceding screenshot (*Figure 27*), we can see that a new column, `label`
    has been added, which shows the label texts as separate words, separated by a
    comma.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, since we have a bunch of texts available, to make the prediction and topic
    modeling easier, we need to make the indexing for the words we split. But before
    that, we need to swap the `labelText` and `text` in a new data frame, as shown
    in *Figure 28*. To check if it has really happened, just print the newly created
    data frame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: '![Topic modeling with Spark MLlib](img/00049.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 28: Swapping the labelText and text in a new data frame'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 5: Feature transformation by means of string indexer**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the code for feature transformation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Now create a new data frame for the data frame `newDF` we created in *Step
    2*, and look at the contents of the data frame. Note that we have selected the
    old column `labelText` and set the new column simply as `label`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: '![Topic modeling with Spark MLlib](img/00131.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 29: Corresponding labels against the labelText column'
  prefs: []
  type: TYPE_NORMAL
- en: So, as shown in *Figure 29*, we got a new column, `label`, which contains the
    corresponding labels against the `labelText` column. The very next step is about
    removing the stop words.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 6: Feature transformation (removing the stop words)**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the code for feature transformation for removing the stop words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'The current implementation of the `StopWordsRemover` class of Spark contains
    the following words as stop words. Since we don''t have any precondition, we have
    used those words directly:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Topic modeling with Spark MLlib](img/00074.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 30: A bunch of stop words provided by Spark for text analytics'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 7: Create a filtered dataset by removing stop words**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the code to create a filtered dataset by removing stop words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Now create a new data frame for the filtered words (that is, excluding the
    stop words). Let''s look at the contents of the filtered dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: '![Topic modeling with Spark MLlib](img/00107.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 31: Filtered words excluding the stop words'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 8: Feature extraction using HashingTF**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the code for feature extraction using HashingTF:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'In the previous code, we have done the HashingTF for only five features, for
    simplicity. Now create another data frame out of the extracted features on the
    old data frame (that is, `filteredDF`) and show the same output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: '![Topic modeling with Spark MLlib](img/00024.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 32: Data frame out of the extracted features on the old data frame,
    filteredDF'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For more information and API documentation details for feature transformation,
    estimator, and hashing, please refer to the Spark website at [https://spark.apache.org/docs/latest/ml-features.html](https://spark.apache.org/docs/latest/ml-features.html).
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 9: Feature extraction using IDF estimator**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code creates new features from the raw features by fitting the
    `idfModel` that takes the featured data frame (that is, `featurizedData`) in step
    5\. Now let''s create and show a new data frame for the rescaled data using the
    estimator we just created (that is, `idfModel`), which consumes the old data frame
    for the featurized data (that is, `featurizedData`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: '![Topic modeling with Spark MLlib](img/00137.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 33: The rescaled data using the estimator'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 10: Chi-Squared feature selection**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The Chi-Squared feature selection selects categorical features to use for predicting
    a categorical label. The following code segment does this selection:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'Now create another data frame for the selected features, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: '![Topic modeling with Spark MLlib](img/00130.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 34: Chi-Squared feature selection'
  prefs: []
  type: TYPE_NORMAL
- en: You can see from the preceding output/screenshot that our data is ready for
    the LDA model to be trained and do the topic modeling.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 11: Create and train an LDA model**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create and train the LDA model using the training datasets (that is, the data
    frame result) by specifying the *K* (which is the number of clusters the topic
    modeling must have to be >1, where the default value is 10) and max iteration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have the model trained, fitted, and ready for our purposes, let''s
    look at our output. However, before doing that, we need to have a data frame that
    could capture the topic-related metrics. Use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s see the topics distribution. Look at the previous Dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: '![Topic modeling with Spark MLlib](img/00003.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 35: Corresponding term weight, topic name, and term indices'
  prefs: []
  type: TYPE_NORMAL
- en: If you look at the preceding output carefully, we have found the corresponding
    term weight, topic name, and term indices. The preceding terms and their corresponding
    weights will be used in [Chapter 9](part0073_split_000.html#25JP22-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 9.  Advanced Machine Learning with Streaming and Graph Data"), *Advanced
    Machine Learning with Streaming and Graph Data*, for finding connected components
    using GraphX with Scala.
  prefs: []
  type: TYPE_NORMAL
- en: However, we also need to have the actual terms. We will show the detailed technique
    of retrieving the terms in [Chapter 9](part0073_split_000.html#25JP22-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 9.  Advanced Machine Learning with Streaming and Graph Data"), *Advanced
    Machine Learning with Streaming and Graph Data* hat extensively depends on the
    vocabulary of the terms that need to be developed  or generated using the terms
    from the bag of words concept.
  prefs: []
  type: TYPE_NORMAL
- en: Scalability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The previous example shows how to perform topic modeling using the LDA algorithm
    as a standalone application. However, according to a Databricks blog by Joseph
    B. at [https://databricks.com/blog/2015/03/25/topic-modeling-with-lda-mllib-meets-graphx.html](https://databricks.com/blog/2015/03/25/topic-modeling-with-lda-mllib-meets-graphx.html),
    the parallelization of LDA is not straightforward, and there have been many research
    papers proposing different strategies. The key obstacle in this regard is that
    all methods involve a large amount of communication. According to the blog on
    the Databricks website, here are the statistics of the dataset and related training
    and test sets that were used during the experimentation:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Training set size: 4.6 million documents'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vocabulary size: 1.1 million terms'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Training set size: 1.1 billion tokens (~239 words/document)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 100 topics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 16-worker EC2 cluster, for example, M4.large or M3.medium depending upon budget
    and requirements
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Timing results: 176 secs/iteration on average over 10 iterations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Credit risk analysis pipeline with Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will develop a credit risk pipeline that is commonly used
    in financial institutions such as banks and credit unions. First we will discuss
    what credit risk analysis is and why it is important before developing a Spark
    ML-based pipeline using a Random-Forest-based classifier. Finally, we will provide
    some performance improvement suggestions.
  prefs: []
  type: TYPE_NORMAL
- en: What is credit risk analysis? Why is it important?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When an applicant applies for a loan and a bank receives that application, based
    on the applicant's profile, the bank has to make a decision whether to approve
    the loan application or not.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this regard, there are two types of risk associated with the bank''s decision
    on the loan application:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Applicant is a good credit risk**: That means the client or applicant is
    more likely to repay the loan. Then, if the loan is not approved, the bank can
    potentially suffer loss of business.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Applicant is a bad credit risk**: That means that the client or applicant
    is most likely not to repay the loan. In that case, approving the loan to the
    client will result in financial loss to the bank.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our common sense says that the second risk is the greater risk, as the bank
    has a higher chance of not being reimbursed the borrowed amount.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, most banks or credit unions evaluate the risks associated with lending
    money to a client, applicant, or customer. In business analytics, minimizing the
    risk tends to maximize the profit to the bank itself. In other words, maximizing
    the profit and minimizing the loss from a financial perspective is important.
  prefs: []
  type: TYPE_NORMAL
- en: Often, the bank makes a decision about approving a loan application based on
    different factor and parameters of an applicant. For example, the demographic
    and socio-economic conditions regarding their loan application.
  prefs: []
  type: TYPE_NORMAL
- en: Developing a credit risk analysis pipeline with Spark ML
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will first discuss the credit risk dataset in detail in
    order to gain some insight. After that, we will look at how to develop a large-scale
    credit risk pipeline. Finally, we will provide some performance improvement suggestions
    toward better prediction accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: The dataset exploration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The German Credit dataset was downloaded from the UCI Machine Learning Repository
    at [https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/](https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/).
    Although a detailed description of the dataset is available in the link, we provide
    some brief insights here in *Table 3*. The data contains credit-related data on
    21 variables and the classification of whether an applicant is considered a good
    or a bad credit risk for 1000 loan applicants. *Table 3* shows details about each
    variable that was considered before making the dataset available online:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Entry** | **Variable** | **Explanation** |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | `creditability` | Capable of repaying |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | `balance` | Current balance |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | `duration` | Duration of the loan being applied for |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | `history` | Is there any bad loan history? |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | `purpose` | Purpose of the loan |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | `amount` | Amount being applied for |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | `savings` | Monthly saving |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | `employment` | Employment status |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | `instPercent` | Interest percent |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | `sexMarried` | Sex and marriage status |'
  prefs: []
  type: TYPE_TB
- en: '| 11 | `guarantors` | Are there any guarantors? |'
  prefs: []
  type: TYPE_TB
- en: '| 12 | `residenceDuration` | Duration of residence at the current address |'
  prefs: []
  type: TYPE_TB
- en: '| 13 | `assets` | Net assets |'
  prefs: []
  type: TYPE_TB
- en: '| 14 | `age` | Age of the applicant |'
  prefs: []
  type: TYPE_TB
- en: '| 15 | `concCredit` | Concurrent credit |'
  prefs: []
  type: TYPE_TB
- en: '| 16 | `apartment` | Residential status |'
  prefs: []
  type: TYPE_TB
- en: '| 17 | `credits` | Current credits |'
  prefs: []
  type: TYPE_TB
- en: '| 18 | `occupation` | Occupation |'
  prefs: []
  type: TYPE_TB
- en: '| 19 | `dependents` | Number of dependents |'
  prefs: []
  type: TYPE_TB
- en: '| 20 | `hasPhone` | If the applicant uses a phone |'
  prefs: []
  type: TYPE_TB
- en: '| 21 | `foreign` | If the applicant is a foreigner |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: German credit dataset properties'
  prefs: []
  type: TYPE_NORMAL
- en: Note that, although *Table 3* describes the variables in the dataset, there
    is no associated header. In *Table 3*, we have shown the variable, position, and
    associated significance of each variable.
  prefs: []
  type: TYPE_NORMAL
- en: Credit risk pipeline with Spark ML
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There will be several steps involved, from data loading, parsing, data preparation,
    training testing set preparation, model training, model evaluation, and result
    interpretation. Let's go through the steps one by one.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1: Load required APIs and libraries**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the code for loading the required APIs and libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 2: Create a Spark session**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is another code for creating a Spark session:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 3: Load and parse the credit risk dataset**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that the dataset is in **Comma-Separated Value** (**CSV**) format. Now
    load and parse the dataset using the Databricks-provided CSV readers and prepare
    a Dataset of Row, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, show the Dataset to get to know the exact structure, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: '![Credit risk pipeline with Spark ML](img/00104.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 36: A snapshot of the credit risk dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 4: Create an RDD of type Credit**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create an RDD of typed class `Credit`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code segments creates an RDD of type `Credit` after taking the
    variable as double values by using the `parseDouble()` method, which takes a string
    and returns the corresponding value in `Double` format. The `parseDouble()` method
    goes as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: Now we need to know the structure of the `Credit` class so that the structure
    itself helps to create the RDDs using the typed class.
  prefs: []
  type: TYPE_NORMAL
- en: 'Well, the `Credit` class is basically a singleton class that initializes all
    the setter and getter methods for the 21 variables from the dataset through the
    constructor. Here is the class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: If you look at the flow of the class, at first it declares 21 variables for
    the 21 features in the dataset. Then it initializes them using the constructor.
    The rest are simple setter and getter methods.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 5: Create a Dataset of type Row from the RDD of type Credit**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code shows how to create a Dataset of type Row:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'Now save the Dataset as a temporary view, or more formally, a table in-memory
    for query purposes, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s get to know the schema of the table as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: '![Credit risk pipeline with Spark ML](img/00102.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 37: The schema of the Dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 6: Create the feature vector using the VectorAssembler**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new feature vector for the 21 variables using the `VectorAssembler`
    class of Spark, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 7: Create a Dataset by combining and transforming the assembler**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a Dataset by transforming the assembler using the `creditData` Dataset
    previously created, and print the first top 20 rows of the Dataset, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: '![Credit risk pipeline with Spark ML](img/00087.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 38: Newly created featured Credit Dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 8: Create label for making predictions**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a label column out of the creditability column of the preceding Dataset
    (*Figure 38*), as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s explore the new Dataset using the `show()` method as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: '![Credit risk pipeline with Spark ML](img/00056.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 39: Dataset with a new label column'
  prefs: []
  type: TYPE_NORMAL
- en: From the preceding figure, we can understand that there are only two labels
    associated with the Dataset, which are 1.0 and 0.0\. That signifies the problem
    as a binary classification problem.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 9: Prepare the training and test set**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Prepare the training and test set as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: Here, the ratio is 70% and 30% for the training and testing set, respectively,
    with a long seed value to disallow the random result generation in each iteration.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 10: Train the Random Forest model**'
  prefs: []
  type: TYPE_NORMAL
- en: 'To train the Random Forest model, use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: 'As previously mentioned, the problem is a binary classification problem. Therefore,
    we will evaluate the Random Forest model using a binary evaluator for the `label`
    column, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we need to collect the model performance metric on the test set that goes
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 11: Print the performance parameters**'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will observe several performance parameters of the binary evaluator, for
    example, accuracy after fitting the model, **Mean Square Error** (**MSE**), **Mean
    Absolutize Error** (**MAE**), **Root Mean Squared Error** (**RMSE**), R Squared
    and explained variable, and so on. Let''s do it as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code segment generates the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: Performance tuning and suggestions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you look at the performance metrics in Step 11, it is obvious that the credit
    risk predictions are not satisfactory, especially in terms of accuracy, which
    is only 76.22%. That means that for the given test data, our model can predict
    if there is a credit risk with 76.22% precision. Since we need to be more careful
    about such sensitive financial sectors, therefore, more accuracy is desired no
    doubt.
  prefs: []
  type: TYPE_NORMAL
- en: Now, if you want to increase the prediction performance, you should try training
    your model using a model other than the Random-Forest-based classifier. For example,
    a Logistic Regression or NaÃ¯ve Baseyan-based classifier.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, you can use the SVM-based classifier or neural-network-based Multilayer
    Perceptron classifier. In [Chapter 7](part0059_split_000.html#1O8H62-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 7. Tuning Machine Learning Models"), *Tuning Machine Learning Models*,
    we will look at how to tune the hyper parameters in order to select the best model.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling the ML pipelines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data mining and machine learning algorithms impose outstanding challenges on
    parallel and distributed computing platforms. Furthermore, parallelizing the machine
    learning algorithms is highly task-specific and often depends on the preceding
    questions. In [Chapter 1](part0014_split_000.html#DB7S2-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 1. Introduction to Data Analytics with Spark"), *Introduction to Data
    Analytics with Spark*, we discussed and showed how to deploy the same machine
    learning application on top of a cluster or cloud computing infrastructure (that
    is, Amazon AWS/EC2).
  prefs: []
  type: TYPE_NORMAL
- en: Following that method, we can handle datasets with enormous batch sizes or in
    real time. In addition to this, scaling up the machine learning applications evolves
    another trade-off such as cost, complexity, run-time, and technical requirements.
    Furthermore, making task-appropriate algorithm and platform choices for large-scale
    machine learning requires an understanding of the benefits, trade-offs, and constraints
    of the available options.
  prefs: []
  type: TYPE_NORMAL
- en: 'To handle these issues, in this section, we will provide some theoretical aspects
    of handling big Datasets for deploying large-scale machine learning applications.
    However, before going any further, we need to know the answer to some questions.
    For example:'
  prefs: []
  type: TYPE_NORMAL
- en: How do we collect the big dataset to fulfil our needs?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How large are the big datasets and how do we handle them?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How much training data is enough to scale-up the ML application on a big dataset?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is an alternative approach if we don't have enough training data?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What sorts of machine learning algorithms should be used for fulfilling our
    needs?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What platform should be chosen for parallel learning?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here we discuss some important aspects of deploying and scaling up a machine
    learning application that handles the preceding big data challenges, including
    size, data skewness, cost, and infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: Size matters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Big data is data in volume, variety, veracity, velocity, and value that is too
    great to process by traditional in-memory computer systems. Scaling up machine
    learning applications by handling big data involves tasks such as classification,
    clustering, regression, feature selection, boosted decision trees, and SVMs. How
    do we handle 1 billion or 1 trillion data instances? Moreover, 5 billion cell
    phones, social networks such as Twitter produce big datasets in an unprecedented
    way. On the other hand, crowdsourcing is the reality, which is labeling of 100,000+
    data instances within a week.
  prefs: []
  type: TYPE_NORMAL
- en: In terms of sparsity, Big datasets cannot be too sparse but dense from a content
    perspective. From the machine learning perspective, to justify this claim, let's
    think of an example of data labeling. For instance, 1M data instances cannot belong
    to 1M classes, simply because it's not practical to have 1M classes but more than
    once data instances belong to a particular class. Therefore, based on the sparsity
    and size of such a large-scale dataset, making predictive analytics is another
    challenge, which needs to be considered and handled while scaling up.
  prefs: []
  type: TYPE_NORMAL
- en: Size versus skewness considerations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Machine learning also depends on the availability of labeled data, and its trustworthiness
    is based on the learning task, such as supervised, unsupervised, or semi-supervised.
    You might have a structured dataset, but with extreme skewness. More specifically,
    suppose you have 1K labeled and 1M unlabeled data points, so the labeled and unlabeled
    ratio is 0.1%.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, do you think that only the 1K label points are enough to train a
    supervised model? As another example, suppose, for instance, that you have 1M
    labeled and 1B unlabeled data points, where the labeled and unlabeled ratio is
    also 0.1%. Again, the same question arises, which is, is it enough to have only
    the 1M labels to train a supervised model?
  prefs: []
  type: TYPE_NORMAL
- en: Now the concern is what can be done or approach using existing labels as only
    guidance rather than a directive for the semi-supervised clustering, classification,
    or regression. Alternatively, label more data, either manually, or with a little
    help from the crowd. For example, suppose someone wants to cluster or classify
    analysis on a disease. More specifically, suppose we want to classify tweets,
    if particular tweets indicate an Ebola- or flu-related disease. In this case,
    we should use the semi-supervised approach for labeling the tweets.
  prefs: []
  type: TYPE_NORMAL
- en: However, in this case, a dataset might be very skewed, or labeling might be
    biased. Usually, the training data comes from different users, where an explicit
    user feedback might often be misleading.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, learning from the implicit feedback is a better idea; for example,
    collecting data by clicking on web search results. In these types of large-scale
    datasets, the skewness of the training data is hard to detect, as discussed in
    [Chapter 4](part0038_split_000.html#147LC2-5afe140a04e845e0842b44be7971e11a "Chapter 4. Extracting
    Knowledge through Feature Engineering"), *Extracting Knowledge through Feature
    Engineering*. Therefore, be wary of this skewness in big Datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Cost and infrastructure
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To scale-up your machine learning application, you will need better infrastructure
    and computing power to handle such big datasets. Initially, you might want to
    utilize a local cluster. However, sometimes, the cluster might not be enough to
    scale-up your ML application if the dataset increases exponentially.
  prefs: []
  type: TYPE_NORMAL
- en: As discussed in the chapter about deploying the ML pipeline on powerful infrastructure
    such as Amazon AWS cloud computing like EC2, you will have to go for pay-as-you-go
    to enjoy the cloud as Platform as a Service and Infrastructure as a Service, even
    though you use your own ML application as the Software as a Service.
  prefs: []
  type: TYPE_NORMAL
- en: Tips and performance considerations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spark also supports cross-validation for hyperparameter tuning that will be
    discussed broadly in the following chapter. Spark's view toward the cross-validation
    as a meta-algorithm, which fits the underlying estimator with user-specified combinations
    of parameters, cross-evaluates the fitted models and outputs the best one.
  prefs: []
  type: TYPE_NORMAL
- en: However, there is no specific requirement of the underlying estimator, which
    could be a pipeline, as long as it can be paired with an evaluator that outputs
    a scalar metric, such as precision and recall, from the predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s recall the OCR prediction, where we found that the precision is 75%,
    which is obviously not satisfactory, once again. To further investigate the reason
    now, let''s prints the confusion matrix for the label 8.0 or "I". If you look
    at the following matrix in *Figure 40*, you will find the number of correctly
    predicted instances is low:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Tips and performance considerations](img/00045.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 40: The confusion matrix for the label 8.0 or "I"'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s try to use the Random Forest model to do the prediction. But before
    going into the model training step, let''s do some initialization of the parameters
    that are needed for the Random Forest classifier, which also supports multiclass
    classification such as the logistic regression model with LBFGS:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: 'Now train the model by specifying the previous parameters, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s see how it performs. We''ll reuse the same code segments that we
    used in step 9\. Refer to the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Tips and performance considerations](img/00027.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 41: Performance metrics for the precision and recall'
  prefs: []
  type: TYPE_NORMAL
- en: '![Tips and performance considerations](img/00154.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 42: The improved confusion matrix for the label 8.0 or "I"'
  prefs: []
  type: TYPE_NORMAL
- en: If you look at *Figure 42*, we have a significant improvement in terms of all
    the parameters printed, and the precision has been increased from 75.30% to 89.20%.
    The reason behind this is the improved interpretation of the Random Forest model
    toward the global maxima calculation for prediction accuracy and the confusion
    matrix, as shown in *Figure 38*; you will find significant improvements in the
    number of predicted instances marked by a diagonal arrow.
  prefs: []
  type: TYPE_NORMAL
- en: Through a process of trial and error, you can settle on a short list of algorithms
    that show promise, but how do you know which is the best? Moreover, as previously
    mentioned, it is difficult to find a well-performing machine learning algorithm
    for your dataset. Therefore, if you are still unsatisfied with the accuracy of
    89.20%, I suggest you tune the parametric values and look at the precision and
    the recall.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have shown several machine learning applications and tried
    to differentiate between Spark MLlib and Spark ML. We also showed that it's really
    difficult to develop a complete machine learning application using only Spark
    ML or Spark MLlib.
  prefs: []
  type: TYPE_NORMAL
- en: However, we would like to argue that a combined approach, or interoperability,
    between these two APIs would be best for these purposes. In addition, we learned
    how to build an ML pipeline by using both Spark ML libraries and how to scale-up
    the basic model by considering some performance considerations.
  prefs: []
  type: TYPE_NORMAL
- en: Tuning an algorithm or machine learning application can simply be thought of
    as a process by which one goes through to optimize the parameters that impact
    the model in order to enable the algorithm to perform at its best (in terms of
    runtime and memory usage).
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 7](part0059_split_000.html#1O8H62-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 7. Tuning Machine Learning Models"), *Tuning Machine Learning Models*,
    we will discuss more about tuning machine learning models. We will try to reuse
    some of the applications from this chapter and [Chapter 5](part0043_split_000.html#190862-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 5.  Supervised and Unsupervised Learning by Examples"), *Supervised and
    Unsupervised Learning by Examples*, to tune up the performance by tuning several
    parameters.
  prefs: []
  type: TYPE_NORMAL
