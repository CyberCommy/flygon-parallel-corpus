- en: Deploying Your Application in Containers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the past few chapters, we focused on the actual development of our Go application.
    However, there is more to software engineering than just writing code. Usually,
    you will also need to concern yourself with the question of how you will deploy
    your application into its runtime environment. Especially in microservice architectures,
    where each Service may be built on a completely different technology stack, deployment
    can quickly become a challenge.
  prefs: []
  type: TYPE_NORMAL
- en: When you are deploying Services that use different technologies (for example,
    when you have Services written in Go, Node.js, and Java), you will need to provide
    an environment in which all these Services can actually be run. Using traditional
    virtual machines or bare-metal servers, this can become quite a hassle. Even though
    modern cloud providers make quickly spawning and disposing VMs easily, maintaining
    an infrastructure for all possible kinds of Services becomes an operational challenge.
  prefs: []
  type: TYPE_NORMAL
- en: This is where modern container technologies such asa60;**Docker** or **RKT**
    shine. Using containers, you can package an application with all its dependencies
    into a container image and then use that image to quickly spawn a container running
    your application on any server that can run these containers. The only software
    that needs to run on your servers themselves (be it virtualized or bare-metal)
    is the container runtime environment (typically, Docker or RKT).
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will show you how to package the MyEvents application, which
    we built over the last few chapters, in container images and how to deploy these
    images. Since we are thinking big, we will also take a look at cluster managers,
    such as **Kubernetes**, that allows you to deploy containers over many servers
    at once, allowing you to make your application deployment more resilient and scalable.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Using Docker to build and run container images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up complex multi-container applications with Docker Compose
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Container cloud infrastructures with Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are containers?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Container technologies such as Docker use isolation features offered by modern
    operating systems, such as **namespaces** and **control groups** (**cgroups**)
    in Linux. Using these features allows the operating system to isolate multiple
    running processes from each other to a very large extent. For example, a container
    runtime might provide two processes with two entirely separate filmount namespaces
    or two separate networking stacks using network namespaces. In addition to namespaces,
    cgroups can be used to ensure that each process does not use more than a previously
    allocated amount of resources (such as CPU time, memory or I/O, and network bandwidth).
  prefs: []
  type: TYPE_NORMAL
- en: In contrast to traditional virtual machines, a container runs completely within
    the operating system of the host environment; there is no virtualized hardware
    and OS running on that. Also, in many container runtimes, you do not even have
    all the typical processes that you will find in a regular operating system. For
    example, a Docker container will typically not have an init process like regular
    Linux systems have; instead, the root process (PID 1) in your container will be
    your application (also, as the container only exists as long as its **PID 1**
    process exists, it will cease to exist as soon as your application exists).
  prefs: []
  type: TYPE_NORMAL
- en: Of course, this does not apply to all container runtimes. LXC, for example,
    will give you a complete Linux system within your container (at least the user-space
    part of it), including an init process as PID 1.
  prefs: []
  type: TYPE_NORMAL
- en: Most container runtimes also come with a concept of **container images**. These
    contain prepackaged filesystems from which you can spawn new containers. Many
    container-based deployments actually use container images as deployment artifacts,
    in which the actual build artifact (for example, a compiled Go binary, Java application,
    or Node.js app) are packaged together with their runtime dependencies (which are
    not that many for compiled Go binaries; however, for other applications, a container
    image might contain a Java runtime, a Node.js installation, or anything else required
    for the application to work). Having a container image for your applications can
    also help make your application scalable and resilient since it is easy to spawn
    new containers from your application image.
  prefs: []
  type: TYPE_NORMAL
- en: Container runtimes such as Docker also tend to treat containers as **immutable**
    (meaning that containers are typically not changed in any way after having been
    started). When deploying your application in containers, the typical way to deploy
    a new version of your application would be to build a new container image (containing
    the updated version of your application), then create a new container from that
    new image and delete the container running the old version of your application.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to Docker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Currently, the de facto standard for application container runtimes is **Docker**,
    although there are other runtimes, for example, RKT (pronounced rocket). In this
    chapter, we will focus on Docker. However, many container runtimes are interoperable
    and built on common standards. For example, RKT containers can easily be spawned
    from Docker images. This means that even if you decide to deploy your application
    using Docker images, you are not running into a vendor lock-in.
  prefs: []
  type: TYPE_NORMAL
- en: Running simple containers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have worked with Docker before in [Chapter 4](0a806398-0654-46f9-8e6f-02af9334821b.xhtml),
    *Asynchronous Microservice Architectures Using Message Queues*, to quickly set
    up RabbitMQ and Kafka message brokers; however, we did not go into details on
    how Docker actually works. We will assume that you already have a working Docker
    installation on your local machine. If not, take a look at the official installation
    instructions to learn how you can install Docker on your operating system: [https://docs.docker.com/engine/installation/](https://docs.docker.com/engine/installation/).
  prefs: []
  type: TYPE_NORMAL
- en: 'To test whether your Docker installation is working correctly, try the following
    command on the command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The preceding command uses the new Docker command structure introduced in Docker
    1.13\. If you are running an older version of Docker, use `docker run` instead
    of `docker container run`. You can test your current Docker version using the
    `docker version` command. Also, note that Docker changed its versioning scheme
    after version 1.13, so the next version after 1.13 will be 17.03.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Docker run command follows the `docker container run [flags...] [image
    name] [arguments...]` pattern. In this case, `hello-world` is the name of the
    image to run, and the `--rm` flag indicates that the container should be removed
    immediately after it has finished running. When running the preceding command,
    you should receive an output similar to the one in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c105a19e-13ea-4866-b495-8bd31540f1b4.png)'
  prefs: []
  type: TYPE_IMG
- en: docker container run output
  prefs: []
  type: TYPE_NORMAL
- en: Actually, the `docker run` command did multiple things, here. First of all,
    it detected that the `hello-world` image was not present on the local machine
    and downloaded it from the official Docker image registry (if you run the same
    command again, you will note that the image will not be downloaded since it is
    already present on the local machine).
  prefs: []
  type: TYPE_NORMAL
- en: It then created a new container from the `hello-world` image that it just downloaded
    and started that container. The container image consists only of a small program
    that prints some text to the command line and then immediately exists.
  prefs: []
  type: TYPE_NORMAL
- en: Remember that a Docker container has no init system and typically has one process
    running in it. As soon as that process terminates, the container will stop running.
    Since we created the container with the `--rm` flag, the Docker engine will also
    delete the container automatically after it has stopped running.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let''s do something more complex. Execute the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This command will download the `nginx` image and spawn a new container from
    it. In contrast to the `hello-world` image, this image will run a web server that
    runs for an indefinite time. In order to not block your shell indefinitely, the
    `-d` flag (short for `--detach`) is used to start the new container in the background.
    The `--name` flag is responsible for giving the new container an actual name (if
    omitted, a random name will be generated for the container).
  prefs: []
  type: TYPE_NORMAL
- en: 'The **NGINX** web server running within the container by default listens on
    TCP port 80\. However, each Docker container has its own separate networking stack,
    so you cannot just access this port by navigating to `http://localhost`. The `-p
    80:80` flag tells the Docker Engine to forward the container''s TCP port 80 to
    localhost''s port 80\. To check whether the container is actually running now,
    run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding command lists all currently running containers, the images from
    which they were created, and their port mappings. You should receive an output
    similar to the one in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a55cd8f5-2137-449c-bc44-e91a42579439.png)'
  prefs: []
  type: TYPE_IMG
- en: docker container ls output
  prefs: []
  type: TYPE_NORMAL
- en: When the container is running, you can now access the web server you just started
    via `http://localhost`.
  prefs: []
  type: TYPE_NORMAL
- en: Building your own images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Up until now, you have worked with publicly available, premade images from the
    Docker Hub, such as the `nginx` image (or the RabbitMQ and Spotify/Kafka images
    in [Chapter 4](0a806398-0654-46f9-8e6f-02af9334821b.xhtml), *Asynchronous Microservice
    Architectures Using Message Queues*). However, with Docker, it is also easy to
    build your own images. Generally, Docker images are built from a **Dockerfile**.
    A Dockerfile is a kind of a construction manual for new Docker images and describes
    how a Docker image should be built, starting from a given base image. Since it
    rarely makes sense to start with a completely empty filesystem (empty as *in not
    even a shell or standard libraries*), images are often built on distribution images
    that contain the user-space tools of popular Linux distributions. Popular base
    images include U*buntu*, *Debian*, or C*entOS*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s build a short example `Dockerfile`. For demonstration purposes, we will
    build our own version of the `hello-world` image. For this, create a new empty
    directory and create a new file named `Dockerfile` with the following contents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The line starting with `FROM` denotes the base image on which you are building
    your custom image. It always needs to be the first line of a `Dockerfile`. The
    `MAINTAINER` statement contains only metadata.
  prefs: []
  type: TYPE_NORMAL
- en: The `RUN` statement is executed when the container image is being built (meaning
    that the final container image will have a `/hello.txt` file with the contents
    `Hello World` in its filesystem). A `Dockerfile` may contain many such `RUN` statements.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast to that, the `CMD` statement is executed when a container created
    from the image is being run. The command specified here will be the first and
    main process (PID 1) of a container created from the image.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can build the actual Docker image using the `docker image build` command
    (`docker build` in versions older than 1.13), as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/cb54a379-ad3a-435c-808a-f90e39cd9075.png)'
  prefs: []
  type: TYPE_IMG
- en: docker image build output
  prefs: []
  type: TYPE_NORMAL
- en: 'The `-t test-image` flag contains the name that your new image should get.
    After building the image, you can find it using the `docker image ls` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6834320f-4f42-402d-976f-cef8450e908f.png)'
  prefs: []
  type: TYPE_IMG
- en: docker image ls output
  prefs: []
  type: TYPE_NORMAL
- en: 'The name specified with `-t` allows you to create and run a new container from
    the preceding image using the already known `docker container run` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: As before, this command will create a new container (this time, from our freshly
    created image), start it (actually, start the command specified by the `CMD` statement
    in the `Dockerfile`), and then remove the container after the command has finished
    (thanks to the `--rm` flag).
  prefs: []
  type: TYPE_NORMAL
- en: Networking containers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Often, your application consists of multiple processes that communicate with
    each other (starting from relatively simple cases such as an application server
    talking to a database up to complex microservice architectures). When using containers
    to manage all these processes, you will typically have one container per process.
    In this section, we will take a look at how you can have multiple Docker containers
    communicate with each other over their network interfaces.
  prefs: []
  type: TYPE_NORMAL
- en: In order to enable container-to-container communication, Docker offers a network
    management feature. The command line allows you to create new virtual networks
    and then add containers to these virtual networks. Containers within one network
    can communicate with each other and resolve their internal IP addresses via Docker's
    built-in DNS server.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s test this by creating a new network with Docker using the `docker network
    create` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'After this, you will be able to see the new network then running `docker network
    ls`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/827535c6-bac6-4153-9588-2900e0d82ac0.png)'
  prefs: []
  type: TYPE_IMG
- en: docker network ls output
  prefs: []
  type: TYPE_NORMAL
- en: 'After having created a new network, you can attach containers to this network.
    For starters, begin by creating a new container from the `nginx` image and attaching
    it to the test network using the `--network` flag:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, create a new container in the same network. Since we have already started
    a web server, our new container will contain an HTTP client that we will use to
    connect to our new web server (note that we did not bind the container''s HTTP
    port to the localhost using the `-p` flag as we did before). For this, we will
    use the appropriate/curl image. This is an image that basically contains a containerized
    version of the cURL command-line utility. Since our web server container has the
    name web, we can now simply use that name for establishing a network connection:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'This command will simply print the web server''s index page to the command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4dd1c98a-6d10-4638-855e-f7855e635eba.png)'
  prefs: []
  type: TYPE_IMG
- en: docker container run output
  prefs: []
  type: TYPE_NORMAL
- en: This demonstrates that the cURL container created from the appropriate/curl
    image is able to reach the web container via HTTP. When establishing the connection,
    you can simply use the container's name (in this case, *web*). Docker will automatically
    resolve this name to the container's IP address.
  prefs: []
  type: TYPE_NORMAL
- en: Armed with the knowledge of Docker images and networking, you can now get to
    packaging the MyEvents application into container images and running them on Docker.
  prefs: []
  type: TYPE_NORMAL
- en: Working with volumes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An individual Docker container is often very short-lived. Deploying a new version
    of your application may result in a number of containers being deleted and new
    ones being spawned. If your application is running in a cloud environment (we
    will have a look at cloud-based container environments later in this chapter),
    your container may suffer from a node failure and will be re-scheduled on another
    cloud instance. This is completely tolerable for stateless applications (in our
    example, the event service and booking service).
  prefs: []
  type: TYPE_NORMAL
- en: However, this gets difficult for stateful containers (in our example, this would
    be both the message broker and database containers). After all, if you delete
    a MongoDB container and create a new one with a similar configuration, the actual
    data managed by the database will be gone. This is where **volumes** come into
    play.
  prefs: []
  type: TYPE_NORMAL
- en: Volumes are Docker's way to make data persist beyond the lifecycle of an individual
    container. They contain files and exist independently of individual containers.
    Each volume can be *mounted* into an arbitrary number of containers, allowing
    you to share files between containers.
  prefs: []
  type: TYPE_NORMAL
- en: 'To test this, create a new volume using the `docker volume create` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'This will create a new volume named *test*. You can find this volume again
    by using the `docker volume ls` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'After having created a volume, you can mount it into a container using the
    `-v` flag of the `docker container run` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: This command creates a new container that has the test volume mounted into the
    `/my-volume` directory. The container's command will be a bash shell that creates
    a `test.txt` file within this directory. After this, the container will terminate
    and be deleted.
  prefs: []
  type: TYPE_NORMAL
- en: 'To ensure that the files within the volume are still there, you can now mount
    this volume into a second container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: This container will print the `test.txt` file's contents to the command line.
    This demonstrates that the test volume still contains all its data, even though
    the container that initially populated it with data has already been deleted.
  prefs: []
  type: TYPE_NORMAL
- en: Building containers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will start with building container images for the components of the MyEvents
    application. As of now, our application consists of three components—two backend
    services (event and booking service) and the React frontend application. While
    the frontend application does not contain any kind of backend logic itself, we
    will at least need a web server to deliver this application to the user. This
    makes three container images in total that we need to build. Let's start with
    the backend components.
  prefs: []
  type: TYPE_NORMAL
- en: Building containers for the backend services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Both the event and booking service are Go applications that are compiled into
    single executable binaries. For this reason, it is not necessary to include any
    kind of source files or even the Go tool chain in the Docker image.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is important to note at this point that you will need compiled Linux binaries
    of your Go applications for the next steps. When on macOS or Windows, you will
    need to set the `GOOS` environment variable when invoking `go build`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'On macOS and Linux, you can check for the correct binary type using the `file`
    command. For a Linux `ELF` binary, the `file` command should print a output similar
    to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Start by compiling Linux binaries for both the event service and the booking
    service.
  prefs: []
  type: TYPE_NORMAL
- en: 'When you have compiled both services, continue by defining the Docker image
    build process for the event service. For this, create a new file named `Dockerfile`
    in the event service''s root directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: This Dockerfile contains some new statements that we did not cover before. The
    `COPY` statement copies a file from the host's local filesystem into the container
    image. This means that we are assuming that you have built your Go application
    using `go build` before starting the Docker build. The `USER` command causes all
    subsequent `RUN` statements and the `CMD` statement to run as that user (and not
    root). The `ENV` command sets an environment variable that will be available to
    the application. Finally, the `EXPOSE` statement declares that containers created
    from this image will need the TCP port `8181`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Continue by building the container image using the `docker image build` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, add a similar Docker file to the `bookingservice`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Again, build the image using `docker image build`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: To test our new images, we can now spawn the respective containers. However,
    before starting the actual application containers, we will need to create a virtual
    network for these container and the required persistence services. Both event
    and booking service each require a MongoDB instance and a shared AMQP (or Kafka)
    message broker.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start by creating the container network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, add a RabbitMQ container to your network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Continue by adding two MongoDB containers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, you can start the actual application containers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Note the port mappings. Currently, both services have their REST API listen
    on TCP port `8181`. As long as these two APIs run in different containers, it
    is completely valid. However, when mapping these ports to host ports (for example,
    for testing purposes), we would have a port conflict that we resolve here by mapping
    the booking service's port `8181` to `8282`.
  prefs: []
  type: TYPE_NORMAL
- en: Also, note how the `-e` flags are used to pass environment variables into the
    running containers. For example, using the `MONGO_URL` environment variable, it
    becomes easy to connect the two application containers to different databases.
  prefs: []
  type: TYPE_NORMAL
- en: 'After having started all these containers, you will be able to reach the event
    service via `http://localhost:8181` and the booking service via `http://localhost:8282`
    from your local machine. The following `docker container ls` command should now
    show you five running containers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ecbca662-2530-4267-a40b-8d8974f8304c.png)'
  prefs: []
  type: TYPE_IMG
- en: docker container ls output
  prefs: []
  type: TYPE_NORMAL
- en: Using static compilation for smaller images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Currently, we are building our application images on top of the `debian:jessie`
    image. This image contains the user-space tools and libraries of a typical Debian
    installation and takes about 123 MB in size (you can find this out using the `docker
    image ls` command). Add another 10 MB for the compiled Go application that you
    are adding to that base image, each of the resulting images will be about 133
    MB in size (which does not mean that our two images for the event service and
    booking service will together take up 266 MB of your disk space. They are both
    built on the same base image, and Docker is very efficient at optimizing disk
    space usage for container images).
  prefs: []
  type: TYPE_NORMAL
- en: However, our application does not use most of these tools and libraries, so
    our container images could be much smaller. By this, we can optimize the local
    disk space usage (although the Docker Engine is already quite efficient at this),
    optimize the transfer times when the image is downloaded from an image repository,
    and reduce the attack surface against malicious users.
  prefs: []
  type: TYPE_NORMAL
- en: 'Typically, compiled Go binaries have very few dependencies. You do not need
    any kind of runtime libraries or VMs, and all Go libraries that you use in your
    project are embedded directly into the resulting executable file. However, if
    you compile your application in Linux, the Go compiler will link the resulting
    binary against a few C standard libraries that are typically available on any
    Linux system. If you are on Linux, you can easily find out against which libraries
    your program was linked by invoking the `ldd` binary with one of your compiled
    Go binaries as argument. If your binary is linked against the C standard library,
    you will receive the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: This means that your Go application actually needs these Linux libraries to
    run and that you cannot just arbitrarily delete them from your image to make it
    smaller.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you cross-compiled your application on Windows or macOS using the `GOOS=linux`
    environment variable, you will probably not have this issue. As the compiler on
    those systems do not have access to the Linux standard C libraries, it will, by
    default, produce a statically linked binary that does not have any dependencies
    at all. When invoked with such a binary, `ldd` will render the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'On Linux, you can force the Go compiler to create statically linked binaries
    by setting the `CGO_ENABLED=0` environment variable for your Go build command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Having completely statically linked binary allows you to create a much smaller
    container image. Instead of building on `debian:jessie` as a base image, you can
    now use the *scratch* image. The `scratch` image is a special image. It is directly
    built into the Docker Engine, and you cannot download it from the Docker Hub.
    What is special about the `scratch` image is that it is completely empty, empty
    as in does not contain one single file—this means no standard libraries, no system
    utilities, and not even a shell. Although these properties typically make the
    scratch image cumbersome to use, it is perfectly suited for building minimal container
    images for statically linked applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'Change the event service''s `Dockerfile` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, change the booking service''s `Dockerfile` in a similar way. Build both
    container images again using the `docker image build` command from the preceding
    code. After that, verify your image size using the `docker image ls` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2e70b403-4ec6-4fd8-a171-df7d4bee38b8.png)'
  prefs: []
  type: TYPE_IMG
- en: docker image ls output
  prefs: []
  type: TYPE_NORMAL
- en: Building containers for the frontend
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have container images for our backend applications, we can direct
    our attention to the frontend application. Since this application runs in the
    user's browser, we do not really need a containerized runtime environment for
    it. What we do need though is a way to deliver the application to the user. Since
    the entire application consists of a bit of HTML and JavaScript files, we can
    build a container image that contains a simple NGINX web server that serves these
    files to the users.
  prefs: []
  type: TYPE_NORMAL
- en: For this, we will be building on the `nginx:1.11-alpine` image. This image contains
    a minimal version of the NGINX web server, built on Alpine Linux. Alpine is a
    Linux distribution optimized for small size. The entire `nginx:1.11-alpine` image
    is only 50 MB in size.
  prefs: []
  type: TYPE_NORMAL
- en: 'Add the following `Dockerfile` to your frontend application directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Obviously, our web server will need to service both the `index.html` and the
    compiled Webpack bundle at `dist/bundle.js` to the users, so these are copied
    into the container image with `COPY`. However, from all the dependencies installed
    into the `node_modules/` directory, our users will need only a very specific subset.
    For these reasons, we are copying these five files explicitly into the container
    image, instead of just using `COPY` for the entire `node_modules/` directory.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before actually building the container image, ensure that you have a recent
    Webpack build of your application and all dependencies installed. You can also
    use the `-p` flag to trigger Webpack to create a production build of your application
    that will be optimized for size:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'After this, build your container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'You can now start this container using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Note that we are not passing the `--network=myevents` flag in this case. This
    is because the frontend container does not actually need to communicate with the
    backend services directly. All communication is initiated from the user's browser,
    not from within the actual frontend container.
  prefs: []
  type: TYPE_NORMAL
- en: The `-p 80:80` flag binds the container's TCP port 80 to your local TCP port
    80\. This allows you to now open `http://localhost` in your browser and view the
    MyEvents frontend application. If you still have the backend containers from the
    previous sections running, the application should work out of the box.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying your application with Docker Compose
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Up until now, actually deploying the MyEvents application from existing container
    images involved a number of `docker container run` commands. Although this works
    reasonably well for testing, it becomes tedious once your application runs in
    production, especially when you want to deploy updates or scale the application.
  prefs: []
  type: TYPE_NORMAL
- en: One possible solution for this is **Docker Compose**. Compose is a tool that
    allows you to describe applications composed of multiple containers in a declarative
    way (in this case, a YAML file that describes which components have built your
    application).
  prefs: []
  type: TYPE_NORMAL
- en: 'Docker Compose is part of the regular Docker installation packages, so if you
    have Docker installed in your local machine, you should also have Docker Compose
    available. You can easily test this by invoking the following command on your
    command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: If Compose is not available on your local machine, consult the installation
    manual at [https://docs.docker.com/compose/install](https://docs.docker.com/compose/install)
    for a detailed description on how to set up Compose.
  prefs: []
  type: TYPE_NORMAL
- en: Every Compose project is described by a `docker-compose.yml` file. The Compose
    file will later contain a description of all containers, networks, and volumes
    that you need for your application. Compose will then try to reconcile the desired
    state expressed in the Compose file with the actual state of the local Docker
    engine (for example, by creating, deleting, starting, or stopping containers).
  prefs: []
  type: TYPE_NORMAL
- en: 'Create such a file at the root of your project directory with the following
    contents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Note the `version: "3"` declaration in the Compose file. Compose supports multiple
    declaration formats, the most recent being version 3\. In some documentations,
    examples, or open source projects, you will most likely stumble across Compose
    files written for older versions. Compose files that do not declare a version
    at all are interpreted as version 1 files.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Right now, the preceding Compose file does nothing more than declare that your
    application requires a virtual network named `myevents`. Nevertheless, you can
    use Compose to reconcile the desired state (one network named `myevents` must
    exist) by running the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Right now, the preceding command will print a warning message since we are declaring
    a container network that is not used by any container.
  prefs: []
  type: TYPE_NORMAL
- en: 'Containers are declared in the Compose file under the `services`. Each container
    has a name (used as a key in the YAML structure) and can have various properties
    (such as the image to be used). Let''s continue by adding a new container to the
    Compose file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: This is the RabbitMQ container that you have created manually earlier using
    the `docker container run -d --network myevents -p 15672:15672 rabbitmq:3-management` command.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can now create this container by running the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: The `-d` flag has the same effect as with the docker container run command;
    it will cause the container(s) to be started in the background.
  prefs: []
  type: TYPE_NORMAL
- en: As soon as the RabbitMQ container starts running, you can actually invoke `docker-compose
    up` as many times as you like. Since the already running RabbitMQ container matches
    the specification from the Compose file, Compose will not take any further action.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s continue by adding the two MongoDB containers to the Compose file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Run `docker-compose up -d` another time. Compose will still not touch the RabbitMQ
    container, as it is still up to spec. However, it will create two new MongoDB
    containers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we can add the two application Services:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Note that we are not specifying an `image` attribute for these two containers,
    but a `build` attribute, instead. This will cause Compose to actually build the
    images for these containers on-demand from the Dockerfile found in the respective
    directories.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that the Docker build does not compile your Go binaries.
    Instead, it will rely on them being already there. In [Chapter 9](465ef76f-a2b2-42f7-b9c4-3c60ac552f77.xhtml),
    *Continuous Delivery*, you will learn how to use CI pipelines to automate these
    build steps.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also use the `docker-compose` command to trigger individual steps of
    this pipeline separately. For example, use `docker-compose pull` to download recent
    versions of all images used in the `Compose` file from the Docker Hub:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'For containers that do not use a predefined image, use `docker-compose build`
    to rebuild all images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Create the new containers with another `docker-compose up -d`.
  prefs: []
  type: TYPE_NORMAL
- en: Ensure that you have stopped any previously created containers that might be
    bound to the TCP ports 8181 or 8282\. Use the `docker container ls` and `docker
    container stop` commands to locate and stop these containers.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also use the `docker-compose ps` command to get an overview of the
    currently running containers associated with the current Compose project:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bbe15948-950f-4e04-948f-1eab0d05b679.png)'
  prefs: []
  type: TYPE_IMG
- en: docker-compose ps output
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, add the frontend application to the Compose file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: As you learned in this section, Docker Compose enables you to describe your
    application's architecture in a declarative way, allowing easy deployment and
    updates of your application on any server that supports a Docker instance.
  prefs: []
  type: TYPE_NORMAL
- en: Up until now, we have always worked on a single host (most probably, your local
    machine). This is good for development, but for a production setup, you will need
    to concern yourself with deploying your application to a remote server. Also,
    since cloud architectures are all about scale, over the next few sections, we
    will also take a look at how to manage containerized applications at scale.
  prefs: []
  type: TYPE_NORMAL
- en: Publishing your images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You now have the ability to build container images from your application components
    and to run containers from these images on your local machine. However, in a production
    context, the machine on which you have built a container image is rarely the machine
    that you will run it on. To actually be able to deploy your application to any
    cloud environment, you will need a way to distribute built container images to
    any number of hosts.
  prefs: []
  type: TYPE_NORMAL
- en: This is where container registries come into play. In fact, you have already
    worked with a container registry earlier in this chapter, that is, the Docker
    Hub. Whenever you use a Docker image that is not present on your local machine
    (let's say, for example, the `nginx` image), the Docker engine will pull this
    image from the Docker Hub onto your local machine. However, you can also use a
    container registry such as the Docker Hub to publish your own container images
    and then pull them from another instance.
  prefs: []
  type: TYPE_NORMAL
- en: At the Docker Hub (which you can access in your browser via [https://hub.docker.com](https://hub.docker.com)),
    you can register as a user and then upload your own images. For this, click on
    the Create Repository after logging in and choose a new name for your image.
  prefs: []
  type: TYPE_NORMAL
- en: 'To push a new image into your newly created repository, you will first need
    to log in with your Docker Hub account on your local machine. Use the following `docker
    login` command for this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, you will be able to push images into the new repository. The image names
    will need to start with your Docker Hub username, followed by a slash:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: By default, images pushed to the Docker Hub will be publicly visible. The Docker
    Hub also offers the possibility to push private images as a paid feature. Private
    images can only be pulled after you have successfully authenticated using the
    `docker login` command.
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, you do not have to use the Docker Hub to distribute your own images.
    There are alternative providers, such as Quay ([https://quay.io](https://quay.io/)),
    and all major cloud providers also offer the possibility to host managed container
    registries for you. However, when using a registry other than the Docker Hub,
    some of the preceding commands will change slightly. For starters, you will have
    to tell the `docker login` command the registry that you will be signing in:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Also, container images that you want to push will not only need to start with
    your Docker Hub username, but with the entire registry hostname:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'If you do not want to entrust your container images to a third-party provider,
    you can also roll out your own container registry. Fittingly, there is a Docker
    image that you can use to quickly set up your own registry:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'This will set up a container registry that is reachable at: `http://localhost:5000`.
    You can treat it like any other third-party registry:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Having a private container registry listening on `localhost:5000` is fine for
    development, but for a production setup, you will need additional configuration
    options. For example, you will need to configure TLS transfer encryption for your
    registry (by default, the Docker engine will refuse to any non-encrypted Docker
    registry other than localhost), and you will also need to set up authentication
    (unless you explicitly intend to run a publicly accessible container registry).
    Take a look a the registry's official deployment guide to learn how to set up
    encryption and authentication: [https://docs.docker.com/registry/deploying/](https://docs.docker.com/registry/deploying/).
  prefs: []
  type: TYPE_NORMAL
- en: Deploying your application to the cloud
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To conclude this chapter, we will have a look at how you can deploy your containerized
    application to a cloud environment.
  prefs: []
  type: TYPE_NORMAL
- en: Container engines, such as, Docker allow you to provision multiple Services
    in isolated environments, without having to provision separate virtual machines
    for individual Services. However, as typical for Cloud applications, our container
    architecture needs to be easily scalable and also resilient to failure.
  prefs: []
  type: TYPE_NORMAL
- en: This is where container orchestration system such as Kubernetes comes into play.
    These are systems that allow you to deploy containerized applications over entire
    clusters of hosts. They allow for easy scaling since you can easily add new hosts
    to an existing cluster (after which new container workloads may automatically
    be scheduled on them) and also make your system resilient; node failures can be
    quickly detected, which allows containers on those nodes to be started somewhere
    else to ensure their availability.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the most prominent container orchestrators is Kubernetes (which is Greek
    for *helmsman*). Kubernetes is an open source product originally developed by
    Google and now owned by the Cloud Native Computing Foundation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows the basic architecture of a Kubernetes cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/392103eb-b9ad-4cdc-9ccc-5413b070389c.png)'
  prefs: []
  type: TYPE_IMG
- en: The central component of each Kubernetes cluster is the master server (which,
    of course, does not have to be an actual single server. In production setups,
    you will often have multiple master servers that are configured for high availability).
    The master server stores the entire cluster state in an end data store. The API
    Server is the component that offers a REST API that can be used by both internal
    components (such as the scheduler, controllers, or Kubelets) and external users
    (you!). The scheduler tracks available resources on the individual nodes (such
    as memory and CPU usage) and decides on which node in the cluster new containers
    should be scheduled. Controllers are components that manage high-level concepts
    such as replication controllers or autoscaling groups.
  prefs: []
  type: TYPE_NORMAL
- en: The Kubernetes nodes are the place where the actual application containers managed
    by the master server are started. Each node runs a Docker Engine and a **Kubelet**.
    The Kubelet is connected to the master server's REST API and is responsible for
    actually starting the containers that were scheduled for this node by the scheduler.
  prefs: []
  type: TYPE_NORMAL
- en: In Kubernetes, containers are organized in Pods. A Pod is Kubernetes' smallest
    possible scheduling unit and consists of one or more Docker containers. All containers
    in a Pod are guaranteed to be run on the same host. Each Pod will receive an IP
    address that is unique and routable within the whole cluster (meaning that Pods
    running on one host will be able to communicate with Pods running on other nodes
    via their IP addresses).
  prefs: []
  type: TYPE_NORMAL
- en: The Kube Proxy is the component that ensures that users can actually reach your
    applications. In Kubernetes, you can define Services that group multiple Pods.
    The Kube Proxy assigns a unique IP address to each Service, and forward network
    traffic to all Pods matched by a Service. This way, the Kube Proxy also implements
    a very simple but effective load balancing, when there are multiple instances
    of an application running in multiple Pods.
  prefs: []
  type: TYPE_NORMAL
- en: You may have noticed that Kubernetes' architecture is quite complex. Setting
    up a Kubernetes cluster is a challenging task, which we will not cover in detail
    in this book. For local development and testing, we will use the Minikube tool,
    which automatically creates a virtualized Kubernetes environment on your local
    machine. When you are running your application in a public cloud environment,
    you can also use tools that automatically set up a production-ready Kubernetes
    environment for you. Some cloud providers even provide managed Kubernetes clusters
    for you (for example, the **Google Container Engine** and the **Azure Container
    Service** are both built on Kubernetes).
  prefs: []
  type: TYPE_NORMAL
- en: Setting up a local Kubernetes with Minikube
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To get started with Minikube, you will need three tools on your local machine:
    Minikube itself (which will handle setting up the virtual Kubernetes environment
    on your machine), VirtualBox (which will be used as a virtualization environment),
    and kubectl (which is the command-line client for working with Kubernetes). Although
    we are using Minikube in this example, each and every kubectl command that we
    are showing in the following sections will work on nearly every Kubernetes cluster,
    regardless of how it was set up.'
  prefs: []
  type: TYPE_NORMAL
- en: Start by setting up VirtualBox. For this, download an installer from the official
    download page at [https://www.virtualbox.org/wiki/Downloads](https://www.virtualbox.org/wiki/Downloads)
    and follow the installation instructions for your operating system.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, download a recent build of Minikube. You can find all releases at: [https://github.com/kubernetes/minikube/releases](https://github.com/kubernetes/minikube/releases)
    (at the time of writing, the most recent release was 0.18.0). Again, follow the
    installation instructions for your operating system. Alternatively, use the following
    command to quickly download and set up Minikube (replace `linux` with `darwin`
    or `windows`, respectively):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Lastly, set up kubectl. You can find the installation instructions at: [https://kubernetes.io/docs/tasks/kubectl/install](https://kubernetes.io/docs/tasks/kubectl/install).
    Alternatively, use the following command (again, replace `linux` with `darwin`
    or `windows` as necessary):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'After having set up all the requirements, you can use the `minikube start`
    command to start your local Kubernetes environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'This command will download an ISO image, then start a new virtual machine from
    this image and install various Kubernetes components. Grab a coffee and do not
    be surprised if this takes a few minutes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e2c578b1-67b9-475e-8318-1b47a0864fd6.png)'
  prefs: []
  type: TYPE_IMG
- en: minikube start output
  prefs: []
  type: TYPE_NORMAL
- en: The `minikube start` command also creates a configuration file for kubectl that
    enables you to use kubectl with your minikube VM without any further configuration.
    You can find this file in your home directory at `~/.kube/config`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To test whether the entire setup works as expected, run the `kubectl get nodes`
    command. This command will print a list of all nodes that are part of the Kubernetes
    cluster. In a Minikube setup, you should see exactly one node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/2259d431-8edc-496e-8ab7-8b073fe91bf1.png)'
  prefs: []
  type: TYPE_IMG
- en: kubectl get nodes output
  prefs: []
  type: TYPE_NORMAL
- en: Core concepts of Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before diving back into MyEvents, let's take a more thorough look at some of
    Kubernetes' core concepts. We will start by creating a new Pod that contains a
    simple NGINX web server.
  prefs: []
  type: TYPE_NORMAL
- en: 'Kubernetes resources (such as Pods and Services) are usually defined in YAML
    files that declaratively describe the desired state of your cluster (similar to
    the Docker Compose configuration files that you have worked with before). For
    our new NGINX Pod, create a new file named `nginx-pod.yaml` anywhere in your local
    filesystem:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: This so-called manifest file describes what your new Pod should look like. In
    the `metadata` section, you can set basic metadata, such as the Pod's name or
    any additional labels (we will need those later). The `spec` section contains
    the actual specification of what the Pod should look like. As you can see, the
    `spec.containers` section is formatted as a list; in theory, you could add additional
    containers here that would then run within the same Pod.
  prefs: []
  type: TYPE_NORMAL
- en: 'After having created this file, use the `kubectl apply` command to create the
    Pod:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'After this, you can use the `kubectl get pods` command to verify that your
    Pod has successfully been created. Note that it may take a few seconds to minutes
    until the Pod changes its status from `ContainerCreating` to `Running`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/fcfee8cc-9752-48f7-b3ae-d564aec74df2.png)'
  prefs: []
  type: TYPE_IMG
- en: kubectl get pods output
  prefs: []
  type: TYPE_NORMAL
- en: Note that the `kubectl` command communicates directly with the Kubernetes API
    server (although when using Minikube, that is not a very big distinction, since
    all components are running on the same virtual machine anyway), not with the cluster
    nodes. In theory, your Kubernetes cluster could be made of many hosts, and the
    Kubernetes scheduler would automatically pick the best-suited host on which to
    run your new Pod on.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are more things that you can configure for a single Pod. For example,
    you might want to restrict your application''s memory or CPU usage. In this case,
    you could add the following settings to your newly created Pod manifest:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: The `resources.limits` section will instruct Kubernetes to create a container
    with a memory limit of 128 MB and a CPU limit of one half CPU core.
  prefs: []
  type: TYPE_NORMAL
- en: The important thing to note about Kubernetes Pods is that they are not meant
    to be persistent. Pods may be terminated at a moment's notice and may get lost
    whenever a node fails. For this reason, it is recommended to use a Kubernetes
    controller (such as the Deployment controller) to create Pods for you.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before continuing, delete your Pod using the `kubectl delete` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, create a new `nginx-deployment.yaml` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: This manifest will create a so-called Deployment controller for you. A Deployment
    controller will make sure that a given number of Pods of a given configuration
    is running at any time—in this case, two Pods (specified in the `spec.replicas`
    field) that are described by the `spec.template` field (note that the `spec.template`
    fields matches the Pod definition that we have already written before, minus the
    name).
  prefs: []
  type: TYPE_NORMAL
- en: 'As before, create the Deployment using the `kubectl apply` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Verify the success of your actions using the `kubectl get pods` command. You
    should note that two Pods will be scheduled (having names like `nginx-deployment-1397492275-qz8k5`):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/24c3dc96-8f3e-4085-9ad2-16fea98b64b0.png)'
  prefs: []
  type: TYPE_IMG
- en: kubectl get pods output
  prefs: []
  type: TYPE_NORMAL
- en: 'There''s more that you can do with Deployments. For starters, try deleting
    one of the automatically generated Pods using the `kubectl delete` command (keep
    in mind that on your machine, you will have a different Pod name):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: After deleting the Pod, call `kubectl get pods` again. You will note that the
    Deployment controller almost immediately created a new Pod.
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, you might decide that two instances of your application is not sufficient
    and that you want to scale your application further. For this, you can simply
    increase the Deployment controller''s `spec.scale` property. To increase (or decrease)
    the scale, you can either edit your existing YAML file and then call `kubectl
    apply` again. Alternatively, you can directly edit the resource using the `kubectl
    edit` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Especially for the `spec.scale` property, there is also a special `kubectl
    scale` command that you can use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/0f53bd75-2285-45b3-ab56-5e01be6f9f7f.png)'
  prefs: []
  type: TYPE_IMG
- en: kubectl get pods output
  prefs: []
  type: TYPE_NORMAL
- en: Services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Currently, we have four NGINX containers running, but no way to actually access
    them. This is where Services come into play. Create a new YAML file named `nginx-service.yaml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that the `spec.selector` property matches the `metadata.labels` property
    that you have specified in the Deployment manifest. All Pods created by a Deployment
    controller will have a given set of labels (which are really just arbitrary key/value
    mappings). The `spec.selector` property of a Service now specifies which labels
    a Pod should have to be recognized by this Service. Also, note the `type: NodePort`
    property, which is going to be important later.'
  prefs: []
  type: TYPE_NORMAL
- en: 'After creating the file, use `kubectl apply` as usual to create the service
    definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b7f872bb-8f02-4beb-87a1-89a89edd01c0.png)'
  prefs: []
  type: TYPE_IMG
- en: kubectl apply output
  prefs: []
  type: TYPE_NORMAL
- en: <pre>**$ kubectl apply -f nginx-service.yaml**
  prefs: []
  type: TYPE_NORMAL
- en: Next, call `kubectl get services` to inspect the newly created Service definition.
  prefs: []
  type: TYPE_NORMAL
- en: In the `kubectl get services` output, you will find your newly created `nginx`
    Service (along with the Kubernetes Service, which is always there).
  prefs: []
  type: TYPE_NORMAL
- en: 'Remember the `type: NodePort` property that you specified when creating the
    Service? The effect of this property is that the Kube proxy on each node now opened
    up a TCP port. The port number of this port is chosen randomly. In the preceding
    example, this is the TCP port 31455\. You can use this Port to connect to your
    Service from outside the Kubernetes cluster (for example, from your local machine).
    Any and all traffic received on this port is forwarded to one of the Pods matched
    by the `selector` specified in the Service''s specification.'
  prefs: []
  type: TYPE_NORMAL
- en: The special thing about services is that typically they will have a (much) longer
    lifespan than your average Pod. When new Pods are added (maybe because you have
    increased the replica count of the Deployment controller), these will automatically
    be added. Also, when Pods are removed (again, maybe because of a changed replica
    count, but also because of a node failure or just because a Pod was manually deleted),
    they will stop receiving traffic.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are using Minikube, you can now use the `minikube service` command to
    quickly find a node''s public IP address to open this service in your browser:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: In addition to the node port, also note the cluster IP property in the preceding
    output; this is an IP address that you can use within the cluster to reach any
    Pod matched by this Service. So, in this example, you could start a new Pod running
    your own application, and use the IP address `10.0.0.223` to access the `nginx`
    Service within this application. Also, since IP addresses are cumbersome to work
    with, you will also be able to use the Service name (`nginx`, in this case) as
    a DNS name.
  prefs: []
  type: TYPE_NORMAL
- en: Persistent volumes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Often, you will need a place to store files and data in a persistent way. Since
    individual Pods are fairly short-lived in a Kubernetes environment, it is usually
    not a good solution to store files directly in a container's filesystem. In Kubernetes,
    this issue is solved using persistent volumes, which are basically a more flexible
    abstraction of the Docker volumes that you have already worked with before.
  prefs: []
  type: TYPE_NORMAL
- en: 'To create a new persistent volume, create a new `example-volume.yaml` file with
    the following contents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: Create the volume using `kubectl apply -f example-volume.yaml`. After this,
    you can find it again by running `kubectl get pv`.
  prefs: []
  type: TYPE_NORMAL
- en: The preceding manifest file creates a new volume that stores its files in the
    `/data/volume01` directory on the host that the volume is used on.
  prefs: []
  type: TYPE_NORMAL
- en: Other than in a local development environment, using a hostPath volume for persistent
    data is a terrible idea. If a Pod using this Persistent Volume is rescheduled
    on another node, it does not have access to the same data that it had before.
    Kubernetes support a large variety of volume types that you can use to make volumes
    accessible across multiple hosts.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, in AWS, you could use the following volume definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: Before using a persistent volume in a Pod, you will need to claim it. Kubernetes
    makes an important distinction between creating persistent volumes and using them
    in containers. This is because the person creating a persistent volume and the
    one using (claiming) it are often different. Also, by decoupling the creation
    of volumes and their usage, Kubernetes also decouples the usage of volumes in
    Pods from the actual underlying storage technology.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/65c9aafa-74bd-4d3c-9404-2c32b6da6253.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, create a `PersistentVolumeClaim` by creating a `example-volume-claim.yaml`
    file and then calling `kubectl apply -f example-volume-claim.yaml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'When calling `kubectl get pv` again, you will find that the status field of
    the `volume01` volume has changed to `Bound`. You can now use the newly created
    persistent volume claim when creating a Pod or Deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: When you operate your Kubernetes cluster in a cloud environment, Kubernetes
    is also able to create new persistent volumes automatically by talking to the
    cloud provider's API, for example, to create new EBS devices.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying MyEvents to Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that you took your first steps with Kubernetes, we can work on deploying
    the MyEvents application into a Kubernetes cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the RabbitMQ broker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's start by creating the RabbitMQ broker. Since RabbitMQ is not a stateless
    component, we will use a special controller offered by Kubernetes—the `StatefulSet`
    Controller. This works similar to a Deployment controller, but will create Pods
    with a persistent identity.
  prefs: []
  type: TYPE_NORMAL
- en: 'To create a new `StatefulSet`, create a new file named `rabbitmq-statefulset.yaml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: This definition is missing one important thing, though, that is, persistence.
    Currently, if the RabbitMQ Pod should fail for any reason, a new one will be scheduled
    without any of the state (in this case, exchanges, queues, and not-yet-dispatched
    messages) that the broker previously had. For this reason, we should also declare
    a persistent volume that can be used by this `StatefulSet`. Instead of manually
    creating a new `PersistentVolume` and a new `PersistentVolumeClaim`, we can simply
    declare a `volumeClaimTemplate` for the `StatefulSet` and let Kubernetes provision
    the new volume automatically. In a Minikube environment this is possible, because
    Minikube ships with an automatic provisioner for such volumes. In cloud environments,
    you will find similar volume provisioners.
  prefs: []
  type: TYPE_NORMAL
- en: 'Add the following section to the `StatefulSet`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: The `volumeClaimTemplate` will instruct the `StatefulSet` controller to automatically
    provision a new `PersistentVolume` and a new `PersistentVolumeClaim` for each
    instance of the `StatefulSet`. If you increase the replica count, the controller
    will automatically create more volumes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The last thing left to do is actually use the volume claim within the `rabbitmq`
    container. For this, modify the container spec as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: Create the `StatefulSet` using `kubectl apply -f rabbitmq-statefulset.yaml`.
    After this, you should see a new Pod named `rmq-0` starting up when you run `kubectl
    get pods`. You should also see the automatically generated persistent volumes
    and the respective claims when running `kubectl get pv` and `kubectl get pvc`,
    respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, create a `Service` to allow other Pods to access your RabbitMQ broker:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: As usual, create the Service using `kubectl apply -f rabbitmq-service.yaml`.
    After creating the Service, you will be able to resolve it via DNS using the hostname
    `amqp-broker` (or in its long form, `amqp-broker.default.svc.cluster.local`).
  prefs: []
  type: TYPE_NORMAL
- en: Creating the MongoDB containers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Next, let''s create the MongoDB containers. Conceptually, they are not much
    different than the RabbitMQ container that you created in the preceding section.
    Just as before, we will use a `StatefulSet` with automatically provisioned volumes.
    Place the following contents in a new file called `events-db-statefulset.yaml`
    and then call `kubectl apply` on this file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, define a Service that matches this `StatefulSet` by creating a new file, `events-db-service.yaml`,
    and calling `kubectl apply`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: Now, we need to repeat this for the booking service's MongoDB containers. You
    can reuse almost the same definitions from above; simply replace `events` with
    `bookings` and create the `StatefulSet` and Service `bookings-db`.
  prefs: []
  type: TYPE_NORMAL
- en: Making images available to Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before you can now deploy your actual microservices, you need to make sure
    that the Kubernetes cluster has access to your images. Typically, you will need
    to have your self-built images available in a container registry for this. If
    you are using Minikube and want to save yourself the hassle of setting up your
    own image registry, you can do the following instead:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: The first command will instruct your local shell to connect not to your local
    Docker Engine, but the Docker Engine within the Minikube VM, instead. Then, using
    a regular `docker container build` command, you can build the container image
    you are going to use directly on the Minikube VM.
  prefs: []
  type: TYPE_NORMAL
- en: 'If your images are available in a private registry (like, for example, the
    Docker Hub, Quay.io or a self-hosted registry), you will need to configure your
    Kubernetes cluster so that it is authorized to actually access these images. For
    this, you will add your registry credentials as a `Secret` object. For this, use
    the `kubectl create secret` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: In the code example above, `my-private-registry` is an arbitrarily chosen name
    for your set of Docker credentials. The `--docker-server` flag `https://index.docker.io/v1/`
    specifies the URL of the official Docker Hub. If you are using a third-party registry,
    remember to change this value accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can now use this newly created `Secret` object when creating a new Pod,
    by adding an `imagePullSecrets` attribute to the Pod specification:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: Using the `imagePullSecrets` attribute also works when you are creating Pods
    using a `StatefulSet` or a Deploymet controller.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying the MyEvents components
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that you have your container images available on your Kubernetes cluster
    (either by building them locally on your Minikube VM or by pushing them to a registry
    and authorizing your cluster to access that registry), we can begin deploying
    the actual event service. Since the event service itself is stateless, we will
    deploy it using a regular Deployment object, not as `StatefulSet`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Continue by creating a new file—`events-deployment.yaml`—with the following
    contents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'Note the  `imagePullPolicy: Never` property. This is necessary if you have
    built the `myevents/eventservice` image directly on the Minikube VM. If you have
    an actual container registry available to which you can push your image, you should
    omit this property (and add a `imagePullSecrets` attribute, instead).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, create the respective Service by creating a new file, `events-service.yaml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: Create both Deployment and Service with the respective `kubectl apply` calls.
    Shortly thereafter, you should see the respective containers showing up in the
    `kubectl get pods` output.
  prefs: []
  type: TYPE_NORMAL
- en: Proceed similarly for the booking service. You can find the full manifest files
    for the booking service in the code examples of this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let''s deploy the frontend application. Create another Deployment
    with the following manifest:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the corresponding `Service` with the following manifest:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: Configuring HTTP Ingress
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At this point, you have all required services for the MyEvents application running
    in your Kubernetes cluster. However, there is no convenient way (yet) to access
    these services from outside the cluster. One possible solution to make them accessible
    would be to use **NodePort** services (which we have done before in one of the
    previous sections). However, this would result in your services being exposed
    at some randomly chosen high TCP ports, which is not desirable for a production
    setup (HTTP(S) services should be available at TCP ports `80` and `443`).
  prefs: []
  type: TYPE_NORMAL
- en: 'If your Kubernetes cluster is running in a public cloud environment (more precisely,
    AWS, GCE, or Azure), you can create a `LoadBalancer` `Service` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: This will provision the appropriate cloud provider resources (for example, an
    **Elastic Load Balancer** in AWS) to make your service publicly accessible at
    a standard port.
  prefs: []
  type: TYPE_NORMAL
- en: However, Kubernetes also offers another feature that allows you to handle incoming
    HTTP traffic called **Ingress**. Ingress resources offer you a more fine-grained
    control of how your HTTP services should be accessible from the outside world.
    For example, our application consists of two backend services and one frontend
    application, all three of which need to be publicly accessible via HTTP. While
    it is possible to create separate `LoadBalancer` services for each of these components,
    this would result in each of these three services receiving its own IP address
    and requiring its own hostname (for example, serving the frontend app on `https://myevents.example`,
    and the two backend services on `https://events.myevents.example` and `https://bookings.myevents.example`).
    This may get cumbersome to use, and in many microservice architecture, there is
    often a requirement to present a single entry point for external API access. Using
    Ingress, we can declare a path-to-service mapping that, for example, makes all
    backend services accessible at `https://api.myevents.example`.
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/kubernetes/ingress/blob/master/controllers/nginx/README.md](https://github.com/kubernetes/ingress/blob/master/controllers/nginx/README.md).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Before using Ingress resources, you will need to enable an Ingress controller
    for your Kubernetes clusters. This is highly specific to your individual environment;
    some cloud-providers offer special solutions for handling Kubernetes Ingress traffic,
    while in other environments, you will need to run your own. Using Minikube, however,
    enabling Ingress is a simple command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: If instead, you intend to run your own Ingress controller on Kubernetes, take
    a look at the official documentation of the NGINX Ingress controller. It may seem
    complicated at first, but just as many internal Kubernetes services, an Ingress
    controller also just consists of Deployment and Service recources.
  prefs: []
  type: TYPE_NORMAL
- en: After enabling the Ingress controller in Minikube, your Minikube VM will start
    responding to HTTP requests on ports `80` and `443`. To determine which IP address
    you need to connect to, run the `minikube ip` command.
  prefs: []
  type: TYPE_NORMAL
- en: 'To make our services accessible to the open world, create a new Kubernetes
    resource in a new file—`ingress.yaml`—with the following contents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the Ingress resource using `kubectl apply -f ingress.yaml`. Of course,
    the `myevents.example` domain will not be publicly accessible (that''s the entire
    point of the `.example` top-level domain); so, to actually test this setup, you
    can add a few entries to your host file (`/etc/hosts` on macOS and Linux; `C:\Windows\System32\drivers\etc\hosts`
    on Windows):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: Typically, `192.168.99.100` should be the (only locally routable) IP address
    of the Minikube VM. Cross-check with the output of the `minikube ip` command to
    be sure.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned how to use container technologies such as Docker
    to package your application, including all its dependencies into container images.
    You learned how to build container images from your application and deploy them
    in a production container environment built on Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: We will get back to building container images in Chapter 9, where you will learn
    how to further automate your container build tool chain, allowing you to completely
    automate your application deployment, starting with a git push command and ending
    with an updated container image running in your Kubernetes cloud.
  prefs: []
  type: TYPE_NORMAL
- en: Up until now, we have been fairly cloud-agnostic. Each and every example that
    we have looked at so far will work in any major public or private cloud, be it
    AWS, Azure, GCE, or OpenStack. If fact, container technologies are often considered
    an excellent way to abstract from the individual quirks of the cloud providers
    and avoid a (potentially costly) vendor lock-in.
  prefs: []
  type: TYPE_NORMAL
- en: All this will change in the next two chapters, where we will take a look at
    one of the major cloud providers—**Amazon Web Services** (**AWS**). You will learn
    about the intricacies of each of these providers, how to deploy the MyEvents application
    onto these platforms, and how to use the unique features offered by them.
  prefs: []
  type: TYPE_NORMAL
