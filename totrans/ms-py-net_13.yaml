- en: Test-Driven Development for Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The idea of **Test-Driven Development** (**TDD**) has been around for a while.
    American software engineer Kent Beck, among others, is typically credited with
    bringing and leading the TDD movement along with agile software development. Agile
    software development requires very short build-test-deploy development cycles;
    all of the software requirements are turned into test cases. These test cases
    are usually written before the code is written, and the software code is only
    accepted when the test passes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The same idea can be drawn in parallel with network engineering. When we face
    the challenge of designing a modern network, we can break the process down into
    the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: We start with the overall requirement for the new network. Why do we need to
    design a new or part of a new network? Maybe it is for new server hardware, a
    new storage network, or a new micro-service software architecture.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The new requirements are broken down into smaller, more specific requirements.
    This can be looking at a new switch platform, a more efficient routing protocol,
    or a new network topology (for example, fat-tree). Each of the smaller requirements
    can be broken down into the categories of must-have and optional.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We draw out the test plan and evaluate it against the potential candidates for
    solutions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The test plan will work in reverse order; we will start by testing the features,
    then integrate the new feature into a bigger topology. Finally, we will try to
    run our test as close to a production environment as possible.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The point is, even if we don't realize it, we might already be adopting a test-driven
    development methodology in network engineering. This was part of my revelation
    when I was studying the TDD mindset. We are already implicitly following this
    best practice without formalizing the method.
  prefs: []
  type: TYPE_NORMAL
- en: By gradually moving parts of the network as code, we can use TDD for the network
    even more. If our network topology is described in a hierarchical format in XML
    or JSON, each of the components can be correctly mapped and expressed in the desired
    state. This is the desired state that we can write test cases against. For example,
    if our desired state calls for a full mesh of switches, we can always write a
    test case to check against our production devices for the number of BGP neighbors
    it has.
  prefs: []
  type: TYPE_NORMAL
- en: Test-driven development overview
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The sequence of TDD is loosely based on the following six steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Write a test with the result in mind
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run all tests and see whether the new test fails
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write the code
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the test again
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Make necessary changes if the test fails
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: I just follow the guidelines loosely. The TDD process calls for writing the
    test cases before writing any code, or in our instance, before any components
    of the network are built. As a matter of personal preference, I always like to
    see a working version of the working network or code before writing test cases.
    It gives me a higher level of confidence. I also jump around the levels of testing;
    sometimes I test a small portion of the network; other times I conduct a system-level
    end-to-end test, such as a ping or traceroute test.
  prefs: []
  type: TYPE_NORMAL
- en: The point is, I do not believe there is a one-size-fits-all approach when it
    comes to testing. It depends on personal preference and the scope of the project.
    This is true for most of the engineers I have worked with. It is a good idea to
    keep the framework in mind, so we have a working blueprint to follow, but you
    are the best judge of your style of problem-solving.
  prefs: []
  type: TYPE_NORMAL
- en: Test definitions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s look at some of the terms commonly used in TDD:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Unit test**: Checks a small piece of code. This is a test that is run against
    a single function or class'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Integration test**: Checks multiple components of a code base; multiple units
    are combined and tested as a group. This can be a test that checks against a Python
    module or multiple modules'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**System test**: Checks from end to end. This is a test that runs as close
    to what an end user would see'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Functional test**: Checks against a single function'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Test coverage**: A term defined as the determination of whether our test
    cases cover the application code. This is typically done by examining how much
    code is exercised when we run the test cases'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Test fixtures**: A fixed state that forms a baseline for running our tests.
    The purpose of a test fixture is to ensure there is a well-known and fixed environment
    in which tests are run, so they are repeatable'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Setup and teardown**: All the prerequisite steps are added in the setup and
    cleaned up in the teardown'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The terms might seem very software-development-centric, and some might not be
    relevant to network engineering. Keep in mind that the terms are a way for us
    to communicate a concept or step we will be using these terms in the rest of this
    chapter. As we use the terms more in the network engineering context, they might
    become clearer. Let's dive into treating network topology as code.
  prefs: []
  type: TYPE_NORMAL
- en: Topology as code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we declare that the network is too complex, it is impossible to summarize
    it into code! Let's keep an open mind. Would it help if I tell you we have been
    using code to describe our topology in this book already?
  prefs: []
  type: TYPE_NORMAL
- en: If you take a look at any of the VIRL topology graphs that we have been using
    in this book, they are simply XML files that include a description of the relationship
    between nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will use the following topology for our lab:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/7a7b33f1-6e47-4a88-9d3e-11e5497c9237.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If we open up the topology file, `chapter13_topology.virl`, with a text editor,
    we will see that the file is an XML file describing the node and the relationship
    between the nodes. The top root level is the `<topology>` node with child nodes
    of `<node>`. Each of the child nodes consists of various extensions and entries.
    The device configurations are embedded in the file as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'By expressing the network as code, we can declare a source of truth for our
    network. We can write test code to compare the actual production value against
    this blueprint. We will use this topology file as the base, and compare the production
    network value against it. But first, we will need to grab the values we want from
    the XML file. In `chapter13_1_xml.py`, we will use `ElementTree` to parse the
    `virl` topology file and construct a dictionary consisting of the information
    of our devices:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is a Python dictionary that consists of the devices according to
    our topology file. We can also add customary items to the dictionary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We can use our example from [Chapter 3](d2c76e60-c005-4efc-85de-c7a3253e4b47.xhtml),
    *APIs and Intent-Driven Networking*, `cisco_nxapi_2.py`, to retrieve the NX-OSv
    version. When we combine the two files, we can compare the value we received from
    our topology file as well as the production device information. We can use Python's
    built-in `unittest` module to write test cases.
  prefs: []
  type: TYPE_NORMAL
- en: We will discuss the `unittest` module later. Feel free to skip ahead and come
    back to this example if you'd like.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the relevant `unittest` portion in `chapter13_2_validation.py`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'When we run the validation test, we can see that the test passes because the
    software version in production matches what we expected:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'If we manually change the expected NX-OSv version value to introduce a failure
    case, we will see the following failed output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We can see that the test case result was returned as failed; the reason for
    failure was the version mismatch between the two values.
  prefs: []
  type: TYPE_NORMAL
- en: Python's unittest module
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous example, we saw how we could use the `assertEqual()` method
    to compare the two values to return either `True` or `False`. Here is an example
    of the built-in `unittest` module to compare two values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the `python3` command-line interface, the `unittest` module can automatically
    discover the test cases in the script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Besides comparing two values, here are more examples of testing if the expected
    value is `True` or `False`. We can also generate custom failure messages when
    a failure occurs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We can use `-v` for the option to display a more detailed output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Starting from Python 3.3, the `unittest` module includes the `module` object
    library by default ([https://docs.python.org/3/library/unittest.mock.html](https://docs.python.org/3/library/unittest.mock.html)).
    This is a very useful module to make a fake HTTP API call to a remote resource
    without actually making the call. For example, we have seen the example of using
    NX-API to retrieve the NX-OS version number. What if we want to run our test,
    but we do not have an NX-OS device available? We can use the `unittest` mock object.
  prefs: []
  type: TYPE_NORMAL
- en: 'In `chapter13_5_more_unittest_mocks.py`, we created a simple class with a method
    to make HTTP API calls and expect a JSON response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We also created a function that mocks two URL calls:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we make the API call to the two URLs in our test case. However, we
    are using the `mock.patch` decorator to intercept the API calls:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'When we run the test, we will see that the test passes without needing to make
    an actual API call to the remote endpoint:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: For more information on the `unittest` module, Doug Hellmann's Python module
    of the week ([https://pymotw.com/3/unittest/index.html#module-unittest](https://pymotw.com/3/unittest/index.html#module-unittest))
    is an excellent source of short and precise examples on the `unittest` module.
    As always, the Python documentation is a good source of information as well: [https://docs.python.org/3/library/unittest.html](https://docs.python.org/3/library/unittest.html).
  prefs: []
  type: TYPE_NORMAL
- en: More on Python testing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In addition to the built-in library of `unittest`, there are lots of other Python
    testing frameworks in the community. Pytest is another robust Python testing framework
    that is worth a look. `pytest` can be used for all types and levels of software
    testing. It can be used by developers, QA engineers, individuals practicing Test-Driven
    Development, and open source projects. Many of the large-scale open source projects
    have switched from `unittest` or `nose` to `pytest`, including Mozilla and Dropbox.
    The main attractive features of `pytest` were a third-party plugin model, a simple
    fixture model, and assert rewriting.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to learn more about the `pytest` framework, I would highly recommend *Python
    Testing with PyTest* by Brian Okken (ISBN 978-1-68050-240-4). Another great source
    is the `pytest` documentation: [https://docs.pytest.org/en/latest/](https://docs.pytest.org/en/latest/).
  prefs: []
  type: TYPE_NORMAL
- en: '`pytest` is command-line-driven; it can find the tests we have written automatically
    and run them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Let's look at some examples using `pytest`.
  prefs: []
  type: TYPE_NORMAL
- en: pytest examples
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first `pytest` example will be a simple assert for two values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'When you run with the `-v` option, `pytest` will give us a pretty robust answer
    for the failure reason:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'In the second example, we will create a `router` object. The `router` object
    will be initiated with some values in `None` and some values with default values.
    We will use `pytest` to test one instance with the default and one instance without:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'When we run the test, we will see whether the instance was accurately applied
    with the default values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'If we were to replace the previous `unittest` example with `pytest`, in `chapter13_8_pytest_3.py` we
    will have a simple test case:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we run the test with the `pytest` command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: If we are writing tests for ourselves, we are free to choose any modules. Between
    `unittest` and `pytest`, I find `pytest` a more intuitive tool to use. However,
    since `unittest` is included in the standard library, many teams might have a
    preference for using the `unittest` module for their testing.
  prefs: []
  type: TYPE_NORMAL
- en: Writing tests for networking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have been mostly writing tests for our Python code. We have used
    both the `unittest` and `pytest` libraries to assert `True/False` and `equal/Non-equal`
    values. We were also able to write mocks to intercept our API calls when we do
    not have an actual API-capable device but still want to run our tests.
  prefs: []
  type: TYPE_NORMAL
- en: 'A few years ago, Matt Oswalt announced the **Testing On Demand: Distributed**
    (**ToDD**) validation tool for network changes. It is an open source framework
    aimed at testing network connectivity and distributed capacity. You can find more
    information about the project on its GitHub page: [https://github.com/toddproject/todd](https://github.com/toddproject/todd).
    Oswalt also talked about the project on this Packet Pushers Priority Queue 81,
    Network Testing with ToDD: [https://packetpushers.net/podcast/podcasts/pq-show-81-network-testing-todd/](https://packetpushers.net/podcast/podcasts/pq-show-81-network-testing-todd/).'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, let's look at how we can write tests that are relevant to the
    networking world. There is no shortage of commercial products when it comes to
    network monitoring and testing. Over the years, I have come across many of them.
    However, in this section, I prefer to use simple, open source tools for our tests.
  prefs: []
  type: TYPE_NORMAL
- en: Testing for reachability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Often, the first step of troubleshooting is to conduct a small reachability
    test. For network engineers, `ping` is our best friend when it comes to network
    reachability tests. It is a way to test the reachability of a host on an IP network
    by sending a small package across the network to the destination.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can automate the `ping` test via the `OS` module or the `subprocess` module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The `subprocess` module offers the additional benefit of catching the output
    back:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: These two modules prove to be very useful in many situations. Any command we
    can execute in the Linux and Unix environment can be executed via the `OS` or
    `subprocess` module.
  prefs: []
  type: TYPE_NORMAL
- en: Testing for network latency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The topic of network latency can sometimes be subjective. Working as a network
    engineer, we are often faced with the user saying that the network is slow. However,
    slow is a very subjective term. If we could construct tests that turn subjective
    terms into objective values, it would be very helpful. We should do this consistently
    so that we can compare the values over a time series of data.
  prefs: []
  type: TYPE_NORMAL
- en: 'This can sometimes be difficult to do since the network is stateless by design.
    Just because one packet is successful does not guarantee success for the next
    packet. The best approach I have seen over the years is just to use ping across
    many hosts frequently and log the data, conducting a ping-mesh graph. We can leverage
    the same tools we used in the previous example, catch the return-result time,
    and keep a record:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, the result is kept in a tuple and put into a list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: This is by no means perfect, and is merely a starting point for monitoring and
    troubleshooting. However, in the absence of other tools, this offers some baseline
    of objective values.
  prefs: []
  type: TYPE_NORMAL
- en: Testing for security
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We already saw the best tool for security testing in [Chapter 6](30262891-a82e-4bef-aae2-2e8fe530a16f.xhtml), *Network
    Security with Python*, with Scapy, in my opinion. There are lots of open source
    tools for security, but none offers the flexibility that comes with constructing
    our packets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another great tool for network security testing is `hping3` ([http://www.hping.org/](http://www.hping.org/)).
    It offers a simple way to generate a lot of packets at once. For example, you
    can use the following one-liner to generate a TCP Syn flood:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Again, since this is a command-line tool, we can use the `subprocess` module
    to automate any `hping3` test we want.
  prefs: []
  type: TYPE_NORMAL
- en: Testing for transactions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The network is a crucial part of the infrastructure, but it is only a part of
    it. What the users care about is often the service that runs on top of the network.
    If the user is trying to watch a YouTube video or listen to a podcast but cannot,
    in their opinion, the service is broken. We might know that it is not the network
    transport, but that doesn't comfort the user.
  prefs: []
  type: TYPE_NORMAL
- en: For this reason, we should implement tests that are as similar to the user's
    experience as possible. In the example of a YouTube video, we might not be able
    to duplicate the YouTube experience 100% (unless you are part of Google), but
    we can implement a layer-seven service as close to the network edge as possible.
    We can then simulate the transaction from a client at a regular interval as a
    transactional test.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Python `HTTP` standard library module is a module that I often use when
    I need to quickly test layer-seven reachability on a web service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: If we can simulate a full transaction for the expected service, that is even
    better. But the Python simple `HTTP` server module in the standard library is
    always a great one for running some ad hoc web service tests.
  prefs: []
  type: TYPE_NORMAL
- en: Testing for network configuration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In my opinion, the best test for network configuration is using standardized
    templates to generate the configuration and back up the production configuration
    often. We have seen how we can use the Jinja2 template to standardize our configuration
    per device type or role. This will eliminate many of the mistakes caused by human
    error, such as copy and paste.
  prefs: []
  type: TYPE_NORMAL
- en: Once the configuration is generated, we can write tests against the configuration
    for known characteristics that we would expect before we push the configuration
    to production devices. For example, there should be no overlap of IP address in
    all of the network when it comes to loopback IP, so we can write a test to see
    whether the new configuration contains a loopback IP that is unique across our
    devices.
  prefs: []
  type: TYPE_NORMAL
- en: Testing for Ansible
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For the time I have been using Ansible, I can not recall using a `unittest` like
    tool to test a Playbook. For the most part, the Playbooks are utilizing modules
    that were tested by the module developers.
  prefs: []
  type: TYPE_NORMAL
- en: Ansible provides unit tests for their library of modules. Unit tests in Ansible
    are currently the only way to drive tests from Python within Ansible's continuous-integration
    process. The unit tests that are run today can be found under `/test/units` ([https://github.com/ansible/ansible/tree/devel/test/units](https://github.com/ansible/ansible/tree/devel/test/units)).
  prefs: []
  type: TYPE_NORMAL
- en: 'The Ansible testing strategy can be found in the following documents:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Testing Ansible**: [https://docs.ansible.com/ansible/2.5/dev_guide/testing.html](https://docs.ansible.com/ansible/2.5/dev_guide/testing.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unit tests**: [https://docs.ansible.com/ansible/2.5/dev_guide/testing_units.html](https://docs.ansible.com/ansible/2.5/dev_guide/testing_units.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unit testing Ansible modules**: [https://docs.ansible.com/ansible/2.5/dev_guide/testing_units_modules.html](https://docs.ansible.com/ansible/2.5/dev_guide/testing_units_modules.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One of the interesting Ansible testing frameworks is **molecule** ([https://pypi.org/project/molecule/2.16.0/](https://pypi.org/project/molecule/2.16.0/)).
    It intends to aid in the development and testing of Ansible roles. Molecule provides
    support for testing with multiple instances, operating systems, and distributions.
    I have not used this tool, but it is where I would start if I wanted to perform
    more testing on my Ansible roles.
  prefs: []
  type: TYPE_NORMAL
- en: Pytest in Jenkins
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Continuous-integration** (**CI**) systems, such as Jenkins, are frequently
    used to launch tests after each of the code commits. This is one of the major
    benefits of using a CI system. Imagine that there is an invisible engineer who
    is always watching for any change in the network; upon detecting change, the engineer
    will faithfully test a bunch of functions to make sure that nothing breaks. Who
    wouldn''t want that?'
  prefs: []
  type: TYPE_NORMAL
- en: Let's look at an example of integrating `pytest` into the Jenkins tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Jenkins integration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we can insert the test cases into our continuous integration, let''s
    install some of the plugins that can help us visualize the operation. The two
    plugins we will install are build-name-setter and Test Result Analyzer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/cd23c244-3916-4292-bcf2-4a708cd95195.png)'
  prefs: []
  type: TYPE_IMG
- en: Jenkins plugin installation
  prefs: []
  type: TYPE_NORMAL
- en: 'The test we will run will reach out to the NXOS device and retrieve the operating
    system version number. This will ensure that we have API reachability to the Nexus
    device. The full script content can be read in `chapter13_9_pytest_4.py` the relevant
    `pytest` portion and result are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'We will use the `--junit-xml=results.xml` option to produce the file Jenkins
    needs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step would be to check this script into the GitHub repository. I prefer
    to put the test under its directory. Therefore, I created a `/test` directory
    and put the test file there:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/974185c8-eb51-4ed4-86d5-5470266069bd.png)'
  prefs: []
  type: TYPE_IMG
- en: Project repository
  prefs: []
  type: TYPE_NORMAL
- en: 'We will create a new project named `chapter13_example1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/5456017e-169a-4ecd-af1c-350097468fc4.png)'
  prefs: []
  type: TYPE_IMG
- en: Chapter 13 example 1
  prefs: []
  type: TYPE_NORMAL
- en: 'We can copy over the previous task, so we do not need to repeat all the steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/1705abee-1ac9-4b3c-b55b-4cffa1080c59.png)'
  prefs: []
  type: TYPE_IMG
- en: Copy task from chapter 12 example 2
  prefs: []
  type: TYPE_NORMAL
- en: 'In the execute shell section, we will add the `pytest` step:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/10883f4c-c1a0-4230-8588-97ed8a5f2b02.png)'
  prefs: []
  type: TYPE_IMG
- en: Project execute shell
  prefs: []
  type: TYPE_NORMAL
- en: 'We will add a post-build step of Publish JUnit test result report:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/628c4028-6460-47d6-9802-16b8133f00bb.png)'
  prefs: []
  type: TYPE_IMG
- en: Post-build step
  prefs: []
  type: TYPE_NORMAL
- en: 'We will specify the `results.xml` file as the JUnit result file:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/5fc9e8fd-82ee-4259-9692-6b27b45c5cd8.png)'
  prefs: []
  type: TYPE_IMG
- en: Test report XML location
  prefs: []
  type: TYPE_NORMAL
- en: 'After we run the build a few times, we will be able to see the Test Result
    Analyzer graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/91ab28c8-3cf3-475c-a267-5ebf0147fef7.png)'
  prefs: []
  type: TYPE_IMG
- en: Test result analyzer
  prefs: []
  type: TYPE_NORMAL
- en: 'The test result can also be seen on the project homepage. Let''s introduce
    a test failure by shutting down the management interface of the Nexus device.
    If there is a test failure, we will be able to see it right away on the Test Result
    Trend graph on the project dashboard:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/b17f8844-1578-47c8-b6ee-32dce3b3d765.png)'
  prefs: []
  type: TYPE_IMG
- en: Test result trend
  prefs: []
  type: TYPE_NORMAL
- en: This is a simple but complete example. There are many ways we can integrate
    testing into Jenkins.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we looked at test-driven development and how it can be applied
    to network engineering. We started with an overview of TDD; then we looked at
    examples of using the `unittest` and `pytest` Python modules. Python and simple
    Linux command-line tools can be used to construct various tests for network reachability,
    configuration, and security.
  prefs: []
  type: TYPE_NORMAL
- en: We also looked at how we can utilize testing in Jenkins, a continuous-integration
    tool. By integrating tests into our CI tool, we can gain more confidence in the
    sanity of our change. At the very least, we hope to catch any errors before our
    users do.
  prefs: []
  type: TYPE_NORMAL
- en: Simply put, if it is not tested, it is not trusted. Everything in our network
    should be programmatically tested as much as possible. As with many software concepts,
    test-driven development is a never-ending service wheel. We strive to have as
    much test coverage as possible, but even at 100% test coverage, we can always
    find new ways and test cases to implement. This is especially true in networking,
    where the network is often the internet, and 100% test coverage of the internet
    is just not possible.
  prefs: []
  type: TYPE_NORMAL
