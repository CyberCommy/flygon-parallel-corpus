- en: Prefer local thread variables over static and shared when possible
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Thread local variables are a special kind of variable. Every task will have
    an independent value for this variable, so you don't need any synchronization
    mechanism to protect access to this variable.
  prefs: []
  type: TYPE_NORMAL
- en: This can sound a little strange. Every object has its own copy of the attributes
    of the class, so why do we need the thread local variables? Consider this situation.
    You create a `Runnable` task, and you want to execute multiple instances of that
    task. You can create a `Runnable` object for each thread you want to execute,
    but another option is to create a `Runnable` object and use that object to create
    all the threads. In the last case, all the threads will have access to the same
    copy of the attributes of the class except if you use the `ThreadLocal` class.
    The `ThreadLocal` class guarantees you that every thread will access its own instance
    of the variable without the use of a Lock, a semaphore, or a similar class.
  prefs: []
  type: TYPE_NORMAL
- en: Another situation when you can take advantage of Thread local variables is with
    static attributes. All instances of a class share the static attributes, but you
    declare them with the `ThreadLocal` class. In this case, every thread will have
    access to its own copy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another option you have is to use something like `ConcurrentHashMap<Thread,
    MyType>` and use it like `var.get(Thread.currentThread())` or `var.put(Thread.currentThread(),
    newValue)`. Usually, this approach is significantly slower than `ThreadLocal`
    because of possible contention (`ThreadLocal` has no contention at all). It has
    an advantage though: you can clear the map completely and the value will disappear
    for every thread; thus, sometimes it''s useful to use such an approach.'
  prefs: []
  type: TYPE_NORMAL
- en: Find the more easily parallelizable version of the algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can define an algorithm as a sequence of steps to solve a problem. There
    are different ways to solve the same problem. Some are faster, some use fewer
    resources, and others fit better with special characteristics of the input data.
    For example, if you want to order a set of numbers, you can use one of the multiple
    sorting algorithms that have been implemented.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a previous section of this chapter, we recommended you use a sequential
    algorithm as the starting point to implement a concurrent algorithm. There are
    two main advantages to this approach:'
  prefs: []
  type: TYPE_NORMAL
- en: You can easily test the correctness of the results of your parallel algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can measure the improvement in performance obtained with the use of concurrency
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: But not every algorithm can be parallelized, at least not so easily. You might
    think that the best starting point could be the sequential algorithm with the
    best performance solving the problem you want to parallelize, but this can be
    a wrong assumption. You should look for an algorithm than can be easily parallelized.
    Then, you can compare the concurrent algorithm with the sequential one with the
    best performance to see which offers the best throughput.
  prefs: []
  type: TYPE_NORMAL
- en: Using immutable objects when possible
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the main problems you can have in a concurrent application is a data
    race condition. As we explained before, this happens when two or more tasks modify
    the data stored in a shared variable and access to that variable is not implemented
    inside a critical section.
  prefs: []
  type: TYPE_NORMAL
- en: For example, when you work with an object-oriented language such as Java, you
    implement your application as a collection of objects. Each object has a number
    of attributes and some methods to read and change the values of the attributes.
    If some tasks share an object and call to a method to change a value of an attribute
    of that object and that method is not protected by a synchronization mechanism,
    you probably will have data inconsistency problems.
  prefs: []
  type: TYPE_NORMAL
- en: There are special kinds of object named immutable objects. Their main characteristic
    is that you can't modify any attributes after initialization. If you want to modify
    the value of an attribute, you must create another object. The `String` class
    in Java is the best example of immutable objects. When you use an operator (for
    example, `=` or +`=`) that we might think changes the value of a String, you are
    really creating a new object.
  prefs: []
  type: TYPE_NORMAL
- en: 'The use of immutable objects in a concurrent application has two very important
    advantages:'
  prefs: []
  type: TYPE_NORMAL
- en: You don't need any synchronization mechanism to protect the methods of these
    classes. If two tasks want to modify the same object, they will create new objects,
    so it will never occur that two tasks modify the same object at a time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You won't have any data inconsistency problems, as a conclusion of the first
    point.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is a drawback with immutable objects. If you create too many objects,
    this may affect the throughput and memory use of the application. If you have
    a simple object without internal data structures, it's usually not a problem to
    make it immutable. However, making immutable complex objects that incorporate
    collections of other objects usually leads to serious performance problems.
  prefs: []
  type: TYPE_NORMAL
- en: Avoiding deadlocks by ordering the locks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the best mechanisms to avoid a deadlock situation in a concurrent application
    is to force tasks to get shared resources always in the same order. An easy way
    to do this is to assign a number to every resource. When a task needs more than
    one resource, it has to request them in order.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if you have two tasks, T1 and T2, and both need two resources,
    R1 and R2, you can force both to request first the R1 resource and then the R2
    resource. You will never have a deadlock.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, if T1 first requests R1 and then R2 and T2 first requests
    R2 and then R1, you can have a deadlock.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, a bad use of this tip is as follows. You have two tasks that need
    to get two `Lock` objects. They try to get the locks in different order:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: It's possible that `operation1()` executes its first sentence and `operation2()`
    its first sentence too, so they will be waiting for the other `Lock` and you will
    have a deadlock.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can avoid this simply by getting the locks in the same order. If you change
    `operation2()`, you will never have a deadlock as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Using atomic variables instead of synchronization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When you have to share data between two or more tasks, you have to use a synchronization
    mechanism to protect the access to that data and avoid any data inconsistency
    problems.
  prefs: []
  type: TYPE_NORMAL
- en: Under some circumstances, you can use the `volatile` keyword and not use a synchronization
    mechanism. If only one of the tasks modifies the data and the rest of the tasks
    read it, you can use the volatile keyword without any synchronization or data
    inconsistency problem. In other scenarios, you need to use a lock, the synchronized
    keyword, or any other synchronization method.
  prefs: []
  type: TYPE_NORMAL
- en: In Java 5, the concurrency API included a new kind of variable called atomic
    variables. These variables are classes that support atomic operations on single
    variables. They include a method, denominated by `compareAndSet(oldValue, newValue)`,
    that includes a mechanism to detect if assigning to the new value to the variable
    is done in one step. If the value of the variable is equal to `oldValue`, it changes
    it to `newValue` and returns true. Otherwise, it returns `false`. There are more
    methods that work in a similar way, such as `getAndIncrement()` or `getAndDecrement()`.
    These methods are also atomic.
  prefs: []
  type: TYPE_NORMAL
- en: This solution is lock-free; that is to say, it doesn't use locks or any synchronization
    mechanism, so its performance is better than any synchronized solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'The most important atomic variables that you can use in Java are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`AtomicInteger`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`AtomicLong`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`AtomicReference`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`AtomicBoolean`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`LongAdder`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DoubleAdder`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Holding locks for as short a time as possible
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Locks, as with any other synchronization mechanism, allow you to define a critical
    section that only one task can execute at a time. While a task is executing the
    critical section, the other tasks that want to execute it are blocked and have
    to wait for the liberation of the critical section. The application is working
    in a sequential way.
  prefs: []
  type: TYPE_NORMAL
- en: You have to pay special attention to the instructions you include in your critical
    sections because you can degrade the performance of your application without realizing
    it. You must make your critical section as small as possible, and it must include
    only the instructions that work on shared data with other tasks, so the time that
    the application is executing in a sequential way will be minimal.
  prefs: []
  type: TYPE_NORMAL
- en: Avoid executing inside the critical section the code you don't control. For
    example, you are writing a library that accepts a user-defined `Callable`, which
    you need to launch sometimes. You don't know what exactly will be in that `Callable`.
    Maybe it blocks input/output, acquires some locks, calls other methods of your
    library, or just works for a very long time. Thus, whenever possible, try to execute
    it when your library does not hold any locks. If it's impossible for your algorithm,
    specify this behavior in your library documentation and possibly specify the limitations
    to the user-supplied code (for example, it should not take any locks). A good
    example of such documentation can be found in the `compute()` method of the `ConcurrentHashMap`
    class.
  prefs: []
  type: TYPE_NORMAL
- en: Taking precautions using lazy initialization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Lazy initialization is a mechanism that delays object creation until the object
    is used in the application for the first time. Its main advantage is it minimizes
    the use of memory because you only create the objects that are really needed,
    but it can be a problem in concurrent applications.
  prefs: []
  type: TYPE_NORMAL
- en: If you have a method that initializes an object and this method is called by
    two different tasks at once, you can initialize two different objects. This, for
    example, can be a problem with singleton classes because you only want to create
    one object of these classes.
  prefs: []
  type: TYPE_NORMAL
- en: A elegant solution to this problem has been implemented, as the Initialization-on-demand
    holder idiom ([https://en.wikipedia.org/wiki/Initialization-on-demand_holder_idiom](https://en.wikipedia.org/wiki/Initialization-on-demand_holder_idiom)).
  prefs: []
  type: TYPE_NORMAL
- en: Avoiding the use of blocking operations inside a critical section
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Blocking operations** are those operations that block the task that calls
    them until an event occurs. For example, when you read data from a file or write
    data to the console, the task that calls these operations must wait until they
    finish.'
  prefs: []
  type: TYPE_NORMAL
- en: If you include one of these operations into a critical section, you are degrading
    the performance of your application because none of the tasks that want to execute
    that critical section can execute it. The one that is inside the critical section
    is waiting for the finalization of an I/O operation, and the others are waiting
    for the critical section.
  prefs: []
  type: TYPE_NORMAL
- en: Unless it is imperative, don't include blocking operations inside a critical
    section.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Concurrent programming includes all the necessary tools and techniques to have
    multiple tasks or process running at the same time in a computer, communicating
    and synchronizing between them without data loss or inconsistency.
  prefs: []
  type: TYPE_NORMAL
- en: We started this chapter by introducing the basic concepts of concurrency. You
    must know and understand terms such as concurrency, parallelism, and synchronization
    to fully understand the examples of this book. However, concurrency can generate
    some problems, such as data race conditions, deadlocks, livelocks, and others.
    You must also know the potential problems of a concurrent application. It will
    help you identify and solve these problems.
  prefs: []
  type: TYPE_NORMAL
- en: We also explained a simple methodology of five steps introduced by Intel to
    convert a sequential algorithm into a concurrent one and showed you some concurrency
    design patterns implemented in the Java language and some tips to take into account
    when you implement a concurrent application.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we explained briefly the components of the Java concurrency API. It's
    a very rich API with low- and very high-level mechanisms that allow you to implement
    powerful concurrency applications easily. We also described the Java memory model,
    which determines how concurrent applications manage the memory and the execution
    order of the instructions internally.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will learn how to implement applications that use a
    lot of threads using the executor framework. This allows you to execute a big
    number of threads by controlling the resources you use and reducing the overhead
    introduced by thread creation (it reuses `Thread` objects to execute different
    tasks).
  prefs: []
  type: TYPE_NORMAL
