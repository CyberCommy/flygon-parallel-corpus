- en: Working with the Spark Key/Value API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we'll be working with the Spark key/value API. We will start
    by looking at the available transformations on key/value pairs. We will then learn
    how to use the `aggregateByKey` method instead of the `groupBy()` method. Later,
    we'll be looking at actions on key/value pairs and looking at the available partitioners
    on key/value data. At the end of this chapter, we'll be implementing an advanced
    partitioner that will be able to partition our data by range.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will be covering the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Available actions on key/value pairs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using aggregateByKey instead of groupBy()
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Actions on key/value pairs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Available partitioners on key/value data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing a custom partitioner
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Available actions on key/value pairs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will be covering the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Available transformations on key/value pairs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using `countByKey()`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the other methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, this is our well-known test in which we will be using transformations on
    key/value pairs.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will create an array of user transactions for users `A`, `B`, `A`,
    `B`, and `C` for some amount, as per the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We then need to key our data by a specific field, as per the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: We will key it by `userId`, by invoking the `keyBy` method with a `userId` parameter.
  prefs: []
  type: TYPE_NORMAL
- en: Now, our data is assigned to the `keyed` variable and its type is a tuple. The
    first element is a string, that is, `userId` and the second element is `UserTransaction`.
  prefs: []
  type: TYPE_NORMAL
- en: Let's look at the transformations that are available. First, we will look at
    `countByKey`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at its implementation, as shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This returns a `Map` of key `K`, and `Long` is a generic type because it can
    be any type of key. In this example, the key will be a string. Every operation
    that returns map is not entirely safe. If you see a signature of the method that
    is returning map, it is a sign that this data will be sent to the driver and it
    needs to fit in the memory. If there is too much data to fit into one driver's
    memory, then we will run out of memory. Hence, we need to be cautious when using
    this method.
  prefs: []
  type: TYPE_NORMAL
- en: 'We then perform an assert count that should contain the same elements as the
    map, as per the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '`B` is `2` because we have two values for it. Also, `A` is one similar to `C`
    as they have only one value. `CountByKey()` is not memory expensive because it
    only stores key and counter. However, if the key is a complex and a big object,
    for example, a transaction with multiple fields, which is more than two, then
    that map could be really big.'
  prefs: []
  type: TYPE_NORMAL
- en: 'But let''s start this test, as shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/18dbf0ad-9949-4e70-acfc-f826330185be.png)'
  prefs: []
  type: TYPE_IMG
- en: From the preceding screenshot, we can see that our test passed.
  prefs: []
  type: TYPE_NORMAL
- en: We also have a `combineByKey()` method, which combines the same elements for
    the same key, and shares the negative `aggregateByKey()` that is able to aggregate
    different types. We have `foldByKey`, which is taking the current state and value,
    but returns the same type as the value for the key.
  prefs: []
  type: TYPE_NORMAL
- en: We also have `groupByKey()`, which we learned about in the previous section.
    This groups everything by the specific key and returns the iterator of values
    for a key. This is a very memory expensive operation as well, so we need to be
    careful when we use it.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we'll be using `aggregateByKey` instead of `groupBy`. We
    will learn how `groupBy` works and fix its shortcomings.
  prefs: []
  type: TYPE_NORMAL
- en: Using aggregateByKey instead of groupBy()
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will explore the reason why we use `aggregateByKey` instead
    of `groupBy`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Why we should avoid the use of `groupByKey`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What `aggregateByKey` gives us
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing logic using `aggregateByKey`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'First, we will create our array of user transactions, as shown in the following
    example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We will then use `parallelize` to create an RDD, as we want our data to be
    key-wise. This is shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we invoked `keyBy` for `userId` to have the data of payers,
    key, and user transaction.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s consider that we want to aggregate, where we want to execute some specific
    logic for the same key, as shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The reasoning for this can be for choosing a maximum element, minimum element,
    or to calculate average. `aggregateByKey` needs to take three parameters, as shown
    in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The first parameter is an initial parameter of type T, and defining `amountForUser`
    is an initial parameter that has a type of `ArrayBuffer`. This is very important
    because the Scala compiler will infer that type, and argument numbers `1` and
    `2` need to have exactly the same type T in this example: `ArrayBuffer.empty[long]`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The next argument is a method that takes the current element that we are processing.
    In this example, `transaction: UserTransaction) =>` is a current transaction and
    also needs to take the state that we were initializing our function with, and,
    hence, it will be an array buffer here.'
  prefs: []
  type: TYPE_NORMAL
- en: 'It needs to be of the same type that''s as shown in the following code block,
    so this is our type T:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: At this point, we are able to take any transaction and add it to the specific
    state. This is done in a distributed way. For one key, execution is done on one
    executor and, for exactly the same key, on different executors. This happens in
    parallel, so multiple trades will be added for the same key.
  prefs: []
  type: TYPE_NORMAL
- en: Now, Spark knows that, for exactly the same key, it has multiple states of type
    T `ArrayBuffer` that it needs to merge. So, we need to `mergeAmounts` for our
    transactions for the same key.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `mergeArgument` is a method that takes two states, both of which are intermediate
    states of type T, as shown in the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we want to merge the release buffers into one array buffer.
    Therefore, we issue `p1 ++= p2`. This will merge two array buffers into one.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we have all arguments ready and we are able to execute `aggregateByKey`
    and see what the results look like. The result is an RDD of string and type T,
    the `ArrayBuffer[long]`, which is our state. We will not be keeping `UserTransaction`
    in our RDD anymore, which helps in reducing the amount of memory. `UserTransaction`
    is a heavy object because it can have multiple fields and, in this example, we
    are only interested in the amount field. So, this way, we can reduce the memory
    that is used.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following example shows what our result should look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: We should have a key, `A`, and an `ArrayBuffer` of `100` and `10001`, since
    it is our input data. `B` should be `4` and `10`, and lastly, `C` should be `10`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start the test to check if we have implemented `aggregateByKey` properly,
    as shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/26cd4ebd-1378-493b-a9d9-3e2b5f821a41.png)'
  prefs: []
  type: TYPE_IMG
- en: From the preceding output, we can see that it worked as expected.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we'll be looking at the actions that are available on key/value
    pairs.
  prefs: []
  type: TYPE_NORMAL
- en: Actions on key/value pairs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we'll be looking at the actions on key/value pairs.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Examining actions on key/value pairs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using `collect()`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Examining the output for the key/value RDD
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the first section of this chapter, we covered transformations that are available
    on key/value pairs. We saw that they are a bit different compared to RDDs. Also,
    for actions, it is slightly different in terms of result but not in the method
    name.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, we'll be using `collect()` and we'll be examining the output of our
    action on these key/value pairs.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will create our transactions array and RDD according to `userId`,
    as shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The first action that comes to our mind is to `collect()`. `collect()` takes
    every element and assigns it to the result, and thus our result is very different
    than the result of `keyBy`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our result is a pair of keys, `userId`, and a value, that is, `UserTransaction`.
    We can see, from the following example, that we can have a duplicated key:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: As we can see in the preceding code, we have multiple occurrences of the same
    order. For a simple key as a string, duplication is not very expensive. However,
    if we have a more complex key, it will be expensive.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, let''s start this test, as shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0bcc197e-ee8b-4de6-ae59-4cb32a6655ff.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see, from the preceding output, that our test has passed. To see the
    other actions, we will look at different methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'If a method is returning RDD, such as `collect[U] (f: PartialFunction[(String,
    UserTransaction), U])`, it means that this is not an action. If something returns
    RDD, it means that it is not an action. This is the case for key/value pairs.'
  prefs: []
  type: TYPE_NORMAL
- en: '`collect()` does not return an RDD but returns an array, thus it is an action. `count` returns
    `long`, so this is also an action. `countByKey` returns map. If we want to `reduce`
    our elements, then this is an action, but `reduceByKey` is not an action. This
    is the big difference between `reduce` and `reduceByKey`.'
  prefs: []
  type: TYPE_NORMAL
- en: We can see that everything is normal according to the RDD, so actions are the
    same and differences are only in transformation.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will be looking at the available partitioners on key/value
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Available partitioners on key/value data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We know that partitioning and partitioners are the key components of Apache
    Spark. They influence how our data is partitioned, which means they influence
    where the data actually resides on which executors. If we have a good partitioner,
    then we will have good data locality, which will reduce shuffle. We know that
    shuffle is not desirable for processing, so reducing shuffle is crucial, and,
    therefore, choosing a proper partitioner is also crucial for our systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Examining `HashPartitioner`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Examining `RangePartitioner`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will first examine our `HashPartitioner` and `RangePartitioner`. We will
    then compare them and test the code using both the partitioners.
  prefs: []
  type: TYPE_NORMAL
- en: 'First we will create a `UserTransaction` array, as per the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'We will then use `keyBy` (as shown in the following example) because the partitioner
    will automatically work on the key for our data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We will then take a `partitioner` of key data, as shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The code shows `partitioner.isEmpty`, because we have not defined any `partitioner`
    and thus it is empty at this point, as can be seen in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We can specify a `partitioner` by using the `partitionBy` method, as shown
    in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The method is expecting a `partitioner` abstract class implementation. We will
    have a couple of implementations, but first, let's focus on `HashPartitioner`.
  prefs: []
  type: TYPE_NORMAL
- en: '`HashPartitioner` takes a number of partitions and has a number of partitions.
    `numPartition` returns our argument, but `getPartition` gets a bit more involved,
    as shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: It first checks if our `key` is `null`. If it is `null`, it will land in partition
    number `0`. If we have data with `null` keys, they will all land in the same executors,
    and, as we know, this is not a good situation because the executors will have
    a lot of memory overhead and they can fail without memory exceptions.
  prefs: []
  type: TYPE_NORMAL
- en: If the `key` is not `null`, then it does a `nonNegativeMod` from `hashCode`
    and the number of partitions. It has to be the modulus of the number of partitions
    so that it can be assigned to the proper partition. Thus, the `hashCode` method
    is very important for our key.
  prefs: []
  type: TYPE_NORMAL
- en: If we are supplying a custom key and not a primitive type like an integer or
    string, which has a well-known `hashCode`, we need to supply and implement a proper
    `hashCode` as well. But the best practice is to use the `case` class from Scala
    as they have `hashCode` and equals implemented for you.
  prefs: []
  type: TYPE_NORMAL
- en: We have defined `partitioner` now, but `partitioner` is something that could
    be changed dynamically. We can change our `partitioner` to be `rangePartitioner`.
    `rangePartitioner` takes the partitions in an RDD.
  prefs: []
  type: TYPE_NORMAL
- en: '`rangePartitioner` is more complex as it tries to divide our data into ranges,
    which is not as simple as `HashPartitioner` is in getting partition. The method
    is really complex as it is trying to spread our data evenly and has complex logic
    for spreading that into ranges.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start our test to check if we were able to assign `partitioner` properly,
    as shown in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d0cd68c3-e380-4192-895c-307a2cd974ec.png)'
  prefs: []
  type: TYPE_IMG
- en: Our tests have passed. This means that, at the initial point, the `partitioner`
    was empty and then we had to shuffle RDD at `partitionBy`, and also a `branchPartitioner`.
    But it shows us only the number line where we created an instance of the `partitioner`
    interface.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we'll try to improve it or try to tweak and play with the
    partitioner by implementing a custom partitioner.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a custom partitioner
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we'll implement a custom partitioner and create a partitioner
    that takes a list of parses with ranges. If our key falls into a specific range,
    we will assign the partition number index of the list.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a custom partitioner
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing a range partitioner
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing our partitioner
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will implement the logic range partitioning based on our own range partitioning
    and then test our partitioner. Let's start with the black box test without looking
    at the implementation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first part of the code is similar to what we have used already, but this
    time we have `keyBy` amount of data, as shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'We are keying by the amount and we have the following keys: `100`, `4`, `100001`,
    `10`, and `10`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will then create a partitioner and call it `CustomRangePartitioner`, which
    will take a list of tuples, as shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The first element is from `0` to `100`, which means if the key is within the
    range of `0` to `100`, it should go to partition `0`. So, we have four keys that
    should fall into that partition. The next partition number has a range of `100`
    and `10000`, so every record within that range should fall into partition number
    `1`, inclusive of both ends. The last range is between `10000` and `1000000` elements,
    so, if the record is between that range, it should fall into that partition. If
    we have an element out of range, then the partitioner will fail with an illegal
    argument exception.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at the following example, which shows the implementation of our
    custom range partitioner:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'It takes ranges as an argument list of tuples, as shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Our `numPartitions` should be equal to `ranges.size`, so the number of partitions
    is equal to the number of ranges in size.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we have the `getPartition` method. First, our partitioner will work only
    for integers, as shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: We can see that this is an integer and cannot be used for other types. For the
    same reason, we first need to check whether our key is an instance of integer,
    and, if it is not, we get an `IllegalArgumentException` because that partitioner
    works only for the int type.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now test our `keyInt` by using `asInstanceOf`. Once this is done, we
    are able to iterate over ranges and take the last range when the index is between
    predicates. Our predicate is a tuple `v`, and should be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '`KeyInt` should be more than or equal to `v._1`, which is the first element
    of the tuple, but it should also be lower than the second element, `v._2`.'
  prefs: []
  type: TYPE_NORMAL
- en: The start of the range is `v._1` and the end of the range is `v._2`, so we can
    check that our element is within range.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the end, we will print the for key we found in the index for debugging purposes,
    and we will return the index, which will be our partition. This is shown in the
    following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s start the following test:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/485d4143-f478-4f9b-be3f-35c580667323.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see that for key `100001`, the code returned partition number `2`, which
    is as expected. For key `100` returns partition one and for `10`, `4`, `10` it
    returns partition zero, which means our code works as expected.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we first saw available the transformations on key/value pairs.
    We then learned how to use `aggregateByKey` instead of `groupBy`. We also covered
    actions on key/value pairs. Later, we looked at available partitioners like `rangePartitioner`
    and `HashPartition` on key/value data. By the end of this chapter, we had implemented
    our custom partitioner, which was able to assign partitions, based on the end
    and start of the range for learning purposes.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn how to test our Spark jobs and Apache Spark
    jobs.
  prefs: []
  type: TYPE_NORMAL
