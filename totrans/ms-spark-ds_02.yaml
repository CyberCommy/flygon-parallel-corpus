- en: Chapter 2. Data Acquisition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As a data scientist, one of the most important tasks is to load data into your
    data science platform. Rather than having uncontrolled, ad hoc processes, this
    chapter explains how a general data ingestion pipeline in Spark can be constructed
    that serves as a reusable component across many feeds of input data. We walk through
    a configuration and demonstrate how it delivers vital feed management information
    under a variety of running conditions.
  prefs: []
  type: TYPE_NORMAL
- en: Readers will learn how to construct a *content register* and use it to track
    all input loaded to the system and to deliver metrics on ingestion pipelines,
    so that these flows can be reliably run as an automated, lights-out process.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduce the **Global Database of Events, Language, and Tone** (**GDELT**)
    dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data pipelines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Universal ingestion framework
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Real-time monitoring for new data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Receiving streaming data via Kafka
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Registering new content and vaulting for tracking purposes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualization of content metrics in Kibana to monitor ingestion processes and
    data health
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data pipelines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Even with the most basic of analytics, we always require some data. In fact,
    finding the *right data* is probably among the hardest problems to solve in data
    science (but that''s a whole topic for another book!). We have already seen in
    the last chapter that the way in which we obtain our data can be as simple or
    complicated as is needed. In practice, we can break this decision down into two
    distinct areas: *ad hoc* and *scheduled*.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Ad hoc data acquisition**: is the most common method during prototyping and
    small scale analytics as it usually doesn''t require any additional software to
    implement. The user acquires some data and simply downloads it from source as
    and when required. This method is often a matter of clicking on a web link and
    storing the data somewhere convenient, although the data may still need to be
    versioned and secure.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scheduled data acquisition**: is used in more controlled environments for
    large scale and production analytics; there is also an excellent case for ingesting
    a dataset into a data lake for possible future use. With the **Internet of Things**
    (**IoT**) on the increase, huge volumes of data are being produced in many cases,
    if the data is not ingested immediately it is lost forever. Much of this data
    may not have an apparent use today, but could have in the future; so the mindset
    is to gather all of the data in case it is needed and delete it later when we
    are sure it is not.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It's clear we need a flexible approach to data acquisition that supports a variety
    of procurement options.
  prefs: []
  type: TYPE_NORMAL
- en: Universal ingestion framework
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are many ways to approach data acquisition, ranging from home-grown bash
    scripts through to high-end commercial tools. The aim of this section is to introduce
    a highly-flexible framework that we can use for small-scale data ingest, and then
    grow as our requirements change all the way through to a full, corporately-managed
    workflow if needed. That framework will be built using **Apache NiFi**. NiFi enables
    us to build large-scale, integrated data pipelines that move data around the planet.
    In addition, it's also incredibly flexible and easy to build simple pipelines
    usually quicker even than using bash or any other traditional scripting method.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If an ad hoc approach is taken to source the same dataset on a number of occasions,
    then some serious thought should be given as to whether it falls into the scheduled
    category, or at least whether a more robust storage and versioning setup should
    be introduced.
  prefs: []
  type: TYPE_NORMAL
- en: We have chosen to use Apache NiFi as it offers a solution that provides the
    ability to create many pipelines of varying complexity that can be scaled to truly
    big data and IoT levels, and it also provides a great drag and drop interface
    (using what's known as *flow-based programming*  [*https://en.wikipedia.org/wiki/Flow-based_programming*](https://en.wikipedia.org/wiki/Flow-based_programming)
    ). With patterns, templates, and modules for workflow production, it automatically
    takes care of many of the complex features that traditionally plague developers
    such as multithreading, connection management, and scalable processing. For our
    purposes, it will enable us to quickly build simple pipelines for prototyping,
    and scale these to full production where required.
  prefs: []
  type: TYPE_NORMAL
- en: 'It''s pretty well documented and easy to get running by following the information
    on [https://nifi.apache.org/download.html](https://nifi.apache.org/download.html).
    It runs in a browser and looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Universal ingestion framework](img/image_02_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We leave the installation of NiFi as an exercise for the reader, which we would
    encourage you to do as we will be using it in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing the GDELT news stream
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Hopefully, we have NiFi up and running now and can start to ingest some data.
    So, let''s start with some global news media data from GDELT. Here''s our brief,
    taken from the GDELT website, [http://blog.gdeltproject.org/gdelt-2-0-our-global-world-in-realtime/](http://blog.gdeltproject.org/gdelt-2-0-our-global-world-in-realtime/):'
  prefs: []
  type: TYPE_NORMAL
- en: '*"Within 15 minutes of GDELT monitoring a news report breaking anywhere the
    world, it has translated it, processed it to identify all events, counts, quotes,
    people, organizations, locations, themes, emotions, relevant imagery, video, and
    embedded social media posts, placed it into global context, and made all of this
    available via a live open metadata firehose enabling open research on the planet
    itself.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*[As] the single largest deployment in the world of sentiment analysis, we
    hope that by bringing together so many emotional and thematic dimensions crossing
    so many languages and disciplines, and applying all of it in realtime to breaking
    news from across the planet, that this will spur an entirely new era in how we
    think about emotion and the ways in which it can help us better understand how
    we contextualize, interpret, respond to, and understand global events."*'
  prefs: []
  type: TYPE_NORMAL
- en: Quite a challenging remit I think you'd agree! Therefore, rather than delay,
    pausing to specify the details here, let's get going straight away. We'll introduce
    the aspects of GDELT as we use them throughout the coming chapters.
  prefs: []
  type: TYPE_NORMAL
- en: In order to start consuming this open data, we'll need to hook into that metadata
    firehose and ingest the news streams onto our platform. How do we do this? Let's
    start by finding out what data is available.
  prefs: []
  type: TYPE_NORMAL
- en: Discovering GDELT in real-time
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: GDELT publishes a list of the latest files on their website. This list is updated
    every 15 minutes. In NiFi, we can set up a dataflow that will poll the GDELT website,
    source a file from this list, and save it to HDFS so we can use it later.
  prefs: []
  type: TYPE_NORMAL
- en: Inside the NiFi dataflow designer, create a HTTP connector by dragging a processor
    onto the canvas and selecting `GetHTTP` function.
  prefs: []
  type: TYPE_NORMAL
- en: '![Discovering GDELT in real-time](img/image_02_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'To configure this processor, you''ll need to enter the URL of the file list
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://data.gdeltproject.org/gdeltv2/lastupdate.txt](http://data.gdeltproject.org/gdeltv2/lastupdate.txt)'
  prefs: []
  type: TYPE_NORMAL
- en: Also, provide a temporary filename for the file list you will download. In the
    example below, we've used NiFi's expression language to generate a universally
    unique key so that files are not overwritten (`UUID()`).
  prefs: []
  type: TYPE_NORMAL
- en: '![Discovering GDELT in real-time](img/image_02_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: It's worth noting that with this type of processor (`GetHTTP` method), NiFi
    supports a number of scheduling and timing options for the polling and retrieval.
    For now, we're just going to use the default options and let NiFi manage the polling
    intervals for us.
  prefs: []
  type: TYPE_NORMAL
- en: 'An example of the latest file list from GDELT is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Discovering GDELT in real-time](img/ch-02-image.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Next, we will parse the URL of the GKG news stream so that we can fetch it in
    a moment. Create a regular expression parser by dragging a processor onto the
    canvas and selecting `ExtractText`. Now, position the new processor underneath
    the existing one and drag a line from the top processor to the bottom one. Finish
    by selecting the `success` relationship in the connection dialog that pops up.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Discovering GDELT in real-time](img/image_02_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, let''s configure the `ExtractText` processor to use a regular expression
    that matches only the relevant text of the file list, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: From this regular expression, NiFi will create a new property (in this case,
    called `url`) associated with the flow design, which will take on a new value
    as each particular instance goes through the flow. It can even be configured to
    support multiple threads.
  prefs: []
  type: TYPE_NORMAL
- en: 'Again, this is example is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Discovering GDELT in real-time](img/image_02_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: It's worth noting here, that while this is a fairly specific example, the technique
    is deliberately general purpose and can be used in many situations.
  prefs: []
  type: TYPE_NORMAL
- en: Our first GDELT feed
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that we have the URL of the GKG feed, we fetch it by configuring an `InvokeHTTP`
    processor to use the `url` property we previously created as it's remote endpoint,
    and dragging the line as before.
  prefs: []
  type: TYPE_NORMAL
- en: '![Our first GDELT feed](img/image_02_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'All that remains is to decompress the zipped content with an `UnpackContent`
    processor (using the basic `.zip` format) and save to HDFS using a `PutHDFS` processor,
    like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Our first GDELT feed](img/image_02_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Improving with publish and subscribe
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'So far, this flow looks very *point-to-point*, meaning that if we were to introduce
    a new consumer of data, for example, a Spark-streaming job, the flow must be changed.
    For example, the flow design might have to change to look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Improving with publish and subscribe](img/image_02_009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: If we add yet another, the flow must change again. In fact, each time we add
    a new consumer, the flow gets a little more complicated, particularly when all
    the error handling is added. This is clearly not always desirable, as introducing
    or removing consumers (or producers) of data, might be something we want to do
    often, even frequently. Plus, it's also a good idea to try to keep your flows
    as simple and reusable as possible.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, for a more flexible pattern, instead of writing directly to HDFS,
    we can publish to *Apache Kafka*. This gives us the ability to add and remove
    consumers at any time without changing the data ingestion pipeline. We can also
    still write to HDFS from Kafka if needed, possibly even by designing a separate
    NiFi flow, or connect directly to Kafka using the Spark-streaming.
  prefs: []
  type: TYPE_NORMAL
- en: To do this, we create a Kafka writer by dragging a processor onto the canvas
    and selecting `PutKafka`.
  prefs: []
  type: TYPE_NORMAL
- en: '![Improving with publish and subscribe](img/image_02_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We now have a simple flow that continuously polls for an available file list,
    routinely retrieving the latest copy of a new stream over the web as it becomes
    available, decompressing the content, and streaming it record-by-record into Kafka,
    a durable, fault-tolerant, distributed message queue, for processing by the Spark-streaming
    or storage in HDFS. And what's more, without writing a single line of bash!
  prefs: []
  type: TYPE_NORMAL
- en: Content registry
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have seen in this chapter that data ingestion is an area that is often overlooked,
    and that its importance cannot be underestimated. At this point, we have a pipeline
    that enables us to ingest data from a source, schedule that ingest, and direct
    the data to our repository of choice. But the story does not end there. Now we
    have the data, we need to fulfil our data management responsibilities. Enter the
    *content registry*.
  prefs: []
  type: TYPE_NORMAL
- en: We're going to build an index of metadata related to that data we have ingested.
    The data itself will still be directed to storage (HDFS, in our example) but,
    in addition, we will store metadata about the data, so that we can track what
    we've received and understand basic information about it, such as, when we received
    it, where it came from, how big it is, what type it is, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Choices and more choices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The choice of which technology we use to store this metadata is, as we have
    seen, one based upon knowledge and experience. For metadata indexing, we will
    require at least the following attributes:'
  prefs: []
  type: TYPE_NORMAL
- en: Easily searchable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scalable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parallel write ability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Redundancy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are many ways to meet these requirements, for example we could write the
    metadata to Parquet, store in HDFS, and search using Spark SQL. However, here
    we will use *Elasticsearch* as it meets the requirements a little better, most
    notably because it facilitates low latency queries of our metadata over a REST
    API, very useful for creating dashboards. In fact, Elasticsearch has the advantage
    of integrating directly with **Kibana**, meaning it can quickly produce rich visualizations
    of our content registry. For this reason, we will proceed with Elasticsearch in
    mind.
  prefs: []
  type: TYPE_NORMAL
- en: Going with the flow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Using our current NiFi pipeline flow, let''s fork the output from "Fetch GKG
    files from URL" to add an additional set of steps to allow us to capture and store
    this metadata in Elasticsearch. These are:'
  prefs: []
  type: TYPE_NORMAL
- en: Replace the flow content with our metadata model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Capture the metadata.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Store directly in Elasticsearch.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here''s what this looks like in NiFi:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Going with the flow](img/image_02_011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Metadata model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'So, the first step here is to define our metadata model. And there are many
    areas we could consider, but let''s select a set that helps tackle a few key points
    from earlier discussions. This will provide a good basis upon which further data
    can be added in the future, if required. So, let''s keep it simple and use the
    following three attributes:'
  prefs: []
  type: TYPE_NORMAL
- en: File size
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Date ingested
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: File name
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These will provide basic registration of received files.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, inside the NiFi flow, we''ll need to replace the actual data content
    with this new metadata model. An easy way to do this, is to create a JSON template
    file from our model. We''ll save it to local disk and use it inside a `FetchFile`
    processor to replace the flow''s content with this skeleton object. This template
    will look something like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Note the use of placeholder names (`SIZE, FILENAME, DATE`) in place of the attribute
    values. These will be substituted, one-by-one, by a sequence of `ReplaceText`
    processors, that swap the placeholder names for an appropriate flow attribute
    using regular expressions provided by the NiFi Expression Language, for example
    `DATE` becomes `${now()}`.
  prefs: []
  type: TYPE_NORMAL
- en: The last step is to output the new metadata payload to Elasticsearch. Once again,
    NiFi comes ready with a processor for this; the `PutElasticsearch` processor.
  prefs: []
  type: TYPE_NORMAL
- en: 'An example metadata entry in Elasticsearch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have added the ability to collect and interrogate metadata, we
    now have access to more statistics that can be used for analysis. This includes:'
  prefs: []
  type: TYPE_NORMAL
- en: Time-based analysis, for example, file sizes over time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loss of data, for example, are there data holes in the timeline?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If there is a particular analytic that is required, the NIFI metadata component
    can be adjusted to provide the relevant data points. Indeed, an analytic could
    be built to look at historical data and update the index accordingly if the metadata
    does not exist in current data.
  prefs: []
  type: TYPE_NORMAL
- en: Kibana dashboard
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have mentioned Kibana a number of times in this chapter. Now that we have
    an index of metadata in Elasticsearch, we can use the tool to visualize some analytics.
    The purpose of this brief section is to demonstrate that we can immediately start
    to model and visualize our data. To see Kibana used in a more complex scenario,
    have a look at [Chapter 9](ch09.xhtml "Chapter 9.  News Dictionary and Real-Time
    Tagging System") *, News Dictionary and Real-Time Tagging System*. In this simple
    example, we have completed the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Added the Elasticsearch index for our GDELT metadata to the **Settings** tab.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Selected file size under the **Discover** tab.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Selected **Visualize** for file size.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Changed the `Aggregation` field to `Range`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Entered values for the ranges.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The resulting graph displays the file size distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Kibana dashboard](img/image_02_012.jpg)'
  prefs: []
  type: TYPE_IMG
- en: From here, we are free to create new visualizations or even a fully-featured
    dashboard that can be used to monitor the status of our file ingest. By increasing
    the variety of metadata written to Elasticsearch from NiFi, we can make more fields
    available in Kibana and even start our data science journey right here with some
    ingest-based actionable insights.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a fully-functioning data pipeline delivering us real-time feeds
    of data, how do we ensure data quality of the payload we are receiving? Let's
    take a look at the options.
  prefs: []
  type: TYPE_NORMAL
- en: Quality assurance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With an initial data ingestion capability implemented, and data streaming onto
    your platform, you will need to decide how much quality assurance is required
    at the "front door". It's perfectly viable to start with no initial quality controls
    and build them up over time (retrospectively scanning historical data as time
    and resources allow). However, it may be prudent to install a basic level of verification
    to begin with. For example, basic checks such as file integrity, parity checking,
    completeness, checksums, type checking, field counting, overdue files, security
    field pre-population, denormalization, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: You should take care that your up-front checks do not take too long. Depending
    on the intensity of your examinations and the size of your data, it's not uncommon
    to encounter a situation where there is not enough time to perform all processing
    before the next dataset arrives. You will always need to monitor your cluster
    resources and calculate the most efficient use of time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some examples of the types of rough capacity planning calculations
    you can perform:'
  prefs: []
  type: TYPE_NORMAL
- en: Example 1 - Basic quality checking, no contending users
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Data is ingested every 15 minutes and takes 1 minute to pull from the source
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quality checking (integrity, field count, field pre-population) takes 4 minutes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are no other users on the compute cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*There are 10 minutes of resources available for other tasks.*'
  prefs: []
  type: TYPE_NORMAL
- en: As there are no other users on the cluster, this is satisfactory - no action
    needs to be taken.
  prefs: []
  type: TYPE_NORMAL
- en: Example 2 - Advanced quality checking, no contending users
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Data is ingested every 15 minutes and takes 1 minute to pull from the source
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quality checking (integrity, field count, field pre-population, denormalization,
    sub dataset building) takes 13 minutes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are no other users on the compute cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*There is only 1 minute of resource available* for *other tasks.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'You probably need to consider, either:'
  prefs: []
  type: TYPE_NORMAL
- en: Configuring a resource scheduling policy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reducing the amount of data ingested
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reducing the amount of processing we undertake
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding additional compute resources to the cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Example 3 - Basic quality checking, 50% utility due to contending users
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Data is ingested every 15 minutes and takes 1 minute to pull from the source
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quality checking (integrity, field count, field pre-population) takes 4 minutes
    (100% utility)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are other users on the compute cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*There are 6 minutes of resources available for other tasks (15 - 1 - (4 *
    (100 / 50))). Since there are other users, there is a danger that, at least some
    of the time, we will not be able to complete our processing and a backlog of jobs
    will occur.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'When you run into timing issues, you have a number of options available to
    you in order to circumvent any backlog:'
  prefs: []
  type: TYPE_NORMAL
- en: Negotiating sole use of the resources at certain times
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Configuring a resource scheduling policy, including:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'YARN fair scheduler: allows you to define queues with differing priorities
    and target your Spark jobs by setting the `spark.yarn.queue` property on start-up
    so your job always takes precedence'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dynamic resource allocation: allows concurrently running jobs to automatically
    scale to match their utilization'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Spark scheduler pool: allows you to define queues when sharing a `SparkContext`
    using a multithreading model, and target your Spark job by setting the `spark.scheduler.pool`
    property per execution thread so your thread takes precedence'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running processing jobs overnight when the cluster is quiet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In any case, you will eventually get a good idea of how the various parts to
    your jobs perform and will then be in a position to calculate what changes could
    be made to improve efficiency. There's always the option of throwing more resources
    at the problem, especially when using a cloud provider, but we would certainly
    encourage the intelligent use of existing resources - this is far more scalable,
    cheaper, and builds data expertise.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we walked through the full setup of an Apache NiFi GDELT ingest
    pipeline, complete with metadata forks and a brief introduction to visualizing
    the resulting data. This section is particularly important as GDELT is used extensively
    throughout the book and the NiFi method is a highly effective way to source data
    in a scalable and modular way.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will get to grips with what to do with the data once
    it's landed, by looking at schemas and formats.
  prefs: []
  type: TYPE_NORMAL
