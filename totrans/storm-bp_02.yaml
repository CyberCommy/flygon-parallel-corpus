- en: Chapter 2. Configuring Storm Clusters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you'll take a deeper look at the Storm technology stack, its
    software dependencies, and the process of setting up and deploying it to a Storm
    cluster.
  prefs: []
  type: TYPE_NORMAL
- en: We will begin by installing Storm in the pseudo-distributed mode where all components
    are collocated on the same machine, rather than distributed across multiple machines.
    Once you have an understanding of the basic steps involved in installing and configuring
    Storm, we will move on to automating these processes using the Puppet provisioning
    tool, which will greatly reduce the time and effort required to set up a multi-node
    cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, we will cover:'
  prefs: []
  type: TYPE_NORMAL
- en: The various components and services that compose a cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Storm technology stack
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing and configuring Storm on Linux
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Storm's configuration parameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Storm's command-line interface
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the Puppet provisioning tool to automate the installation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing the anatomy of a Storm cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Storm clusters follow a master/slave architecture similar to distributed computing
    technologies such as Hadoop but with slightly different semantics. In a master/slave
    architecture, there is typically a master node that is either statically assigned
    through configuration or dynamically elected at runtime. Storm uses the former
    approach. While the master/slave architecture can be criticized as a setup that
    introduces a single point of failure, we'll show that Storm is semi-tolerant of
    a master node failure.
  prefs: []
  type: TYPE_NORMAL
- en: 'A Storm cluster consists of one master node (called **nimbus**) and one or
    more worker nodes (called **supervisors**). In addition to the nimbus and supervisor
    nodes, Storm also requires an instance of Apache ZooKeeper, which itself may consist
    of one or more nodes as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Introducing the anatomy of a Storm cluster](img/8294OS_02_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Both the nimbus and supervisor processes are daemon processes provided by Storm
    and do not need to be isolated from individual machines. In fact, it is possible
    to create a single-node pseudo-cluster with the nimbus, supervisor, and ZooKeeper
    processes all running on the same machine.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the nimbus daemon
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The nimbus daemon's primary responsibility is to manage, coordinate, and monitor
    topologies running on a cluster, including topology deployment, task assignment,
    and task reassignment in the event of a failure.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying a topology to a Storm cluster involves *submitting* the prepackaged
    topology JAR file to the nimbus server along with topology configuration information.
    Once nimbus has received the topology archive, it in turn distributes the JAR
    file to the necessary number of supervisor nodes. When the supervisor nodes receive
    the topology archive, nimbus then assigns tasks (spout and bolt instances) to
    each supervisor and signals them to spawn the necessary workers to perform the
    assigned tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Nimbus tracks the status of all supervisor nodes and the tasks assigned to each.
    If nimbus detects that a specific supervisor node has failed to heartbeat or has
    become unavailable, it will reassign that supervisor's tasks to other supervisor
    nodes in the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned earlier, nimbus is not a single point of failure in the strictest
    sense. This quality is due to the fact that nimbus does not take part in topology
    data processing, rather it merely manages the initial deployment, task assignment,
    and monitoring of a topology. In fact, if a nimbus daemon dies while a topology
    is running, the topology will continue to process data as long as the supervisors
    and workers assigned with tasks remain healthy. The main caveat is that if a supervisor
    fails while nimbus is down, data processing will fail since there is no nimbus
    daemon to reassign the failed supervisor's tasks to another node.
  prefs: []
  type: TYPE_NORMAL
- en: Working with the supervisor daemon
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The supervisor daemon waits for task assignments from nimbus and spawns and
    monitors workers (JVM processes) to execute tasks. Both the supervisor daemon
    and the workers it spawns are separate JVM processes. If a worker process spawned
    by a supervisor exits unexpectedly due to an error (or even if the process is
    being forcibly terminated with the UNIX `kill -9` or Windows `taskkill` command),
    the supervisor daemon will attempt to respawn the worker process.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, you may be wondering how Storm's guaranteed delivery features
    fit into its fault tolerance model. If a worker or even an entire supervisor node
    fails, how does Storm guarantee the delivery of the tuples that were in process
    at the time of failure?
  prefs: []
  type: TYPE_NORMAL
- en: The answer lies in Storm's tuple anchoring and acknowledgement mechanism. When
    reliable delivery is enabled, tuples routed to the task on the failed node will
    not be acknowledged, and the original tuple will eventually be replayed by the
    spout after it is timed out. This process will repeat until the topology has recovered
    and normal processing has resumed.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Apache ZooKeeper
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: ZooKeeper provides a service for maintaining centralized information in a distributed
    environment using a small set of primitives and group services. It has a simple
    yet powerful distributed synchronization mechanism that allows client applications
    to watch or subscribe to individual data or sets of data and receive notifications
    when that data is created, updated, or modified. Using common ZooKeeper patterns
    or recipes, developers can implement a number of different constructs needed by
    distributed applications such as leader election, distributed locks and queues.
  prefs: []
  type: TYPE_NORMAL
- en: Storm uses ZooKeeper primarily to coordinate state information such as task
    assignments, worker status, and topology metrics between nimbus and supervisors
    in a cluster. Nimbus and supervisor node communication is largely handled through
    a combination of ZooKeeper's state modifications and watch notifications.
  prefs: []
  type: TYPE_NORMAL
- en: Storm's use of ZooKeeper is relatively lightweight by design and does not incur
    a heavy resource burden. For heavier-weight data transfer operations, such as
    a one-time (at deployment time) transfer of topology JAR files, Storm relies on
    Thrift for communication. And as we'll see, data transfer operations between components
    in a topology—where performance matters most—is handled at a low level and optimized
    for performance.
  prefs: []
  type: TYPE_NORMAL
- en: Working with Storm's DRPC server
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A common pattern among Storm applications involves the desire to leverage Storm's
    parallelization and distributed computation capabilities within a request-response
    paradigm where a client process or application submits a request and waits for
    a response synchronously. While such a paradigm may seem to counter the highly
    asynchronous, long-lived nature of a typical Storm topology, Storm includes a
    transactional capability that enables such a use case.
  prefs: []
  type: TYPE_NORMAL
- en: '![Working with Storm''s DRPC server](img/8294OS_02_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: To enable this functionality, Storm uses the combination of an extra service
    (Storm DRPC) and a specialized spout and bolt that work together to provide a
    highly scalable Distributed RPC capability.
  prefs: []
  type: TYPE_NORMAL
- en: The use of Storm's DRPC capability is entirely optional. DRPC server nodes are
    only necessary when a Storm application leverages this functionality.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing the Storm UI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Storm UI is an optional, but very useful, service that provides a web-based
    GUI to monitor Storm clusters and manage the running topologies to a certain degree.
    The Storm UI provides statistics for a given Storm cluster and its deployed topologies
    and is very useful when monitoring and tuning cluster and topology performance.
  prefs: []
  type: TYPE_NORMAL
- en: '![Introducing the Storm UI](img/8294OS_02_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Storm UI only reports information gleaned from the nimbus thrift API and does
    not impart any other functionality to a Storm cluster. The Storm UI service can
    be started and stopped at any time without affecting any topology or cluster functionality
    and is in that respect completely stateless. It can also be configurated to start,
    stop, pause, and rebalance topologies for easy management.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing the Storm technology stack
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we jump into installing Storm, let's take a look at the technologies
    with which Storm and topologies are built.
  prefs: []
  type: TYPE_NORMAL
- en: Java and Clojure
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Storm runs on the Java Virtual Machine and is written with a roughly equal combination
    of Java and Clojure. Storm's primary interfaces are defined in Java, with the
    core logic being implemented mostly in Clojure. In addition to JVM languages,
    Storm uses Python to implement the Storm executable. Beyond those languages, Storm
    is a highly polyglot-friendly technology due in part to the fact that a number
    of its interfaces use Apache Thrift.
  prefs: []
  type: TYPE_NORMAL
- en: The components of Storm topologies (spouts and bolts) can be written in virtually
    any programming language supported by the operating system on which it's installed.
    JVM language implementations can run natively, and other implementations are possible
    through JNI and Storm's multilang protocol.
  prefs: []
  type: TYPE_NORMAL
- en: Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: All Storm daemons and management commands are run from a single executable file
    written in Python. This includes the nimbus and supervisor daemons, and as we'll
    see, all the commands to deploy and manage topologies. It is for this reason that
    a properly configured Python interpreter be installed on all machines participating
    in a Storm cluster as well as any workstation used for management purposes.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Storm on Linux
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Storm was originally designed to run on Unix-style operating systems, but as
    of Version 0.9.1, it supports deployment on Windows as well.
  prefs: []
  type: TYPE_NORMAL
- en: For our purposes, we will be using Ubuntu 12.04 LTS for its relative ease of
    use. We'll use the server version which by default does not include a graphical
    user interface since we won't need or use it. The Ubuntu 12.04 LTS server can
    be downloaded from [http://releases.ubuntu.com/precise/ubuntu-12.04.2-server-i386.iso](http://releases.ubuntu.com/precise/ubuntu-12.04.2-server-i386.iso).
  prefs: []
  type: TYPE_NORMAL
- en: The instructions that follow the command work equally well on both the actual
    hardware as well as virtual machines. For the purpose of learning and development,
    you will likely find it much more convenient to work with virtual machines, especially
    if you don't have several networked computers readily available.
  prefs: []
  type: TYPE_NORMAL
- en: 'Virtualization software is readily available for OSX, Linux, and Windows. We
    recommend any one of the following software options:'
  prefs: []
  type: TYPE_NORMAL
- en: VMWare (OSX, Linux, and Windows)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This software would need to be purchased. It is available at [http://www.vmware.com](http://www.vmware.com).
  prefs: []
  type: TYPE_NORMAL
- en: VirtualBox (OSX, Linux, and Windows)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This software is available for free. It is available at [https://www.virtualbox.org](https://www.virtualbox.org).
  prefs: []
  type: TYPE_NORMAL
- en: Parallels Desktop (OSX)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This software would need to be purchased. It is available at [http://www.parallels.com](http://www.parallels.com).
  prefs: []
  type: TYPE_NORMAL
- en: Installing the base operating system
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can begin by booting from the Ubuntu installation disk (or disk image) and
    follow the onscreen instructions for a basic installation. When the **Package
    Selection** screen comes up, choose the option to install OpenSSH Server. This
    package will allow you to use `ssh` to remotely log into the server. In all other
    cases, you can simply accept the default options unless you choose to make modifications
    specific to your hardware.
  prefs: []
  type: TYPE_NORMAL
- en: By default, the primary user under Ubuntu will have administrative (sudo) privileges.
    If you are using a different user account or Linux distribution, make sure your
    account has administration privileges.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Java
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, install a JVM. Storm is known to work with Java 1.6 and 1.7 JVMs from
    both the open source OpenJDK and Oracle. In this example, we''ll update the apt
    repository information and install the OpenJDK distribution of Java 1.6:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: ZooKeeper installation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For our single-node pseudo-cluster, we''ll install ZooKeeper alongside all
    other Storm components. Storm currently requires Version 3.3.x, so we''ll install
    that version rather than the latest one using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This command will install both the ZooKeeper binaries as well as the service
    scripts to start and stop ZooKeeper. It will also create a cron job that will
    periodically purge old ZooKeeper transaction logs and snapshot files, which will
    quickly consume large amounts of disk space if not purged on a regular basis as
    this is ZooKeeper's default behavior.
  prefs: []
  type: TYPE_NORMAL
- en: Storm installation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Storm's binary release distributions can be downloaded from the Storm website
    ([http://storm.incubator.apache.org](http://storm.incubator.apache.org)). The
    layout of the binary archives is geared more toward development activities than
    running a production system, so we'll make a few modifications to more closely
    follow UNIX conventions (such as logging to `/var/log` rather than Storm's home
    directory).
  prefs: []
  type: TYPE_NORMAL
- en: 'We begin by creating a Storm user and group. This will allow us to run the
    Storm daemons as a specific user rather than the default or root users:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, download and unzip the Storm distribution. We''ll install Storm in `/usr/share`
    and symlink the version-specific directory to `/usr/share/storm`. This approach
    will allow us to easily install other versions and activate (or revert) the new
    version by changing a single symbolic link. We''ll also link the Storm executable
    to `/usr/bin/storm`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'By default, Storm will log information to `$STORM_HOME/logs` rather than the
    `/var/log` directory that most UNIX services use. To change this, execute the
    following commands to create the `storm` directory under `/var/log/` and configure
    Storm to write its log data there:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we''ll move Storm''s configuration file to `/etc/storm` and create
    a symbolic link so Storm can find it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: With Storm installed, we're now ready to configure Storm and set up the Storm
    daemons so they start automatically.
  prefs: []
  type: TYPE_NORMAL
- en: Running the Storm daemons
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: All of the Storm daemons are fail-fast by design, meaning the process will halt
    whenever an unexpected error occurs. This allows individual components to safely
    fail and successfully recover without affecting the rest of the system.
  prefs: []
  type: TYPE_NORMAL
- en: This means that the Storm daemons need to be restarted immediately whenever
    they die unexpectedly. The technique for this is known as running a process under
    *supervision*, and fortunately there are a number of utilities available to perform
    this function. In fact, ZooKeeper is also a fail-fast system, and the upstart-based
    `init` scripts included in the ZooKeeper Debian distributions (Ubuntu is a Debian-based
    distribution) provide just that functionality—if the ZooKeeper process exits abnormally
    at any time, upstart will ensure it is restarted so the cluster can recover.
  prefs: []
  type: TYPE_NORMAL
- en: While the Debian upstart system is perfect for this situation, there are simpler
    options that are also available on other Linux distributions. To keep things simple,
    we'll use the supervisor package that's readily available on most distributions.
    Unfortunately, the supervisor name collides with the name of Storm's supervisor
    daemon. To clarify this distinction, we'll refer to the non-Storm process supervision
    daemon as *supervisord* (note the added *d* at the end) in the text, even though
    sample code and commands will use the proper name without the added *d*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Under Debian-based Linux distributions, the `supervisord` package is named
    supervisor, while other distributions such as Red Hat use the name supervisord.
    To install it on Ubuntu, use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This will install and start the supervisord service. The main configuration
    file will be located at `/etc/supervisor/supervisord.conf`. Supervisord's configuration
    file will automatically include any files matching the pattern `*.conf` in the
    `/etc/supervisord/conf.d/` directory, and this is where we'll place our `config`
    files for to run the Storm daemons under supervision.
  prefs: []
  type: TYPE_NORMAL
- en: 'For each Storm daemon command we want to run under supervision, we''ll create
    a configuration file that contains the following:'
  prefs: []
  type: TYPE_NORMAL
- en: A unique (within the supervisord configuration) name for the service under supervision.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The command to run.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The working directory in which to run the command.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Whether or not the command/service should be automatically restarted if it exits.
    For fail-fast services, this should always be true.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The user that will own the process. In this case, we will run all Storm daemons
    with the Storm user as the process owner.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Create the following three files to set up the Storm daemons to be automatically
    started (and restarted in the event of unexpected failure) by the supervisord
    service:'
  prefs: []
  type: TYPE_NORMAL
- en: '`/etc/supervisord/conf.d/storm-nimbus.conf`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Use the following code to create the file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '`/etc/supervisord/conf.d/storm-supervisor.conf`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Use the following code to create the file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '`/etc/supervisord/conf.d/storm-ui.conf`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Use the following code to create the file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Once those files have been created, stop and start the supervisord service
    with the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The supervisord service will load the new configurations and start the Storm
    daemons. Wait a moment or two for the Storm services to start and then verify
    the Storm pseudo-cluster is up and running by visiting the following URL in a
    web browser (replace `localhost` with the host name or IP address of the actual
    machine):'
  prefs: []
  type: TYPE_NORMAL
- en: '`http://localhost:8080`'
  prefs: []
  type: TYPE_NORMAL
- en: This will bring up the Storm UI graphical interface. It should indicate that
    the cluster is up with one supervisor node running with four available worker
    slots and no topologies are running (we'll deploy a topology to the cluster later).
  prefs: []
  type: TYPE_NORMAL
- en: 'If for some reason the Storm UI does not come up or fails to show an active
    supervisor in the cluster, check the following log files for errors:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Storm UI**: Check the `ui.log` file under `/var/log/storm` to check for errors'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Nimbus**: Check the `nimbus.log` file under `/var/log/storm` to check for
    errors'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Supervisor**: Check the `supervisor.log` file under `/var/log/storm` to check
    for errors'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So far, we've relied on the default Storm configuration that defaults to using
    `localhost` for many cluster hostname parameters such as the ZooKeeper hosts as
    well as the location of the nimbus master. This is fine for a single-node pseudo-cluster
    where everything runs on the same machine, but setting up a real multi-node cluster
    requires overriding the default values. Next, we'll explore the various configuration
    options Storm provides and how they affect the behavior of a cluster and its topologies.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring Storm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Storm's configuration consists of a series of YAML properties. When a Storm
    daemon starts, it loads the default values and then loads the `storm.yaml` (which
    we've symlinked to `/etc/storm/storm.yaml`) file under `$STORM_HOME/conf/`, substituting
    any values found there with the defaults.
  prefs: []
  type: TYPE_NORMAL
- en: 'The listing below provides a minimal `storm.yaml` file with entries that you
    must override:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Mandatory settings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The following settings are mandatory for configuring working, multihost Storm
    clusters.
  prefs: []
  type: TYPE_NORMAL
- en: '`storm.zookeeper.servers`: This setting is a list of the hostnames in the ZooKeeper
    cluster. Since we''re running a single node ZooKeeper on the same machine as the
    other Storm daemons, the default value of localhost is acceptable.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nimbus.host`: This is the hostname of the cluster''s nimbus node. Workers
    need to know which node is the master in order to download topology JAR files
    and configurations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`supervisor.slots.ports`: This setting controls how many worker processes run
    on a supervisor node. It is defined as a list of port numbers that the workers
    will listen on, and the number of port numbers listed will control how many worker
    slots are available on the supervisor node. For example, if we have a cluster
    with three supervisor nodes, and each node is configured with three ports, the
    cluster will have a total of nine (3 * 3 = 9) worker slots. By default, Storm
    will use ports 6700-6703, a total of four slots per supervisor node.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`storm.local.dir`: Both the nimbus and supervisor daemons store a small amount
    of transient state information as well as JAR and configuration files required
    by workers. This setting determines where the nimbus and supervisor processes
    will store that information. The directory specified here must exist with appropriate
    permissions so the process owner (in our case, the Storm user) can read and write
    to the directory. The contents of this directory must persist as long as the cluster
    is running, so it is best to avoid using `/tmp` where the contents might be deleted
    by the operating system.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optional settings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In addition to the settings that are mandatory for an operational cluster,
    there are several other settings that you may find necessary to override. Storm
    configuration settings follow a dotted naming convention where the prefix identifies
    the category of the setting; this is shown in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Prefix | Category |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `storm.*` | General configuration |'
  prefs: []
  type: TYPE_TB
- en: '| `nimbus.*` | Nimbus configuration |'
  prefs: []
  type: TYPE_TB
- en: '| `ui.*` | Storm UI configuration |'
  prefs: []
  type: TYPE_TB
- en: '| `drpc.*` | DRPC server configuration |'
  prefs: []
  type: TYPE_TB
- en: '| `supervisor.*` | Supervisor configuration |'
  prefs: []
  type: TYPE_TB
- en: '| `worker.*` | Worker configuration |'
  prefs: []
  type: TYPE_TB
- en: '| `zmq.*` | ZeroMQ configuration |'
  prefs: []
  type: TYPE_TB
- en: '| `topology.*` | Topology configuration |'
  prefs: []
  type: TYPE_TB
- en: 'For a complete list of the default configuration settings that are available,
    take a look at the `defaults.yaml` file in the Storm source code ([https://github.com/nathanmarz/storm/blob/master/conf/defaults.yaml](https://github.com/nathanmarz/storm/blob/master/conf/defaults.yaml)).
    Some of the more frequently overridden settings are outlined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`nimbus.childopts` (default: "-Xmx1024m"): This setting is a list of JVM options
    that will be added to the Java command line when starting the nimbus daemon.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ui.port` (default: 8080): This specifies the listening port for the Storm
    UI web server.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ui.childopts` (default: "-Xmx1024m"): This specifies the JVM options that
    will be added to the Java command line when starting the Storm UI service.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`supervisor.childopts` (default: "-Xmx1024m"): This specifies the JVM options
    that will be added to the Java command line when starting the supervisor daemon.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`worker.childopts` (default: "-Xmx768m"): This specifies the JVM options that
    will be added to the Java command line when starting worker processes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`topology.message.timeout.secs` (default: 30): This configures the maximum
    amount of time (in seconds) for a tuple''s tree to be acknowledged (fully processed)
    before it is considered failed (timed out). Setting this value too low may cause
    tuples to be replayed repeatedly. For this setting to take effect, a spout must
    be configured to emit anchored tuples.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`topology.max.spout.pending` (default: null): With the default value of null,
    Storm will stream tuples from a spout as fast as the spout can produce them. Depending
    on the execute latency of downstream bolts, the default behavior can overwhelm
    the topology, leading to message timeouts. Setting this value to a non-null number
    greater than 0 will cause Storm to pause streaming tuples from spouts until the
    number of outstanding tuples falls below that number, essentially throttling the
    spout. This setting, along with `topology.message.timeout.secs`, are two of the
    most important parameters when tuning a topology for performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`topology.enable.message.timeouts` (default: true): This sets the timeout behavior
    for anchored tuples. If false, anchored tuples will not time out. Use this setting
    with care. Consider altering `topology.message.timeout.secs` before setting this
    to false. For this setting to take effect, a spout must be configured to emit
    anchored tuples.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Storm executable
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Storm executable is a multipurpose command used for everything from launching
    Storm daemons to performing topology management functions, such as deploying new
    topologies to a cluster, or simply running a topology in local mode during development
    and testing phases.
  prefs: []
  type: TYPE_NORMAL
- en: 'The basic syntax for the Storm command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Setting up the Storm executable on a workstation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For running Storm commands that connect to a remote cluster, you will need
    to have the Storm distribution installed locally. Installing the distribution
    on a workstation is simple; just unzip the Storm distribution archive and add
    the Storm bin directory (`$STORM_HOME/bin`) to your `PATH` environment variable.
    Next, create the `storm.yaml` file under `~/.storm/` with a single line that tells
    Storm where to find the nimbus server for the cluster with which you want to interact:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In order for a Storm cluster to operate properly, it is imperative that the
    IP address name resolution be set up properly, either through the DNS system or
    entries in the `hosts` file under `/etc`.
  prefs: []
  type: TYPE_NORMAL
- en: While it is possible to use IP addresses instead of hostnames throughout Storm's
    configuration, using the DNS system is preferred.
  prefs: []
  type: TYPE_NORMAL
- en: The daemon commands
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Storm's daemon commands are used to launch Storm services, and should be run
    under supervision so they are relaunched in the event of unexpected failures.
    When starting, Storm daemons read configuration from `$STORM_HOME/conf/storm.yaml`.
    Any configuration parameters in this file will override Storm's built-in defaults.
  prefs: []
  type: TYPE_NORMAL
- en: Nimbus
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Usage: `storm nimbus`'
  prefs: []
  type: TYPE_NORMAL
- en: This launches the nimbus daemon.
  prefs: []
  type: TYPE_NORMAL
- en: Supervisor
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Usage: `storm supervisor`'
  prefs: []
  type: TYPE_NORMAL
- en: This launches the supervisor daemon.
  prefs: []
  type: TYPE_NORMAL
- en: UI
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Usage: `storm ui`'
  prefs: []
  type: TYPE_NORMAL
- en: This launches the Storm UI daemon that provides a web-based UI for monitoring
    Storm clusters.
  prefs: []
  type: TYPE_NORMAL
- en: DRPC
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Usage: `storm drpc`'
  prefs: []
  type: TYPE_NORMAL
- en: This launches the DRPC daemon.
  prefs: []
  type: TYPE_NORMAL
- en: The management commands
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Storm''s management commands are used to deploy and manage topologies running
    in a cluster. Management commands typically, but not necessarily, run from a workstation
    outside of the Storm cluster. They communicate to the nimbus Thrift API and thus
    need to know the hostname of the nimbus node. The management commands look for
    configuration from the `~/.storm/storm.yaml` file, and Storm''s jars are appended
    to the classpath. The only required configuration parameter is the hostname of
    the nimbus node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Jar
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Usage: `storm jar topology_jar topology_class [arguments...]`'
  prefs: []
  type: TYPE_NORMAL
- en: The `jar` command is used to submit a topology to a cluster. It runs the `main()`
    method of `topology_class` with the specified arguments and uploads the `topology_jar`
    file to nimbus for distribution to the cluster. Once submitted, Storm will activate
    the topology and start processing.
  prefs: []
  type: TYPE_NORMAL
- en: The `main()` method in the topology class is responsible for calling the `StormSubmitter.submitTopology()`
    method and supplying a unique (within the cluster) name for the topology. If a
    topology with that name already exists on the cluster, the `jar` command will
    fail. It is common practice to specify the topology name in the command-line arguments
    so that the topology can be named at the time of submission.
  prefs: []
  type: TYPE_NORMAL
- en: Kill
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Usage: `storm kill topology_name [-w wait_time]`'
  prefs: []
  type: TYPE_NORMAL
- en: The `kill` command is used to undeploy. It kills the topology with the name
    `topology_name`. Storm will first deactivate the topology's spouts for the duration
    of the topology's configured `topology.message.timeout.secs` to allow all tuples
    actively being processed to complete. Storm will then halt the workers and attempt
    to clean up any saved states. Specifying a wait time with the `-w` switch will
    override `topology.message.timeout.secs` with the specified interval.
  prefs: []
  type: TYPE_NORMAL
- en: The functionality of the `kill` command is also available in the Storm UI.
  prefs: []
  type: TYPE_NORMAL
- en: Deactivate
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Usage: `storm deactivate topology_name`'
  prefs: []
  type: TYPE_NORMAL
- en: The `deactivate` command tells Storm to stop streaming tuples from the specified
    topology's spouts.
  prefs: []
  type: TYPE_NORMAL
- en: Topologies can also be deactivated from the Storm UI.
  prefs: []
  type: TYPE_NORMAL
- en: Activate
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Usage: `storm activate topology_name`'
  prefs: []
  type: TYPE_NORMAL
- en: The `activate` command tells Storm to resume streaming tuples from the specified
    topology's spouts.
  prefs: []
  type: TYPE_NORMAL
- en: Topologies can also be reactivated from the Storm UI.
  prefs: []
  type: TYPE_NORMAL
- en: Rebalance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Usage: `storm rebalance topology_name [-w wait_time] [-n worker_count] [-e
    component_name=executer_count]`...'
  prefs: []
  type: TYPE_NORMAL
- en: The `rebalance` command instructs Storm to redistribute tasks among workers
    in a cluster without killing and resubmitting the topology. For example, this
    might be necessary when a new supervisor node has been added to a cluster—since
    it is a new node, none of the tasks of existing topologies would have been assigned
    to workers on that node.
  prefs: []
  type: TYPE_NORMAL
- en: The `rebalance` command also allows you to alter the number of workers assigned
    to a topology and change the number of executors assigned to a given task with
    the `-n` and `-e` switches respectively.
  prefs: []
  type: TYPE_NORMAL
- en: When the `rebalance` command is run, Storm will first deactivate the topology,
    wait for the configured time for outstanding tuples to finish processing, then
    redistribute workers evenly among supervisor nodes. After rebalancing, Storm will
    return the topology to its previous activation state (that is, if it was activated,
    Storm will reactivate it and vice versa).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following example will rebalance the topology with the name `wordcount-topology`
    with a waiting time of 15 seconds, assign five workers to the topology, and set
    `sentence-spout` and `split-bolt` to use 4 and 8 executor threads respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Remoteconfvalue
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Usage: `storm remoteconfvalue conf-name`'
  prefs: []
  type: TYPE_NORMAL
- en: The `remoteconfvalue` command is used to look up a configuration parameter on
    a remote cluster. Note that this applies to the global cluster configuration and
    does not take into account individual overrides made at the topology level.
  prefs: []
  type: TYPE_NORMAL
- en: Local debug/development commands
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Storm's local commands are utilities for debugging and testing. Like the management
    commands, Storm's debug commands read `~/.storm/storm.yaml` and use those values
    to override Storm's built-in defaults.
  prefs: []
  type: TYPE_NORMAL
- en: REPL
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Usage: `storm repl`'
  prefs: []
  type: TYPE_NORMAL
- en: The `repl` command opens a Clojure REPL session configured with Storm's local
    classpath.
  prefs: []
  type: TYPE_NORMAL
- en: Classpath
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Usage: `storm classpath`'
  prefs: []
  type: TYPE_NORMAL
- en: The `classpath` command prints the classpath used by the Storm client.
  prefs: []
  type: TYPE_NORMAL
- en: Localconfvalue
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Usage: `storm localconfvalue conf-name`'
  prefs: []
  type: TYPE_NORMAL
- en: The `localconfvalue` command looks up a configuration key from the consolidated
    configuration, that is, from `~/.storm/storm.yaml` and Storm's built-in defaults.
  prefs: []
  type: TYPE_NORMAL
- en: Submitting topologies to a Storm cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have a running cluster, let''s revisit our earlier word count example
    and modify it so we can deploy it to a cluster as well as run it in local mode.
    The previous example used Storm''s `LocalCluster` class to run in local mode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Submitting a topology to a remote cluster is simply a matter of using Storm''s
    `StormSubmitter` class, which exposes a method with the same name and signature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'When developing Storm topologies, you usually aren''t going to want to change
    code and recompile them to switch between running in local mode and deploying
    to a cluster. The standard way to handle this is to add an if/else block that
    makes that determination based on a command-line argument. In our updated example,
    if there are no command line arguments, we run the topology in local mode; otherwise,
    we use the first argument as the topology name and submit it to the cluster, as
    shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'To deploy the updated word count topology to a running cluster, first perform
    a Maven build in the `Chapter 2` source code directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, run the `storm jar` command to deploy the topology:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: When the command completes, you should see the topology become active in the
    Storm UI and be able to click on the topology name to drill down and view the
    topology statistics.
  prefs: []
  type: TYPE_NORMAL
- en: '![Submitting topologies to a Storm cluster](img/8294OS_02_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Automating the cluster configuration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we've configured a single-node pseudo-cluster manually from the command
    line. While this approach certainly works with small clusters, it will quickly
    become untenable as the cluster size increases. Consider the situation where one
    needs to configure clusters consisting of tens, hundreds, or even thousands of
    nodes. The configuration tasks can be automated using shell scripts, but even
    a shell script-based automation solution is questionable in terms of scalability.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, there are a number of technologies available to help address the
    issue of configuration and provisioning of large numbers of managed servers. Both
    Chef and Puppet offer a declarative approach to configuration that allows you
    to define **states** (that is, what packages are installed and how they are configured)
    as well as **classes** of machines (for example, an *Apache web server* class
    machine needs to have the Apache `httpd` daemon installed).
  prefs: []
  type: TYPE_NORMAL
- en: Automating the process of provisioning and configuring servers is a very broad
    topic that is far beyond the scope of this book. For our purposes, we will use
    Puppet and leverage a subset of its functionality in the hope that it will provide
    a basic introduction to the topic and encourage further exploration.
  prefs: []
  type: TYPE_NORMAL
- en: A rapid introduction to Puppet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Puppet ([https://puppetlabs.com](https://puppetlabs.com)) is an IT automation
    framework that helps system administrators manage large network infrastructure
    resources using a flexible, declarative approach to IT automation.
  prefs: []
  type: TYPE_NORMAL
- en: 'At the heart of Puppet is the concept of a *manifest* that describes the desired
    *state* of an infrastructure resource. In Puppet terms, a state can include the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: Which software packages are installed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Which services are running and which aren't
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Software configuration details
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Puppet manifests
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Puppet uses a declarative Ruby-based DSL to describe system configuration in
    collections of files known as manifests. An example Puppet manifest for ZooKeeper
    is listed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: This simple manifest can be used to make sure ZooKeeper is installed as a service
    and that the service is running. The first package block tells Puppet to use the
    operating system's package manager (for example, apt-get for Ubuntu/Debian, yum
    for Red Hat, and so on) to ensure that the Version 3.3.5 of the zookeeper package
    is installed. The second package block ensures that the zookeeperd package is
    installed; it requires that the zookeeper package is already installed. Finally,
    the `service` block tells Puppet that it should ensure that the zookeeperd system
    service is running and that the service requires the zookeeperd package to be
    installed.
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate how Puppet manifests translate to installed software and system's
    state, let's install Puppet and use the preceding example to install and start
    the zookeeperd service.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get the latest version of Puppet, we need to configure apt-get to use the
    Puppet labs repository. Execute the following commands to do so and install the
    latest version of puppet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, save the preceding example manifest to a file named `init.pp` and use
    Puppet to apply the manifest:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'When the command completes, check to see whether the zookeeper service is in
    fact running:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: If we were to manually stop the zookeeper service and rerun the `puppet apply`
    command, Puppet would not install the packages again (since they are already there);
    however, it would restart the zookeeper service since the state defined in the
    manifest defines the service as *running*.
  prefs: []
  type: TYPE_NORMAL
- en: Puppet classes and modules
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While standalone Puppet manifests make it easy to define the state of an individual
    resource, such an approach can quickly become unwieldy when the number of resources
    you're managing increases.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, Puppet has the concept of classes and modules that can be leveraged
    to better organize and isolate specific configuration details.
  prefs: []
  type: TYPE_NORMAL
- en: Consider a situation with Storm where we have multiple classes of nodes. For
    example, a node in a Storm cluster may be a nimbus node, a supervisor node, or
    both. Puppet classes and modules provide a way to distinguish between multiple
    configuration roles that you can mix and match to easily define a network resource
    that performs multiple roles.
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate this capability, let''s revisit the manifest we used to install
    the zookeeper package and redefine it as a class that can be reused and included
    in multiple class types and manifests:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding example, we've redefined the zookeeper manifest to be a `puppet`
    class that can be used in other classes and manifests. On the second line, the
    `zookeeper` class includes another class, `jdk`, which will include the class
    definition for a resource that will include the state necessary for a machine
    that requires a Java JDK.
  prefs: []
  type: TYPE_NORMAL
- en: Puppet templates
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Puppet also leverages the Ruby ERB templating system that allows you to define
    templates for various files that will be populated when Puppet applies a manifest
    file. Placeholders in Puppet ERB templates are Ruby expressions and constructs
    that will be evaluated and replaced when Puppet runs. The Ruby code in ERB templates
    has full access to the Puppet variables defined in manifest files.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following Puppet file declaration that''s used to generate the
    `storm.yaml` configuration file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'This declaration tells Puppet to create the file, `storm.yaml`, under `/etc/storm/`
    from the `storm.yaml.erb` template:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The conditional logic and variable expansion in the template allow us to define
    a single file that can be used for many environments. For example, if the environment
    we're configuring does not have any Storm DRPC servers, then the `drpc.servers`
    section of the generated `storm.yaml` file will be omitted.
  prefs: []
  type: TYPE_NORMAL
- en: Managing environments with Puppet Hiera
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We''ve briefly introduced the concepts of Puppet manifests, classes, and templates.
    At this point, you''re probably wondering how to define variables in a puppet
    class or manifest. Defining a variable within a `puppet` class or manifest is
    easy; simply define it at the beginning of the manifest or class definition as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Once defined, the `java_version` variable will be available throughout the class
    or manifest definition as well as any ERB templates; however, there is a drawback
    here in terms of reusability. If we hard-code information such as version numbers,
    we're effectively limiting the reuse of our class by pinning it to a hard-coded
    value. It would be better if we could externalize all potentially frequently changing
    variables to make configuration management more maintainable. This is where Hiera
    comes into play.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Hiera
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Hiera is a key-value lookup tool that has been integrated into the latest version
    of the Puppet framework. Hiera allows you to define key-value hierarchies (hence
    the name) such that keys in a parent definition source can be overridden by child
    definition sources.
  prefs: []
  type: TYPE_NORMAL
- en: For example, consider a situation where we are defining configuration parameters
    for a number of machines that will participate in a Storm cluster. All machines
    will share a common set of key-values such as the version of Java we'd like to
    use. So, we'd define those values in a file called "`common.yaml`.`"`
  prefs: []
  type: TYPE_NORMAL
- en: From there on, things start to diverge. We may have environments that are single-node
    pseudo-clusters, and we may have environments that are multi-node. For that, we'd
    like to store environment-specific configuration values in separate files such
    as "`single-node.yaml"` and "`cluster.yaml`."
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we'd like to store true host-specific information in files that follow
    the naming convenion "**[hostname].yaml**."
  prefs: []
  type: TYPE_NORMAL
- en: '![Introducing Hiera](img/8294OS_02_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Puppet's Hiera integration allows you to do just that and use built-in Puppet
    variables to resolve filenames appropriately.
  prefs: []
  type: TYPE_NORMAL
- en: The examples in the `Chapter 2` source code directory demonstrate how to implement
    this type of organization.
  prefs: []
  type: TYPE_NORMAL
- en: 'A typical `common.yaml` file might define global properties common to all hosts
    and looks like the following :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'At the environment level, we may want to distinguish between *standalone* and
    *cluster* configurations, in which case a `cluster.yaml` file might look like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Finally, we may want to define host-specific parameters in files that use the
    naming convention [hostname].yaml, and define the Puppet classes that should be
    applied for that node.
  prefs: []
  type: TYPE_NORMAL
- en: 'For `nimbus01.yaml`, use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'For `zookeeper01.yaml`, use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: We've only scratched the surface of what's possible with Puppet and Hiera. The
    `Chapter 2` source code directory contains additional examples and documentation
    on how to use Puppet to automate deployment and configuration tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we've covered the steps necessary to install and configure
    Storm in both a single-node (pseudo-distributed) configuration as well as a fully
    distributed multi-node configuration. We've also introduced you to the Storm daemons
    and command line utilities used to deploy and manage running topologies.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we offered a brief introduction to the Puppet framework and showed
    how it can be used to manage multiple environment configurations.
  prefs: []
  type: TYPE_NORMAL
- en: We'd encourage you to explore the additional code and documentation included
    in the accompanied downloads.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will introduce Trident, which is a high-level abstraction
    layer on top of Storm for transactions and state management.
  prefs: []
  type: TYPE_NORMAL
