- en: Chapter 8. Client and Server Applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous chapter, we looked at exchanging data between devices by using
    the sockets interface. In this chapter, we''re going to use sockets to build network
    applications. Sockets follow one of the main models of computer networking, that
    is, the **client/server** model. We''ll look at this with a focus on structuring
    server applications. We''ll cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Designing a simple protocol
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building an echo server and client
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a chat server and client
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multithreaded and event-driven server architectures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `eventlet` and `asyncio` libraries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The examples in this chapter are best run on Linux or a Unix operating system.
    The Windows sockets implementation has some idiosyncrasies, and these can create
    some error conditions, which we will not be covering here. Note that Windows does
    not support the `poll` interface that we'll use in one example. If you do use
    Windows, then you'll probably need to use *ctrl* + *break* to kill these processes
    in the console, rather than using *ctrl* - *c* because Python in a Windows command
    prompt doesn't respond to *ctrl* – *c* when it's blocking on a socket send or
    receive, which will be quite often in this chapter! (and if, like me, you're unfortunate
    enough to try testing these on a Windows laptop without a *break* key, then be
    prepared to get very familiar with the Windows Task Manager's **End task** button).
  prefs: []
  type: TYPE_NORMAL
- en: Client and server
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The basic setup in the client/server model is one device, the server that runs
    a service and patiently waits for clients to connect and make requests to the
    service. A 24-hour grocery shop may be a real world analogy. The shop waits for
    customers to come in and when they do, they request certain products, purchase
    them and leave. The shop might advertise itself so people know where to find it,
    but the actual transactions happen while the customers are visiting the shop.
  prefs: []
  type: TYPE_NORMAL
- en: A typical computing example is a web server. The server listens on a TCP port
    for clients that need its web pages. When a client, for example a web browser,
    requires a web page that the server hosts, it connects to the server and then
    makes a request for that page. The server replies with the content of the page
    and then the client disconnects. The server advertises itself by having a hostname,
    which the clients can use to discover the IP address so that they can connect
    to it.
  prefs: []
  type: TYPE_NORMAL
- en: In both of these situations, it is the client that initiates any interaction
    – the server is purely responsive to that interaction. So, the needs of the programs
    that run on the client and server are quite different.
  prefs: []
  type: TYPE_NORMAL
- en: Client programs are typically oriented towards the interface between the user
    and the service. They retrieve and display the service, and allow the user to
    interact with it. Server programs are written to stay running for indefinite periods
    of time, to be stable, to efficiently deliver the service to the clients that
    are requesting it, and to potentially handle a large number of simultaneous connections
    with a minimal impact on the experience of any one client.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will look at this model by writing a simple echo server
    and client, and then upgrading it to a chat server, which can handle a session
    with multiple clients. The `socket` module in Python perfectly suits this task.
  prefs: []
  type: TYPE_NORMAL
- en: An echo protocol
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we write our first client and server programs, we need to decide how
    they are going to interact with each other, that is we need to design a protocol
    for their communication.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our echo server should listen until a client connects and sends a bytes string,
    then we want it to echo that string back to the client. We only need a few basic
    rules for doing this. These rules are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Communication will take place over TCP.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The client will initiate an echo session by creating a socket connection to
    the server.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The server will accept the connection and listen for the client to send a bytes
    string.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The client will send a bytes string to the server.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once it sends the bytes string, the client will listen for a reply from the
    server
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When it receives the bytes string from the client, the server will send the
    bytes string back to the client.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When the client has received the bytes string from the server, it will close
    its socket to end the session.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These steps are straightforward enough. The missing element here is how the
    server and the client will know when a complete message has been sent. Remember
    that an application sees a TCP connection as an endless stream of bytes, so we
    need to decide what in that byte stream will signal the end of a message.
  prefs: []
  type: TYPE_NORMAL
- en: Framing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This problem is called **framing**, and there are several approaches that we
    can take to handle it. The main ones are described here:'
  prefs: []
  type: TYPE_NORMAL
- en: Make it a protocol rule that only one message will be sent per connection, and
    once a message has been sent, the sender will immediately close the socket.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use fixed length messages. The receiver will read the number of bytes and know
    that they have the whole message.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Prefix the message with the length of the message. The receiver will read the
    length of the message from the stream first, then it will read the indicated number
    of bytes to get the rest of the message.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use special character delimiters for indicating the end of a message. The receiver
    will scan the incoming stream for a delimiter, and the message comprises everything
    up to the delimiter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Option 1 is a good choice for very simple protocols. It's easy to implement
    and it doesn't require any special handling of the received stream. However, it
    requires the setting up and tearing down of a socket for every message, and this
    can impact performance when a server is handling many messages at once.
  prefs: []
  type: TYPE_NORMAL
- en: Option 2 is again simple to implement, but it only makes efficient use of the
    network when our data comes in neat, fixed-length blocks. For example in a chat
    server the message lengths are variable, so we will have to use a special character,
    such as the null byte, to pad messages to the block size. This only works where
    we know for sure that the padding character will never appear in the actual message
    data. There is also the additional issue of how to handle messages longer than
    the block length.
  prefs: []
  type: TYPE_NORMAL
- en: Option 3 is usually considered as one of the best approaches. Although it can
    be more complex to code than the other options, the implementations are still
    reasonably straightforward, and it makes efficient use of bandwidth. The overhead
    imposed by including the length of each message is usually minimal as compared
    to the message length. It also avoids the need for any additional processing of
    the received data, which may be needed by certain implementations of option 4.
  prefs: []
  type: TYPE_NORMAL
- en: Option 4 is the most bandwidth-efficient option, and is a good choice when we
    know that only a limited set of characters, such as the ASCII alphanumeric characters,
    will be used in messages. If this is the case, then we can choose a delimiter
    character, such as the null byte, which will never appear in the message data,
    and then the received data can be easily broken into messages as this character
    is encountered. Implementations are usually simpler than option 3\. Although it
    is possible to employ this method for arbitrary data, that is, where the delimiter
    could also appear as a valid character in a message, this requires the use of
    character escaping, which needs an additional round of processing of the data.
    Hence in these situations, it's usually simpler to use length-prefixing.
  prefs: []
  type: TYPE_NORMAL
- en: For our echo and chat applications, we'll be using the UTF-8 character set to
    send messages. The null byte isn't used in any character in UTF-8 except for the
    null byte itself, so it makes a good delimiter. Thus, we'll be using method 4
    with the null byte as the delimiter to frame our messages.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, our rule number 8 will become:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Messages will be encoded in the UTF-8 character set for transmission, and
    they will be terminated by the null byte.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Now, let's write our echo programs.
  prefs: []
  type: TYPE_NORMAL
- en: A simple echo server
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we work through this chapter, we''ll find ourselves reusing several pieces
    of code, so to save ourselves from repetition, we''ll set up a module with useful
    functions that we can reuse as we go along. Create a file called `tincanchat.py`
    and save the following code in it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: First we define a default interface and a port number to listen on. The empty
    `''` interface, specified in the `HOST` variable, tells `socket.bind()` to listen
    on all available interfaces. If you want to restrict access to just your machine,
    then change the value of the `HOST` variable at the beginning of the code to `127.0.0.1`.
  prefs: []
  type: TYPE_NORMAL
- en: We'll be using `create_listen_socket()` to set up our server listening connections.
    This code is the same for several of our server programs, so it makes sense to
    reuse it.
  prefs: []
  type: TYPE_NORMAL
- en: The `recv_msg()` function will be used by our echo server and client for receiving
    messages from a socket. In our echo protocol, there isn't anything that our programs
    may need to do while they're waiting to receive a message, so this function just
    calls `socket.recv()` in a loop until it has received the whole message. As per
    our framing rule, it will check the accumulated data on each iteration to see
    if it has received a null byte, and if so, then it will return the received data,
    stripping off the null byte and decoding it from UTF-8.
  prefs: []
  type: TYPE_NORMAL
- en: The `send_msg()` and `prep_msg()` functions work together for framing and sending
    a message. We've separated the null byte termination and the UTF-8 encoding into
    `prep_msg()` because we will use them in isolation later on.
  prefs: []
  type: TYPE_NORMAL
- en: Handling the received data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Note that we're drawing ourselves a careful line with these send and receive
    functions as regards string encoding. Python 3 strings are Unicode, while the
    data that we receive over the network is bytes. The last thing that we want to
    be doing is handling a mixture of these in the rest of our program code, so we're
    going to carefully encode and decode the data at the boundary of our program,
    where the data enters and leaves the network. This will ensure that any functions
    in the rest of our code can assume that they'll be working with Python strings,
    which will later on make things much easier for us.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, not all the data that we may want to send or receive over a network
    will be text. For example, images, compressed files, and music, can't be decoded
    to a Unicode string, so a different kind of handling is needed. Usually this will
    involve loading the data into a class, such as a **Python Image Library** (**PIL**)
    image for example, if we are going to manipulate the object in some way.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are basic checks that could be done here on the received data, before
    performing full processing on it, to quickly flag any problems with the data.
    Some examples of such checks are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Checking the length of the received data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Checking the first few bytes of a file for a magic number to confirm a file
    type
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Checking values of higher level protocol headers, such as the `Host` header
    in an `HTTP` request
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This kind of checking will allow our application to fail fast if there is an
    obvious problem.
  prefs: []
  type: TYPE_NORMAL
- en: The server itself
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, let''s write our echo server. Open a new file called `1.1-echo-server-uni.py`
    and save the following code in it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This is about as simple as a server can get! First, we set up our listening
    socket with the `create_listen_socket()` call. Second, we enter our main loop,
    where we listen forever for incoming connections from clients, blocking on `listen_sock.accept()`.
    When a client connection comes in, we invoke the `handle_client()` function, which
    handles the client as per our protocol. We've created a separate function for
    this code, partly to keep the main loop tidy, and partly because we'll want to
    reuse this set of operations in later programs.
  prefs: []
  type: TYPE_NORMAL
- en: That's our server, now we just need to make a client to talk to it.
  prefs: []
  type: TYPE_NORMAL
- en: A simple echo client
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Create a file called `1.2-echo_client-uni.py` and save the following code in
    it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: If we're running our server on a different machine from the one on which we
    are running the client, then we can supply the IP address or the hostname of the
    server as a command line argument to the client program. If we don't, then it
    will default to trying to connect to the localhost.
  prefs: []
  type: TYPE_NORMAL
- en: The third and forth lines of the code check the command line arguments for a
    server address. Once we've determined which server to connect to, we enter our
    main loop, which loops forever until we kill the client by entering `q` as a message.
    Within the main loop, we first create a connection to the server. Second, we prompt
    the user to enter the message to send and then we send the message using the `tincanchat.send_msg()`
    function. We then wait for the server's reply. Once we get the reply, we print
    it and then we close the connection as per our protocol.
  prefs: []
  type: TYPE_NORMAL
- en: 'Give our client and server a try. Run the server in a terminal by using the
    following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'In another terminal, run the client and note that you will need to specify
    the server if you need to connect to another computer, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Running the terminals side by side is a good idea, because you can simultaneously
    see how the programs behave.
  prefs: []
  type: TYPE_NORMAL
- en: Type a few messages into the client and see how the server picks them up and
    sends them back. Disconnecting with the client should also prompt a notification
    on the server.
  prefs: []
  type: TYPE_NORMAL
- en: Concurrent I/O
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you're adventurous, then you may have tried connecting to our server using
    more than one client at once. If you tried sending messages from both of them,
    then you'd have seen that it does not work as we might have hoped. If you haven't
    tried this, then give it a go.
  prefs: []
  type: TYPE_NORMAL
- en: 'A working echo session on the client should look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'However, when trying to send a message by using a second connected client,
    we''ll see something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The client will hang when the message is sent, and it won't get an echo reply.
    You may also notice that if we send a message by using the first connected client,
    then the second client will get its response. So, what's going on here?
  prefs: []
  type: TYPE_NORMAL
- en: The problem is that the server can only listen for the messages from one client
    at a time. As soon as the first client connects, the server blocks at the `socket.recv()`
    call in `tincanchat.recv_msg()`, waiting for the first client to send a message.
    The server isn't able to receive messages from other clients while this is happening
    and so, when another client sends a message, that client blocks too, waiting for
    the server to send a reply.
  prefs: []
  type: TYPE_NORMAL
- en: This is a slightly contrived example. The problem in this case could easily
    be fixed in the client end by asking the user for an input before establishing
    a connection to the server. However in our full chat service, the client will
    need to be able to listen for messages from the server while simultaneously waiting
    for user input. This is not possible in our present procedural setup.
  prefs: []
  type: TYPE_NORMAL
- en: There are two solutions to this problem. We can either use more than one thread
    or process, or use **non-blocking** sockets along with an **event-driven** architecture.
    We're going to look at both of these approaches, starting with **multithreading**.
  prefs: []
  type: TYPE_NORMAL
- en: Multithreading and multiprocessing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Python has APIs that allow us to write both multithreading and multiprocessing
    applications. The principle behind multithreading and multiprocessing is simply
    to take copies of our code and run them in additional threads or processes. The
    operating system automatically schedules the threads and processes across available
    CPU cores to provide fair processing time allocation to all the threads and processes.
    This effectively allows a program to simultaneously run multiple operations. In
    addition, when a thread or process blocks, for example, when waiting for IO, the
    thread or process can be de-prioritized by the OS, and the CPU cores can be allocated
    to other threads or processes that have actual computation to do.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an overview of how threads and processes relate to each other:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Multithreading and multiprocessing](graphics/6008OS_08_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Threads exist within processes. A process can contain multiple threads but it
    always contains at least one thread, sometimes called the **main thread**. Threads
    within the same process share memory, so data transfer between threads is just
    a case of referencing the shared objects. Processes do not share memory, so other
    interfaces, such as files, sockets, or specially allocated areas of shared memory,
    must be used for transferring data between processes.
  prefs: []
  type: TYPE_NORMAL
- en: When threads have operations to execute, they ask the operating system thread
    scheduler to allocate them some time on a CPU, and the scheduler allocates the
    waiting threads to CPU cores based on various parameters, which vary from OS to
    OS. Threads in the same process may run on separate CPU cores at the same time.
  prefs: []
  type: TYPE_NORMAL
- en: Although two processes have been displayed in the preceding diagram, multiprocessing
    is not going on here, since the processes belong to different applications. The
    second process is displayed to illustrate a key difference between Python threading
    and threading in most other programs. This difference is the presence of the GIL.
  prefs: []
  type: TYPE_NORMAL
- en: Threading and the GIL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The CPython interpreter (the standard version of Python available for download
    from [www.python.org](http://www.python.org)) contains something called the **Global
    Interpreter Lock** (**GIL**). The GIL exists to ensure that only a single thread
    in a Python process can run at a time, even if multiple CPU cores are present.
    The reason for having the GIL is that it makes the underlying C code of the Python
    interpreter much easier to write and maintain. The drawback of this is that Python
    programs using multithreading cannot take advantage of multiple cores for parallel
    computation.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is a cause of much contention; however, for us this is not so much of
    a problem. Even with the GIL present, threads that are blocking on I/O are still
    de-prioritized by the OS and put into the background, so threads that do have
    computational work to do can run instead. The following figure is a simplified
    illustration of this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Threading and the GIL](graphics/6008OS_08_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The **Waiting for GIL** state is where a thread has sent or received some data
    and so is ready to come out of the blocking state, but another thread has the
    GIL, so the ready thread is forced to wait. In many network applications, including
    our echo and chat servers, the time spent waiting on I/O is much higher than the
    time spent processing data. As long as we don't have a very large number of connections
    (a situation we'll discuss later on when we come to event driven architectures),
    thread contention caused by the GIL is relatively low, and hence threading is
    still a suitable architecture for these network server applications.
  prefs: []
  type: TYPE_NORMAL
- en: With this in mind, we're going to use multithreading rather than multiprocessing
    in our echo server. The shared data model will simplify the code that we'll need
    for allowing our chat clients to exchange messages with each other, and because
    we're I/O bound, we don't need processes for parallel computation. Another reason
    for not using processes in this case is that processes are more "heavyweight"
    in terms of the OS resources, so creating a new process takes longer than creating
    a new thread. Processes also use more memory.
  prefs: []
  type: TYPE_NORMAL
- en: 'One thing to note is that if you need to perform an intensive computation in
    your network server application (maybe you need to compress a large file before
    sending it over the network), then you should investigate methods for running
    this in a separate process. Because of quirks in the implementation of the GIL,
    having even a single computationally intensive thread in a mainly I/O bound process
    when multiple CPU cores are available can severely impact the performance of all
    the I/O bound threads. For more details, go through the David Beazley presentations
    linked to in the following information box:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Processes and threads are different beasts, and if you're not clear on the distinctions,
    it's worthwhile to read up. A good starting point is the Wikipedia article on
    threads, which can be found at [http://en.wikipedia.org/wiki/Thread_(computing)](http://en.wikipedia.org/wiki/Thread_(computing)).
  prefs: []
  type: TYPE_NORMAL
- en: A good overview of the topic is given in *Chapter 4* of Benjamin Erb's thesis,
    which is available at [http://berb.github.io/diploma-thesis/community/](http://berb.github.io/diploma-thesis/community/).
  prefs: []
  type: TYPE_NORMAL
- en: Additional information on the GIL, including the reasoning behind keeping it
    in Python can be found in the official Python documentation at [https://wiki.python.org/moin/GlobalInterpreterLock](https://wiki.python.org/moin/GlobalInterpreterLock).
  prefs: []
  type: TYPE_NORMAL
- en: You can also read more on this topic in Nick Coghlan's Python 3 Q&A, which can
    be found at [http://python-notes.curiousefficiency.org/en/latest/python3/questions_and_answers.html#but-but-surely-fixing-the-gil-is-more-important-than-fixing-unicode](http://python-notes.curiousefficiency.org/en/latest/python3/questions_and_answers.html#but-but-surely-fixing-the-gil-is-more-important-than-fixing-unicode).
  prefs: []
  type: TYPE_NORMAL
- en: Finally, David Beazley has done some fascinating research on the performance
    of the GIL on multi-core systems. Two presentations of importance are available
    online. They give a good technical background, which is relevant to this chapter.
    These can be found at [http://pyvideo.org/video/353/pycon-2010--understanding-the-python-gil---82](http://pyvideo.org/video/353/pycon-2010--understanding-the-python-gil---82)
    and at [https://www.youtube.com/watch?v=5jbG7UKT1l4](https://www.youtube.com/watch?v=5jbG7UKT1l4).
  prefs: []
  type: TYPE_NORMAL
- en: A multithreaded echo server
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A benefit of the multithreading approach is that the OS handles the thread switches
    for us, which means we can continue to write our program in a procedural style.
    Hence we only need to make small adjustments to our server program to make it
    multithreaded, and thus, capable of handling multiple clients simultaneously.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new file called `1.3-echo_server-multi.py` and add the following code
    to it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: You can see that we've just imported an extra module and modified our main loop
    to run our `handle_client()` function in separate threads, rather than running
    it in the main thread. For each client that connects, we create a new thread that
    just runs the `handle_client()` function. When the thread blocks on a receive
    or send, the OS checks the other threads to see if they have come out of a blocking
    state, and if any have, then it switches to one of them.
  prefs: []
  type: TYPE_NORMAL
- en: Notice that we have set the `daemon` argument in the thread constructor call
    to `True`. This will allow the program to exit if we hit *ctrl* - *c* without
    us having to explicitly close all of our threads first.
  prefs: []
  type: TYPE_NORMAL
- en: If you try this echo server with multiple clients, then you'll see that a second
    client that connects and sends a message will immediately get a response.
  prefs: []
  type: TYPE_NORMAL
- en: Designing a chat server
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We've got a working echo server and it can handle multiple clients simultaneously,
    so we're pretty close to having a functional chat client. However, our server
    needs to broadcast the messages it receives to all the connected clients. Sounds
    simple, but there are two problems that we need to overcome to make this happen.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, our protocol needs an overhaul. If we think about what needs to happen
    from a client''s point of view, then we can no longer rely on the simple work
    flow:'
  prefs: []
  type: TYPE_NORMAL
- en: client connect > client send > server send > client disconnect.
  prefs: []
  type: TYPE_NORMAL
- en: Clients can now potentially receive messages at any time, and not just when
    they send a message to the server themselves.
  prefs: []
  type: TYPE_NORMAL
- en: Second, we need to modify our server to send messages to all of the connected
    clients. As we are using multiple threads to handle our clients, this means that
    we need to set up communication between the threads. With this, we're dipping
    our toe into the world of concurrent programming, and it should be approached
    with care and forethought. While the shared state of threads is useful, it is
    also deceptive in its simplicity. Having multiple threads of control asynchronously
    accessing and changing the same resources is a perfect breeding ground for race
    conditions and subtle deadlock bugs. While a full discussion on concurrent programming
    is well beyond the scope of this text, we'll cover some simple principles, which
    can help preserve your sanity.
  prefs: []
  type: TYPE_NORMAL
- en: A chat protocol
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The main purpose of our protocol update will be to specify that clients must
    be able to accept all messages that are sent to them, whenever they are sent.
  prefs: []
  type: TYPE_NORMAL
- en: In theory, one solution for this would be for our client itself to set up a
    listening socket, so that the server can connect to it whenever it has a new message
    to deliver. In the real world, this solution will rarely be applicable. Clients
    are almost always protected by some kind of firewall, which prevents any new inbound
    connections from connecting to the client. In order for our server to make a connection
    to a port on our client, we would need to ensure that any intervening firewalls
    are configured to allow our server to connect. This requirement would make our
    software much less appealing to most users since there are already chat solutions
    which don't require this.
  prefs: []
  type: TYPE_NORMAL
- en: If we can't assume that the server can connect to the client, then we need to
    meet our requirement by only using the client-initiated connection to the server.
    There are two ways in which we can do this. First, we can have our clients run
    in a disconnected state by default, then have them periodically connect to the
    server, download any waiting messages, and then disconnect again. Alternatively,
    we can have our clients connect to the server and then leave the connection open.
    They can then continuously listen on the connection and handle new messages sent
    by the server in one thread, while accepting user input and sending messages over
    the same connection in another thread.
  prefs: []
  type: TYPE_NORMAL
- en: You may recognize these scenarios as the **pull** and **push** options that
    are available in some e-mail clients. They are called pull and push because of
    how the operations appear to the client. The client either pulls data from the
    server, or the server pushes data to the client.
  prefs: []
  type: TYPE_NORMAL
- en: There are pros and cons to using either of the two approaches, and the decision
    depends on an application's needs. Pull results in a lower load on the server,
    but higher latency for the client in receiving messages. While this is fine for
    many applications, such as e-mail, in a chat server we usually expect immediate
    updates. While we could poll very frequently, this imposes unneeded load on the
    client, server, and network as the connections are repeatedly set up and torn
    down.
  prefs: []
  type: TYPE_NORMAL
- en: Push is better suited for a chat server. As the connection remains open continuously
    the amount of network traffic is limited to the initial connection setup, and
    the messages themselves. Also, the client gets new messages from the server almost
    immediately.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, we''ll use a push approach, and we will now write our chat protocol as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Communication will be conducted over TCP.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The client will initiate a chat session by creating a socket connection to the
    server.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The server will accept the connection, listen for any messages from the client,
    and accept them.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The client will listen on the connection for any messages from the server, and
    accept them.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The server will send any messages from the client to all the other connected
    clients.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Messages will be encoded in the UTF-8 character set for transmission, and they
    will be terminated by the null byte.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Handling data on persistent connections
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A new problem which our persistent connection approach raises is that we can
    no longer assume that our `socket.recv()` call will contain data from only one
    message. In our echo server, because of how we have defined the protocol, we know
    that as soon as we see a null byte, the message that we have received is complete,
    and that the sender won't be sending anything further. That is, everything we
    read in the last `socket.recv()` call is a part of that message.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our new setup, we''ll be reusing the same connection to send an indefinite
    number of messages, and these won''t be synchronized with the chunks of data that
    we will pull from each `socket.recv()`. Hence, it''s quite possible that the data
    from one `recv()` call will contain data from multiple messages. For example,
    if we send the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Then on the wire they will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Due to the vagaries of network transmission though, a set of successive `recv()`
    calls may receive them as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice that `recv 1` and `recv 2,` when taken together contain a complete message,
    but they also contain the beginning of the next message. Clearly, we need to update
    our parsing. One option is to read data from the socket one byte at a time, that
    is, use `recv(1)`, and check every byte to see if it''s a null byte. This is a
    dismally inefficient way to use a network socket though. We want to read as much
    data in our call to `recv()` as we can. Instead, when we encounter an incomplete
    message we can cache the extraneous bytes and use them when we next call `recv()`.
    Lets do this, add these functions to the `tincanchat.py` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: From now on, we'll be using `recv_msgs()` wherever we were using `recv_msg()`
    before. So, what are we doing here? Starting with a quick scan through `recv_msgs()`,
    you can see that it's similar to `recv_msg()`. We make repeated calls to `recv()`
    and accumulate the received data as before, but now we will be using `parse_recvd_data()`
    to parse it, with the expectation that it may contain multiple messages. When
    `parse_recvd_data()` finds one or more complete messages in the received data,
    it splits them into a list and returns them, and if there is anything left after
    the last complete message, then it additionally returns this using the `rest`
    variable. The `recv_msgs()` function then decodes the messages from UTF-8, and
    returns them and the `rest` variable.
  prefs: []
  type: TYPE_NORMAL
- en: The `rest` value is important because we will feed it back to `recv_msgs()`
    next time we call it, and it will be prefixed to the data from the `recv()` calls.
    In this way, the leftover data from the last `recv_msgs()` call won't be lost.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, in our preceding example, parsing the messages would take place as shown
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '| `recv_msgs call` | `data` argument | `recv` result | Accumulated `data` |
    `msgs` | `rest` |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | - | `''caerphil''` | `''caerphil''` | `[]` | `b''''` |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | - | `''ly\0illches''` | `''caerphilly\0illches''` | `[''caerphilly'']`
    | `''illches''` |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | `''illches''` | `''ter\0brie\0''` | `''illchester\0brie\0''` | `[''illchester'',
    ''brie'']` | `b''''` |'
  prefs: []
  type: TYPE_TB
- en: Here, we can see that the first `recv_msgs()` call doesn't return after its
    first iteration. It loops again because `msgs` is still empty. This is why the
    `recv_msgs` call numbers are 1, 1, and 2.
  prefs: []
  type: TYPE_NORMAL
- en: A multithreaded chat server
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So let''s put this to use and write our chat server. Make a new file called
    `2.1-chat_server-multithread.py` and put the following code in it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: We're now using two threads per client. One thread handles the messages received
    and the other thread handles the task of sending messages. The idea here is to
    break out each place a block might happen into its own thread. This will give
    us the lowest latency for each client, but it does come at the cost of system
    resources. We're reducing the potential number of clients that we may be able
    to handle simultaneously. There are other models that we could use, such as having
    a single thread for each client which receives messages and then sends them itself
    to all the connected clients, but I've chosen to optimize for latency.
  prefs: []
  type: TYPE_NORMAL
- en: To facilitate the separate threads, we've broken the receiving code and the
    sending code into the `handle_client_recv()` function and `handle_client_send()`
    function respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Our `handle_client_recv` threads are tasked with receiving messages from the
    clients, and our `handle_client_send` threads are tasked with sending messages
    to the clients, but how do the received messages get from the receive threads
    to the send threads? This is where the `queue`, `send_queue`, `dict` and `lock`
    objects come in.
  prefs: []
  type: TYPE_NORMAL
- en: Queues
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A `Queue` is a **first-in first-out** (**FIFO**) pipe. You add items to it by
    using the `put()` method, and pull them out by using the `get()` method. The important
    thing about `Queue` objects is that they are completely **thread safe**. Objects
    in Python are generally not thread safe unless it is explicitly specified in their
    documentation. Being thread safe means that operations on the object are guaranteed
    to be **atomic**, that is, they will always complete without any chance of another
    thread getting to that object and doing something unexpected to it.
  prefs: []
  type: TYPE_NORMAL
- en: Hang on, you might ask, earlier, didn't you say that because of the GIL the
    OS is running only one Python thread per process at any given moment in time?
    If that's so, then how could two threads perform an operation on an object simultaneously?
    Well, this is a fair question. Most operations in Python are, in fact, made up
    of many operations at the OS level, and it is at the OS level that threads are
    scheduled. A thread could start an operation on an object—say by appending an
    item to a `list`—and when the thread gets halfway through its OS level operations
    the OS could switch to another thread, which also starts appending to the same
    `list`. Since `list` objects provide no warranty of their behavior when abused
    like this by threads (they're not thread safe), anything could happen next, and
    it's unlikely to be a useful outcome. This situation can be called a **race condition**.
  prefs: []
  type: TYPE_NORMAL
- en: Thread safe objects remove this possibility, so they should absolutely be preferred
    for sharing state among threads.
  prefs: []
  type: TYPE_NORMAL
- en: Getting back to our server, the other useful behavior of `Queues` is that if
    `get()` is called on an empty `Queue`, then it will block until something is added
    to the `Queue`. We take advantage of this in our send threads. Notice, how we
    go into an infinite loop, with the first operation being a `get()` method call
    on a `Queue`. The thread will block there and patiently wait until something is
    added to its `Queue`. And, you've probably guessed it, our receive threads add
    the messages to the queues.
  prefs: []
  type: TYPE_NORMAL
- en: We create a `Queue` object for each send thread as it's being created and then
    we store the queues in the `send_queues` dict. For our receive threads to broadcast
    new messages, they just need to add the message to each `Queue` in `send_queues`,
    which we do in the `broadcast_msgs()` function. Our waiting send threads will
    then unblock, pick the message out of their `Queue` and then send it to their
    client.
  prefs: []
  type: TYPE_NORMAL
- en: We've also added a `handle_disconnect()` function, which gets called whenever
    a client disconnects or a socket error occurs. This function ensures that queues
    associated with closed connections are cleaned up, and that the socket is closed
    properly from the server end.
  prefs: []
  type: TYPE_NORMAL
- en: Locks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Contrast our use of the `Queues` object with our use of `send_queues`. `Dict`
    objects are not thread safe, and unfortunately there isn't a thread safe associative
    array type in Python. Since we need to share this `dict`, we need to take extra
    precautions whenever we access it, and this is where the `Lock` comes in. `Lock`
    objects are a type of **synchronization primitive**. These are special objects
    built with functionality to help manage our threads and ensure that they don't
    trip over each others' accesses.
  prefs: []
  type: TYPE_NORMAL
- en: A `Lock` is either locked or unlocked. A thread can lock a thread by either
    calling `acquire()` on it, or as in our program, using it as a context manager.
    If a thread has acquired a lock and another thread also tries to acquire the lock,
    then the second thread will block on the `acquire()` call until the first thread
    releases the lock or exits the context. There is no limit on the number of threads
    that can try to acquire a lock at once – all but the first will block. By wrapping
    all the accesses to a non-thread safe object with a lock, we can ensure that no
    two threads operate on the object at the same time.
  prefs: []
  type: TYPE_NORMAL
- en: So, every time we add or remove something from `send_queues`, we wrap it in
    a `Lock` context`.` Notice that we're also protecting `send_queues` when we iterate
    over it. Even though we're not changing it, we want to be sure that it doesn't
    get modified while we're working with it.
  prefs: []
  type: TYPE_NORMAL
- en: Although we're being careful and using locks and thread safe primitives, we're
    not protected against all possible thread related pitfalls. Since the thread synchronization
    mechanisms themselves block, it's still quite possible to create deadlocks, where
    two threads are simultaneously blocking on objects locked by the other thread.
    The best approach to managing thread communication is to keep all the accesses
    to your shared state restricted to as small an area of your code as you can. In
    the case of this server, this module could be reworked as a class providing a
    minimum number of public methods. It could also be documented such that it discourages
    the changing of any internal state. This will keep this chunk of threading strictly
    confined to this class.
  prefs: []
  type: TYPE_NORMAL
- en: A multithreaded chat client
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have a new, all receiving and broadcasting chat server, we just
    need a client to go with it. We have mentioned before that we will hit a problem
    with our procedural client when trying to listen for both network data and user
    input at the same time. Well, now that we have some idea of how to employ threads,
    we can have a go at addressing this. Create a new text file called `2.2-chat_client-multithread.py`
    and save the following code in it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: We've updated our client to honor our new chat protocol by creating a new thread
    to handle user input and send messages, while handling receiving messages in the
    main thread. This allows the client to deal with the user input and receive the
    messages at the same time.
  prefs: []
  type: TYPE_NORMAL
- en: Note that there's no shared state here, so we didn't have to get clever with
    `Queues` or synchronization primitives.
  prefs: []
  type: TYPE_NORMAL
- en: Let's give our new programs a try. Fire up the multithreaded chat server, and
    then launch at least two clients. If you can, run them in terminals such that
    you can watch all of them at once. Now, try and send some messages from the clients
    and see how they are sent to all of the other clients.
  prefs: []
  type: TYPE_NORMAL
- en: Event-driven servers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For many purposes threads are great, especially because we can still program
    in the familiar procedural, blocking-IO style. But they suffer from the drawback
    that they struggle when managing large numbers of connections simultaneously,
    because they are required to maintain a thread for each connection. Each thread
    consumes memory, and switching between threads incurs a type of CPU overhead called
    **context switching**. Although these aren't a problem for small numbers of threads,
    they can impact performance when there are many threads to manage. Multiprocessing
    suffers from similar problems.
  prefs: []
  type: TYPE_NORMAL
- en: An alternative to threading and multiprocessing is using the **event-driven**
    model. In this model, instead of having the OS automatically switch between active
    threads or processes for us, we use a single thread which registers blocking objects,
    such as sockets, with the OS. When these objects become ready to leave the blocking
    state, for example a socket receives some data, the OS notifies our program; our
    program can then access these objects in non-blocking mode, since it knows that
    they are in a state that is ready for immediate use. Calls made to objects in
    non-blocking mode always return immediately. We structure our application around
    a loop, where we wait for the OS to notify us of activity on our blocking objects,
    then we handle that activity, and then we go back to waiting. This loop is called
    the **event loop**.
  prefs: []
  type: TYPE_NORMAL
- en: This approach provides comparable performance to threading and multiprocessing,
    but without the memory or context switching overheads, and hence allows for greater
    scaling on the same hardware. The challenge of engineering applications that can
    efficiently handle very large numbers of simultaneous connections has historically
    been called the **c10k problem**, referring to the handling of ten-thousand concurrent
    connections in a single thread. With the help of event-driven architectures, this
    problem was solved, though the term is still often used to refer to the challenges
    of scaling when it comes to handling many concurrent connections.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: On modern hardware it's actually possible to handle ten-thousand concurrent
    connections using a multithreading approach as well, see this Stack Overflow question
    for some numbers [https://stackoverflow.com/questions/17593699/tcp-ip-solving-the-c10k-with-the-thread-per-client-approach](https://stackoverflow.com/questions/17593699/tcp-ip-solving-the-c10k-with-the-thread-per-client-approach).
  prefs: []
  type: TYPE_NORMAL
- en: The modern challenge is the "c10m problem", that is, ten million concurrent
    connections. Solving this involves some drastic software and even operating system
    architecture changes. Although this is unlikely to be manageable with Python any
    time soon, an interesting (though unfortunately incomplete) general introduction
    to the topic can be found at [http://c10m.robertgraham.com/p/blog-page.html](http://c10m.robertgraham.com/p/blog-page.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows the relationship of processes and threads in an
    event-driven server:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Event-driven servers](graphics/6008OS_08_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Although the GIL and the OS thread scheduler are shown here for completeness,
    in the case of an event-driven server, they have no impact on performance because
    the server only uses a single thread. The scheduling of I/O handling is done by
    the application.
  prefs: []
  type: TYPE_NORMAL
- en: A low-level event-driven chat server
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So the event-driven architecture has a few great benefits, the catch is that
    for a low-level implementation, we need to write our code in a completely different
    style. Let's write an event-driven chat server to illustrate this.
  prefs: []
  type: TYPE_NORMAL
- en: Note that this example will not at all work on Windows as Windows lacks the
    `poll` interface which we will be employing here. There is an older interface,
    called `select`, which Windows does support, however it is slower and more complicated
    to work with. The event-driven frameworks that we look at later do automatically
    switch to `select` for us though, if we're running on Windows.
  prefs: []
  type: TYPE_NORMAL
- en: There is a higher performance alternative to `poll` called `epoll`, available
    on Linux operating systems, however it also more complicated to use, so for simplicity
    we'll stick with `poll` here. Again, the frameworks we discuss later automatically
    take advantage of `epoll` if it is available.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, counter-intuitively, Python's `poll` interface lives in a module called
    `select`, hence we will import `select` in our program.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a file called `3.1-chat_server-poll.py` and save the following code
    in it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The crux of this program is the `poll` object, which we create at the start
    of execution. This is an interface for the kernel's poll service, which lets us
    register sockets for the OS to watch and notify us when they are ready for us
    work with them.
  prefs: []
  type: TYPE_NORMAL
- en: We register a socket by calling the `poll.register()` method, passing the socket
    as an argument along with the type of activity that we want the kernel to watch
    out for. There are several conditions which we can monitor by specifying various
    `select.POLL*` constants. We're using `POLLIN` and `POLLOUT` in this program to
    watch out for when a socket is ready to receive and send data respectively. Accepting
    a new incoming connection on our listening socket will be counted as a read.
  prefs: []
  type: TYPE_NORMAL
- en: Once a socket is registered with `poll`, the OS will watch it and record when
    the socket is ready to carry out the activity that we requested. When we call
    `poll.poll()`, it returns a list of all the sockets that have become ready for
    us to work with. For each socket, it also returns an `event` flag, which indicates
    the state of the socket. We can use this event flag to tell whether we can read
    from (`POLLIN` event) or write to the socket (`POLLOUT` event), or whether an
    error has occurred (`POLLHUP`, `POLLERR`, `POLLNVAL` events).
  prefs: []
  type: TYPE_NORMAL
- en: To make use of this, we enter our event loop, repeatedly calling `poll.poll()`,
    iterating through the ready objects it returns and operating on them as per their
    `event` flags.
  prefs: []
  type: TYPE_NORMAL
- en: 'Because we''re only running in a single thread, we don''t need any of the synchronization
    mechanisms which we had to employ in the multithreaded server. We''re just using
    a regular `dict` to keep track of our clients. If you''ve not come across it before,
    the `SimpleNamespace` object that we use in the `create_client()` function is
    just a new idiom for creating an empty object with a `__dict__` (this is needed
    because `Object` instances don''t have a `__dict__` so they won''t accept arbitrary
    attributes). Previously, we may have used the following to give us an object which
    we can assign arbitrary attributes to:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Python version 3.3 and later versions give us the new, more explicit `SimpleNamespace`
    object.
  prefs: []
  type: TYPE_NORMAL
- en: We can run our multithreaded client against this server. The server is still
    using the same network protocol, and the architecture of the two programs won't
    affect the communication. Give it a try and verify if it works as expected.
  prefs: []
  type: TYPE_NORMAL
- en: This style of programming, employing `poll` and non-blocking sockets, is often
    referred to as **non-blocking** and **asynchronous,** since we use sockets in
    non-blocking mode, and the thread of control handles I/O reactively, as it needs
    to happen, rather than locking to a single I/O channel until it's done. However,
    you should note that our program isn't completely non-blocking, since it still
    blocks on the `poll.poll()` call. This is pretty much inevitable in an I/O bound
    system because when nothing's happening, you've got to wait for the I/O activity
    somewhere.
  prefs: []
  type: TYPE_NORMAL
- en: Frameworks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you can see, writing servers using these lower level threading and `poll`
    APIs can be quite involved, especially considering that various things which would
    be expected in a production system, such as logging and comprehensive error handling,
    haven't been included in our examples due to brevity.
  prefs: []
  type: TYPE_NORMAL
- en: Many people have hit these problems before us, and several libraries and frameworks
    are available for taking some of the leg work out of writing the network servers.
  prefs: []
  type: TYPE_NORMAL
- en: An eventlet-based chat server
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `eventlet` library provides a high-level API for event-driven programming,
    but it does so in a style that mimics the procedural, blocking-IO style that we
    used in our multithreaded servers. The upshot is that we can effectively take
    our multithreaded chat server code, make a few minor modifications to it to use
    `eventlet` instead, and immediately gain the benefits of the event-driven model!
  prefs: []
  type: TYPE_NORMAL
- en: 'The `eventlet` library is available in PyPi, and it can be installed with `pip`,
    as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `eventlet` library automatically falls back to `select` if `poll` is not
    available, so it will run properly on Windows.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once it''s installed, create a new file called `4.1-chat_server-eventlet.py`
    and save the following code in it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: We can test this with our multithreaded client to ensure that it works as expected.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, it's pretty much identical to our multithreaded server, with
    a few changes made so as to use `eventlet`. Notice that we've removed the synchronization
    code and the `lock` around `send_queues`. We're still using queues, although they're
    the `eventlet` library's queues, because we want to retain the blocking behavior
    of `Queue.get()`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are more examples of using eventlet for programming on the eventlet site
    at [http://eventlet.net/doc/examples.html](http://eventlet.net/doc/examples.html).
  prefs: []
  type: TYPE_NORMAL
- en: An asyncio-based chat server
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `asyncio` Standard Library module is new in Python 3.4 and it is an effort
    at bringing some standardization around asynchronous I/O into the Standard Library.
    The `asyncio` library uses a co-routine based style of programming. It provides
    a powerful loop class, which our programs can submit prepared tasks, called co-routines,
    to, for asynchronous execution. The event loop handles the scheduling of the tasks
    and optimization of performance around blocking I/O calls.
  prefs: []
  type: TYPE_NORMAL
- en: 'It has built-in support for socket-based networking, which makes building a
    basic server a straightforward task. Let''s see how this can be done. Create a
    new file called `5.1-chat_server-asyncio.py` and save the following code in it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Again, we can test this with our multithreaded client to make sure that it works
    as we expect it to.
  prefs: []
  type: TYPE_NORMAL
- en: Let's step through the code, as it's quite different from our previous servers.
    We begin by defining our server behavior in a subclass of the `asyncio.Protocol`
    abstract class. We're required to override the three methods `connection_made()`,
    `data_received()`, and `connection_lost()`. By using this class we can instantiate
    a new server scheduled on the event loop, which will listen on a socket and behave
    according to the contents of these three methods. We perform this instantiation
    in the main section further down with the `loop.create_server()` call.
  prefs: []
  type: TYPE_NORMAL
- en: The `connection_made()` method is called when a new client connects to our socket,
    which is equivalent to `socket.accept()` receiving a connection. The `transport`
    argument that it receives is a writable stream object, that is, it is an `asyncio.WriteTransport`
    instance. We will use this to write data to the socket, so we hang on to it by
    assigning it to the `self.transport` attribute. We also grab the client's host
    and port by using `transport.get_extra_info('peername')`. This is the transport's
    equivalent of `socket.getpeername()`. We then set up a `rest` attribute to hold
    the leftover data from `tincanchat.parse_recvd_data()` calls, and then we add
    our instance to the global `clients` list so that the other clients can broadcast
    to it.
  prefs: []
  type: TYPE_NORMAL
- en: The `data_received()` method is where the action happens. This function is called
    every time the `Protocol` instance's socket receives any data. This is equivalent
    to `poll.poll()` returning a `POLLIN` event, and then us performing a `recv()`
    on the socket. When called, this method is passed the data that is received from
    the socket as the `data` argument, which we then parse using `tincanchat.parse_recvd_data()`,
    as we have done before.
  prefs: []
  type: TYPE_NORMAL
- en: We then iterate over any received messages, and for each one, send it to every
    client in the `clients` list by calling the `write()` method on the clients' transport
    objects. The important thing to note here is that the `Transport.write()` call
    is non-blocking and so returns immediately. The send just gets submitted to the
    event loop, to be scheduled for completion soon. Hence the broadcast itself completes
    quickly.
  prefs: []
  type: TYPE_NORMAL
- en: The `connection_lost()` method is called when the client disconnects or the
    connection is lost, which is equivalent to a `socket.recv()` returning an empty
    result, or a `ConnectionError`. Here, we just remove the client from the `clients`
    global list.
  prefs: []
  type: TYPE_NORMAL
- en: In the main module code we acquire an event loop, and then create an instance
    of our `Protocol` server. The call to `loop.run_until_complete()` runs the initialization
    phase of our server on the event loop, setting up the listening socket. Then we
    call `loop.run_forever()`, which starts our server listening for incoming connections.
  prefs: []
  type: TYPE_NORMAL
- en: More on frameworks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I've broken from our usual procedural form and used an object-oriented approach
    in the last example for two reasons. First, although it is possible to write a
    purely procedural style server with `asyncio`, it requires a deeper understanding
    of co-routines than what we were able to provide here. If you're curious, then
    you can go through an example co-routine style echo server, which is in the `asyncio`
    documentation at [https://docs.python.org/3/library/asyncio-stream.html#asyncio-tcp-echo-server-streams](https://docs.python.org/3/library/asyncio-stream.html#asyncio-tcp-echo-server-streams).
  prefs: []
  type: TYPE_NORMAL
- en: The second reason is that this kind of class-based approach is generally a more
    manageable model to follow in a full system.
  prefs: []
  type: TYPE_NORMAL
- en: There is in fact a new module called `selectors` in Python 3.4, which provides
    an API for quickly building an object-oriented server based on the IO primitives
    in the `select` module (including `poll`). The documentation and an example can
    be seen at [https://docs.python.org/3.4/library/selectors.html](https://docs.python.org/3.4/library/selectors.html).
  prefs: []
  type: TYPE_NORMAL
- en: There are other third-party event-driven frameworks available, popular ones
    are Tornado ([www.tornadoweb.org](http://www.tornadoweb.org)) and circuits ([https://github.com/circuits/circuits](https://github.com/circuits/circuits)).
    Both are worth investigating for comparison, if you intend to choose a framework
    for a project.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, no discussion of Python asynchronous I/O would be complete without
    a mention of the Twisted framework. Until Python 3, this has been the go to solution
    for any serious asynchronous I/O work. It is an event-driven engine, with support
    for a large number of network protocols, good performance, and a large and active
    community. Unfortunately, it hasn't finished the jump to Python 3 yet (a view
    of the migration progress can be seen at [https://rawgit.com/mythmon/twisted-py3-graph/master/index.html](https://rawgit.com/mythmon/twisted-py3-graph/master/index.html)).
    Since we're focused squarely on Python 3 in this book, we decided to not include
    a detailed treatment of it. However, once it does get there, Python 3 will have
    another very powerful asynchronous framework, which will be well worth investigating
    for your projects.
  prefs: []
  type: TYPE_NORMAL
- en: Taking our servers forward
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are a number of things that we can do to improve our servers. For multithreaded
    systems, it's common to have a mechanism for capping the number of threads in
    use at any one time. This can be done by keeping a count of the active threads
    and immediately closing any new incoming connections from clients while it's above
    a threshold.
  prefs: []
  type: TYPE_NORMAL
- en: For all our servers, we would also want to add a logging mechanism. I strongly
    recommend the standard library `logging` module for this, the documentation for
    this is complete and full of good examples. The basic tutorial is a good place
    to start if you've not used it before, and it can be found at [https://docs.python.org/3/howto/logging.html#logging-basic-tutorial](https://docs.python.org/3/howto/logging.html#logging-basic-tutorial).
  prefs: []
  type: TYPE_NORMAL
- en: We also want to handle errors more comprehensively. Since the intention is that
    our server should be long running with minimal intervention, we want to make sure
    that nothing less than a critical exception causes the process to exit. We also
    want to make sure that errors that occur when handling one client do not affect
    other connected clients.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally there are some basic features of chat programs that it may be fun to
    add: letting users enter a name, which would be shown beside their messages on
    the other clients; adding chat rooms; and adding TLS encryption to the socket
    connections to provide privacy and security.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We looked at how to develop network protocols while considering aspects such
    as the connection sequence, framing of the data on the wire, and the impact these
    choices will have on the architecture of the client and server programs.
  prefs: []
  type: TYPE_NORMAL
- en: We worked through different architectures for network servers and clients, demonstrating
    the differences between the multithreaded and event-driven models by writing a
    simple echo server and upgrading it to a multi-client chat server. We discussed
    performance issues around threaded and event-driven architectures. Finally, we
    looked at the `eventlet` and `asyncio` frameworks, which can greatly simplify
    the process of writing servers when using an event-driven approach.
  prefs: []
  type: TYPE_NORMAL
- en: In the next and final chapter of this book, we will look at bringing several
    threads of this book together for writing server-side web applications.
  prefs: []
  type: TYPE_NORMAL
