- en: Understanding Asynchronous Event-Driven Programming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '"The best way to predict the future is to invent it."'
  prefs: []
  type: TYPE_NORMAL
- en: – Alan Kay
  prefs: []
  type: TYPE_NORMAL
- en: Eliminating blocking processes through the use of event-driven, asynchronous
    I/O is Node's primary organizational principle. We've learned how this design
    helps developers in shaping information and adding capacity. Node lets you build
    and organize lightweight, independent, and share-nothing processes that communicate
    through callbacks and synchronize with a predictable event loop.
  prefs: []
  type: TYPE_NORMAL
- en: Accompanying the growth in the popularity of Node is a growth in the number
    of well-designed event-driven systems and applications. For a new technology to
    be successful, it must eliminate the existing problems, and/or offer to consumers
    a better solution at a lower cost in terms of time, effort, or price. In its young
    and fertile lifespan, the Node community has collaboratively proven that this
    new development model is a viable alternative to the existing technologies. The
    number and quality of Node-based solutions powering enterprise-level applications
    provides further proof that these new ideas are not only novel, but preferred.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will delve deeper into how Node implements event-driven
    programming. We will begin by unpacking the ideas and theories that event-driven
    languages and environments derive from and grapple with, in an effort to clear
    away misconceptions and encourage mastery. Following this introduction to events,
    we'll look at the key Node.js technology—the event loop. We'll then go into more
    detail on how Node implements timers, callbacks, and I/O events, and how you as
    a Node developer can use them. We'll further discuss management of concurrency
    using modern tools such as **Promises**, **Generators**, and **async/await**.
    We'll practice the theory as we build up some simple but exemplary file and data-driven
    applications. These examples highlight Node's strengths, and show how Node is
    succeeding in its ambition to simplify network application designs.
  prefs: []
  type: TYPE_NORMAL
- en: Node's unique design
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, let''s take an accurate look at the total time cost when your program
    asks the system to perform different kinds of services. I/O is expensive. In the
    following chart (taken from *Ryan Dahl*''s original presentation on Node), we
    can see how many clock cycles typical system tasks consume. The relative cost
    of I/O operations is striking:'
  prefs: []
  type: TYPE_NORMAL
- en: '| L1 cache  |    3 cycles |'
  prefs: []
  type: TYPE_TB
- en: '| L2 cache |   14 cycles |'
  prefs: []
  type: TYPE_TB
- en: '| RAM  | 250 cycles |'
  prefs: []
  type: TYPE_TB
- en: '| Disk   |   41,000,000 cycles |'
  prefs: []
  type: TYPE_TB
- en: '| Network   |   240,000,000 cycles |'
  prefs: []
  type: TYPE_TB
- en: 'The reasons are clear enough: a disk is a physical device, a spinning metal
    platter — storing and retrieving that data is much slower than moving data between
    solid-state devices (such as microprocessors and memory chips), or indeed optimized
    on-chip L1/L2 caches. Similarly, data does not move from point to point on a network
    instantaneously. Light itself needs 0.1344 seconds to circle the globe! In a network
    used by many billions of people regularly interacting across great distances at
    speeds much slower than the speed of light, with many detours and few straight
    lines, this sort of latency builds up.'
  prefs: []
  type: TYPE_NORMAL
- en: When our software ran on personal computers on our desks, little or no communication
    was happening over the network. Delays or hiccups in our interactions with a word
    processor or spreadsheet had to do with disk access time. Much work was done to
    improve disk access speeds. Data storage and retrieval became faster, software
    became more responsive, and users now expect this responsiveness in their tools.
  prefs: []
  type: TYPE_NORMAL
- en: With the advent of cloud computing and browser-based software, your data has
    left the local disk and exists on a remote disk, and you access this data via
    a network—the internet. Data access times have slowed down again, dramatically.
    Network I/O is slow. Nevertheless, more companies are migrating sections of their
    applications into the cloud, with some software being entirely network-based.
  prefs: []
  type: TYPE_NORMAL
- en: Node is designed to make I/O fast. It is designed for this new world of networked
    software, where data is in many places and must be assembled quickly. Many of
    the traditional frameworks to build web applications were designed at a time when
    a single user working on a desktop computer used a browser to periodically make
    HTTP requests to a single server running a relational database. Modern software
    must anticipate tens of thousands of simultaneously connected clients concurrently
    altering enormous, shared data pools via a variety of network protocols, on any
    number of unique devices. Node is designed specifically to help those building
    that kind of network software.
  prefs: []
  type: TYPE_NORMAL
- en: 'The breakthrough in thinking reflected by Node''s design is simple to understand
    once one recognizes that most worker threads spend their time waiting—for more
    instructions, a sub-task to complete, and so on. For example, a process assigned
    to service the command *format my hard drive* will dedicate all of its allotted
    resources to managing a workflow, something like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Communicate to a device driver that a format request has been made
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Idle, waiting for an *unknowable* length of time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Receive the signal format as complete
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Notify the client
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Clean up; shut down:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/654909f4-ef43-4a75-9199-e145e067c376.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding figure, we see that an expensive worker is charging the client
    a fixed fee per unit of time, regardless of whether any useful work is being done
    (the client is paying equally for activity and idleness). To put it another way,
    it is not necessarily true, and most often not true, that the sub-tasks comprising
    a total task each require similar effort or expertise. It's therefore wasteful
    to pay a premium price for such cheap labor.
  prefs: []
  type: TYPE_NORMAL
- en: Sympathetically, we must also recognize that this worker can do no better even
    if ready and able to handle more work — even the best-intentioned worker cannot
    do anything about I/O bottlenecks. The worker here is **I/O bound**.
  prefs: []
  type: TYPE_NORMAL
- en: Instead, imagine an alternative design. What if multiple clients could share
    the same worker, such that the moment a worker announces availability due to an
    I/O bottleneck, another job from another client can start?
  prefs: []
  type: TYPE_NORMAL
- en: Node has commoditized I/O through the introduction of an environment where system
    resources are (ideally) **never** idle. Event-driven programming as implemented
    by Node reflects the simple goal of lowering overall system costs by encouraging
    the sharing of expensive labor, mainly by reducing the number of I/O bottlenecks
    to **zero**. We no longer have a powerless chunk of rigidly-priced unsophisticated
    labor; we can reduce all effort into discrete units with precisely delineated
    shapes, and therefore admit much more accurate pricing.
  prefs: []
  type: TYPE_NORMAL
- en: What would an environment within which many client jobs are cooperatively scheduled
    look like? And how is this message passing between events handled? Additionally,
    what do concurrency, parallelism, asynchronous execution, callbacks, and events
    mean to the Node developer?
  prefs: []
  type: TYPE_NORMAL
- en: Collaboration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What would be preferable to the blocking system described previously is a collaborative
    work environment, where workers are regularly assigned new tasks to do, instead
    of idling. In order to achieve such a goal, what we need is a virtual switchboard,
    where requests for services are dispatched to available workers, and where workers
    notify the switchboard of their availability.
  prefs: []
  type: TYPE_NORMAL
- en: 'One way to achieve this goal is to have a pool of available labors, improving
    efficiency by delegating tasks to different workers as the tasks come in:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/57b70667-0b0f-44fa-a901-76806eda58ba.png)'
  prefs: []
  type: TYPE_IMG
- en: One drawback to this method is the amount of scheduling and worker surveillance
    that needs to be done. The dispatcher must field a steady stream of requests,
    while managing messages coming from workers about their availability, neatly breaking
    up requests into manageable tasks and efficiently sorting them, so that the fewest
    number of workers are idling.
  prefs: []
  type: TYPE_NORMAL
- en: Perhaps most importantly, what happens when all workers are fully booked? Does
    the dispatcher begin to drop requests from clients? Dispatching is resource-intensive
    as well, and there are limits even to the dispatcher's resources. If requests
    continue to arrive and no worker is available to service them, what does the dispatcher
    do? Manage a queue? We now have a situation where the dispatcher is no longer
    doing the right job (dispatching), and has become responsible for bookkeeping
    and keeping lists, further extending the time each task takes to complete. Each
    task takes some amount of time, and must be processed in arrival order. This task
    execution model stacks fixed time intervals — *ticks* of time. This is *synchronous* execution.
  prefs: []
  type: TYPE_NORMAL
- en: Queueing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to avoid overwhelming anyone, we might add a buffer between the clients
    and the dispatcher. This new worker is responsible for managing customer relations.
    Instead of speaking directly with the dispatcher, the client speaks to the services
    manager, passing the manager requests, and at some point in the future, getting
    a call that their task has been completed. Requests for work are added to a prioritized
    work queue (a stack of orders with the most important one on top), and this manager
    waits for another client to walk through the door.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure describes the situations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bcb39362-b03e-413f-9c99-a8e37f043fc4.png)'
  prefs: []
  type: TYPE_IMG
- en: The dispatcher tries to keep all workers busy by pulling tasks from this queue,
    passing back any packages workers have completed, and generally maintaining a
    sane work environment where nothing gets dropped or lost. Rather than proceeding
    task-by-task along a single timeline, multiple simultaneous jobs, on their own
    timelines, run in parallel. If it comes to a point where all the workers are idle
    and the task queue is empty, the office can sleep for a while, until the next
    client arrives.
  prefs: []
  type: TYPE_NORMAL
- en: This is a rough schematic of how Node gains speed by working *asynchronously,*
    rather than *synchronously*. Now, let's dive deeper into how Node's event loop
    works.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the event loop
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following three points are important to remember, as we break down the
    event loop:'
  prefs: []
  type: TYPE_NORMAL
- en: The event loop runs in the same (single) thread your JavaScript code runs in.
    Blocking the event loop means blocking the entire thread.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You don't start and/or stop the event loop. The event loop starts as soon as
    a process starts, and ends when no further callbacks remain to be performed. The
    event loop may, therefore, run forever.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The event loop delegates many I/O operations to `libuv`, which manages these
    operations (using the power of the OS itself, such as thread pools), notifying
    the event loop when results are available. An easy-to-reason-about single-threaded
    programming model is reinforced with the efficiency of multithreading.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For example, the following `while` loop will never terminate:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Even though one might expect, in approximately one second, the assignment of
    a Boolean `true` to the variable `stop`, tripping the `while` conditional and
    interrupting its loop; this will never happen. Why? This `while` loop starves
    the event loop by running infinitely, greedily checking and rechecking a value
    that is never given a chance to change, as the event loop is never given a chance
    to schedule our timer callback for execution. This proves the event loop (which
    manages timers), and runs on the same thread.
  prefs: []
  type: TYPE_NORMAL
- en: According to the Node documentation, "The event loop is what allows Node.js
    to perform non-blocking I/O operations — despite the fact that JavaScript is single-threaded
    — by offloading operations to the system kernel whenever possible." The key design
    choice made by Node's designers was the implementation of an event loop as a concurrency
    manager. For example, notifying your Node-based HTTP server of network connections
    to your local hardware is handled by the OS passing along, via `libuv`, network
    interface events.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following description of event-driven programming (taken from: [http://www.princeton.edu/~achaney/tmve/wiki100k/docs/Event-driven_programming.html](http://www.princeton.edu/~achaney/tmve/wiki100k/docs/Event-driven_programming.html))
    clearly not only describes the event-driven paradigm, but also introduces us to
    how events are handled in Node, and how JavaScript is an ideal language for such
    a paradigm.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In computer programming, event-driven programming or event-based programming
    is a programming paradigm in which the flow of the program is determined by events—that
    is, sensor outputs or user actions (mouse clicks, key presses) or messages from
    other programs or threads. Event-driven programming can also be defined as an
    application architecture technique in which the application has a main loop that
    is clearly divided down to two sections: the first is event selection (or event
    detection), and the second is event handling […]. Event-driven programs can be
    written in any language, although the task is easier in languages that provide
    high-level abstractions, such as closures. Visit [https://www.youtube.com/watch?v=QQnz4QHNZKc](https://www.youtube.com/watch?v=QQnz4QHNZKc) for
    more information.'
  prefs: []
  type: TYPE_NORMAL
- en: Node makes a single thread more efficient by delegating many blocking operations
    to OS subsystems to process, bothering the main V8 thread only when there is data
    available for use. The main thread (your executing Node program) expresses interest
    in some data (such as via `fs.readFile`) by passing a callback, and is notified
    when that data is available. Until that data arrives, no further burden is placed
    on V8's main JavaScript thread. How? Node delegates I/O work to `libuv`, as quoted
    at: [http://nikhilm.github.io/uvbook/basics.html#event-loops](http://nikhilm.github.io/uvbook/basics.html#event-loops).
  prefs: []
  type: TYPE_NORMAL
- en: In event-driven programming, an application expresses interest in certain events,
    and responds to them when they occur. The responsibility of gathering events from
    the operating system or monitoring other sources of events is handled by `libuv`,
    and the user can register callbacks to be invoked when an event occurs.
  prefs: []
  type: TYPE_NORMAL
- en: '*Matteo Collina* has created an interesting module for benchmarking the event
    loop, which is available at: [https://github.com/mcollina/loopbench](https://github.com/mcollina/loopbench).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of this program is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Here''s what Node does when executing this program:'
  prefs: []
  type: TYPE_NORMAL
- en: A process object is created in C++ using the V8 API. The Node.js runtime is
    then imported into this V8 process.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `fs` module is attached to the Node runtime. V8 exposes C++ to JavaScript.
    This provides access to native filesystem bindings for your JavaScript code.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `fs.readFile` method has passed instructions and a JavaScript callback.
    Through `fs.binding`, `libuv` is notified of the file read request, and is passed
    a specially prepared version of the callback sent by the original program.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`libuv` invokes the OS-level functions necessary to read a file.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The JavaScript program continues, printing `This happens first`. Because there
    is a callback outstanding, the event loop continues to spin, waiting for that
    callback to resolve.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When the file descriptor has been fully read by the OS, `libuv` (via internal
    mechanisms) is informed, and the callback passed to `libuv` is invoked, which
    essentially prepares the original JavaScript callback for re-entrance into the
    main (V8) thread.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The original JavaScript callback is pushed onto the event loop, and is invoked
    on a near-future tick of the loop.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The file contents are printed to the console.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As there are no further callbacks in flight, the process exits.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Here, we see the key ideas that Node implements to achieve fast, manageable,
    and scalable I/O. If, for example, there were 10 read calls made for `foo.js`
    in the preceding program, the execution time would, nevertheless, remain roughly
    the same. Each call will be managed by `libuv` as efficiently as possible (by,
    for example, parallelizing the calls using threads). Even though we wrote our
    code in JavaScript, we are actually deploying a very efficient multithreaded execution
    engine while avoiding the difficulties of OS asynchronous process management.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know how a filesystem operation might work, let's dig into how every
    type of asynchronous operation Node capable of spawning is treated on the event
    loop.
  prefs: []
  type: TYPE_NORMAL
- en: Event loop ordering, phases, and priorities
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The event loop proceeds through phases, and each phase has a queue of events
    to process. From the Node documentation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d3cdf6c5-7bf6-4a11-8fc0-fdba8cd39ffb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The phases relevant to developers are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Timers**: Callbacks deferred to some time in the future specified in milliseconds,
    such as `setTimeout` and `setInterval`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**I/O callbacks**: Prepared callbacks returned to the main thread after being
    delegated to Node''s managed thread pool, such as filesystem calls and network
    listeners'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Poll/check**: Mainly the functions slotted on the stack according to the
    rules of `setImmediate` and `nextTick`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When data becomes available on a socket or other stream interface, we cannot
    simply execute our callback immediately. JavaScript is single-threaded, so results
    must be synchronized. We can't suddenly change the state in the middle of an event
    loop tick — this would create some of the classic multithreaded application problems
    of race conditions, memory access conflicts, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'To learn more about how Node is bound to `libuv` and other core libraries,
    parse through the `fs` module code at: [https://github.com/nodejs/node/blob/master/lib/fs.js](https://github.com/nodejs/node/blob/master/lib/fs.js). Compare
    the `fs.read` and the `fs.readSync` methods to observe the difference between
    how synchronous and asynchronous actions are implemented; note the wrapper callback
    that is passed to the native `binding.read` method in `fs.read`. To take an even
    deeper dive into the very heart of Node''s design, including the queue implementation,
    read through the Node source at: [https://github.com/joyent/node/tree/master/src](https://github.com/joyent/node/tree/master/src).
    Follow `FSEventWrap` within `fs_event_wrap.cc`. Investigate the `req_wrap` class,
    a wrapper for the V8 engine, deployed in `node_file.cc` and elsewhere and defined
    in `req_wrap.h`.'
  prefs: []
  type: TYPE_NORMAL
- en: Upon entering an event loop, Node, in effect, makes a copy of the current instruction
    queue (also known as **stack**), empties the original queue, and executes its
    copy. The processing of this instruction queue is referred to as a **tick**. If
    `libuv`, asynchronously, receives results while the chain of instructions copied
    at the start of this tick are being processed on the single main thread (V8),
    these results (wrapped as callbacks) are queued. Once the current queue is emptied
    and its last instruction has completed, the queue is again checked for instructions
    to execute on the next tick. This pattern of checking and executing the queue
    will repeat (loop) until the queue is emptied, and no further data events are
    expected, at which point the Node process exits.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let's look at the event interfaces of Node.
  prefs: []
  type: TYPE_NORMAL
- en: Listening for events
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Modern network software, for various reasons, is growing in complexity and,
    in many ways, changing how we think about application development. Most new platforms
    and languages are attempting to address these changes. Node is no exception —
    and JavaScript is no exception.
  prefs: []
  type: TYPE_NORMAL
- en: Learning about Node means learning about event-driven programming, composing
    software out of modules, creating and linking data streams, and producing and
    consuming events and their related data. Node-based architectures are often composed
    of many small processes and/or services communicating with events — internally,
    by extending the `EventEmitter` interface and using callbacks, and externally,
    over one of several common transport layers (for example, HTTP, TCP), or through
    a thin messaging layer covering one of these transport layers (for example, 0MQ,
    Redis PUBSUB, and Kafka).
  prefs: []
  type: TYPE_NORMAL
- en: It is likely that these processes are composed of several free, open source,
    and high-quality npm modules, each distributed with unit tests and/or examples
    and/or documentation.
  prefs: []
  type: TYPE_NORMAL
- en: The previous chapter introduced you to the `EventEmitter` interface. This is
    the primary event interface we will be encountering as we move chapter to chapter,
    as it provides the prototype class for the many Node objects exposing evented
    interfaces, such as file and network streams. Various `close`, `exit`, `data`,
    and other events exposed by different module APIs signal the presence of an `EventEmitter`
    interface, and we will be learning about these modules and use cases as we progress.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, our goal is to discuss some lesser-known event sources: signals,
    child process communication, filesystem change events, and deferred execution.'
  prefs: []
  type: TYPE_NORMAL
- en: Signals
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Evented programming is like hardware interrupt programming. Interrupts do exactly
    what their name suggests. They use their ability to interrupt whatever a controller,
    or the CPU, or any other device is doing, demanding that their particular need
    be serviced immediately.
  prefs: []
  type: TYPE_NORMAL
- en: In fact, the Node process object exposes standard **Portable Operating System
    Interface (POSIX)** signal names, such that a Node process can subscribe to these
    system events.
  prefs: []
  type: TYPE_NORMAL
- en: As [http://en.wikipedia.org/wiki/POSIX_signal](http://en.wikipedia.org/wiki/POSIX_signal) defines, "A
    signal is a limited form of inter-process communication used in Unix, Unix-like,
    and other POSIX-compliant operating systems. It is an asynchronous notification
    sent to a process, or to a specific thread, within the same process in order to
    notify it of an event that occurred."
  prefs: []
  type: TYPE_NORMAL
- en: This is a very elegant and natural way to expose a Node process to operating
    system signal events. One might configure listeners to catch signals instructing
    a Node process to restart or update some configuration files, or simply clean
    up and shut down.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, the **SIGINT** signal is sent to a process when its controlling
    terminal detects a *Ctrl* + *C* (or equivalent) keystroke. This signal tells a
    process that an interrupt has been requested. If a Node process has bound a callback
    to this event, that function might log the request prior to terminating, do some
    other cleanup work, or even ignore the request:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output for `sigint.js`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This example starts a long interval, so Node doesn't exit with nothing else
    to do. When you send a *Ctrl* + *C* from your keyboard through the terminal controlling
    the process, Node gets the signal from the operating system. Your code has subscribed
    to that event, and Node runs your function.
  prefs: []
  type: TYPE_NORMAL
- en: Now, consider a situation in which a Node process is doing some ongoing work,
    such as parsing logs. It might be useful to be able to send that process a signal,
    such as update your configuration files, or restart the scan. You may want to
    send such signals from the command line. You might prefer to have another process
    do so — a practice known as **Inter-Process Communication** (IPC).
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a file named `ipc.js`, and type in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: As before, Node will wait for around 16 minutes before running the empty function,
    keeping the process open, so you'll have to *Ctrl *+ *C* to get your prompt back.
    Note that this works just fine even though here, we haven't subscribed to the
    SIGINT signal.
  prefs: []
  type: TYPE_NORMAL
- en: '`SIGUSR1` (and `SIGUSR2`) are user-defined signals, triggered by no specific
    action known to the operating system. They''re meant for custom functionality.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To send a command to a process, you must determine its **process ID.** With
    a PID in hand, you can address a process and communicate with it. If the PID assigned
    to `ipc.js` after being run through Node is `123`, then we can send that process
    a `SIGUSR1` signal using the `kill` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'A simple way to find the PID for a given Node process in UNIX is to search
    the system process list for the name of the program that says the process is running.
    If `ipc.js` is currently executing, its PID is found by entering the following
    command line in the console/terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '`ps aux | grep ipc.js`. Try it.'
  prefs: []
  type: TYPE_NORMAL
- en: Child processes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A fundamental part of Node's design is to create or fork processes when parallelizing
    execution or scaling a system, as opposed to creating a thread pool, for instance.
    We will be using these child processes in various ways throughout this book. Right
    now, the focus will be on understanding how to handle communication events between
    child processes.
  prefs: []
  type: TYPE_NORMAL
- en: 'To create a child process, require Node''s `child_process` module, and call
    the `fork` method. Pass the name of the program file the new process should execute:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: You can keep any number of subprocesses running with this method. On multicore
    machines, the operating system will distribute forked processes across the available
    hardware cores. Spreading Node processes across cores, even onto other machines,
    and managing IPC is one way to scale a Node application in a stable, understandable,
    and predictable way.
  prefs: []
  type: TYPE_NORMAL
- en: 'Extending the preceding example, we can now have the forking process (`parent`)
    send, and listen for, messages from the forked process (`child`). Here''s the
    code for `parent.js`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output for `parent.js`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Alongside that file, make another one and name it `lovechild.js`.  The code
    of the child in here can listen for messages and send them back up:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Don't run `lovechild.js` yourself; `--parent.js` will do that for you with fork!
  prefs: []
  type: TYPE_NORMAL
- en: 'Running `parent.js` should fork a child process and send that child a message.
    The child should respond in kind:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: With `parent.js` running, check your operating system's task manager. There
    will be two Node processes, not one, as there were with preceeding examples.
  prefs: []
  type: TYPE_NORMAL
- en: Another very powerful idea is to pass a network server an object to a child.
    This technique allows multiple processes, including the parent, to share the responsibility
    for servicing connection requests, spreading load across cores.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, the following program will start a network server, fork a child
    process, and pass the server reference from the parent down to the child:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'In addition to passing a message to a child process as the first argument to
    send, the preceding code also sends the server handle to itself as a second argument.
    Our child server can now help out with the family''s service business:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: This child process should print out the sent message to your console, and begin
    listening for connections, sharing the sent server handle.
  prefs: []
  type: TYPE_NORMAL
- en: Repeatedly connecting to this server at `localhost:8080` will result in either
    child-handled connection or parent-handled connection being displayed; two separate
    processes are balancing the server load. This technique, when combined with the
    simple inter-process messaging protocol discussed previously, demonstrates how
    *Ryan Dahl's* creation succeeds in providing an easy way to build scalable network
    programs.
  prefs: []
  type: TYPE_NORMAL
- en: We've connected two nodes with just a few lines of code.
  prefs: []
  type: TYPE_NORMAL
- en: We will discuss Node's new cluster module, which expands and simplifies the
    previously discussed technique in [Chapter 7](48e16668-9318-4577-b3c0-3f4dbce035d9.xhtml),
    *Using Multiple Processes*. If you are interested in how server handles are shared,
    visit the cluster documentation: [https://nodejs.org/dist/latest-v9.x/docs/api/cluster.html](https://nodejs.org/dist/latest-v9.x/docs/api/cluster.html)
  prefs: []
  type: TYPE_NORMAL
- en: File events
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most applications make some use of the filesystem, in particular, those that
    function as web services. As well, a professional application will likely log
    information about usage, cache pre-rendered data views, or make other changes
    to files and directory structures. Node allows developers to register for notifications
    on file events through the `fs.watch` method. The `watch` method broadcasts changed
    events on both files and directories.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `watch` method accepts three arguments, in order:'
  prefs: []
  type: TYPE_NORMAL
- en: The file or directory path being watched. If the file does not exist, an **ENOENT
    (no entity)** error will be thrown, so using `fs.exists` at some prior useful
    point is encouraged.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'An optional options object, including:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Persistent (Boolean default true): Node keeps processes alive, as long as there
    is *something to do*. Set this option to *false* to let Node close the process
    even if your code still has a file watcher watching.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Recursive (Boolean default false): Whether to automatically descend into subdirectories.
    Note: This is not consistently implemented across platforms. For this reason,
    and for performance reasons, you should explicitly control the file list you are
    watching, rather than randomly watching directories.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Encoding (String default `utf8`): Character encoding of passed filenames. You
    probably don''t need to change this.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `listener` function, which receives two arguments:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The name of the change event (one of rename or change)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The filename that was changed (important when watching directories)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This example will set up a watcher on itself, change its own filename, and
    exit:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Two lines, `rename` and the name of the original file, should have been printed
    to the console.
  prefs: []
  type: TYPE_NORMAL
- en: 'Close your watcher channel whenever you want to use code like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'It should be noted that `fs.watch` depends a great deal on how the host OS
    handles file events, and the Node documentation says this:'
  prefs: []
  type: TYPE_NORMAL
- en: '"The fs.watch API is not 100% consistent across platforms, and is unavailable
    in some situations."'
  prefs: []
  type: TYPE_NORMAL
- en: The author has had very good experiences with the module across many different
    systems, noting only that the filename argument is null in callbacks on OS X implementations.
    Different systems may also enforce case sensitivity, one way or the other. Nevertheless,
    be sure to run tests on your specific architecture — trust, but verify.
  prefs: []
  type: TYPE_NORMAL
- en: 'Alternatively, use a third-party package! If you encounter difficulties with
    a Node module, check npm for alternatives. Here, as a problem-fixing wrapper on
    top of `fs.watch`, consider *Paul Miller''s* *chokidar*. It is used as the file-watching
    tool for build systems like gulp, and in many other projects. Refer to: [https://www.npmjs.com/package/chokidar](https://www.npmjs.com/package/chokidar).'
  prefs: []
  type: TYPE_NORMAL
- en: Deferred execution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One occasionally needs to defer the execution of a function. Traditional JavaScript
    uses timers for this purpose, with the well-known `setTimeout` and `setInterval` functions.
    Node introduces another perspective on defers, primarily as means of controlling
    the order in which a callback executes in relation to I/O events, as well as timer
    events properly.
  prefs: []
  type: TYPE_NORMAL
- en: As we saw earlier, managing timers is one of the main jobs of Node's event loop.
    Two types of deferred event sources that give a developer the ability to schedule
    callback executions to occur either before, or after, the processing of queued
    I/O events are `process.nextTick` and `setImmediate`. Let's look at those now.
  prefs: []
  type: TYPE_NORMAL
- en: process.nextTick
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A method of the native Node process module, `process.nextTick` is similar to
    the familiar `setTimeout` method in which it delays execution of its callback
    function until some point in the future. However, the comparison is not exact;
    a list of all requested `nextTick` callbacks are placed at the head of the event
    queue, and is processed, in its entirety and in order, before I/O or timer events
    and after execution of the current script (the JavaScript code executing synchronously
    on the V8 thread).
  prefs: []
  type: TYPE_NORMAL
- en: The primary use of `nextTick` in a function is to postpone the broadcast of
    result events to listeners on the current execution stack until the caller has
    had an opportunity to register event listeners, giving the currently executing
    program a chance to bind callbacks to `EventEmitter.emit` events.
  prefs: []
  type: TYPE_NORMAL
- en: Think of this as a pattern to use wherever you want to create your own asynchronous
    behavior. For instance, imagine a lookup system that may either fetch from a cache,
    or pull fresh data from a data store. The cache is fast and doesn't need callbacks,
    while the data I/O call would need them.
  prefs: []
  type: TYPE_NORMAL
- en: The need for callbacks in the second case argues for emulation of the callback
    behavior, with `nextTick` in the first case. This allows a consistent API, improving
    clarity of implementation without burdening the developer with the responsibility
    of determining whether or not to use a callback.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code seems to set up a simple transaction; when an instance of
    `EventEmitter` emits a start event, log `Started` to the console:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: However, the result you might expect will not occur! The event emitter instantiated
    within `getEmitter` emits `start` previous to being returned, wrong-footing the
    subsequent assignment of a listener, which arrives a step late, missing the event
    notification.
  prefs: []
  type: TYPE_NORMAL
- en: 'To solve this race condition, we can use `process.nextTick`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: This code attaches the `on("start")` handler before Node gives us the `start`
    event, and works properly.
  prefs: []
  type: TYPE_NORMAL
- en: Erroneous code can recursively call `nextTick`, causing an unending loop of
    code to run. Note that unlike a recursive call to a function within a single turn
    of the event loop, doing this won't cause a stack overflow. Rather, it will starve
    the event loop, churn your process on the microprocessor, and could prevent your
    program from discovering the I/O that Node has finished.
  prefs: []
  type: TYPE_NORMAL
- en: setImmediate
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`setImmediate` is technically a member of the class of timers, along with `setInterval`
    and `setTimeout` . However, there is no sense of time associated with it — there
    is no *number of milliseconds* to wait for an argument to be sent.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This method is really more of a sibling to `process.nextTick`, differing in
    one very important way: while callbacks queued by `nextTick` will execute before
    I/O and timer events, callbacks queued by `setImmediate` will be called after
    I/O events.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The naming of these two methods is confusing: Node will actually run the function
    you give to `nextTick` before the one you pass to `setImmediate`.'
  prefs: []
  type: TYPE_NORMAL
- en: This method does reflect the standard behavior of timers in that its invocation
    will return an object that can be passed to `clearImmediate`, cancelling your
    request to run your function later on in the same way `clearTimeout` cancels timers
    set with `setTimeout`.
  prefs: []
  type: TYPE_NORMAL
- en: Timers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Timers are used to schedule events in the future. They are used when one seeks
    to delay the execution of some block of code until a specified number of milliseconds
    have passed, to schedule periodic execution of a particular function, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'JavaScript provides two asynchronous timers: `setInterval()` and `setTimeout()`.
    It is assumed that the reader is fully aware of how to set (and cancel) these
    timers, so very little time will be spent discussing the syntax. We''ll instead
    focus more on gotchas and less well-known details about timeouts and intervals.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The key takeaway will be this: when using timers, one should make no assumptions
    about the amount of actual time that will expire before the callback registered
    for this timer fires, or about the ordering of callbacks. Node timers are not
    interrupts. Timers simply promise to execute as close as possible to the specified
    time (though never before), beholden, as with every other event source, to event
    loop scheduling.'
  prefs: []
  type: TYPE_NORMAL
- en: 'At least one thing you may not know about timers-we are all familiar with the
    standard arguments to `setTimeout`: a callback function and timeout interval.
    Did you know that many additional arguments are passed to the `callback` function?
    `setTimeout(callback, time, [passArg1, passArg2…])`'
  prefs: []
  type: TYPE_NORMAL
- en: setTimeout
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Timeouts can be used to defer the execution of a function until some number
    of milliseconds into the future.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: One would expect that function `b` would execute after function `a`. However,
    this cannot be guaranteed — `a` may follow `b`, or the other way around.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, consider the subtle difference present in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The execution order of `a` and `b` are predictable in this case. Node essentially
    maintains an object map grouping callbacks with identical timeout lengths. *Isaac
    Schlueter*, a former leader of the Node project and now CEO of npm Inc., puts
    it in this way:'
  prefs: []
  type: TYPE_NORMAL
- en: As we can find on [https://groups.google.com/forum/#!msg/nodejs-dev/kiowz4iht4Q/T0RuSwAeJV0J](https://groups.google.com/forum/#!msg/nodejs-dev/kiowz4iht4Q/T0RuSwAeJV0J), "[N]ode
    uses a single low level timer object for each timeout value. If you attach multiple
    callbacks for a single timeout value, they'll occur in order, because they're
    sitting in a queue. However, if they're on different timeout values, then they'll
    be using timers in different threads, and are thus subject to the vagaries of
    the [CPU] scheduler."
  prefs: []
  type: TYPE_NORMAL
- en: The ordering of timer callbacks registered within an identical execution scope
    does not predictably determine the eventual execution order in all cases. Additionally,
    there exists a minimum wait time of one millisecond for a timeout. Passing a value
    of zero, -1, or a non-number will be translated into this minimum value.
  prefs: []
  type: TYPE_NORMAL
- en: To cancel a timeout, use `clearTimeout(timerReference)`.
  prefs: []
  type: TYPE_NORMAL
- en: setInterval
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One can think of many cases where being able to periodically execute a function
    would be useful. Polling a data source every few seconds and pushing updates is
    a common pattern. Running the next step in an animation every few milliseconds
    is another use case, as is collecting garbage. For these cases, `setInterval`
    is a good tool:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Every 100 milliseconds the sent callback function will execute, a process that
    can be cancelled with `clearInterval(intervalReference)`.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, as with `setTimeout`, this behavior is not always reliable. Importantly,
    if a system delay (such as some badly written blocking `while` loop) occupies
    the event loop for some period of time, intervals set prior and completing within
    that interim will have their results queued on the stack. When the event loop
    becomes unblocked and unwinds, all the interval callbacks will be fired in sequence,
    essentially immediately, losing any sort of timing delays they intended.
  prefs: []
  type: TYPE_NORMAL
- en: Luckily, unlike browser-based JavaScript, intervals are rather more reliable
    in Node, generally able to maintain expected periodicity in normal use scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: unref and ref
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A Node program does not stay alive without a reason to do so. A process will
    keep running for as long as there are callbacks still waiting to be processed.
    Once those are cleared, the Node process has nothing left to do, and it will exit.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, the following silly code fragment will keep a Node process running
    forever:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Even though the set callback function does nothing useful or interesting, it
    continues to be called. This is the correct behavior, as an interval should keep
    running until `clearInterval` is used to stop it.
  prefs: []
  type: TYPE_NORMAL
- en: There are cases of using a timer to do something interesting with external I/O,
    or some data structure, or a network interface, where once those external event
    sources stop occurring or disappear, the timer itself becomes unnecessary. Normally,
    one would trap that irrelevant state of a timer somewhere else in the program,
    and cancel the timer from there. This can become difficult or even clumsy, as
    an unnecessary tangling of concerns is now necessary, an added level of complexity.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `unref` method allows the developer to assert the following instructions:
    when this timer is the only event source remaining for the event loop to process,
    go ahead and terminate the process.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s test this functionality to our previous silly example, which will result
    in the process terminating rather than running forever:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Note that `unref` is a method of the opaque value returned when starting a timer,
    which is an object.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s add an external event source, a timer. Once that external source
    gets cleaned up (in about 100 milliseconds), the process will terminate. We send
    information to the console to log what is happening:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'You may return a timer to its normal behavior with `ref`, which will undo an
    `unref` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The listed process will continue indefinitely, as in our original silly example.
  prefs: []
  type: TYPE_NORMAL
- en: Snap quiz! After running the following code, what is the expected order of logged
    messages?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of this program is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s break the preceding code down:'
  prefs: []
  type: TYPE_NORMAL
- en: A, F, and I execute in the main program flow, and as such, they will have the
    first priority in the main thread. This is obvious; your JavaScript executes its
    instructions in the order they are written, including the synchronous execution
    of the emit callback.
  prefs: []
  type: TYPE_NORMAL
- en: With the main call stack exhausted, the event loop is now almost reading to
    process I/O operations. This is the moment when `nextTick` requests are honored,
    slotting in at the head of the event queue. This is when B is displayed.
  prefs: []
  type: TYPE_NORMAL
- en: The rest of the order should be clear. Timers and I/O operations will be processed
    next, (C, G, H) followed by the results of the `setImmediate` callback (E), always
    arriving after any I/O and timer responses are executed.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the long timeout (D) arrives, being a relatively far-future event.
  prefs: []
  type: TYPE_NORMAL
- en: Note that reordering the expressions in this program will not change the output
    order outside of possible reordering of the STAT results, which only implies that
    they have been returned from the thread pool in a different order, remaining as
    a group in the correct order as related to the event queue.
  prefs: []
  type: TYPE_NORMAL
- en: Concurrency and errors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Members of the Node community develop new packages and projects every day. Because
    of Node's evented nature, callbacks permeate these codebases. We've considered
    several of the key ways in which events might be queued, dispatched, and handled
    through the use of callbacks. Let's spend a little time outlining the best practices,
    in particular, about conventions for designing callbacks and handling errors,
    and discuss some useful patterns when designing complex chains of events and callbacks.
    In particular, let's look at the new Promise, Generator, and async/await patterns
    that you will see in this book, and other examples of modern Node code.
  prefs: []
  type: TYPE_NORMAL
- en: Managing concurrency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Simplifying control flows has been a concern of the Node community since the
    very beginning of the project. Indeed, this potential criticism was one of the
    very first anticipated by *Ryan Dahl*, who discussed it at length during the talk
    in which he introduced Node to the JavaScript developer community.
  prefs: []
  type: TYPE_NORMAL
- en: 'Because deferred code execution often requires the nesting of callbacks within
    callbacks, a Node program can sometimes begin to resemble a sideways pyramid,
    also known as *The Pyramid of Doom*. You''ve seen it: deeply nested code, 4 or
    5 or even more levels deep, curly braces everywhere. Apart from syntactical annoyances,
    you can also imagine that tracking errors across such a call stack might be difficult—if
    a callback at the third level throws, who is responsible for handling that error?
    The second level? Even if level 2 is reading a file and level 3 is querying a
    database? Does that make sense? It can be hard to make sense of asynchronous program
    flows.'
  prefs: []
  type: TYPE_NORMAL
- en: Callbacks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Luckily, Node creators agreed upon sane conventions on how to structure callbacks
    early on. It is important to follow this tradition. Deviation leads to surprises,
    sometimes very bad surprises, and in general, to do so automatically makes an
    API awkward, a characteristic other developers will rapidly tire of.
  prefs: []
  type: TYPE_NORMAL
- en: One is either returning a function result by executing a `callback`, handling
    the arguments received by a `callback`, or designing the signature for a `callback`
    within your API. Whichever situation is being considered, one should follow the
    convention relevant to that case.
  prefs: []
  type: TYPE_NORMAL
- en: The first argument returned to a `callback` function is any error message, preferably
    in the form of an error object. If no error is to be reported, this slot should
    contain a null value.
  prefs: []
  type: TYPE_NORMAL
- en: When passing a `callback` to a function, it should be assigned the last slot
    of the function signature. APIs should be consistently designed this way.
  prefs: []
  type: TYPE_NORMAL
- en: Any number of arguments may exist between the error and the `callback` slots.
  prefs: []
  type: TYPE_NORMAL
- en: To create an error object: `new Error("Argument must be a String!")`
  prefs: []
  type: TYPE_NORMAL
- en: Promises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Like some politicians, the Node core was against Promises before it was for
    them. *Mikeal Rogers*, in discussing why Promises were removed from the original
    Node core, makes a strong argument for why leaving feature development to the
    community leads to a stronger core product. You can view this discussion at: [https://web.archive.org/posts/broken-promises.html](https://web.archive.org/posts/broken-promises.html)
  prefs: []
  type: TYPE_NORMAL
- en: 'Promises have gained a very large following since then, and Node core has changed
    in response. Promises are essentially a replacement for the standard callback
    pattern seen everywhere in Node. Once, you might have written this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'If API was instead "Promisified" (recall `util.promisify` from the previous
    chapter?), your description of the preceding asynchronous control flow would be
    described using a Promise chain:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: This is at least a tighter syntax that reads a little more easily, with long
    chains of operations; however, there is much more going on here that is of value.
  prefs: []
  type: TYPE_NORMAL
- en: '`promiseProfile` references a Promise object. Promises only execute once, reaching
    either an error state (unfulfilled) or fulfilled state, where you can extract
    the last, immutable value via `then`, as we did with profile, previously. Of course,
    Promises can be assigned to a variable, and that variable can be passed around
    to as many consumers as you''d like, even prior to resolving. Since `then` is
    only called when there is a value available, whenever that may be, Promises are
    aptly named as promises of a future state.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Perhaps most importantly, Promises, unlike callbacks, are able to manage errors
    across many asynchronous actions. If you go back and look at the example callback
    code at the head of this section, you''ll see err parameters in each callback,
    reflecting the core error-first callback style of Node. Each of those error objects
    must be handled individually, so the preceding code would actually start to look
    more like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Observe how each error condition must be handled individually. In practice,
    developers would like to be responsible for "hand-rolling" a wrapper around this
    code, such as a `try...catch` block, which would, in some way, catch all errors
    in this logical unit and manage them in a centralized way.
  prefs: []
  type: TYPE_NORMAL
- en: 'With Promises, you get that for free. Any `catch` statement will catch any
    errors thrown by any `then` prior to it in the chain. This makes creating a common
    error handler a snap. Even more, Promises allows the execution chain to continue
    past an error. You can add the following to the previous Promise chain:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: In this way, Promises allows you to compose rather complex, asynchronous, logical
    flows in much less space, with limited indentation, where error handling is much
    easier to work with and values are immutable and exchangeable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another extremely useful feature of the Promise object is that these future-resolving
    states can be managed as a block. For instance, imagine that to fulfill a query
    for a user profile, you needed to make three database calls. Rather than chaining
    these calls which always run serially, one at a time in order, you might use `Promise.all`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Here, all three of the Promises will be triggered *simultaneously,* and will *run
    in parallel*. Running calls in parallel is, of course, much more efficient than
    running them serially. Also, `Promise.all` guarantees that the final thennable
    receives an array of results ordered to synchronize result position with caller
    position.
  prefs: []
  type: TYPE_NORMAL
- en: It would be good for you to familiarize yourself with the full Promise API,
    which you can read about at MDN: [https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Promise](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Promise)
  prefs: []
  type: TYPE_NORMAL
- en: Even though Promises are now native, there remains a "userland" module, bluebird,
    which continues to offer a compelling alternative Promises implementation, with
    added features and oftentimes faster execution speed. You can read more about
    bluebird here: [http://bluebirdjs.com/docs/api-reference.html](http://bluebirdjs.com/docs/api-reference.html).
  prefs: []
  type: TYPE_NORMAL
- en: async/await
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Rather than wrap fulfillment in a specialized data structure like a Promise
    with so many function blocks and parentheses and special contexts, why not simply
    make it so that asynchronous expressions can have their cake and eat it, too?
    These expressions do not block the process (asynchronous execution), but they
    nevertheless halt further execution of a program (synchronous) until resolved.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `await` operator is used to wait for a Promise. It only executes within
    an `async` function. The `async/await` concurrency modeling syntax has been available
    since Node 8.x. Here''s a demonstration of `async/await` being used to replicate
    the preceding `Promise.all` example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Nice, right? You'll note that `profile()` returned a Promise. An `async` function *always *returns
    a Promise, though as we see here, the function itself can return anything it would
    like.
  prefs: []
  type: TYPE_NORMAL
- en: 'Promises and `async`/`await` work together like old pals. Here is a recursive
    directory walker that demonstrates this teamwork:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: It's a testament to how terse the code is for this recursive directory walker,
    that it is only slightly longer than the setup code above it. Since `await` expects
    a Promise, which `Promise.all` will return, run through every file that the `readDir`
    Promise returns, and map each file to another awaited Promise that will handle
    any recursive descent into subdirectories, updating the accumulator where appropriate.
    Read like this, the `Promise.all((await readdir(dir)).map` construct reads not
    unlike a basic looping construct, where deep asynchronous recursion  is being
    modelled in a simple and easy-to-follow procedural, synchronous way.
  prefs: []
  type: TYPE_NORMAL
- en: 'A pure Promise drop-in replacement version might look like this, assuming the
    same dependencies as the `async`/`await` version:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Both versions are cleaner than what you would have with callbacks. The `async/await` version
    does take the best of both worlds, and creates a succinct representation resembling
    synchronous code, making it perhaps easier to follow and reason about.
  prefs: []
  type: TYPE_NORMAL
- en: Error handling with `async/await` is also quite easy, as it requires no special
    new syntax. With Promises and `catch`, there is a slight problem with synchronous
    code errors. Promises catch errors that occur in `then` blocks. If, for example,
    a third-party library your code is calling throws, that code is not wrapped by
    the Promise and that error *will not be caught by `catch`*.
  prefs: []
  type: TYPE_NORMAL
- en: 'With `async/await`, you can use the familiar `try...catch` statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: This avoids all problems with special error-catching constructs. This native,
    rock-solid method will catch anything that throws anywhere in the `try` block,
    regardless of whether execution is synchronous or not.
  prefs: []
  type: TYPE_NORMAL
- en: Generators and Iterators
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Generators are function execution contexts that can be paused and resumed. When
    you call a normal function, it will likely `return` a value; the function fully
    executes, then terminates. A Generator function will yield a value then stop but
    the function context of a Generator is not disposed of (as it is with normal functions).
    You can re-enter the Generator at a later point in time and pick up further results.
  prefs: []
  type: TYPE_NORMAL
- en: 'An example might help:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: A Generator is declared by marking it with an asterisk (`*`). On the first call
    to `threeThings`, we get don't get a result, but an Generator object.
  prefs: []
  type: TYPE_NORMAL
- en: Generators conform to the new JavaScript iteration protocols ([https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Iteration_protocols#iterator](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Iteration_protocols#iterator)),
    which for our purposes mean that a Generator object exposes a `next` method, which
    is used to pull out as many values from a Generator as it is willing to yield.
    This power comes from the fact that Generators implement the JavaScript Iteration
    protocol. So, what's an iterator?
  prefs: []
  type: TYPE_NORMAL
- en: As [https://developer.mozilla.org/en-US/docs/Web/JavaScript/Guide/Iterators_and_Generators](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Guide/Iterators_and_Generators) says,
  prefs: []
  type: TYPE_NORMAL
- en: '"An object is an iterator when it knows how to access items from a collection
    one at a time, while keeping track of its current position within that sequence.
    In JavaScript an iterator is an object that provides a next() method which returns
    the next item in the sequence. This method returns an object with two properties:
    done and value."'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can replicate the Generator example using just an Iterator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'You''ll note that the results are nearly identical with the Generator example,
    with one important difference we can see in the first result: an Iterator is simply
    an object with a next method. It must do all the work of maintaining its own internal
    state (tracking `idx` in the previous example). Generators are factories for Iterators;
    furthermore, they do all the work of maintaining and yielding their own state.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Descended from Iterators, Generators yield objects with two properties:'
  prefs: []
  type: TYPE_NORMAL
- en: '**done** : A Boolean. If true, the Generator is indicating that it has nothing
    left to `yield`. If you were to think of Generators as streams (not a bad parallel),
    then you might compare this pattern to the pattern of `Readable.read()` returning
    null when a stream has ended (or if you prefer, the way a `Readable` will push
    null when finished).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**value**: The value of the last `yield`. Should be ignored if `done` is true.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Generators are designed for iterative contexts, not unlike a loop, providing
    the powerful advantage of a function execution context. You may have written something
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: This is fine, but there are downsides, such as needing to create a local reference
    to an external data provider and maintaining that reference when this block or
    function terminates. Do we make `state` a global? Should it be immutable? If the
    underlying data changes, for example, a new element is added to the array, how
    do we make sure `state` is updated, disconnected as it is from the true state
    of our application? What if something accidentally overwrites `state`? Data observation
    and binding libraries exist, design theories exist, frameworks exist to properly
    encapsulate your data sources and inject immutable versions into execution contexts;
    but what if there was a better way?
  prefs: []
  type: TYPE_NORMAL
- en: 'Generators can contain and manage their own data and `yield` the right answer
    even through change. We can implement the previous code with Generators:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: The Generator method handles all the "boilerplate" for sending back a value,
    and naturally encapsulates the state. But there doesn't seem to be a significant
    advantage here. This is because we are using a Generator to execute iterations
    that run sequentially and immediately. Generators are really for situations when
    a series of values are promised, with individual values being generated only when
    requested, over time. Rather than processing an array all at once and in order,
    what we really want to create is a sequential chain of communicating processes,
    each process "tick" calculating a result with visibility into previous process
    results.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: You can pass arguments to Generators. We create a `range` state machine by passing
    range bounds, where further calls to the machine will cause an internal state
    change, and yield the current state representation to the caller. While for demonstration
    purposes we use the `for...of` method of traversing Iterators (and therefore Generators),
    this sequential processing (which blocks the main thread until it is finished)
    can be made *asynchronous*.
  prefs: []
  type: TYPE_NORMAL
- en: The run/halt (not run/stop) design of Generators means that we can think of
    iteration not as running through a list, but of capturing a set of transition
    events over time. This idea is central to the idea of **Reactive Programming**
    ([https://en.wikipedia.org/wiki/Reactive_programming](https://en.wikipedia.org/wiki/Reactive_programming)),
    for example. Let's think through another example where this particular advantage
    of Generators can be displayed.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many other things you can do with these sorts of data structures.
    It might be helpful to think this way: Generators are to a sequence of future
    values as Promises are to a single future value. Both Promises and Generators
    can be passed around the instant they are generated (even if some eventual values
    are still resolving, or haven''t yet been queued for resolution), with one getting
    values via the `next()` interface, and the other via the `then()` interface.'
  prefs: []
  type: TYPE_NORMAL
- en: Errors and exceptions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Generally in programming, the terms *error* and *exception* are often used interchangeably.
    Within the Node environment, these two concepts are not identical. Errors and
    exceptions are different. Additionally, the definition of error and exception
    within Node does not necessarily align with similar definitions in other languages
    and development environments.
  prefs: []
  type: TYPE_NORMAL
- en: Conventionally, an **error** condition in a Node program is a non-fatal condition
    that should be caught and handled, seen most explicitly in the *Error as first
    argument* convention displayed by the typical Node callback pattern. An **exception**
    is a serious error (a system error) that a sane environment should not ignore
    or try to handle.
  prefs: []
  type: TYPE_NORMAL
- en: 'One comes across four common error contexts in Node, and should respond predictably:'
  prefs: []
  type: TYPE_NORMAL
- en: '**A synchronous context**: This will normally happen in the context of a function,
    where a bad call signature or another non-fatal error is detected. The function
    should simply return an error object; `new Error(…)`, or some other consistent
    indicator that the function call has failed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**An asynchronous context**: When expected to respond by firing a `callback`
    function, the execution context should pass an `Error` object, with appropriate
    message, as the first argument to that `callback`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**An event context**: Quoting the Node documentation: *"When an `EventEmitter`
    instance experiences an error, the typical action is to emit an error event. Error
    events are treated as a special case in node. If there is no listener for it,
    then the default action is to print a stack trace and exit the program."* Use
    events where events are expected.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**A Promise context**: A Promise throws or is otherwise rejected, and this
    error is caught within a `.catch` block. Important note: you should *always* reject
    Promises with true `Error` objects. *Petka Antonov*, author of the popular B*luebird*
    Promises implementation, discusses why: [https://github.com/petkaantonov/bluebird/blob/master/docs/docs/warning-explanations.md](https://github.com/petkaantonov/bluebird/blob/master/docs/docs/warning-explanations.md)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clearly, these are situations where an error is caught in a controlled manner,
    prior to it destabilizing the entire application. Without falling too far into
    defensive coding, an effort should be made to check inputs and other sources for
    errors, and properly dismiss them.
  prefs: []
  type: TYPE_NORMAL
- en: 'An additional benefit of always returning a proper `Error` object is access
    to the stack property of that object. The error stack shows the provenance of
    an error, each link in the chain of function, and calls the function that led
    to the error. A typical `Error.stack` trace would look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, the stack is always available via the `console.trace` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: It should be clear how this information aids in debugging, helping to ensure
    that the logical flow of our application is sound.
  prefs: []
  type: TYPE_NORMAL
- en: A normal stack trace truncates after a dozen or so levels. If longer stack traces
    are useful to you, try *Matt Insler's* **longjohn**: [https://github.com/mattinsler/longjohn](https://github.com/mattinsler/longjohn)
  prefs: []
  type: TYPE_NORMAL
- en: As well, run and examine the `js/stacktrace.js` file in your bundle for some
    ideas on how stack information might be used when reporting errors, or even test
    results.
  prefs: []
  type: TYPE_NORMAL
- en: Exception handling is different. Exceptions are unexpected or fatal errors that
    have destabilized the application. These should be handled with care; a system
    in an exception state is unstable, with indeterminate future states, and should
    be gracefully shut down and restarted. This is the smart thing to do.
  prefs: []
  type: TYPE_NORMAL
- en: 'Typically, exceptions are caught in `try`/`catch` blocks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Peppering a codebase with `try`/`catch` blocks and trying to anticipate all
    errors can become unmanageable and unwieldy. Additionally, what if an exception
    you didn't anticipate, an uncaught exception, occurs? How do you pick up from
    where you left off?
  prefs: []
  type: TYPE_NORMAL
- en: 'Node does not have a standard built-in way to handle uncaught critical exceptions.
    This is a weakness of the platform. An exception that is uncaught will continue
    to bubble up through the execution stack until it hits the event loop where, like
    a wrench in the gears of a machine, it will take down the entire process. The
    best we have is to attach an `uncaughtException` handler to the process itself:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: While nothing that follows our exception code will execute, the timeout will
    still fire, as the process managed to catch the exception, saving itself. However,
    this is a very clumsy way of handling exceptions. The `domain` module aimed to
    fix this hole in Node's design, but it has since been deprecated. Properly handling
    and reporting errors remains a real weakness in the Node platform. Work continues
    by the core team to address this problem: [https://nodejs.org/en/docs/guides/domain-postmortem/](https://nodejs.org/en/docs/guides/domain-postmortem/)
  prefs: []
  type: TYPE_NORMAL
- en: 'Recently, a similar mechanism was introduced to catch runaway Promises, which
    occur when you do not attach a catch handler to your Promise chain:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: The `unhandledRejection` handler is fired whenever a Promise is rejected and
    no error handler is attached to the Promise within one turn of the event loop.
  prefs: []
  type: TYPE_NORMAL
- en: Considerations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Any developer is regularly making decisions with a far-reaching impact. It
    is very hard to predict all the possible consequences resulting from a new bit
    of code, or a new design theory. For this reason, it may be useful to keep the
    shape of your code simple, and to force yourself to consistently follow the common
    practices of other Node developers. These are some guidelines you may find useful,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Generally, try to aim for shallow code. This type of refactoring is uncommon
    in non-evented environments. Remind yourself of it by regularly re-evaluating
    entry and exit points, and shared functions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consider building your systems using distinct, composable microservices, which
    we'll discuss in [Chapter 9](c8e13bc3-e661-441c-9fbc-bfdf6019f5f8.xhtml), *Microservices*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Where possible, provide a common context for `callback` re-entry. Closures are
    very powerful tools in JavaScript, and by extension, Node, as long as the context
    frame length of the enclosed callbacks is not excessive.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Name your functions. In addition to being useful in deeply recursive constructs,
    debugging code is much easier when a stack trace contains distinct function names,
    as opposed to anonymous.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Think hard about priorities. Does the order, in which a given result arrives
    or a `callback` is executed, actually matter? More importantly, does it matter
    in relation to I/O operations? If so, consider `nextTick` and `setImmediate`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consider using finite state machines for managing your events. State machines
    are surprisingly under-represented in JavaScript codebases. When a `callback`
    re-enters program flow, it has likely changed the state of your application, and
    the issuing of the asynchronous call itself is a likely indicator that state is
    about to change.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a Twitter feed using file events
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's apply what we've learned. The goal is to create a server that a client
    can connect to and receive updates from Twitter. We will first create a process
    to query Twitter for any messages with the hashtag `#nodejs`, and write any found
    messages to a `tweets.txt` file in 140-byte chunks. We will then create a network
    server that broadcasts these messages to a single client. Those broadcasts will
    be triggered by write events on the `tweets.txt` file. Whenever a write occurs,
    140-byte chunks are asynchronously read from the last-known client read pointer.
    This will happen until we reach the end of the file, broadcasting as we go. Finally,
    we will create a simple `client.html` page, which asks for, receives, and displays
    these messages.
  prefs: []
  type: TYPE_NORMAL
- en: 'While this example is certainly contrived, it demonstrates:'
  prefs: []
  type: TYPE_NORMAL
- en: Listening to the filesystem for changes, and responding to those events
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using data stream events for reading and writing files
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Responding to network events
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using timeouts for polling state
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using a Node server itself as a network event broadcaster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To handle server broadcasting, we are going to use the **Server Sent Events**
    (**SSE**) protocol, a new protocol being standardized as part of HTML5.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''re first going to create a Node server that listens for changes on a file
    and broadcasts any new content to the client. Open your editor and create a file
    `server.js`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'We will be accepting a single user connection, whose pointer will be `theUser`.
    The `userPos` will store the last position this client read from in `tweetFile`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Create an HTTP server listening on port `8080`, which will listen for and handle
    a single connection, storing the `response` argument, representing the pipe connecting
    the server to the client. The `response` argument implements the writable stream
    interface, allowing us to write messages to the client:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'We create a function to send the client messages. We will be pulling buffers
    of 140 bytes out of the readable stream bound to our `tweets.txt` file, incrementing
    our file position counter by one on each read. We write this buffer to the writable
    stream binding our server to the client. When done, we queue up a repeat call
    of the same function using `nextTick,` repeating until we get an error, receive
    no data, or the client disconnects:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: Finally, we start the process by opening the `tweets.txt` file and watching
    for any changes, calling `sendNext` whenever new tweets are written. When we start
    the server, there may not yet exist a file to read from, so we poll using `setTimeout`
    until one exists.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a server looking for file changes to broadcast, we need to
    generate data. We first install the **TWiT** Twitter package for Node, via **npm.**
  prefs: []
  type: TYPE_NORMAL
- en: 'We then create a process whose sole job is to write new data to a file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: To use this example, you will need a Twitter Developer account. Alternatively,
    there is also the option of changing the relevant code to simply write random
    140-byte strings to `tweets.txt: require("crypto").randomBytes(70).toString('hex'):`
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: This establishes a stream pointer to the same file that our server will be watching.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will be writing to this file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Because Twitter messages are never longer than 140 bytes, we can simplify the
    read/write operation by always writing 140-byte chunks, even if some of that space
    is empty. Once we receive updates, we will create a buffer that is *number of
    messages* x 140 bytes wide, and write those 140-byte chunks to this buffer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: We now create a function that will be asked every 10 seconds to check for messages
    containing the hashtag `#nodejs`. Twitter returns an array of message objects.
    The one object property we are interested in is the `#text` of the message. Calculate
    the number of bytes necessary to represent these new messages (140 x message count),
    fetch a clean buffer, and fill it with 140-byte chunks until all messages are
    written. Finally, this data is written to our `tweets.txt` file, causing a change
    event to occur that our server is notified of.
  prefs: []
  type: TYPE_NORMAL
- en: 'The final piece is the client page itself. This is a rather simple page, and
    how it operates should be familiar to the reader. The only thing to note is the
    use of SSE that listens to port `8080` on localhost. It should be clear how, on
    receipt of a new tweet from the server, a list element is added to the unordered
    list container `#list`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: To read more about SSE, refer to [Chapter 6](7c71fdd2-8060-4363-9ab3-1c5cdc24c8cf.xhtml),
    *Creating Real-time Applications*,
  prefs: []
  type: TYPE_NORMAL
- en: 'or you can visit: [https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events).'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Programming with events is not always easy. The control and context switches,
    defining the paradigm, often confound those new to evented systems. This seemingly
    reckless loss of control and the resulting complexity drives many developers away
    from these ideas. Students in introductory programming courses normally develop
    a mindset in which program flow can be dictated, where a program whose execution
    flow does not proceed sequentially from A to B can bend understanding.
  prefs: []
  type: TYPE_NORMAL
- en: By examining the evolution of the architectural problems, Node is now attempting
    to solve for network applications—in terms of scaling and code organization, in
    general terms of data and complexity volume, in terms of state awareness, and
    in terms of well-defined data and process boundaries. We learned how managing
    these event queues can be done intelligently. We saw how different event sources
    are predictably stacked for an event loop to process, and how far-future events
    can enter and reenter contexts using closures and smart callback ordering. We
    also learned about the newer Promise, Generator, and async/await structures designed
    to help with managing concurrency.
  prefs: []
  type: TYPE_NORMAL
- en: We now have a basic domain understanding of the design and characteristics of
    Node, in particular, how evented programming is done using it. Let's now move
    on to larger, more advanced applications of this knowledge.
  prefs: []
  type: TYPE_NORMAL
