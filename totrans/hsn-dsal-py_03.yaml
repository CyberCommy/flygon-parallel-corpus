- en: Principles of Algorithm Design
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Why do we study algorithm design? There are, of course, many reasons, and our
    motivations for learning something is very much dependent on our own circumstances.
    There are, without a doubt, important professional reasons for being interested
    in algorithm design. Algorithms are the foundation of all computing. We can think
    of a computer as being a piece of hardware, with a hard drive, memory chips, processors,
    and so on. However, the essential component, the thing that, if missing, would
    render modern technology impossible, is algorithms. Let's learn more about it
    in the upcoming sections.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will look at the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: An introduction to algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recursion and backtracking
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Big O notation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will need to install the `matplotlib` library with Python to plot the diagram
    in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'It can be installed on Ubuntu/Linux by running the following commands on the
    terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also use the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'To install `matplotlib` on Windows:'
  prefs: []
  type: TYPE_NORMAL
- en: If Python is already installed on the Windows operating system, `matplotlib`
    can be obtained from the following link to install it on Windows: [https://github.com/matplotlib/matplotlib/downloads](https://github.com/matplotlib/matplotlib/downloads)
    or [https://matplotlib.org](https://matplotlib.org).
  prefs: []
  type: TYPE_NORMAL
- en: Code files for this chapter can be found at: [https://github.com/PacktPublishing/Hands-On-Data-Structures-and-Algorithms-with-Python-Second-Edition/tree/master/Chapter03](https://github.com/PacktPublishing/Hands-On-Data-Structures-and-Algorithms-with-Python-Second-Edition/tree/master/Chapter03).
  prefs: []
  type: TYPE_NORMAL
- en: An introduction to algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The theoretical foundation of algorithms, in the form of the Turing machine,
    was established several decades before digital logic circuits could actually implement
    such a machine. The Turing machine is essentially a mathematical model that, using
    a predefined set of rules, translates a set of inputs into a set of outputs. The
    first implementations of Turing machines were mechanical and the next generation
    may likely see digital logic circuits replaced by quantum circuits or something
    similar. Regardless of the platform, algorithms play a central predominant role.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another aspect is the effect algorithms have on technological innovation. As
    an obvious example, consider the page rank search algorithm, a variation of which
    the Google Search engine is based on. Using this and similar algorithms allows
    researchers, scientists, technicians, and others to quickly search through vast
    amounts of information extremely quickly. This has a massive effect on the rate
    at which new research can be carried out, new discoveries made, and new innovative
    technologies developed. An algorithm is a sequential set of instructions to execute
    a particular task. They are very important, as we can break a complex problem
    into a smaller one to prepare simple steps to execute a big problem—that is the
    most important part of algorithms. A good algorithm is key for an efficient program
    to solve a specific problem. The study of algorithms is also important because
    it trains us to think very specifically about certain problems. It can help to
    increase our problem-solving abilities by isolating the components of a problem
    and defining relationships between these components. In summary, there are some
    important reasons for studying algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: They are essential for computer science and *intelligent* systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They are important in many other domains (computational biology, economics,
    ecology, communications, ecology, physics, and so on)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They play a role in technology innovation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They improve problem-solving and analytical thinking
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are mainly two important aspects to solve a given problem. Firstly, we
    need an efficient mechanism to store, manage, and retrieve the data, which is
    important to solve a problem (this comes under data structures); secondly, we
    require an efficient algorithm which is a finite set of instructions to solve
    that problem. Thus, the study of data structures and algorithms is key to solving
    any problem using computer programs. An efficient algorithm should have the following
    characteristics:'
  prefs: []
  type: TYPE_NORMAL
- en: It should be as specific as possible
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It should have each instruction properly defined
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There should not be any ambiguous instruction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All the instructions of the algorithm should be executable in a finite amount
    of time and in a finite number of steps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It should have clear input and output to solve the problem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each instruction of the algorithm should be important in solving the given problem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Algorithms, in their simplest form, are just a sequence of actions—a list of
    instructions. It may just be a linear construct of the form do *x*, then do *y*,
    then do *z*, then finish. However, to make things more useful we add clauses to
    the effect of do *x* then do *y*; in Python, these are if-else statements. Here,
    the future course of action is dependent on some conditions; say the state of
    a data structure. To this, we also add the operation, iteration, the while, and
    the for statements. Expanding our algorithmic literacy further, we add recursion.
    Recursion can often achieve the same results as iteration, however, they are fundamentally
    different. A recursive function calls itself, applying the same function to progressively
    smaller inputs. The input of any recursive step is the output of the previous
    recursive step.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm design paradigms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In general, we can discern three broad approaches to algorithm design. They
    are:'
  prefs: []
  type: TYPE_NORMAL
- en: Divide and conquer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Greedy algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dynamic programming
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As the name suggests, the divide and conquer paradigm involves breaking a problem
    into smaller simple sub-problems, and then solving these sub-problems, and finally,
    combining the results to obtain a global optimal solution. This is a very common
    and natural problem-solving technique, and is, arguably, the most commonly used
    approach to algorithm design. For example, merge sort is an algorithm to sort
    a list of n natural numbers increasingly.
  prefs: []
  type: TYPE_NORMAL
- en: In this algorithm, we divide the list iteratively in equal parts until each
    sub-list contains one element, and then we combine these sub-lists to create a
    new list in a sorted order. We will be discussing merge sort in more detail later
    in this section/chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some examples of divide and conquer algorithm paradigms are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Binary search
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Merge sort
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quick sort
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Karatsuba algorithm for fast multiplication
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Strassen's matrix multiplication
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Closest pair of points
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Greedy algorithms often involve optimization and combinatorial problems. In
    greedy algorithms, the objective is to obtain the best optimum solution from many
    possible solutions in each step, and we try to get the local optimum solution
    which may eventually lead us to obtain the overall optimum solution. Generally,
    greedy algorithms are used for optimization problems. Here are many popular standard
    problems where we can use greedy algorithms to obtain the optimum solution:'
  prefs: []
  type: TYPE_NORMAL
- en: Kruskal's minimum spanning tree
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dijkstra's shortest path
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Knapsack problem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prim's minimal spanning tree algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Travelling salesman problem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Greedy algorithms often involve optimization and combinatorial problems; the
    classic example is to apply the greedy algorithm to the traveling salesperson
    problem, where a greedy approach always chooses the closest destination first.
    This shortest-path strategy involves finding the best solution to a local problem
    in the hope that this will lead to a global solution.
  prefs: []
  type: TYPE_NORMAL
- en: Another classic example is to apply the greedy algorithm   to the traveling
    salesperson problem; it is an NP-hard problem. In this problem, a greedy approach
    always chooses the closest unvisited city first from the current city; in this
    way, we are not sure that we get the best solution, but we surely get an optimal
    solution. This shortest-path strategy involves finding the best solution to a
    local problem in the hope that this will lead to a global solution.
  prefs: []
  type: TYPE_NORMAL
- en: The dynamic programming approach is useful when our sub-problems overlap. This
    is different from divide and conquer. Rather than breaking our problem into independent
    sub-problems, with dynamic programming, intermediate results are cached and can
    be used in subsequent operations. Like divide and conquer, it uses recursion;
    however, dynamic programming allows us to compare results at different stages.
    This can have a performance advantage over the divide and conquer for some problems
    because it is often quicker to retrieve a previously calculated result from memory
    rather than having to recalculate it. Dynamic programming also uses recursion to
    solve the problems. For example, the matrix chain multiplication problem can be
    solved using dynamic programming. The matrix chain multiplication problem determines
    the best effective way to multiply the matrices when a sequence of matrices is
    given, it finds the order of multiplication that requires the minimum number of
    operations.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let''s look at three matrices—*P*, *Q*, and *R*. To compute the
    multiplication of these three matrices, we have many possible choices (because
    the matrix multiplication is associative), such as *(PQ)R = P(QR)*. So, if the
    sizes of these matrices are—*P* is a 20 × 30, *Q* is 30 × 45, *R* is 45 x 50,
    then, the number of multiplications for *(PQ)R* and *P(QR)* will be:'
  prefs: []
  type: TYPE_NORMAL
- en: '*(PQ)R* = 20 x 30 x 45 + 20 x 45 x 50 = 72,000'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*P(QR)* =  20 x 30 x 50 + 30 x 45 x 50 = 97,500'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It can be observed from this example that if we multiply using the first option,
    then we would need 72,000 multiplications, which is less when compared to the
    second option/ This is shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[Chapter 13](fff4acae-cc26-4b4c-a6d1-454703fa9e67.xhtml), *Design Techniques
    and Strategies*, presents a more detailed discussion on the algorithm design strategy.'
  prefs: []
  type: TYPE_NORMAL
- en: Recursion and backtracking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Recursion is particularly useful for divide and conquer problems; however,
    it can be difficult to understand exactly what is happening, since each recursive
    call is itself spinning off other recursive calls. A recursive function can be
    in an infinite loop, therefore, it is required that each recursive function adhere
    to some properties. At the core of a recursive function are two types of cases:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Base cases**: These tell the recursion when to terminate, meaning the recursion
    will be stopped once the base condition is met'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Recursive cases**: The function calls itself and we progress towards achieving
    the base criteria'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A simple problem that naturally lends itself to a recursive solution is calculating
    factorials. The recursive factorial algorithm defines two cases: the base case
    when *n* is zero (the terminating condition), and the recursive case when *n* is
    greater than zero (the call of the function itself). A typical implementation
    is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'To calculate the factorial of `4`, we require four recursive calls plus the
    initial parent call. On each recursion, a copy of the method variables is stored
    in memory. Once the method returns it is removed from memory. The following is
    a way we can visualize this process:'
  prefs: []
  type: TYPE_NORMAL
- en: 'It may not necessarily be clear if recursion or iteration is a better solution
    to a particular problem; after all, they both repeat a series of operations and
    both are very well-suited to divide and conquer approaches and to algorithm design.
    Iteration churns away until the problem is done with. Recursion breaks the problem
    down into smaller and smaller chunks and then combines the results. Iteration
    is often easier for programmers, because control stays local to a loop, whereas
    recursion can more closely represent mathematical concepts such as factorials.
    Recursive calls are stored in memory, whereas iterations are not. This creates
    a trade-off between processor cycles and memory usage, so choosing which one to
    use may depend on whether the task is processor or memory intensive. The following
    table outlines the key differences between recursion and iteration:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Recursion** | **Iteration** |'
  prefs: []
  type: TYPE_TB
- en: '| The function calls itself. | A set of instructions are executed repeatedly
    in the loop. |'
  prefs: []
  type: TYPE_TB
- en: '| It stops when the termination condition is met. | It stops execution when the loop
    condition is met. |'
  prefs: []
  type: TYPE_TB
- en: '| Infinite recursive calls may give an error related to stack overflow. | An
    infinite iteration will run indefinitely until the hardware is powered. |'
  prefs: []
  type: TYPE_TB
- en: '| Each recursive call needs memory space. | Each iteration does not require
    memory storage. |'
  prefs: []
  type: TYPE_TB
- en: '| The code size, in general, is comparatively smaller. | The code size, in
    general, is comparatively smaller. |'
  prefs: []
  type: TYPE_TB
- en: '| Recursion is generally slower than iteration. | It is faster as it does not
    require a stack. |'
  prefs: []
  type: TYPE_TB
- en: Backtracking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Backtracking is a form of recursion that is particularly useful for types of
    problems such as traversing tree structures, where we are presented with a number
    of options for each node, from which we must choose one. Subsequently, we are
    presented with a different set of options, and depending on the series of choices
    made, either a goal state or a dead end is reached. If it is the latter, we must
    backtrack to a previous node and traverse a different branch. Backtracking is
    a divide and conquer method for exhaustive searching. Importantly, backtracking
    **prunes** branches that cannot give a result.
  prefs: []
  type: TYPE_NORMAL
- en: 'An example of backtracking is given next. Here, we have used a recursive approach
    to generate all the possible arrangements of a given string, `s`, of a given length, `n`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This generates the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/933bcc38-2e75-47b4-917e-7d5ee731f5b7.png)'
  prefs: []
  type: TYPE_IMG
- en: Notice the double list compression and the two recursive calls within this comprehension.
    This recursively concatenates each element of the initial sequence, returned when
    *n* =1, with each element of the string generated in the previous recursive call.
    In this sense, it is *backtracking* to uncover previously ungenerated combinations.
    The final string that is returned is all *n* letter combinations of the initial
    string.
  prefs: []
  type: TYPE_NORMAL
- en: Divide and conquer – long multiplication
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For recursion to be more than just a clever trick, we need to understand how
    to compare it to other approaches, such as iteration, and to understand when its
    use will lead to a faster algorithm. An iterative algorithm that we are all familiar
    with is the procedure we learned in primary math classes, and is used to multiply
    two large numbers. That is long multiplication. If you remember, long multiplication
    involved iterative multiplying and carry operations followed by a shifting and
    addition operation.
  prefs: []
  type: TYPE_NORMAL
- en: Our aim here is to examine ways to measure how efficient this procedure is and
    attempt to answer the question—is this the most efficient procedure we can use
    for multiplying two large numbers together?
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following diagram, we can see that multiplying two four-digit numbers
    together requires 16 multiplication operations, and we can generalize and say
    that an *n* digit number requires, approximately, *n*^(*2*) multiplication operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/0bc9b9a7-2672-436c-b651-f1d56260339c.png)'
  prefs: []
  type: TYPE_IMG
- en: This method of analyzing algorithms, in terms of the number of computational
    primitives such as multiplication and addition, is important because it gives
    us a way to understand the relationship between the time it takes to complete
    a certain computation and the size of the input to that computation. In particular,
    we want to know what happens when the input, the number of digits, *n*, is very
    large. This topic, called **asymptotic analysis**, or **time complexity**, is
    essential to our study of algorithms and we will revisit it often during this
    chapter and the rest of this book.
  prefs: []
  type: TYPE_NORMAL
- en: The recursive approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It turns out that in the case of long multiplication the answer is yes, there
    are in fact several algorithms for multiplying large numbers that require less
    operations. One of the most well-known alternatives to long multiplication is
    the **Karatsuba algorithm**, first published in 1962\. This takes a fundamentally
    different approach: rather than iteratively multiplying single-digit numbers,
    it recursively carries out multiplication operations on progressively smaller
    inputs. Recursive programs call themselves on smaller subsets of the input. The
    first step in building a recursive algorithm is to decompose a large number into
    several smaller numbers. The most natural way to do this is to simply split the
    number into two halves, the first half of most-significant digits, and a second
    half of least-significant digits. For example, our four-digit number, 2345, becomes
    a pair of two-digit numbers, 23 and 45\. We can write a more general decomposition
    of any two *n* digit numbers, *x*, and *y* using the following, where *m* is any
    positive integer less than *n*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/a3c99940-655a-414f-81bf-3f12983cecde.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](Images/b3b0c275-29d8-4f19-a38d-f3b935180c21.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So now we can rewrite our multiplication problem *x*, *y* as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/feea5e6e-0ba8-42af-820e-c61603ca563e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'When we expand, we get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/31fa81b7-71ca-471f-a4a5-8d92229fc993.png)'
  prefs: []
  type: TYPE_IMG
- en: 'More conveniently, we can write it like this (equation 3.1):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/3eecd139-882e-4cf6-bfb1-1d89f560583b.png)                       
      ... (3.1)'
  prefs: []
  type: TYPE_IMG
- en: 'Where:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/d46fac6b-adc9-4d6e-aeb8-efdf3c003ddd.png)'
  prefs: []
  type: TYPE_IMG
- en: It should be pointed out that this suggests a recursive approach to multiplying
    two numbers since this procedure does itself involve multiplication. Specifically,
    the products *ac*, *ad*, *bc*, and *bd* all involve numbers smaller than the input
    number and so it is conceivable that we could apply the same operation as a partial
    solution to the overall problem. This algorithm, so far, consists of four recursive
    multiplication steps and it is not immediately clear if it will be faster than
    the classic long multiplication approach.
  prefs: []
  type: TYPE_NORMAL
- en: 'What we have discussed so far in regards to the recursive approach to multiplication,
    has been well-known to mathematicians since the late nineteenth century. The Karatsuba
    algorithm improves on this by making the following observation. We really only
    need to know three quantities: *z*[*2*]= *ac*, *z*[*1*]*=ad +bc*, and *z*[*0*]=
    *bd* to solve equation 3.1\. We need to know the values of *a*, *b*, *c*, and
    *d* only in so far as they contribute to the overall sum and products involved
    in calculating the quantities *z*[*2*], *z*[*1*], and *z*[*0*]. This suggests
    the possibility that perhaps we can reduce the number of recursive steps. It turns
    out that this is indeed the situation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Since the products *ac* and *bd* are already in their simplest form, it seems
    unlikely that we can eliminate these calculations. We can, however, make the following
    observation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/88959a91-37a3-4a23-93c4-083e43baa17a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'When we subtract the quantities *ac* and *bd*, which we have calculated in
    the previous recursive step, we get the quantity we need, namely (*ad + bc*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/4d502318-8937-4c95-a555-5be65f889dce.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This shows that we can indeed compute the sum of *ad + bc* without separately
    computing each of the individual quantities. In summary, we can improve on equation
    3.1 by reducing four recursive steps to three. These three steps are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Recursively calculate *ac*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Recursively calculate *bd*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Recursively calculate (*a + b*)(*c + d*) and subtract *ac* and *bd*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following example shows a Python implementation of the Karatsuba algorithm.
    In the following code, initially, we see if any one of the given numbers is less
    than 10, then there is no need to run recursive functions. Next, we identify the
    number of digits in the larger value, and add one if the number of digits is odd.
    Finally, we recursively call the function three times to calculate *ac*, *bd*,
    and (*a + d*)(*c + d*). The following code prints the multiplication of any two
    digits; for example, it prints `4264704` for the multiplication of `1234` and
    `3456`. The implementation of the Karatsuba algorithm is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Runtime analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The performance of an algorithm is generally measured by the size of its input
    data (**n**) and the time and the memory space used by the algorithm. **Time**
    required is measured by the key operations to be performed by the algorithm (such
    as comparison operations), whereas the space requirements of an algorithm is measured
    by the storage needed to store the variables, constants, and instructions during
    the execution of the program. The space requirements of an algorithm may also
    change dynamically during execution as it depends on variable size, which is to
    be decided at runtime, such as dynamic memory allocation, memory stacks, and so
    on.
  prefs: []
  type: TYPE_NORMAL
- en: The running time required by an algorithm depends on the input size; as the
    input size (**n**) increases, the runtime also increases. For example, a sorting
    algorithm will have more running time to sort the list of input size 5,000 as
    compared to the other list of input size 50\. Therefore, it is clear that to compute
    the time complexity, the input size is important. Further, for a specific input,
    the running time depends on the key operations to be executed in the algorithm.
    For example, the key operation for a sorting algorithm is a **comparison operation**
    that will take most of the time as compared to assignment or any other operation.
    The more is the number of key operations to be executed, the longer it will take
    to run the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'It should be noted that an important aspect to algorithm design is gauging
    the efficiency both in terms of space (memory) and time (number of operations).
    It should be mentioned that an identical metric is used to measure an algorithm''s
    memory performance. There are a number of ways we could, conceivably, measure
    runtime and probably the most obvious way is to simply measure the total time
    taken by the algorithm. The major problem with this approach is that the time
    taken for an algorithm to run is very much dependent on the hardware it is run
    on. A platform-independent way to gauge an algorithm''s runtime is to count the
    number of operations involved. However, this is also problematic as there is no
    definitive way to quantify an operation. This is dependent on the programming
    language, the coding style, and how we decide to count operations. We can use
    this idea, though, of counting operations, if we combine it with the expectation
    that as the size of the input increases the runtime will increase in a specific
    way. That is, there is a mathematical relationship between *n*, the size of the
    input, and the time it takes for the algorithm to run. There are essentially three
    things that characterize an algorithm''s runtime performance; these can be described
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Worst-case complexity is the upper-bound complexity; it is the maximum running
    time required for an algorithm to execute. In this case, the key operations would
    be executed the maximum number of times.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Best-case complexity is the lower-bound complexity; it is the minimum running
    time required for an algorithm to execute. In this case, the key operations would
    be executed the minimum number of times.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Average-case complexity is the average running time required for an algorithm
    to execute.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Worst-case analysis is useful because it gives us a tight upper bound that our
    algorithm is guaranteed not to exceed. Ignoring small constant factors, and lower-order
    terms, is really just about ignoring the things that, at large values of the input
    size, *n*, do not contribute, in a large degree, to the overall run time. Not
    only does this make our work mathematically easier, but it also allows us to focus
    on the things that are having the most impact on performance.
  prefs: []
  type: TYPE_NORMAL
- en: We saw with the Karatsuba algorithm that the number of multiplication operations
    increased to the square of the size, *n*, of the input. If we have a four-digit
    number the number of multiplication operations is 16; an eight-digit number requires
    64 operations. Typically, though, we are not really interested in the behavior
    of an algorithm at small values of *n*, so we most often ignore factors that increase
    at slower rates, say linearly with *n*. This is because at high values of *n*,
    the operations that increase the fastest as we increase *n *will dominate.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will explain this in more detail with an example: the merge sort algorithm.
    Sorting is the subject of [Chapter 10](3b546628-5e98-41b9-a0a8-066c907061c3.xhtml),
    *Sorting*, however, as a precursor and as a useful way to learn about runtime
    performance, we will introduce merge sort here.'
  prefs: []
  type: TYPE_NORMAL
- en: The merge sort algorithm is a classic algorithm developed over 60 years ago.
    It is still used widely in many of the most popular sorting libraries. It is relatively
    simple and efficient. It is a recursive algorithm that uses a divide and conquer
    approach. This involves breaking the problem into smaller sub-problems, recursively
    solving them, and then somehow combining the results. Merge sort is one of the
    most obvious demonstrations of the divide and conquer paradigm.
  prefs: []
  type: TYPE_NORMAL
- en: 'The merge sort algorithm consists of three simple steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Recursively sort the left half of the input array
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Recursively sort the right half of the input array
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Merge two sorted sub-arrays into one
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'A typical problem is sorting a list of numbers into a numerical order. Merge
    sort works by splitting the input into two halves and working on each half in
    parallel. We can illustrate this process schematically with the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the Python code for the merge sort algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We run this program for the following results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/d2f8424e-f06e-47ff-ac59-5a0d32a0f06f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The problem that we are interested in is how we determine the runtime performance,
    that is, what is the rate of growth in the time it takes for the algorithm to
    complete relative to the size of *n*? To understand this a bit better, we can
    map each recursive call onto a tree structure. Each node in the tree is a recursive
    call working on progressively smaller sub-problems:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/40e4b2b0-3c6a-4630-8f5d-643da4b7210c.png)'
  prefs: []
  type: TYPE_IMG
- en: Each invocation of merge sort subsequently creates two recursive calls, so we
    can represent this with a binary tree. Each of the child nodes receives a subset
    of the input. Ultimately, we want to know the total time it takes for the algorithm
    to complete relative to the size of *n*. To begin with, we can calculate the amount
    of work and the number of operations at each level of the tree.
  prefs: []
  type: TYPE_NORMAL
- en: Focusing on the runtime analysis, at level one, the problem is split into two
    *n*/2 sub-problems; at level two, there are four *n*/4 subproblems, and so on.
    The question is, when does the recursion bottom out, that is, when does it reach
    its base case? This is simply when the array is either zero or one.
  prefs: []
  type: TYPE_NORMAL
- en: The number of recursive levels is exactly the number of times you need to divide
    *n* by two until you get a number that is at most one. This is precisely the definition
    of log2\. Since we are counting the initial recursive call as level zero, the
    total number of levels is log[2]*n* + 1.
  prefs: []
  type: TYPE_NORMAL
- en: Let's just pause to refine our definitions. So far, we have been describing
    the number of elements in our input by the letter *n*. This refers to the number
    of elements in the first level of the recursion, that is, the length of the initial
    input. We are going to need to differentiate between the size of the input at
    subsequent recursive levels. For this, we will use the letter *m* or specifically
    *m*[*j*] for the length of the input at recursive level *j.*
  prefs: []
  type: TYPE_NORMAL
- en: Also, there are a few details we have overlooked, and I am sure you are beginning
    to wonder about. For example, what happens when *m*/2 is not an integer, or when
    we have duplicates in our input array? It turns out that this does not have an
    important impact on our analysis here; we will revisit some of the finer details
    of the merge sort algorithm in [Chapter 12](fff4acae-cc26-4b4c-a6d1-454703fa9e67.xhtml),
    *Design Techniques and Strategies*.
  prefs: []
  type: TYPE_NORMAL
- en: The advantage of using a recursion tree to analyze algorithms is that we can
    calculate the work done at each level of the recursion. How we define this work
    is simply by the total number of operations and this, of course, is related to
    the size of the input. It is important to measure and compare the performance
    of algorithms in a platform-independent way. The actual runtime will, of course,
    be dependent on the hardware on which it is run. Counting the number of operations
    is important because it gives us a metric that is directly related to an algorithm's
    performance, independent of the platform.
  prefs: []
  type: TYPE_NORMAL
- en: In general, since each invocation of merge sort is making two recursive calls,
    the number of calls is doubling at each level. At the same time, each of these
    calls is working on an input that is half of its parents. We can formalize this
    and say that for level *j*, where *j* is an integer *0, 1, 2 ... log[2]n*, there
    are two sub-problems each of size *n/2^j*.
  prefs: []
  type: TYPE_NORMAL
- en: To calculate the total number of operations, we need to know the number of operations
    encompassed by a single merge of two sub-arrays. Let's count the number of operations
    in the previous Python code. What we are interested in is all the code after the
    two recursive calls have been made. Firstly, we have the three assignment operations.
    This is followed by three `while` loops. In the first loop, we have an if-else
    statement and within each of our two operations, a comparison followed by an assignment.
    Since there are only one of these sets of operations within the if-else statements,
    we can count this block of code as two operations carried out *m* times. This
    is followed by two `while` loops with an assignment operation each. This makes
    a total of *4m + 3* operations for each recursion of merge sort.
  prefs: []
  type: TYPE_NORMAL
- en: Since *m* must be at least one, the upper bound for the number of operations
    is 7*m*. It has to be said that this has no pretence at being an exact number.
    We could, of course, decide to count operations in a different way. We have not
    counted the increment operations or any of the housekeeping operations; however,
    this is not so important as we are more concerned with the rate of growth of the
    runtime with respect to *n* at high values of *n*.
  prefs: []
  type: TYPE_NORMAL
- en: This may seem a little daunting since each call of a recursive call itself spins
    off more recursive calls, and seemingly explodes exponentially. The key fact that
    makes this manageable is that as the number of recursive calls doubles, the size
    of each subproblem halves. These two opposing forces cancel out nicely, as we
    can demonstrate.
  prefs: []
  type: TYPE_NORMAL
- en: 'To calculate the maximum number of operations at each level of the recursion
    tree we simply multiply the number of subproblems by the number of operations
    in each subproblem as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/f963cdb9-6851-4a7b-9ee5-a7437f7c0f5c.png)'
  prefs: []
  type: TYPE_IMG
- en: Importantly, this shows that, because the *2^j* cancels out the number of operations
    at each level is independent of the level. This gives us an upper bound to the
    number of operations carried out on each level, in this example, 7*n*. It should
    be pointed out that this includes the number of operations performed by each recursive
    call on that level, not the recursive calls made on subsequent levels. This shows
    that the work is done, as the number of recursive calls doubles with each level,
    and is exactly counterbalanced by the fact that the input size for each sub-problem
    is halved.
  prefs: []
  type: TYPE_NORMAL
- en: 'To find the total number of operations for a complete merge sort, we simply
    multiply the number of operations on each level by the number of levels. This
    gives us the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/e06dc99e-5e0a-4f6b-829f-2ad033c7a5ed.png)'
  prefs: []
  type: TYPE_IMG
- en: 'When we expand this out, we get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/d0ad8584-5760-4081-aac4-65ab789871f5.png)'
  prefs: []
  type: TYPE_IMG
- en: The key point to take from this is that there is a logarithmic component to
    the relationship between the size of the input and the total running time. If
    you remember from school mathematics, the distinguishing characteristic of the
    logarithm function is that it flattens off very quickly. As an input variable,
    *x* increases in size; the output variable *y* increases by smaller and smaller
    amounts.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, compare the log function to a linear function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/a70898f7-13ed-4b6b-92d4-a71a04c40e64.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the previous example, multiplying the *n*log[2] *n* component and comparing
    it to ![](Images/2b34c45d-a9f3-4b96-8893-66994aba5875.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/66071838-ae0b-4bb1-b942-01638c4ef2e2.png)'
  prefs: []
  type: TYPE_IMG
- en: Notice how for very low values of *n*, the time to complete, *t*, is actually
    lower for an algorithm that runs in n2 time. However, for values above about 40,
    the log function begins to dominate, flattening the output until, at the comparatively
    moderate size *n* = 100, the performance is more than twice than that of an algorithm
    running in *n*² time. Notice also that the disappearance of the constant factor,
    + 7, is irrelevant at high values of *n*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code used to generate these graphs is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'You will need to install the `matplotlib` library, if it is not installed already,
    for this to work. Details can be found at the following address; I encourage you
    to experiment with this list comprehension expression used to generate the plots.
    For example, we could add the following `plot` statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/4a4614bc-1fd3-49e5-ad08-f9c7ce354585.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding graph shows the difference between counting six operations or
    seven operations. We can see how the two cases diverge, and this is important
    when we are talking about the specifics of an application. However, what we are
    more interested in here is a way to characterize growth rates. We are not so much
    concerned with the absolute values, but how these values change as we increase
    *n*. In this way, we can see that the two lower curves have similar growth rates
    when compared to the top (*x*²) curve. We say that these two lower curves have
    the same **complexity class**. This is a way to understand and describe different
    runtime behaviors. We will formalize this performance metric in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Asymptotic analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Asymptotic analysis of an algorithm refers to the computation of the running
    time of the algorithm. To determine which algorithm is better, given two algorithms,
    a simple approach can be to run both the programs, and the algorithm that takes
    the least time to execute for a given input is better than the other. However,
    it is possible that for a specific input, one algorithm performs better than other,
    whereas for any other input value that the algorithm may perform worse.
  prefs: []
  type: TYPE_NORMAL
- en: 'In asymptotic analysis, we compare two algorithms with respect to input size
    rather than the actual runtime, and we measure how the time taken increases with
    the increase in input size. This is depicted with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Assuming that the size of the array is `n`, and *T(n)* is the total number
    of key operations required to perform a linear search, the key operation in this
    example is the comparison. Let''s consider the linear search as an example to
    understand the worst case, average-case, and best-case complexity:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Worst-case analysis**: We consider the upper-bound running time, that is,
    the maximum time to be taken by the algorithm. In the linear search, the worst
    case happens when the element to be searched is found in the last comparison or
    not found in the list. In this case, there will be a maximum number of comparisons
    and that will be the total number of elements in the array. Therefore, the worst-case
    time complexity is Θ(n).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Average-case analysis**: In this analysis, we consider all the possible cases
    where the element can be found in the list, and then, we compute the average running
    time complexity. For example, in the linear search, the number of comparisons
    at all the positions would be *1* if the element to be searched was found at *0th*
    index, and similarly, the number of comparisons would be 2, 3, and so forth, up
    to *n* respectively for elements found at *1, 2, 3, … (n-1)* index positions*.*
    Thus the average time complexity can defined as `average-case complexity= (1+2+3…n)/n
    = n(n+1)/2`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Best-case analysis**: Best-case running time complexity is the minimum time
    needed for an algorithm to run; it is the lower-bound running time. In a linear
    search, the best case would be if the element to be searched is found in the first
    comparison. In this example, it is clear that the best-case time complexity is
    not dependent upon how long the list is. So, the best-case time complexity would
    be *Θ(1)*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generally, we use worst-case analysis to analyze an algorithm as it provides
    us with the upper bound on the running time, whereas best-case anaylsis is the
    least important as it provides us with the lower bound—that is, a minimum time
    required for an algorithm. Furthermore, the computation of average-case analysis
    is very difficult.
  prefs: []
  type: TYPE_NORMAL
- en: To calculate each of these, we need to know the upper and lower bounds. We have
    looked at a way to represent an algorithm's runtime using mathematical expressions,
    essentially adding and multiplying operations. To use asymptotic analysis, we
    simply create two expressions, one each for the best and worst cases.
  prefs: []
  type: TYPE_NORMAL
- en: Big O notation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The letter O in big *O* notation stands for order, in recognition that rates
    of growth are defined as the order of a function. It measures the worst-case running
    time complexity, that is, the maximum time to be taken by the algorithm. We say
    that one function *T*(*n*) is a big O of another function, *F*(*n*), and we define
    this as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/ca98e7da-6adf-45dd-bb3d-2d818e74f5b1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The function, *g*(*n*), of the input size, *n*, is based on the observation
    that for all sufficiently large values of *n*, *g*(*n*) is bounded above by a
    constant multiple of *f*(*n*). The objective is to find the smallest rate of growth
    that is less than or equal to *f*(*n*). We only care what happens at higher values
    of *n*. The variable *n**0 *represents the threshold below which the rate of growth
    is not important. The function *T(n)* represents the **tight upper bound** F(n).
    In the following plot, we can see that *T*(*n*) = *n*^(*2*) + 500 = *O*(*n*^(*2*)),
    with *C* = 2 and *n*[*0*] being approximately 23:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/34695609-21eb-4181-ad54-229cd55006af.png)'
  prefs: []
  type: TYPE_IMG
- en: You will also see the notation *f*(*n*) = *O*(*g*(*n*)). This describes the
    fact that *O*(*g*(*n*)) is really a set of functions that includes all functions
    with the same or smaller rates of growth than *f*(n). For example, *O*(*n*^(*2*))
    also includes the functions *O(n)*, *O(nlogn)*, and so on. Let's consider another
    example.
  prefs: []
  type: TYPE_NORMAL
- en: The big O time complexity for the function `f(x)= 19n log[2]n  +56 ` is *O(nlogn)*.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following table, we list the most common growth rates in order from
    lowest to highest. We sometimes call these growth rates the **time complexity**
    of a function, or the complexity class of a function:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Complexity class** | **Name** | **Example operations** |'
  prefs: []
  type: TYPE_TB
- en: '| *O(1)* | Constant | append, get item, set item. |'
  prefs: []
  type: TYPE_TB
- en: '| *O(logn)* | Logarithmic | Finding an element in a sorted array. |'
  prefs: []
  type: TYPE_TB
- en: '| *O(n)* | Linear | copy, insert, delete, iteration. |'
  prefs: []
  type: TYPE_TB
- en: '| *nLogn* | Linear-logarithmic | Sort a list, merge-sort. |'
  prefs: []
  type: TYPE_TB
- en: '| *n*^(*2*) | Quadratic | Find the shortest path between two nodes in a graph.
    Nested loops. |'
  prefs: []
  type: TYPE_TB
- en: '| *n*^(*3*) | Cubic | Matrix multiplication. |'
  prefs: []
  type: TYPE_TB
- en: '| 2^(*n*) | Exponential | **Towers of Hanoi** problem, backtracking. |'
  prefs: []
  type: TYPE_TB
- en: Composing complexity classes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Normally, we need to find the total running time of a number of basic operations.
    It turns out that we can combine the complexity classes of simple operations to
    find the complexity class of more complex, combined operations. The goal is to
    analyze the combined statements in a function or method to understand the total
    time complexity of executing several operations. The simplest way to combine two
    complexity classes is to add them. This occurs when we have two sequential operations.
    For example, consider the two operations of inserting an element into a list and
    then sorting that list. We can see that inserting an item occurs in *O(n)* time
    and sorting is in *O(nlogn)* time. We can write the total time complexity as *O(n
    + nlogn)*, that is, we bring the two functions inside the *O(...)*. We are only
    interested in the highest-order term, so this leaves us with just *O(nlogn)*.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we repeat an operation, for example, in a `while` loop, then we multiply
    the complexity class by the number of times the operation is carried out. If an
    operation with time complexity *O(f(n))* is repeated *O(n)* times then we multiply
    the two complexities:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/4e850cfb-5448-44c5-976f-747b075798b0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For example, suppose the function `f(...)` has a time complexity of *O(n²)*
    and it is executed *n *times in a `while` loop, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The time complexity of this loop then becomes *O(n²) * O(n) = O(n * n²) = O(n³)*.
    Here we are simply multiplying the time complexity of the operation by the number
    of times this operation executes. The running time of a loop is at most the running
    time of the statements inside the loop multiplied by the number of iterations.
    A single nested loop, that is, one loop nested inside another loop, will run in
    *n*2 time assuming both loops run `n` times, as demonstrated in the following
    example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Each statement is a constant, *c*, executed *nn* times, so we can express the
    running time as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/252c1e30-173c-4958-923c-369eb1bc0fcb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For consecutive statements within nested loops, we add the time complexities
    of each statement and multiply by the number of times the statement executed,
    for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: This can be written as `c[0] +c[1 ]n + cn^(2 )= O(n²)`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can define (base 2) logarithmic complexity, reducing the size of the problem
    by ½, in constant time. For example, consider the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice that i is doubling on each iteration; if we run this with *n* = 10 we
    see that it prints out four numbers: 2, 4, 8, and 16\. If we double *n* we see
    it prints out five numbers. With each subsequent doubling of *n*, the number of
    iterations is only increased by one. If we assume *k* iterations, we can write
    this as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/b31931aa-a8aa-451c-b30c-b8590dfcf074.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](Images/7ed3d882-ac80-4bc2-93b7-5c00efc6350d.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](Images/8ab8d784-60cc-4838-9816-7f1c9269a84e.png)'
  prefs: []
  type: TYPE_IMG
- en: From this, we can conclude that the total time = ***O**(log(n))*.
  prefs: []
  type: TYPE_NORMAL
- en: Although big O is the most used notation involved in asymptotic analysis, there
    are two other related notations that should be briefly mentioned. They are Omega
    notation and Theta notation.
  prefs: []
  type: TYPE_NORMAL
- en: Omega notation (Ω)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Omega notation describes tight lower bound on algorithms, similar to big O
    notation which describes a tight upper bound. Omega notation computes the best-case
    running time complexity of the algorithm. It provides the highest rate of growth
    *T(n)* which is less than or equal to the given algorithm. It can be computed
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/69e1d0c6-1542-4193-be5e-5e43ebcbc465.png)'
  prefs: []
  type: TYPE_IMG
- en: Theta notation (ϴ )
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It is often the case where both the upper and lower bounds of a given function
    are the same and the purpose of Theta notation is to determine if this is the
    case. The definition is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/391dc9e6-d17d-40c8-a353-f80e5b7576e9.png)'
  prefs: []
  type: TYPE_IMG
- en: Although Omega and Theta notations are required to completely describe growth
    rates, the most practically useful is big O notation and this is the one you will
    see most often.
  prefs: []
  type: TYPE_NORMAL
- en: Amortized analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Often we are not so interested in the time complexity of individual operations;
    we are more interested in the average running time of sequences of operations.
    This is called amortized analysis. It is different from average-case analysis,
    which we will discuss shortly, in that we make no assumptions regarding the data
    distribution of input values. It does, however, take into account the state change
    of data structures. For example, if a list is sorted, any subsequent find operations
    should be quicker. The amortized analysis considers the state change of data structures
    because it analyzes sequences of operations, rather than simply aggregating single
    operations.
  prefs: []
  type: TYPE_NORMAL
- en: Amortized analysis describes an upper bound on the runtime of the algorithm;
    it imposes an additional cost on each operation in the algorithm. The additional
    considered cost of a sequence may be cheaper as compared to the initial expensive
    operation.
  prefs: []
  type: TYPE_NORMAL
- en: When we have a small number of expensive operations, such as sorting, and lots
    of cheaper operations such as lookups, standard worst-case analysis can lead to
    overly pessimistic results, since it assumes that each lookup must compare each
    element in the list until a match is found. We should take into account that once
    we sort the list we can make subsequent find operations cheaper.
  prefs: []
  type: TYPE_NORMAL
- en: 'So far in our runtime analysis, we have assumed that the input data was completely
    random and have only looked at the effect the size of the input has on the runtime.
    There are two other common approaches to algorithm analysis; they are:'
  prefs: []
  type: TYPE_NORMAL
- en: Average-case analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Benchmarking
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Average-case analysis will find the average running time which is based on some
    assumptions regarding the relative frequencies of various input values. Using
    real-world data, or data that replicates the distribution of real-world data,
    is many times on a particular data distribution and the average running time is
    calculated.
  prefs: []
  type: TYPE_NORMAL
- en: Benchmarking is simply having an agreed set of typical inputs that are used
    to measure performance. Both benchmarking and average-time analysis rely on having
    some domain knowledge. We need to know what the typical or expected datasets are.
    Ultimately, we will try to find ways to improve performance by fine-tuning to
    a very specific application setting.
  prefs: []
  type: TYPE_NORMAL
- en: Let's look at a straightforward way to benchmark an algorithm's runtime performance.
    This can be done by simply timing how long the algorithm takes to complete given
    various input sizes. As we mentioned earlier, this way of measuring runtime performance
    is dependent on the hardware that it is run on. Obviously, faster processors will
    give better results, however, the relative growth rates as we increase the input
    size will retain characteristics of the algorithm itself rather than the hardware
    it is run on. The absolute time values will differ between hardware (and software)
    platforms; however, their relative growth will still be bound by the time complexity
    of the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a simple example of a nested loop. It should be fairly obvious
    that the time complexity of this algorithm is *O(n²)* since for each *n* iterations
    in the outer loop there are also *n* iterations in the interloop. For example,
    our simple nested for loop consists of a simple statement executed on the inner
    loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The following code is a simple test function that runs the `nest` function with
    increasing values of `n`. With each iteration, we calculate the time this function
    takes to complete using the `timeit.timeit` function. The `timeit` function, in
    this example, takes three arguments, a string representation of the function to
    be timed, a `setup` function that imports the `nest` function, and an `int` parameter
    that indicates the number of times to execute the main statement.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we are interested in the time the `nest` function takes to complete relative
    to the input size, `n`, it is sufficient, for our purposes, to call the `nest`
    function once on each iteration. The following function returns a list of the
    calculated runtimes for each value of `n`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following code, we run the `test2` function and graph the results, together
    with the appropriately scaled `n²` function, for comparison, represented by the
    dashed line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives the following results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/0f25b101-61b9-454e-9adb-9f8774b28063.png)'
  prefs: []
  type: TYPE_IMG
- en: As we can see, this gives us pretty much what we expect. It should be remembered
    that this represents both the performance of the algorithm itself as well as the
    behavior of underlying software and hardware platforms, as indicated by both the
    variability in the measured runtime and the relative magnitude of the runtime.
    Obviously, a faster processor will result in faster runtimes, and also performance
    will be affected by other running processes, memory constraints, clock speed,
    and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have looked at a general overview of algorithm design. Importantly,
    we studied a platform-independent way to measure an algorithm's performance. We
    looked at some different approaches to algorithmic problems. We looked at a way
    to recursively multiply large numbers and also a recursive approach for merge
    sort. We learned how to use backtracking for exhaustive search and generating
    strings. We also introduced the idea of benchmarking and a simple platform-dependent
    way to measure runtime.
  prefs: []
  type: TYPE_NORMAL
- en: In the following chapters, we will revisit many of these ideas with reference
    to specific data structures. In the next chapter, we will discuss linked lists
    and other pointer structures.
  prefs: []
  type: TYPE_NORMAL
