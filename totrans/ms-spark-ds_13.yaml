- en: Chapter 13. Secure Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Throughout this book, we have visited many areas of data science, often straying
    into those that are not traditionally associated with a data scientist's core
    working knowledge. In particular, we dedicated an entire chapter, [Chapter 2](ch02.xhtml
    "Chapter 2. Data Acquisition"), *Data Acquisition*, to data ingestion, which explains
    how to solve an issue that is always present, but rarely acknowledged or addressed
    adequately. In this chapter, we will visit another of those often overlooked fields,
    secure data. More specifically, how to protect your data and analytic results
    at all stages of the data life cycle. This ranges from ingestion, right through
    to presentation, at all times considering the important architectural and scalability
    requirements that naturally form the Spark paradigm.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: How to implement coarse-grained data access controls using HDFS ACLs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A guide to fine-grained security, with explanations using the Hadoop ecosystem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to ensure data is always encrypted, with an example using Java KeyStore
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Techniques for obfuscating, masking, and tokenizing data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How Spark implements Kerberos
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data security - the ethical and technical issues
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data security
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The final piece to our data architecture is security, and in this chapter we
    will discover that data security is always important, and the reasons for this.
    Given the huge increase in the volume and variety of data in recent times, caused
    by many factors, but in no small part due to the popularity of the Internet and
    related technologies, there is a growing need to provide fully scalable and secure
    solutions. We are going to explore those solutions along with the confidentiality,
    privacy, and legal concerns associated with the storing, processing, and handling
    of data; we will relate these to the tools and techniques introduced in previous
    chapters.
  prefs: []
  type: TYPE_NORMAL
- en: We will continue on by explaining the technical issues involved in securing
    data at scale and introduce ideas and techniques that tackle these concerns using
    a variety of access, classification, and obfuscation strategies. As in previous
    chapters, ideas are demonstrated with examples using the Hadoop ecosystem, and
    public cloud infrastructure strategies will also be present.
  prefs: []
  type: TYPE_NORMAL
- en: The problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have explored many and varied topics in previous chapters, usually concentrating
    on the specifics of a particular issue and the approaches that can be taken to
    solve them. In all of these cases, there has been the implicit idea that the data
    that is being used, and the content of the insights gathered, does not need protecting
    in any way; or at least the protection provided at the operating system level,
    such as login credentials, is sufficient.
  prefs: []
  type: TYPE_NORMAL
- en: In any environment, whether it is a home or a commercial one, data security
    is a huge issue that must always be considered. Perhaps, in a few instances, it
    is enough to write the data to a local hard drive and take no further steps; this
    is rarely an acceptable course of action and certainly should be a conscious decision
    rather than default behavior. In a commercial environment, computing resources
    are often provided with built-in security. In this case, it is still important
    for the user to understand those implications and decide whether further steps
    should be taken; data security is not just about protection from malicious entities
    or accidental deletion, but also everything in-between.
  prefs: []
  type: TYPE_NORMAL
- en: As an example, if you work in a secure, regulated, commercial, air-gapped environment
    (no access to the Internet) and within a team of like-minded data scientists,
    individual security responsibilities are still just as important as in an environment
    where no security exists at all; you may have access to data that must not be
    viewed by any of your peers and you may need to produce analytical results that
    are available to different and diversified user groups, all of whom are not to
    see each other's data. The emphasis may be explicitly or implicitly on you to
    ensure that the data is not compromised; therefore, a strong understanding of
    the security layers in your software stack is imperative.
  prefs: []
  type: TYPE_NORMAL
- en: The basics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Security considerations are everywhere, even in places that you probably hadn't
    even thought of. For example, when Spark is running a parallel job on a cluster,
    do you know the points at which data may touch physical disk during that life
    cycle? If you are thinking that everything is done in RAM, then you have a potential
    security issue right there, as data can be spilled to disk. More on the implications
    of this further on in this chapter. The point here is that you cannot always delegate
    security responsibility to the frameworks you are using. Indeed, the more varied
    the software you use, the more security concerns increase, both user and data
    related.
  prefs: []
  type: TYPE_NORMAL
- en: 'Security can be broadly split into three areas:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Authentication**: determining the legitimacy of the identity of a user'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Authorization**: the privileges that a user holds to perform specific actions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Access**: the security mechanisms used to protect data, both in transit and
    at rest'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are important differences between these points. A user may have full permissions
    to access and edit a file, but if the file has been encrypted outside of the user
    security realm, then the file may still not be readable; user authorization intervenes.
    Equally, a user may send data across a secure link to be processed on a remote
    server before a result is returned, but this does not guarantee that the data
    has not left a footprint on that remote server; the security mechanisms are unknown.
  prefs: []
  type: TYPE_NORMAL
- en: Authentication and authorization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Authentication is related to the mechanisms used to ensure that the user is
    who they say they are and operates at two key levels, namely, local and remote.
  prefs: []
  type: TYPE_NORMAL
- en: Authentication can take various forms, the most common is user login, but other
    examples include fingerprint reading, iris scanning, and PIN number entry. User
    logins can be managed on a local basis, as you would on your personal computer,
    for example, or on a remote basis using a tool such as **Lightweight Directory
    Access Protocol** (**LDAP**). Managing users remotely provides roaming user profiles
    that are independent of any particular hardware and can be managed independently
    of the user. All of these methods execute at the operating system level. There
    are other mechanisms that sit at the application layer and provide authentication
    for services, such as Google OAuth.
  prefs: []
  type: TYPE_NORMAL
- en: Alternative authentication methods have their own pros and cons, a particular
    implementation should be understood thoroughly before declaring a secure system;
    for example, a fingerprint system may seem very secure, but this is not always
    the case. For more information, refer to [http://www.cse.msu.edu/rgroups/biometrics/Publications/Fingerprint/CaoJain_HackingMobilePhonesUsing2DPrintedFingerprint_MSU-CSE-16-2.pdf](http://www.cse.msu.edu/rgroups/biometrics/Publications/Fingerprint/CaoJain_HackingMobilePhonesUsing2DPrintedFingerprint_MSU-CSE-16-2.pdf).
    We are not going to explore authentication any further here, as we have made the
    assumption that most systems will only be implementing user logins; a feature,
    by the way, that is often not a secure solution in its own right and indeed, in
    many cases, provides no security at all. For more information, refer to [http://www.cs.arizona.edu/~collberg/Teaching/466-566/2012/Resources/presentations/2012/topic7-final/report.pdf](http://www.cs.arizona.edu/~collberg/Teaching/466-566/2012/Resources/presentations/2012/topic7-final/report.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: 'Authorization is an area that is of great interest to us as it forms a critical
    part of basic security, is an area that we most often have greatest control over,
    and is something that we can use natively in any modern operating system. There
    are various different ways of implementing resource authorization, the two main
    ones being:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Access control lists** (**ACL**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Role-based access control** (**RBAC**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We'll discuss each of these in turn.
  prefs: []
  type: TYPE_NORMAL
- en: Access control lists (ACL)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In Unix, ACLs are used throughout the filesystem. If we list directory contents
    at the command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see there is a directory called resources that has an assigned owner
    (`mrh`) and group (`mygroup`), has `6` links, a size of `204` bytes, and was last
    modified on the `16 June 2015`. The ACLs `drwxr-xr-x` indicate:'
  prefs: []
  type: TYPE_NORMAL
- en: '`d` this is a directory (- if it is not)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rwx` the owner(`mrh`) has read, write, and executable rights'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`r-x` anyone in the group (`mygroup`) has read and execute rights'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`r-x` everyone else has read and execute rights'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Using ACLs is an excellent first step towards securing our data. It should
    always be the first thing considered, and should always be correct; if we do not
    ensure these settings are correct at all times, then we are potentially making
    it easy for other users to access this data, and we don''t necessarily know who
    the other users on the system are. Always avoid providing full access in the *all*
    part of the ACL:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'It doesn''t matter how secure our system is, any user with access to the filesystem
    can read, write, and delete this file! A far more appropriate setting would be:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Which provides full owner access and read-only access for the group.
  prefs: []
  type: TYPE_NORMAL
- en: 'HDFS implements ACLs natively; these can be administered using the command
    line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This gives full permissions to the file in HDFS for everyone, assuming the file
    already had sufficient permissions for us to make the change.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When Apache released Hadoop in 2008, it was often not understood that a cluster
    set at all of its defaults did not do any authentication of users. The superuser
    in Hadoop, `hdfs`, could be accessed by any user if the cluster had not been correctly
    configured, by simply creating an `hdfs` user on a client machine (`sudo useradd
    hdfs`).
  prefs: []
  type: TYPE_NORMAL
- en: Role-based access control (RBAC)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: RBAC takes a different approach, by assigning users one or more roles. These
    roles are related to common tasks or job functions, such that they can be easily
    added or removed dependent upon the user's responsibilities. For example, in a
    company, there may be many roles, including accounts, stock, and deliveries. An
    accountant may be given all three roles, so that they can compile the end of year
    finances, whereas an administrator booking deliveries would just have the deliveries
    role. This makes it much easier to add new users and manage users when they change
    departments or leave the organization.
  prefs: []
  type: TYPE_NORMAL
- en: 'Three key rules are defined for RBAC:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Role assignment**: a user can exercise a permission only if the user has
    selected or been assigned a role'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Role authorization**: a user''s active role must be authorized for the user'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Permission authorization**: a user can exercise a permission only if the
    permission is authorized for the user''s active role'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The relationships between users and roles can be summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Role-Permissions**: a particular role grants specific permissions to the
    user.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**User-Role**: the relationships between types of users and specific roles.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Role-Role**: the relationships between roles. These can be hierarchical,
    so *role1 => role2* could mean that, if a user has *role1*, then they automatically
    have *role2*, but if they have *role2*, this does not necessarily mean they have
    *role1*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RBAC is realized in Hadoop through Apache Sentry. Organizations can define the
    privileges for datasets that will be enforced from multiple access paths, including
    HDFS, Apache Hive, Impala, as well as Apache Pig and Apache MapReduce/Yarn via
    HCatalog. As an example, each Spark application runs as the requesting user and
    requires access to the underlying files. Spark cannot enforce access control directly,
    since it is running as the requesting user and is untrusted. Therefore, it is
    restricted to filesystem permissions (ACLs). Apache Sentry provides role-based
    control to resources in this case.
  prefs: []
  type: TYPE_NORMAL
- en: Access
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have thus far concentrated only on the specific ideas of ensuring that a
    user is who they say they are and that only the correct users can view and use
    data. However, once we have taken the appropriate steps and confirmed these details,
    we still need to ensure that this data is secure when the user is actually using
    it; there are a number of areas to consider:'
  prefs: []
  type: TYPE_NORMAL
- en: Is the user allowed to see all of the information in the data? Perhaps they
    are to be limited to certain rows, or even certain parts of certain rows.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is the data secure when the user runs analytics across it? We need to ensure
    that the data isn't transmitted as plain text and therefore open to man-in-the-middle
    attacks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is the data secure once the user has completed their task? There's no point
    in ensuring that the data is super secure at all stages, only to write plain text
    results to an insecure area.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can conclusions be made from the aggregation of data? Even if the user only
    has access to certain rows of a dataset, let's say to protect an individual's
    anonymity in this case, it is sometimes possible to make links between apparently
    unrelated information, for example. If the user knows that *A=>B* and *B=>C*,
    they can guess that, probably, *A=>C*, even if they are not allowed to see this
    in the data. In practice, this kind of issue can be very difficult to avoid, as
    data aggregation problems can be very subtle, occurring in unforeseen situations
    and often involving information gleaned over an extended period of time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are a number of mechanisms that we can use to help us protect against
    the preceding scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Encryption
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Arguably the most obvious and well known method of protecting data is encryption.
    We would use this whether our data is in transit or at rest, so, virtually all
    of the time, apart from when the data is actually being processed inside memory.
    The mechanics of encryption are different depending upon the state of the data.
  prefs: []
  type: TYPE_NORMAL
- en: Data at rest
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our data will always need to be stored somewhere, whether it be HDFS, S3, or
    local disk. If we have taken all of the precautions of ensuring that users are
    authorized and authenticated, there is still the issue of plain text actually
    existing on the disk. With direct access to the disk, either physically or by
    accessing it through a lower level in the OSI stack, it is fairly trivial to stream
    the entire contents and glean the plain text data.
  prefs: []
  type: TYPE_NORMAL
- en: If we encrypt data, then we are protected from this type of attack. The encryption
    can also exist at different levels, either by encrypting the data at the application
    layer using software, or by encrypting it at the hardware level, that is, the
    disk itself.
  prefs: []
  type: TYPE_NORMAL
- en: 'Encrypting the data at the application layer is the most common route, as it
    enables the user to make informed choices about the trade-off decisions that need
    to be made, thereby making the right choice of product for their situation. Because
    encryption adds an extra level of processing overhead (the data needs to be encrypted
    at write and decrypted at read), there is a key decision to make regarding the
    processor time versus security strength trade-off. The principal decisions for
    consideration are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Encryption algorithm type**: the algorithm to use to perform encryption,
    that is, AES, RSA, and so on'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Encryption key bit length**: the size of the encryption key roughly equates
    to how difficult it is to crack, but also influences the size of the result (possible
    storage consideration), that is, 64 bit, 128 bit, and so on'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Processor time allowed**: longer encryption keys generally mean greater processing
    time; this can have a serious impact on processing, given data of sufficient volume'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once we have decided upon the correct combination of factors for our use case,
    bearing in mind that some algorithm key length combinations are no longer considered
    safe, we need the software to actually do the encryption. This could be a bespoke
    Hadoop plugin or a commercial application. As mentioned, Hadoop now has a native
    HDFS encryption plugin, so you will not need to write your own! This plugin uses
    a Java KeyStore to safely store the encryption keys, which can be accessed through
    Apache Ranger. Encryption takes place entirely within HDFS, and is essentially
    linked to the ACLs on files. Therefore, when accessing HDFS files in Spark, the
    process is seamless (apart from some extra time to encrypt/decrypt files).
  prefs: []
  type: TYPE_NORMAL
- en: If you wish to implement encryption in Spark to write data to somewhere that
    is not covered in the aforementioned scenarios, then the Java javax.crypto package
    can be used. The weakest link here is now the fact that the key itself must be
    recorded somewhere; therefore, we have potentially simply moved our security issue
    elsewhere. Using a suitable KeyStore, such as Java KeyStore would address this
    issue.
  prefs: []
  type: TYPE_NORMAL
- en: At the time of writing, there is no obvious way of encrypting data when writing
    from Spark to local disk. In the next section, we'll write our own!
  prefs: []
  type: TYPE_NORMAL
- en: 'The idea is to replace the `rdd.saveAsTextFile(filePath)` function with something
    as close as possible to the original, with the further capability of encrypting
    the data. However, that''s not the whole story, as we''ll need to be able to read
    the data back too. To do this, we''ll take advantage of an alternative to `rdd.saveAsTextFile(filePath)`
    function, which also accepts a compression codec argument:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'On the face of it, the way Spark uses the compression codec appears to be similar
    to what we''d want for data encryption. So, let''s adapt one of the existing Hadoop
    compression implementations for our purposes. Looking at a few different existing
    implementations (`GzipCodec`, `BZip2Codec`), we find that we must extend the `CompressionCodec`
    interface to derive our encryption codec, named `CryptoCodec` from here on. Let''s
    look at an implementation in Java:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'It''s worth noting here that this codec class just serves as a wrapper for
    integrating our encryption and decryption routines with the Hadoop API; this class
    provides the entry points for the Hadoop framework to use when the crypto codec
    is called. The two main methods of interest are `createCompressor` and `createDeompressor`,
    which both perform the same initialization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We have used plain text passwords to make things simpler. When using this code,
    the encryption key should be pulled from a secure store; this is discussed in
    detail further on in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we define the encryption methods themselves:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Each time a file is encrypted, the *Initialization Vector* (IV) should be random.
    Randomization is crucial for encryption schemes to achieve semantic security,
    a property whereby repeated usage of the scheme under the same key does not allow
    an attacker to infer relationships between segments of the encrypted message.
  prefs: []
  type: TYPE_NORMAL
- en: The main issue when implementing encryption paradigms is the mishandling of
    byte arrays. A correctly encrypted file size will usually be a multiple of the
    key size when using padding, 16 (bytes) in this case. The encryption/decryption
    process will fail with padding exceptions if the file size is incorrect. In the
    Java libraries used previously, data is fed to the internal encryption routine
    in stages, size `ciphertext.length`, which are encrypted in chunks of 16 bytes.
    If there is a remainder, this is prepended to the data given in the next update.
    If a `doFinal` call is made, the remainder is again prepended and the data is
    padded to the end of the 16 byte block before encryption, whereby the routine
    completes.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now proceed to complete the rest of our `CryptoCodec`, that is, the
    compress and decompress implementations that will implement the preceding code.
    These methods are located in the `CryptoCompressor` and `CryptoDecompressor` classes
    and are called by the Hadoop framework:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: You can see the full implementation for the `CryptoCodec` class in our code
    repository.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have our working `CryptoCodec` class, the Spark driver code is
    then straightforward:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'And we have local disk encryption! To read an encrypted file, we simply define
    the `codec` class within the configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Spark will automatically use the `CryptoCodec` class when it recognizes an appropriate
    file and our implementation ensures a unique IV is used for each file; the IV
    is read from the beginning of the encrypted file.
  prefs: []
  type: TYPE_NORMAL
- en: Java KeyStore
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Depending upon your environment, the preceding code may be enough to keep your
    data secure. However, there is a flaw, in that the key used to encrypt/decrypt
    the data has to be provided in plain text. We can solve this issue by creating
    a Java KeyStore. This can be done via the command line or programmatically. We
    can implement a function to create a `JCEKS` KeyStore and add a key:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We can achieve the same via the command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'And check it exists:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'To retrieve the key from this KeyStore:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We have hardcoded the specifics for ease of reading, this should not be done
    in practice, as Java byte code is relatively simple to reverse engineer and, therefore,
    a malicious third party could easily obtain this secret information.
  prefs: []
  type: TYPE_NORMAL
- en: Our secret key is now protected in a KeyStore and is only accessible using the
    KeyStore password and secret key alias. These still need to be protected, but
    would usually be stored in a database, where they are accessible only to authorized
    users.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now modify our `EncryptionUtils.getPassword` method to retrieve the
    `JCEKS` key rather than the plain text version, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have a `CryptoCodec` class, we can use it throughout Spark to secure
    data anytime we need data encryption. For example, if we set the Spark configuration
    `spark.shuffle.spill.compress` to true, and set `spark.io.compression.codec` to
    `org.apache.hadoop.io.compress.CryptoCodec`, then any spill to disk will be encrypted.
  prefs: []
  type: TYPE_NORMAL
- en: S3 encryption
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'HDFS encryption is great for providing what is essentially a managed service.
    If we now look at S3, this can do the same, but it also offers the ability to
    provide server-side encryption with:'
  prefs: []
  type: TYPE_NORMAL
- en: AWS KMS-Managed keys (SSE-KMS)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Customer-Provided keys (SSE-C)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Server-side encryption can provide more flexibility should you be in an environment
    where the encryption keys need to be explicitly managed.
  prefs: []
  type: TYPE_NORMAL
- en: Hardware encryption is handled within the physical disk architecture. Generally,
    this has the advantage of being quicker (due to the bespoke hardware designated
    for encryption) and being easier to secure, as physical access to the machine
    is required in order to circumvent. The downside being that all data written to
    disk is encrypted, which can result in reduced I/O performance for heavily utilized
    disks.
  prefs: []
  type: TYPE_NORMAL
- en: Data in transit
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If end-to-end security is your goal, an area that is often of concern is that
    of data in transit. This could be the reading/writing from disk or transportation
    of data around a network during analytics processing. In all cases, it is important
    to be aware of the weaknesses of your environment. It is not enough to assume
    that the framework or network administrator have covered these potential issues
    for you, even if your environment does not allow changes to be made directly.
  prefs: []
  type: TYPE_NORMAL
- en: A common mistake is to assume that data is secure when it is not human-readable.
    Although binary data itself isn't human-readable, it is often readily translated
    to the readable content and it can be captured over the network using tools such
    as Wireshark ([www.wireshark.org](http://www.wireshark.org)). So, never assume
    data security on the wire, regardless of whether it's human-readable.
  prefs: []
  type: TYPE_NORMAL
- en: As we have seen previously, even when encrypting data on disk, we cannot assume
    it is necessarily secure. For example, if the data is encrypted at hardware level,
    then it is unencrypted as soon as it leaves the disk itself. In other words, the
    plain text is readable as it traverses the network to any machine and, therefore,
    completely open to being read by unknown entities at any point on that journey.
    Data encrypted at software level is generally not decrypted until it is used by
    the analytic, therefore, generally making it the safer option if the network topology
    is not known.
  prefs: []
  type: TYPE_NORMAL
- en: 'When considering the security of processing systems themselves, such as Spark,
    there are issues here too. Data is constantly moved between nodes with no direct
    control from the user. So, it is vital that we understand where the data may be
    available in plain text at any given time. Consider the following diagram that
    shows the interactions between entities during a Spark YARN job:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Data in transit](img/image_13_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We can see that every connection transmits and receives data. Spark input data
    is transferred via broadcast variables and all channels support encryption apart
    from UI and local shuffle/cache files (see JIRA SPARK-5682 for more information).
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, there is a weakness here, in that cached files are stored as plain
    text. The fix is either to implement the preceding solution, or to set up YARN
    local directories to point to local encrypted disks. To do this, we need to ensure
    that `yarn.nodemanager.local-dirs` in yarn-default.xml are encrypted directories
    on all DataNodes, either using a commercial product or hosting these directories
    on encrypted disks.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have considered the data as a whole, we should address the individual
    parts of the data itself. It is very possible that data may contain sensitive
    information, for example, names, addresses, and credit card numbers. There are
    a number of ways to handle this type of information.
  prefs: []
  type: TYPE_NORMAL
- en: Obfuscation/Anonymizing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'With obfuscation, the sensitive parts of the data are transformed into something
    that can never be linked back to the original content - providing security through
    obscurity. For example, a CSV file containing fields: `Forename`, `Surname`, `Address
    line 1`, `Address line 2`, `Postcode`, `Phone Number`, `Credit Card Number` might
    be obfuscated like so:'
  prefs: []
  type: TYPE_NORMAL
- en: Original
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Obfuscated
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Obfuscating data is great for analytics, as it protects sensitive data while
    still allowing useful calculations, such as counting completed fields. We can
    also be intelligent about the way we obfuscate the data in order to preserve certain
    details while protecting others. For example, a credit card number: `4659 42XX
    XXXX XXXX` can give us a surprising amount of information, as the first six digits
    of payments cards, called the **Bank Identification Number** (**BIN**), tell us
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: BIN 465942
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Card brand: VISA'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Issuing bank: HSBC'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Card type: debit'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Card level: classic'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ISO country number 826 (Great Britain)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data obfuscation should not necessarily be random, but should be carefully tailored
    to ensure that sensitive data is definitely removed. The definition of sensitive
    will entirely depend upon the requirements. In the preceding example, it may be
    very useful to be able to summarize the distribution of customer payments cards
    by type, or it could be deemed as sensitive information that should be removed.
  prefs: []
  type: TYPE_NORMAL
- en: Another phenomenon to be aware of here, as you may recall from previous chapters,
    is data aggregation. For example, if we know that the name of the individual is
    John Smith AND that his credit card starts with 465942, then we know that John
    Smith has an account with HSBC in the UK, a great piece of information for a malicious
    entity to start building on. Therefore, care must be taken to ensure that the
    right amount of obfuscation is applied, bearing in mind that we can never recover
    the original data, unless we have another copy stored elsewhere. Non-recovery
    of data can be a costly event, so data obfuscation should be implemented wisely.
    Indeed, if storage allows, it is not unreasonable to want to store several versions
    of data, each with a different level of obfuscation and different levels of access.
  prefs: []
  type: TYPE_NORMAL
- en: When thinking about implementing this in Spark, it is most likely that we will
    have a scenario where there are many input records that require transformation.
    Thus, our starting point is to write something that works on a single record,
    and then wrap this in an RDD so that the functions can be run across many records
    in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: 'Taking our preceding example, let''s express its schema in Scala as an enumeration.
    Along with the definition, we''ll include in our `Enumeration` class information
    about how any particular field should be obfuscated:'
  prefs: []
  type: TYPE_NORMAL
- en: '*x*, *y* mask the char positions from *x* to *y*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`0`, `len` mask the entire field from 0 to the length of the field text'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prefix` mask everything before the last space character'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`suffix` mask everything after the first space character'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`""` do nothing'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This information is encoded in the enumeration as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we can split the input string and write a function that applies the correct
    obfuscation argument to the correct field:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: To keep things simple, we have hardcoded some of the items that you may want
    to change later, for example the split argument (`,`), and also made the obfuscation
    symbol constant in all cases (`X`).
  prefs: []
  type: TYPE_NORMAL
- en: 'And finally, the actual obfuscation code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Again, we have kept things simple and do not go to great lengths to check for
    exceptions or edge cases. Here is a practical example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'It provides the desired result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'This handy bit of code provides a great basis for obfuscating at scale. We
    can easily extend it to more complicated scenarios, such as the obfuscation of
    different parts of the same field. For example, by changing `StringObfuscator`,
    we could mask the house number and road name differently in the `Address Line
    1` field:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Of course, if you were wishing to scale this out for many different use cases,
    you could also apply the strategy pattern over `StringObfuscator` to allow an
    obfuscation function to be provided at runtime.
  prefs: []
  type: TYPE_NORMAL
- en: 'A key software engineering technique, the strategy pattern is described here:
    [https://sourcemaking.com/design_patterns/strategy](https://sourcemaking.com/design_patterns/strategy).'
  prefs: []
  type: TYPE_NORMAL
- en: At this point, it's worth thinking about obfuscating data using an algorithm,
    such as one-way hashing function or digest, rather than simply replacing with
    characters (`XXX`). This is a versatile technique and is applicable in a wide
    range of use cases. It relies on the computational complexity of performing an
    inverse calculation for some calculations, such as finding factors and modular
    squaring, meaning that once applied they are impractical to reverse. However,
    care should be taken when using hashes because, despite digest calculations being
    NP-complete, there are some scenarios where hashing is still susceptible to compromise
    using implicit knowledge. For example, the predictability of credit card numbers
    means that they have been proved to be cracked quickly by a brute-force approach,
    even using MD5 or SHA-1 hashes.
  prefs: []
  type: TYPE_NORMAL
- en: For more information, refer to [https://www.integrigy.com/security-resources/hashing-credit-card-numbers-unsafe-application-practices](https://www.integrigy.com/security-resources/hashing-credit-card-numbers-unsafe-application-practices).
  prefs: []
  type: TYPE_NORMAL
- en: Masking
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Data masking is about creating a functional substitute of data while ensuring
    that important content is hidden. This is another anonymization method whereby
    the original contents are lost once the masking process has taken place. Therefore,
    it is important to ensure that the changes are carefully planned, as they are
    effectively final. Of course, an original version of the data could be stored
    for emergencies, but this would add additional burden to the security considerations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Masking is a simple process and it relies on generating random data to replace
    any sensitive data. For example, applying a mask to our previous example gives:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: We now have a row, which is functionally equivalent to the original data. We
    have a full name, address, phone number, and credit card number, but they are
    *different*, such that they cannot be linked to the original.
  prefs: []
  type: TYPE_NORMAL
- en: Partial masking is very useful for processing purposes, as we can keep some
    data while masking the rest. In this way, we can perform a number of data auditing
    tasks that are not necessarily possible using obfuscation. For example, we could
    mask data actually present, allowing us to guarantee that populated fields will
    always be valid whilst also being able to detect empty fields.
  prefs: []
  type: TYPE_NORMAL
- en: It is also possible to use complete masking in order to generate mock data without
    having seen the original data at all. In this case, data could be completely generated,
    say, for testing or profiling purposes.
  prefs: []
  type: TYPE_NORMAL
- en: Whatever the use case, care should be taken when using masking, as it is possible
    to unwittingly insert real information into a record. For example, `Simon Jones`
    might actually be a real person. This being the case, it is certainly a good idea
    to store the data provenance, that is, the source and historical record for all
    data held. Therefore, should the real "`Simon, Jones`" submit a **request for
    information** (**RFI**) under the data protection act you have the necessary information
    in order to provide the relevant justifications.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s extend our previously built code to implement a basic masking approach
    using a completely random selection. We have seen that the masking method requires
    that we replace fields with some meaningful alternative. To have something working
    quickly we could simply provide arrays of alternatives:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Later, we can extend these to read from a file containing many more alternatives.
    We can even replace multiple fields in one go using a composite mask:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The processing code is then straightforward:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'We can define a `RandomCCNumber` function to generate a random credit card
    number. Here''s a simple function that provides four sets of randomly generated
    integers using recursion:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Putting this code together and running against our original example, gives
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Again, there are many ways we could develop this code. For example, we could
    generate a credit card number that is valid under the BIN scheme, or we could
    ensure that name selection doesn't randomly choose the same name that it's trying
    to replace. However, the outlined framework is presented here as a demonstration
    of the technique and can be easily extended and generalized to account for any
    additional requirements you might have.
  prefs: []
  type: TYPE_NORMAL
- en: Tokenization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Tokenization** is the process of substituting sensitive information with
    a token that can be later used to retrieve the actual data, if required, subject
    to the relevant authentication and authorization. Using our previous example,
    tokenized text might look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Where the bracketed values are tokens that can be exchanged for the actual values
    when the requesting user fulfills the correct security criteria. This method is
    the most secure of those discussed and allows us to recover the exact original
    underlying data. However, it comes with a significant processing overhead to tokenize
    and detokenize data and, of course, the tokenizer system will require administration
    and careful maintenance.
  prefs: []
  type: TYPE_NORMAL
- en: 'This also means that there is a single point of failure in the tokenization
    system itself and, therefore, it must be subject to the important security processes
    we have discussed: audit, authentication, and authorization.'
  prefs: []
  type: TYPE_NORMAL
- en: Due to the complexities and security issues with tokenization, the most popular
    implementations are commercial products covered by extensive patents. The bulk
    of the effort with this type of system, particularly with big data, is in ensuring
    that the tokenizer system can provide a full, completely secure, robust and scalable
    service at very high levels of throughput. We can, however, build a simple tokenizer
    using Accumulo. In [Chapter 7](ch07.xhtml "Chapter 7. Building Communities"),
    *Building Communities*, there is a section on setting up Apache Accumulo such
    that we can use cell-level security. Apache Accumulo, is an implementation of
    the Google BigTable paper, but it adds the additional security functionality.
    This means that a user can have all of the advantages of loading and retrieving
    data in parallel and at scale, but also be able to control the visibility of that
    data to a very fine degree. The chapter describes all of the information required
    to set up an instance, configure it for multiple users, and load and retrieve
    data with the required security labels (achieved through Accumulo Mutations).
  prefs: []
  type: TYPE_NORMAL
- en: 'For our purposes, we want to take a field and create a token; this could be
    a GUID, hash, or some other object. We can then write an entry to Accumulo using
    the token as the RowID and the field data itself as the contents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'We then write the `uuid` to the related field in the output data. When tokenized
    data is read back in, anything starting with `[` is assumed to be a token and
    the Accumulo read procedure is used to obtain the original field data, assuming
    that the user invoking the Accumulo read has the correct permissions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Using a Hybrid approach
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Obfuscation and masking can be used very effectively together to maximize the
    advantages of both methods. Using this hybrid approach, our example might become:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Using a combination of masking and tokenization is the emerging banking standard
    for securing credit card transactions. The **Primary Account Number** (**PAN**)
    is replaced with a token made up of a unique, randomly generated sequence of numbers,
    alphanumeric characters, or a combination of a truncated PAN and a random alphanumeric
    sequence. This enables the information to be processed as if it were the actual
    data, for example, audit checks or data quality reports, but it does not allow
    the true information to exist in plain text. Should the original information be
    required, the token can be used to request it and the user is only successful
    if they meet the authorization and authentication requirements.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can refactor our code to perform this task; we will define a new function
    that mixes obfuscation and masking together:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Once again, our example becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: As with all tokenization, you will need to be careful to avoid side effects
    with the generated data, for example, `0264` is not a real BIN code. Again, requirements
    will dictate as to whether this is an issue, that is, it's not an issue if we
    are only trying to ensure that the field is populated in the correct format.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to run any of these processes at scale, we simply need to wrap them
    in an RDD:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Data disposal
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Secure data should have an agreed life cycle. This will be set by a data authority
    when working in a commercial context, and it will dictate what state the data
    should be in at any given point during that life cycle. For example, a particular
    dataset may be labeled as *sensitive - requires encryption* for the first year
    of its life, followed by *private - no encryption*, and finally, *disposal*. The
    lengths of time and the rules applied will entirely depend upon the organization
    and the data itself - some data expires after just a few days, some after fifty
    years. The life cycle ensures that everyone knows exactly how the data should
    be treated, and it also ensures that older data is not needlessly taking up valuable
    disk space or breaching any data protection laws.
  prefs: []
  type: TYPE_NORMAL
- en: 'The correct disposal of data from secure systems is perhaps one of the most
    mis-understood areas of data security. Interestingly, it doesn''t always involve
    a complete and/or destructive removal process. Examples where no action is required
    include:'
  prefs: []
  type: TYPE_NORMAL
- en: If data is simply out of date, it may no longer hold any intrinsic value - a
    good example is government records that are released to the public after their
    expiry date; what was top secret during World War Two is generally of no sensitivity
    now due to the elapsed time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If data is encrypted, and no longer required, simply throw the keys away!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As opposed to the examples where some effort is required, leading to the potential
    for mistakes to be made:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Physical destruction**: we often hear of disks being destroyed with a hammer
    or similar, even this is unsafe if not completed thoroughly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multiple writes**: relies upon writing over data blocks multiple times to
    ensure that the original data is physically overwritten. Utilities such as shred
    and scrub on Linux achieve this; however, they still have limited effectiveness
    depending upon the underlying filesystem. For example, RAID and cache type systems
    will not necessarily be overwritten beyond all retrieval with these tools. Overwriting
    tools should be treated with caution and used only with a complete understanding
    of their limitations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When you secure your data, start thinking about your disposal strategy. Even
    if you are not made aware of any organizational rules in existence (in a commercial
    environment), you should still be thinking about how you are going to make sure
    the data is unrecoverable when access is no longer required.
  prefs: []
  type: TYPE_NORMAL
- en: Kerberos authentication
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many installations of Apache Spark use Kerberos to provide security and authentication
    to services such as HDFS and Kafka. It's also especially common when integrating
    with third-party databases and legacy systems. As a commercial data scientist,
    at some point, you'll probably find yourself in a situation where you'll have
    to work with data in a Kerberized environment, so, in this part of the chapter,
    we'll cover the basics of Kerberos - what it is, how it works, and how to use
    it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Kerberos is a third-party authentication technique that''s particularly useful
    where the primary form of communication is over a network, which makes it ideal
    for Apache Spark. It''s used in preference to alternative methods of authentication,
    for example, username and password, because it provides the following benefits:'
  prefs: []
  type: TYPE_NORMAL
- en: No passwords are stored in plain text in application configuration files
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Facilitates centralized management of services, identities, and permissions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Establishes a mutual trust, so both entities are identified
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prevents spoofing - trust is only established temporarily, just for a timed
    session, meaning replay attacks are not possible, but sessions are renewable for
    convenience
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's look at how it works with Apache Spark.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use case 1: Apache Spark accessing data in secure HDFS'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the most basic use case, once you''re logged on to an edge node (or similar)
    of your secure Hadoop cluster and before running your Spark program, Kerberos
    must be initialized. This is done by using the `kinit` command that comes with
    Hadoop and entering your user''s password when prompted:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: At this point, you will be fully authenticated and able to access any data within
    HDFS, subject to the standard permissions model.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, the process seems simple enough, let''s take a deeper look at what happened
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: When the `kinit` command runs, it immediately sends a request to the Kerberos
    **key distribution centre** (**KDC**), to acquire a **ticket granting ticket**
    (**TGT**). The request is sent in plain text, and it essentially contains what
    is known as the **principal**, which is basically the "username@kerberosdomain"
    in this case (you can find out this string using the `klist` command). The **Authentication
    Server (AS)** responds to this request, with a TGT that has been signed using
    client's private key, a key that was shared ahead of time and is already known
    to the AS. This ensures secure transfer of the TGT.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The TGT is cached locally on the client, along with a **Keytab** file - which
    is a container for Kerberos keys and it is accessible to any Spark processes running
    as the same user.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, when the spark-shell is started, Spark uses the cached TGT to request
    that the **Ticket Granting Server** (**TGS**), provide a **session ticket** for
    accessing the HDFS service. This Ticket is signed using the HDFS NameNode's private
    key. In this way, the secure transfer of the Ticket is guaranteed, ensuring that
    only the NameNode can read it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Armed with a ticket, Spark attempts to retrieve a **delegation toke**n from
    the NameNode. The purpose of this token is to prevent a flood of requests into
    the TGT when the executors start reading data (as the TGT was not designed with
    big data in mind!), but it also helps overcome problems Spark has with delayed
    execution times and ticket session expiry.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Spark ensures that all executors have access to the delegation token by placing
    it on the distributed cache so that it's available as a YARN local file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When each executor makes a request to the NameNode for access to a block stored
    in HDFS, it passes across the delegation token it was given previously. The NameNode
    replies with the location of the block, along with a **block token** that is signed
    by the NameNode with a private secret. This key is shared by all of the DataNodes
    in the cluster and is only known by them. The purpose of this added block token
    is to ensure that the access is fully secured and, as such, it is only issued
    to authenticated users and it can only be read by verified DataNodes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The last step is for the executors to supply the block token to the relevant
    DataNode and receive the requested block of data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Use case 1: Apache Spark accessing data in secure HDFS](img/image_13_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Use case 2: extending to automated authentication'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: By default, Kerberos tickets last for 10 hours and then expire, making them
    useless after this time, but they can be renewed. Therefore, when executing long-running
    Spark jobs or Spark Streaming Jobs (or jobs where a user is not directly involved
    and `kinit` cannot be run manually), it is possible to pass enough information
    upon starting a Spark process in order to automate the renewal of tickets issued
    during the previously discussed handshake.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is done by passing in the location of the keytab file and associated principal
    using the command line options provided, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: When attempting to execute a long running job as your local user, the principal
    name can be found using `klist` otherwise, dedicated **service principals** can
    be configured within Kerberos using `ktutils` and `ktadmin`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use case 3: connecting to secure databases from Spark'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When working in a corporate setting, it may be necessary to connect to a third-party
    database that has been secured with Kerberos, such as PostgreSQL or Microsoft
    SQLServer.
  prefs: []
  type: TYPE_NORMAL
- en: In this situation, it is possible to use JDBC RDD to connect directly to the
    database and have Spark issue an SQL query to ingest data in parallel. Care should
    be taken when using this approach, as traditional databases are not built for
    high levels of parallelism, but if used sensibly, it is sometimes a very useful
    technique, particularly well-suited to rapid data exploration.
  prefs: []
  type: TYPE_NORMAL
- en: Firstly, you will need the native JDBC drivers for your particular database
    - here we've used Microsoft SQLServer as an example, but drivers should be available
    for all modern databases that support Kerberos (see RFC 1964).
  prefs: []
  type: TYPE_NORMAL
- en: 'You''ll need to configure spark-shell to use the JDBC drivers on startup, like
    so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, in the shell, type or paste the following (replacing the environment
    specific variables, which are highlighted):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Spark runs the SQL passed into the constructor of `JdbcRDD`, but instead of
    running it as a single query, it is able to chunk it using the last three parameters
    as a guide.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, in this example, in fact, four queries would be run in parallel:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, Kerberos is a huge and complicated subject. The level of knowledge
    required for a data scientist can vary depending upon the role. Some organizations
    will have a DevOps team to ensure that everything is implemented correctly. However,
    in the current climate, where there is a big skills shortage in the market, it
    could well be the case that data scientists will have to solve these issues themselves.
  prefs: []
  type: TYPE_NORMAL
- en: Security ecosystem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will conclude with a brief rundown of some of the popular security tools
    we may encounter while developing with Apache Spark - and some advice about when
    to use them.
  prefs: []
  type: TYPE_NORMAL
- en: Apache sentry
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As the Hadoop ecosystem grows ever larger, products such as Hive, HBase, HDFS,
    Sqoop, and Spark all have different security implementations. This means that
    duplicate policies are often required across the product stack in order to provide
    the user with a seamless experience, as well as enforce the overarching security
    manifest. This can quickly become complicated and time consuming to manage, which
    often leads to mistakes and even security breaches (whether intentional or otherwise).
    Apache Sentry pulls many of the mainstream Hadoop products together, particularly
    with Hive/HS2, to provide fine-grained (up to column level) controls.
  prefs: []
  type: TYPE_NORMAL
- en: Using ACLs is simple, but high maintenance. The setting of permissions for a
    large number of new files and amending umasks is very cumbersome and time consuming.
    As abstractions are created, authorization becomes more complicated. For example,
    the fusing of files and directories can become tables, columns, and partitions.
    Therefore, we need a trusted entity to enforce access control. Hive has a trusted
    service - **HiveServer2** (**HS2**), which parses queries and ensures that users
    have access to the data they are requesting. HS2 runs as a trusted user with access
    to the whole data warehouse. Users don't run code directly in HS2, so there is
    no risk of code bypassing access checks.
  prefs: []
  type: TYPE_NORMAL
- en: To bridge Hive and HDFS data, we can use the Sentry HDFS plugin, which synchronizes
    HDFS file permissions with higher level abstractions. For example, permissions
    to read a table = permission to read table's files and, similarly, permissions
    to create a table = permission to write to a database's directory. We still use
    HDFS ACL's for fine-grained user permissions, however we are restricted to the
    Filesystem view of the world and therefore cannot provide column-level and row-level
    access, it's "all or nothing". As mentioned previously, Accumulo provides a good
    alternative when this scenario is important. There is a product, however, that
    also addresses this issue - see the RecordService section.
  prefs: []
  type: TYPE_NORMAL
- en: The quickest and easiest way to implement Apache Sentry is to use Apache Hue.
    Apache Hue has been developed over the last few years, starting life as a simple
    GUI to pull together a few of the basic Hadoop services, such as HDFS, and has
    grown into a hub for many of the key building blocks in the Hadoop stack; HDFS.
    Hive, Pig, HBase, Sqoop, Zookeeper, and Oozie all feature together with integrated
    Sentry to handle the security. A demonstration of Hue can be found at [http://demo.gethue.com/](http://demo.gethue.com/),
    providing a great introduction to the feature set. We can also see many of the
    ideas discussed in this chapter in practice, including HDFS ACLs, RBACs, and Hive
    HS2 access.
  prefs: []
  type: TYPE_NORMAL
- en: RecordService
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the key aspects of the Hadoop ecosystem is decoupling storage managers
    (for example, HDFS and Apache HBase) and compute frameworks (for example, MapReduce,
    Impala, and Apache Spark). Although this decoupling allows for far greater flexibility,
    thus allowing the user to choose their framework components, it leads to excessive
    complexity due to the compromises required to ensure that everything works together
    seamlessly. As Hadoop becomes an increasingly critical infrastructure component
    for users, the expectations for compatibility, performance, and security also
    increase.
  prefs: []
  type: TYPE_NORMAL
- en: RecordService is a new core security layer for Hadoop that sits between the
    storage managers and compute frameworks to provide a unified data access path,
    fine-grained data permissions, and enforcement across the stack.
  prefs: []
  type: TYPE_NORMAL
- en: '![RecordService](img/image_13_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: RecordService is only compatible with Cloudera 5.4 or later and, thus, cannot
    be used in a standalone capacity, or with Hortonworks, although HDP uses Ranger
    to achieve the same goals. More information can be found at [www.recordservice.io](http://www.recordservice.io).
  prefs: []
  type: TYPE_NORMAL
- en: Apache ranger
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The aims of Apache ranger are broadly the same as RecordService, the primary
    goals being:'
  prefs: []
  type: TYPE_NORMAL
- en: Centralized security administration to manage all security related tasks in
    a central UI, or using REST APIs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fine-grained authorization to perform a specific action and/or operation with
    a Hadoop component/tool and manage through a central administration tool
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Standardize authorization methods across all Hadoop components
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enhanced support for different authorization methods including role-based access
    control and attribute based access control
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Centralized auditing of user access and administrative actions (security related)
    within all components of Hadoop
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At the time of writing, Ranger is an Apache Incubator project and, therefore,
    is not at a major point release. Although, it is fully integrated with Hortonworks
    HDP supporting HDFS, Hive, HBase, Storm, Knox, Solr, Kafka, NiFi, YARN, and, crucially,
    a scalable cryptographic key management service for HDFS encryption. Full details
    can be found at [http://ranger.incubator.apache.org/](http://ranger.incubator.apache.org/)
    and [http://hortonworks.com/apache/ranger/](http://hortonworks.com/apache/ranger/).
  prefs: []
  type: TYPE_NORMAL
- en: Apache Knox
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have discussed many of the security areas of the Spark/Hadoop stack, but
    they are all related to securing individual systems or data. An area that has
    not been mentioned in any detail is that of securing a cluster itself from unauthorized
    external access. Apache Knox fulfills this role by "ring fencing" a cluster and
    providing a REST API Gateway through which all external transactions must pass.
  prefs: []
  type: TYPE_NORMAL
- en: Coupled with a Kerberos secured Hadoop cluster, Knox provides authentication
    and authorization, protecting the specifics of the cluster deployment. Many of
    the common services are catered for, including HDFS (via WEBHDFS), YARN Resource
    Manager, and Hive.
  prefs: []
  type: TYPE_NORMAL
- en: Knox is another project that is heavily contributed to by Hortonworks and, therefore,
    is fully integrated into the Hortonworks HDP platform. Whilst Knox can be deployed
    into virtually any Hadoop cluster, it can be done with a fully integrated approach
    in HDP. More information can be found at [knox.apache.org](http://knox.apache.org).
  prefs: []
  type: TYPE_NORMAL
- en: Your Secure Responsibility
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we''ve covered the common security use cases and discussed some of
    the tools that a data scientist needs to be aware of in their everyday activities,
    there''s one last important item to note. *While in their custody, the responsibility
    for data, including its security and integrity, lies with the data scientist*.
    This is usually true whether or not you are explicitly told. Therefore, it is
    crucial that you take this responsibility seriously and take all the necessary
    precautions when handling and processing data. If needed, also be ready to communicate
    to others their responsibility. We all need to ensure that we are not held responsible
    for a breach off-site; this can be achieved by highlighting the issue or, indeed,
    even having a written contract with the off-site service provider outlining their
    security arrangements. To see a real-world example of what can go wrong when you
    don''t pay proper attention to due diligence, have a look at some security notes
    regarding the Ashley-Madison hack here: [http://blog.erratasec.com/2015/08/notes-on-ashley-madison-dump.html#.V-AGgT4rIUv](http://blog.erratasec.com/2015/08/notes-on-ashley-madison-dump.html#.V-AGgT4rIUv).'
  prefs: []
  type: TYPE_NORMAL
- en: Another area of interest is that of removable media, most commonly DVDs and
    memory sticks. These should be treated in the same way as hard drives, but with
    the assumption that the data is always unsafe and at risk. The same options exist
    for these types of media, meaning data can be secured at the application level
    or at the hardware level (excepting optical disks, for example, DVD/CD). With
    USB key storage, there exists examples that implement hardware encryption. The
    data is always secure when written to them, therefore removing the bulk of the
    responsibility from the user. These types of drive should always be certified
    to **Federal Information Processing Standards** (**FIPS**); generally, FIPS 140
    (Cryptographic modules) or FIPS 197 (AES Cipher).
  prefs: []
  type: TYPE_NORMAL
- en: If an FIPS standard is not required, or the media is optical in nature, then
    data can be encrypted at the application layer, that is, encrypted by software.
    There are a number of ways to do this, including encrypted partitions, encrypted
    files, or raw data encryption. All of these methods involve using third-party
    software to perform the encrypt/decrypt functions at read/write time. Therefore,
    passwords are needed, introducing the issues around password strength, safety,
    and so on. The authors have experienced situations where an encrypted disk was
    handed from one company to another, and the handwritten password handed over at
    the same time! Apart from a risk to data security, there are also possible consequences
    in respect of disciplinary action against the individuals involved. If data is
    put at risk, it's always worth checking best practice and highlighting issues;
    it is very easy to become lax in this area, and sooner or later, data will be
    compromised and someone will have to take responsibility - it may not necessarily
    be the individual who lost the media itself.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have explored the topic of data security and explained some
    of the surrounding issues. We have discovered that not only is there technical
    knowledge to master, but also that a data security mindset is just as important.
    Data security is often overlooked and, therefore, taking a systematic approach,
    and educating others, is a key responsibility for mastering data science.
  prefs: []
  type: TYPE_NORMAL
- en: We have explained the data security life cycle and outlined the most important
    areas of responsibility, including authorization, authentication and access, along
    with related examples and use cases. We have also explored the Hadoop security
    ecosystem and described the important open source solutions currently available.
  prefs: []
  type: TYPE_NORMAL
- en: A significant part of this chapter was dedicated to building a Hadoop `InputFormat`
    *compressor* that operates as a data encryption utility that can be used with
    Spark. Appropriate configuration allows the codec to be used in a variety of key
    areas, crucially when spilling shuffled records to local disk where *currently
    no solution exists*.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will explore Scalable Algorithms, demonstrating the
    key techniques that we can master to enable performance at a truly "big data"
    scale.
  prefs: []
  type: TYPE_NORMAL
