- en: Design Techniques and Strategies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we take a step back and look at broader topics in computer
    algorithm design. As your experience with programming grows, certain patterns
    start to become apparent. The world of algorithms contains a plethora of techniques
    and design principles. A mastery of these techniques is required to tackle harder
    problems in the field.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will discuss the ways in which different kinds of algorithms
    can be categorized. Design techniques will be described and illustrated. We will
    also further discuss the analysis of algorithms. Finally, we will provide detailed
    implementations for a few very important algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: The classification of algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Various algorithm design methodologies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The implementation and explanation of various important algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The source code used in this chapter is available at the following GitHub link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Hands-On-Data-Structures-and-Algorithms-with-Python-3.7-Second-Edition/tree/master/Chapter13](https://github.com/PacktPublishing/Hands-On-Data-Structures-and-Algorithms-with-Python-3.7-Second-Edition/tree/master/Chapter13).'
  prefs: []
  type: TYPE_NORMAL
- en: Classification of algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are a number of classification schemes, based on what the algorithm is
    designed to achieve. In previous chapters, we implemented various algorithms.
    The question to ask is: do these algorithms share the same form or any similarities? If
    the answer is yes, then ask: what are the similarities and characteristics being
    used as the basis for comparison? If the answer is no, then can the algorithms
    be grouped into classes?'
  prefs: []
  type: TYPE_NORMAL
- en: These are the questions that we will discuss in the subsequent subsections.
    Here we present the major methods for classifying algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Classification by implementation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When translating a series of steps or processes into a working algorithm, there
    are a number of forms that it may take. The heart of the algorithm may employ
    one or more of the following assets.
  prefs: []
  type: TYPE_NORMAL
- en: Recursion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Recursive algorithms are the ones that call themselves to repeatedly execute
    code until a certain condition is satisfied. Some problems are more easily expressed
    by implementing their solution through recursion. One classic example is the Towers
    of Hanoi.
  prefs: []
  type: TYPE_NORMAL
- en: In simple terms, an iterative function is one that loops to repeat some part
    of the code, and a recursive function is one that calls itself to repeat the code.
    An iterative algorithm, on the other hand, uses a series of steps or a repetitive
    construct to formulate a solution; it iteratively executes a part of the code.
  prefs: []
  type: TYPE_NORMAL
- en: This repetitive construct could be a simple `while` loop, or any other kind
    of loop. Iterative solutions also come to mind more easily than their recursive
    implementations.
  prefs: []
  type: TYPE_NORMAL
- en: Logic
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One implementation of an algorithm is expressing it as a controlled logical
    deduction. This logic component is comprised of the axioms that will be used in
    the computation. The control component determines the manner in which deduction
    is applied to the axioms. This is expressed in the form a*lgorithm = logic + control*.
    This forms the basis of the logic programming paradigm.
  prefs: []
  type: TYPE_NORMAL
- en: The logic component determines the meaning of the algorithm. The control component
    only affects its efficiency. Without modifying the logic, the efficiency can be
    improved by improving the control component.
  prefs: []
  type: TYPE_NORMAL
- en: Serial or parallel algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The RAM model of most computers allows for the assumption that computing is
    carried out one instruction at a time.
  prefs: []
  type: TYPE_NORMAL
- en: Serial algorithms, also known as **sequential algorithms**, are algorithms that
    are executed sequentially. Execution commences from start to finish without any
    other execution procedure.
  prefs: []
  type: TYPE_NORMAL
- en: To be able to process several instructions at once, a different model or computing
    technique is required. Parallel algorithms perform more than one operation at
    a time. In the PRAM model, there are serial processors that share a global memory.
    The processors can also perform various arithmetic and logical operations in parallel.
    This enables the execution of several instructions at one time.
  prefs: []
  type: TYPE_NORMAL
- en: Parallel/distributed algorithms divide a problem into subproblems among its
    processors to collect the results. Some sorting algorithms can be efficiently
    parallelized, while iterative algorithms are generally parallelizable.
  prefs: []
  type: TYPE_NORMAL
- en: Deterministic versus nondeterministic algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deterministic algorithms produce the same output without fail every time the
    algorithm is run with the same input. There are some sets of problems that are
    so complex in the design of their solutions that expressing their solution in
    a deterministic way can be a challenge.
  prefs: []
  type: TYPE_NORMAL
- en: Nondeterministic algorithms can change the order of execution or some internal
    subprocess, leading to a change in the final result each time the algorithm is
    run.
  prefs: []
  type: TYPE_NORMAL
- en: As such, with every run of a nondeterministic algorithm, the output of the algorithm
    will be different. For instance, an algorithm that makes use of a probabilistic
    value will yield different outputs on successive executions, depending on the
    value of the random number generated.
  prefs: []
  type: TYPE_NORMAL
- en: Classification by complexity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To determine the complexity of an algorithm is to estimate how much space (memory)
    and time is needed during computation or program execution. Generally, the performance
    of the two algorithms is compared with their complexity. The lower complexity
    algorithm—that is, the one requiring less space and time to perform a given task—is
    preferred.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 3, *Principles of Algorithm Design*, presents more comprehensive coverage
    of complexity. We will summarize what we have learned here.
  prefs: []
  type: TYPE_NORMAL
- en: Complexity curves
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's consider a problem of magnitude *n*. To determine the time complexity
    of an algorithm, we denote it with **T**(n). The value may fall under **O**(*1*),
    **O**(*log n*), **O**(*n*), **O**(*n log(n)*), **O**(*n²*), **O**(*n³*), or **O**(*2^n*).
    Depending on the steps an algorithm performs, the time complexity may or may not
    be affected. The notation **O**(*n*) captures the growth rate of an algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Let's now examine a practical scenario, to determine which algorithm is better
    for solving a given problem. How do we come to the conclusion that the bubble
    sort algorithm is slower than the quick sort algorithm? Or, in general, how do
    we measure the efficiency of one algorithm against the other?
  prefs: []
  type: TYPE_NORMAL
- en: Well, we can compare the Big **O** of any number of algorithms to determine
    their efficiency. This approach gives us a time measure or growth rate, which
    charts the behavior of the algorithm as *n* gets bigger.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a graph of the different runtimes that an algorithm''s performance
    may fall under:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/92ff52c1-8d4d-4f7a-901f-117345b34c1d.png)'
  prefs: []
  type: TYPE_IMG
- en: In ascending order, the list of runtimes from best to worst is **O(1)**, **O(log
    n)**, **O(*n*)**, **O(*n log n*)**, **O(*n²*)**, **O(*n³*)**, and **O(*2^n*)**.
    Therefore, if an algorithm has a time complexity of **O****(1)**, and another
    algorithm for the same task has the complexity **O(log n),** the first algorithm
    should be preferred.
  prefs: []
  type: TYPE_NORMAL
- en: Classification by design
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we present categories of algorithms based on their design.
  prefs: []
  type: TYPE_NORMAL
- en: A given problem may have a number of solutions. When these solutions are analyzed,
    it is observed that each one follows a certain pattern or technique. We can categorize
    the algorithms based on how they solve the problem, as in the following subsections.
  prefs: []
  type: TYPE_NORMAL
- en: Divide and conquer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This approach to problem-solving is just as its name suggests. To solve (conquer)
    a certain problem, the algorithm divides it into subproblems that can easily be
    solved. Further, the solutions to each of these subproblems are combined in such
    a way that the final solution is the solution of the original problem.
  prefs: []
  type: TYPE_NORMAL
- en: The way in which the problems are broken down into smaller subproblems is mostly
    done by recursion. We will examine this technique in detail in the subsequent
    subsections. Some algorithms that use this technique include merge sort, quick
    sort, and binary search.
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic programming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This technique is similar to divide and conquer, in that a problem is broken
    down into smaller problems. However, in divide and conquer, each subproblem has
    to be solved before its results can be used to solve bigger problems.
  prefs: []
  type: TYPE_NORMAL
- en: By contrast, dynamic programming does not compute the solution to an already
    encountered subproblem. Rather, it uses a remembering technique to avoid the recomputation.
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic programming problems have two characteristics—**optimal substructure**,
    and **overlapping subproblem**. We will discuss this further in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Greedy algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It may be quite difficult to determine the best solution for a certain problem.
    To overcome this, we resort to an approach where we select the most promising
    choice from multiple available options or choices.
  prefs: []
  type: TYPE_NORMAL
- en: With greedy algorithms, the guiding rule is to always select the option that
    yields the most beneficial results and to continue doing that, hoping to reach
    a perfect solution. This technique aims to find a global optimal final solution
    by making a series of local optimal choices. The local optimal choice seems to
    lead to the solution.
  prefs: []
  type: TYPE_NORMAL
- en: Technical implementation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's dig into the implementation of some of the theoretical programming techniques
    we have discussed. We start with dynamic programming.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation using dynamic programming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we have already described, in this approach, we divide a given problem into
    smaller subproblems. In finding the solution, care is taken not to recompute any
    of the previously encountered subproblems.
  prefs: []
  type: TYPE_NORMAL
- en: This sounds a bit like recursion, but things are a little different here. A
    problem may lend itself to being solved by using dynamic programming, but will
    not necessarily take the form of making recursive calls.
  prefs: []
  type: TYPE_NORMAL
- en: One property that makes a problem an ideal candidate for being solved with dynamic
    programming is that it has an **overlapping set of subproblems**.
  prefs: []
  type: TYPE_NORMAL
- en: Once we realize that the form of subproblems has repeated itself during computation,
    we need not compute it again. Instead, we return a pre-computed result for that previously
    encountered subproblem.
  prefs: []
  type: TYPE_NORMAL
- en: To ensure that we never have to re-evaluate a subproblem, we need an efficient
    way to store the results of each subproblem. The following two techniques are
    readily available.
  prefs: []
  type: TYPE_NORMAL
- en: Memoization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This technique starts from the initial problem set, and divides it into small
    subproblems. After the solution to a subprogram has been determined, we store
    the result to that particular subproblem. In the future, when this subproblem
    is encountered, we only return its pre-computed result.
  prefs: []
  type: TYPE_NORMAL
- en: Tabulation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In tabulation, we fill a table with solutions to subproblems, and then combine
    them to solve bigger problems.
  prefs: []
  type: TYPE_NORMAL
- en: The Fibonacci series
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's consider an example to understand how dynamic programming works. We use
    the Fibonacci series to illustrate both the memoization and tabulation techniques.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Fibonacci series can be demonstrated using a recurrence relation. Recurrence
    relations are recursive functions that are used to define mathematical functions
    or sequences. For example, the following recurrence relation defines the Fibonacci
    sequence [1, 1, 2, 3, 5, 8 ...]:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Note that the Fibonacci sequence can be generated by putting the values of *n*
    in sequence [1, 2, 3, 4, ...].
  prefs: []
  type: TYPE_NORMAL
- en: The memoization technique
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s generate the Fibonacci series to the fifth term:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'A recursive-style program to generate the sequence would be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The code is very simple, but a little tricky to read because of the recursive
    calls being made that end up solving the problem.
  prefs: []
  type: TYPE_NORMAL
- en: When the base case is met, the `fib()` function returns 1\. If *n* is equal
    to or less than 2, the base case is met.
  prefs: []
  type: TYPE_NORMAL
- en: 'If the base case is not met, we will call the `fib()` function again, and this
    time supply the first call with `n-1`, and the second with `n-2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The layout of the strategy to solve the `i^(th)` term in the Fibonacci sequence
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/d80891b2-a57d-4b44-9630-76fafc37a303.png)'
  prefs: []
  type: TYPE_IMG
- en: Careful observation of the tree diagram shows some interesting patterns. The
    call to **fib(1)** happens twice. The call to **fib(2)** happens thrice. Also,
    the call to **fib(3)** happens twice.
  prefs: []
  type: TYPE_NORMAL
- en: The return values of the same function call never change; for example, the return
    value for **fib(2)** will always be the same whenever we call it. It will also
    be the same for **fib(1)** and **fib(3)**. Thus, computational time will be wasted
    if we compute again whenever we encounter the same function, since the same result
    is returned.
  prefs: []
  type: TYPE_NORMAL
- en: These repeated calls to a function with the same parameters and output suggest
    that there is an overlap. Certain computations reoccur down in the smaller subproblems.
  prefs: []
  type: TYPE_NORMAL
- en: A better approach is to store the results of the computation of **fib(1)** the
    first time it is encountered. Similarly, we should store return values for **fib(2)**
    and **fib(3)**. Later, anytime we encounter a call to **fib(1)**, **fib(2)**,
    or **fib(3)**, we simply return their respective results.
  prefs: []
  type: TYPE_NORMAL
- en: 'The diagram of our `fib` calls will now look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/f7165fe9-182b-46e1-9d5d-e83ea6187486.png)'
  prefs: []
  type: TYPE_IMG
- en: We have eliminated the need to compute **fib(3)**, fib(2), and **fib(1)** if
    they are encountered multiple times. This typifies the memoization technique,
    wherein there is no recomputation of overlapping calls to functions when breaking
    a problem into its subproblems.
  prefs: []
  type: TYPE_NORMAL
- en: 'The overlapping function calls in our Fibonacci example are **fib(1)**, **fib(2)**,
    and **fib(3)**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'To create a list of 1,000 elements, we do the following and pass it to the
    lookup parameter of the `dyna_fib` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'This list will store the value of the computation of the various calls to the `dyna_fib()`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Any call to the `dyna_fib()` function with *n* being less than or equal to 2
    will return 1\. When `dyna_fib(1)` is evaluated, we store the value at index 1
    of `map_set`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Write the condition for `lookup[n]` as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: We pass lookup so that it can be referenced when evaluating the subproblems.
    The calls to `dyna_fib(n-1, lookup)` and `dyna_fib(n-2, lookup)` are stored in
    `lookup[n]`.
  prefs: []
  type: TYPE_NORMAL
- en: When we run our updated implementation of the function to find the `*i*^(th)`
    term of the Fibonacci series, we can see a considerable improvement. This implementation
    runs much faster than our initial implementation. Supply the value 20 to both
    implementations, and witness the difference in the execution speed.
  prefs: []
  type: TYPE_NORMAL
- en: However, the updated algorithm has sacrificed space complexity to achieve this,
    because of the use of additional memory in storing the results of the function
    calls.
  prefs: []
  type: TYPE_NORMAL
- en: The tabulation technique
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A second technique in dynamic programming involves the use of a table of results,
    or matrix in some cases, to store the results of computations for later use.
  prefs: []
  type: TYPE_NORMAL
- en: 'This approach solves the bigger problem by first working out a route to the
    final solution. In the case of the `fib()` function, we develop a table with the
    values of `fib(1)` and `fib(2)` predetermined. Based on these two values, we will
    work our way up to `fib(n)`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The `results` variable stores the values 1 and 1 at indices 0 and 1\. This represents
    the return values of `fib(1)` and `fib(2)`. To calculate the values of the `fib()`
    function for values higher than 2, we simply call the `for` loop, which appends
    the sum of the `results[i-1] + results[i-2]` to the list of results.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation using divide and conquer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This programming approach to problem-solving emphasizes the need to break down
    a problem into smaller subproblems of the same type or form of the original problem.
    These subproblems are solved and combined to obtain the solution of the original
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: The following three steps are associated with this kind of programming.
  prefs: []
  type: TYPE_NORMAL
- en: Divide
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To divide means to break down an entity or problem. Here, we devise the means
    to break down the original problem into subproblems. We can achieve this through
    iterative or recursive calls.
  prefs: []
  type: TYPE_NORMAL
- en: Conquer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is impossible to continue breaking the problems into subproblems indefinitely.
    At some point, the smallest indivisible problem will return a solution. Once this
    happens, we can reverse our thought process and say that if we know the solution
    to the smallest subproblem, we can obtain the final solution to the original problem.
  prefs: []
  type: TYPE_NORMAL
- en: Merge
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To obtain the final solution, we need to combine the solutions to the smaller
    problems in order to solve the bigger problem.
  prefs: []
  type: TYPE_NORMAL
- en: There are other variants to the divide and conquer algorithms, such as merge
    and combine, and conquer and solve. Many algorithms make use of the divide and
    conquer principle, such as merge sorting, quick sort, and Strassen's matrix multiplication.
  prefs: []
  type: TYPE_NORMAL
- en: We will now describe implementation of a merge sort algorithm, as we saw earlier
    in Chapter 3, *Principles of Algorithm Design*.
  prefs: []
  type: TYPE_NORMAL
- en: Merge sort
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Merge sort algorithms are based on the divide and conquer rule. Given a list
    of unsorted elements, we split the list into two approximate halves. We continue
    to divide the list into two halves recursively.
  prefs: []
  type: TYPE_NORMAL
- en: 'After a while, the sublists created as a result of the recursive call will
    contain only one element. At that point, we begin to merge the solutions in the
    conquer or merge step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Our implementation starts by accepting the list of unsorted elements into the
    `merge_sort` function. The `if` statement is used to establish the base case,
    where, if there is only one element in the `unsorted_list`, we simply return that
    list again. If there is more than one element in the list, we find the approximate
    middle using `mid_point = int((len(unsorted_list)) // 2)`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using this `mid_point`, we divide the list into two sublists, namely, `first_half`
    and `second_half`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'A recursive call is made by passing the two sublists to the `merge_sort` function
    again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Now for the merge step. When `half_a` and `half_b` have been passed their values,
    we call the `merge` function, which will merge or combine the two solutions stored
    in `half_a` and `half_b`, which are lists:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The `merge` function takes the two lists we want to merge together, `first_sublist`
    and `second_sublist`. The `i` and `j` variables are initialized to 0, and are
    used as pointers to tell us where we are in the two lists with respect to the
    merging process.
  prefs: []
  type: TYPE_NORMAL
- en: 'The final `merged_list` will contain the merged list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The `while` loop starts comparing the elements in `first_sublist` and `second_sublist`.
    The `if` statement selects the smaller of the two, `first_sublist[i]` or `second_sublist[j]`,
    and appends it to `merged_list`. The `i` or `j` index is incremented to reflect
    where we are with the merging step. The `while` loop stops when either sublist is
    empty.
  prefs: []
  type: TYPE_NORMAL
- en: There may be elements left behind in either `first_sublist` or `second_sublist`.
    The last two `while` loops make sure that those elements are added to `merged_list`
    before it is returned.
  prefs: []
  type: TYPE_NORMAL
- en: The last call to `merge(half_a, half_b)` will return the sorted list.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s give the algorithm a dry run by merging the two sublists `[4, 6, 8]`
    and `[5, 7, 11, 40]`:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Step** | `first_sublist` | `second_sublist` | `merged_list` |'
  prefs: []
  type: TYPE_TB
- en: '| Step 0 | `[4 6 8]` | `[5 7 11 40]` | `[]` |'
  prefs: []
  type: TYPE_TB
- en: '| Step 1 | `[ 6 8]` | `[5 7 11 40]` | `[4]` |'
  prefs: []
  type: TYPE_TB
- en: '| Step 2 | `[ 6 8]` | `[ 7 11 40]` | `[4 5]` |'
  prefs: []
  type: TYPE_TB
- en: '| Step 3 | `[ 8]` | `[ 7 11 40]` | `[4 5 6]` |'
  prefs: []
  type: TYPE_TB
- en: '| Step 4 | `[ 8]` | `[ 11 40]` | `[4 5 6 7]` |'
  prefs: []
  type: TYPE_TB
- en: '| Step 5 | `[ ]` | `[ 11 40]` | `[4 5 6 7 8]` |'
  prefs: []
  type: TYPE_TB
- en: '| Step 6 | `[]` | `[ ]` | `[4 5 6 7 8 11 40]` |'
  prefs: []
  type: TYPE_TB
- en: Note that the text in bold represents the current item referenced in the loops
    `first_sublist` (which uses the `i` *index*) and `second_sublist` (which uses
    the `j` index).
  prefs: []
  type: TYPE_NORMAL
- en: At this point in the execution, the third `while` loop in the merge function
    kicks in to move 11 and 40 into `merged_list`. The returned `merged_list` will
    contain the fully sorted list.
  prefs: []
  type: TYPE_NORMAL
- en: Note that while the merge algorithm takes `O(n)` time, the merge sort algorithm
    has a running time complexity of `O(log n) T(n) = O(n)*O(log n) = O(n log n)`.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation using greedy algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we discussed earlier, greedy algorithms make decisions to yield the best
    possible local solution, which in turn provides the optimal solution. It is the
    hope of this technique that by making the best possible choices at each step,
    the total path will lead to an overall optimal solution or end.
  prefs: []
  type: TYPE_NORMAL
- en: Examples of greedy algorithms include **Prim's algorithm** for finding a minimum
    spanning tree, the **Knapsack problem**, and the **Traveling Salesman problem**,
    to mention just a few.
  prefs: []
  type: TYPE_NORMAL
- en: Coin-counting problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To demonstrate how the greedy technique works, let's look at an example. Consider
    a problem in which we wish to compute the minimum number of coin required to make
    a given amount A, where we have an infinite supply of the given coins' values.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, in some arbitrary country, we have the following coin denominations:
    1, 5, and 8 GHC. Given an amount (for example, 12 GHC), we want to find the smallest
    possible number of coins needed to provide this amount.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The algorithm to obtain the minimum number of coins to provide a given amount
    *A* using denominations `{a[1],a[2],a[3]...a[n]}` is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: We sort the list of denominations `{a[1], a[2], a[3] ...a[n]}`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We get the largest denomination in `{a[1], a[2], a[3]...a[n]}` which is smaller
    than A.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We obtain the division by dividing A by the largest denomination.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We get the remaining amount A by getting the remainder using (A % largest denominator).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If the value of A becomes 0, then we return the result.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Else If the value of A is greater than 0, we append the largest denominator
    and division variable in the result variable. And repeat the steps 2-5.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using the greedy approach, we first pick the largest value from the available
    denominations—which is 8—to divide into 12\. The remainder, 4, cannot be divided
    by either 8 or the next lowest denomination, 5\. So, we try the 1 GHC denomination
    coin, of which we need four. In the end, using this greedy algorithm, we return
    an answer of one 8 GHC coin and four 1 GHC coins.
  prefs: []
  type: TYPE_NORMAL
- en: 'So far, our greedy algorithm seems to be doing pretty well. A function that
    returns the respective denominations is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'This greedy algorithm always starts by using the largest denomination possible.
    Note that `denom` is a list of denominations, and that `sorted(denom, reverse=True)`
    will sort the list in reverse so that we can obtain the largest denomination at
    index *`0`*. Now, starting from index `*0*` of the sorted list of denominations,
    `sorted_denominations`, we iterate and apply the greedy technique:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The loop will run through the list of denominations. Each time the loop runs,
    it obtains the quotient, `div`, by dividing the `total_amount` by the current
    denomination, *i*. The `total_amount` variable is updated to store the remainder
    for further processing. If the quotient is greater than 0, we store it in `number_of_denoms`.
  prefs: []
  type: TYPE_NORMAL
- en: However, there are some possible instances where this algorithm may fail. For
    instance, when passed 12 GHC, our algorithm returned one 8 GHC and four 1 GHC
    coins. This output is, however, not the optimal solution. The best solution is
    to use two 5 GHC and two 1 GHC coins.
  prefs: []
  type: TYPE_NORMAL
- en: 'A better greedy algorithm is presented here. This time, the function returns
    a list of tuples that allow us to investigate the best results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The outer `for` loop enables us to limit the denominations from which we find
    our solution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Assuming that we have the list [5, 4, 3] in `sorted_denominations`, slicing
    it with `[j:]` helps us obtain the sublists [5, 4, 3], [4, 3], and [3], from which
    we try to find the right combination.
  prefs: []
  type: TYPE_NORMAL
- en: Shortest path algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The shortest path problem requires us to find out the shortest possible route
    between nodes in a graph. It has important applications for mapping and route
    planning, when plotting the most efficient way to get from point **A** to point
    **B**.
  prefs: []
  type: TYPE_NORMAL
- en: Dijkstra's algorithm is a very popular method of solving this problem. This
    algorithm is used to find the shortest distance from a source to all other nodes
    or vertices in a graph. Here we explain how we can use the greedy approach to
    solve this problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'Dijkstra''s algorithm works for weighted directed and undirected graphs. The
    algorithm produces the output of a list of the shortest path from a given source
    node A in a weighted graph. The algorithm works as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Initially, mark all the nodes as unvisited, and set their distance from the
    given source node to infinity (the source node is set to zero).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set the source node as current.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For the current node, look for all the unvisited adjacent nodes; compute the
    distance to that node from the source node through the current node. Compare the
    newly computed distance to the currently assigned distance, and if it is smaller,
    set this as the new value.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once we have considered all the unvisited adjacent nodes of the current node,
    we mark it as visited.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We next consider the next unvisited node which has the shortest distance from
    the source node. Repeat steps 2 to 4.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We stop when the list of unvisited nodes is empty, meaning we have considered
    all the unvisited nodes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Consider the following example of a weighted graph with six nodes [A, B, C,
    D, E, F] to understand how Dijkstra''s algorithm works:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/30207f74-106b-4b35-8d06-0cf1a2377a4f.png)'
  prefs: []
  type: TYPE_IMG
- en: By manual inspection, the shortest path between node **A** and node **D** at
    first glance seems to be the direct line with a distance of 9\. However, the shortest
    route means the lowest total distance, even if this comprises several parts. By
    comparison, traveling from node **A** to node **E** to node **F** and finally
    to node **D **will incur a total distance of 7, making it a shorter route.
  prefs: []
  type: TYPE_NORMAL
- en: We would implement the shortest path algorithm with a single source. It would determine
    the shortest path from the origin, which in this case is **A**, to any other node
    in the graph.
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 8](7223bbb5-9af3-4c48-88d0-fe2ebeb6900b.xhtml), *Graphs and Other
    Algorithms*, we discussed how to represent a graph with an adjacency list. We
    use an adjacency list along with the weight/cost/distance on every edge to represent
    the graph, as shown in the following Python code. A table is used to keep track
    of the shortest distance from the source in the graph to any other node. A Python
    dictionary will be used to implement this table.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the starting table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Node** | **Shortest distance from source** | **Previous node** |'
  prefs: []
  type: TYPE_TB
- en: '| **A** | 0 | None |'
  prefs: []
  type: TYPE_TB
- en: '| **B** | ∞ | None |'
  prefs: []
  type: TYPE_TB
- en: '| **C** | ∞ | None |'
  prefs: []
  type: TYPE_TB
- en: '| **D** | ∞ | None |'
  prefs: []
  type: TYPE_TB
- en: '| **E** | ∞ | None |'
  prefs: []
  type: TYPE_TB
- en: '| **F** | ∞ | None |'
  prefs: []
  type: TYPE_TB
- en: 'The adjacency list for the diagram and table is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The nested dictionary holds the distance and adjacent nodes.
  prefs: []
  type: TYPE_NORMAL
- en: When the algorithm starts, the shortest distance from the given source node
    (**A**) to any of the nodes is unknown. Thus, we initially set the distance to
    all other nodes to infinity, with the exception of node **A** , as the distance
    from node **A** to node **A** is 0.
  prefs: []
  type: TYPE_NORMAL
- en: No prior nodes have been visited when the algorithm begins. Therefore, we mark
    the previous node column of the node **A** as None.
  prefs: []
  type: TYPE_NORMAL
- en: 'In step 1 of the algorithm, we start by examining the adjacent nodes to node
    **A**. To find the shortest distance from node **A** to node **B**, we need to
    find the distance from the start node to the previous node of node B, which happens
    to be node **A**, and add it to the distance from node **A** to node **B**. We
    do this for other adjacent nodes of **A**, which are **B**, **E**, and **D**.
    This is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/9745d5ee-f528-4bd2-b67b-181e3655f5c4.png)'
  prefs: []
  type: TYPE_IMG
- en: We take the adjacent node **B** as its distance from node **A** is minimum; the
    distance from the start node (**A**) to the previous node (None) is 0, and the
    distance from the previous node to the current node (**B**) is **5**. This sum
    is compared with the data in the shortest distance column of node **B**. Since
    **5** is less than infinity(**∞**), we replace **∞** with the smaller of the two,
    which is **5**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Any time the shortest distance of a node is replaced by a smaller value, we
    need to update the previous node column too for all the adjacent nodes of the
    current node. After this, we mark node **A** as visited:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/220248fc-eec9-4d42-9d69-1ac4a8ca26a2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'At the end of the first step, our table looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Node** | **Shortest distance from source** | **Previous node** |'
  prefs: []
  type: TYPE_TB
- en: '| **A*** | 0 | None |'
  prefs: []
  type: TYPE_TB
- en: '| B | 5 | A |'
  prefs: []
  type: TYPE_TB
- en: '| C | ∞ | None |'
  prefs: []
  type: TYPE_TB
- en: '| D | 9 | A |'
  prefs: []
  type: TYPE_TB
- en: '| E | 2 | A |'
  prefs: []
  type: TYPE_TB
- en: '| F | ∞ | None |'
  prefs: []
  type: TYPE_TB
- en: At this point, node **A** is considered visited. As such, we add node **A** to
    the list of visited nodes. In the table, we show that node **A** has been visited
    by making the text bold and appending an asterisk sign to it.
  prefs: []
  type: TYPE_NORMAL
- en: In the second step, we find the node with the shortest distance using our table
    as a guide. Node **E**, with its value of 2, has the shortest distance. This is
    what we can infer from the table about node **E**. To get to node **E**, we must
    visit node **A** and cover a distance of **2.** From node A, we cover a distance
    of 0 to get to the starting node, which is node **A** itself.
  prefs: []
  type: TYPE_NORMAL
- en: The adjacent nodes to node **E** are **A** and **F**. But node **A** has already
    been visited, so we will only consider node **F**. To find the shortest route
    or distance to node **F**, we must find the distance from the starting node to
    node **E** and add it to the distance between node **E** and **F**. We can find
    the distance from the starting node to node **E** by looking at the shortest distance
    column of node **E**, which has the value **2**. The distance from node **E**
    to **F** can be obtained from the adjacency list we developed in Python earlier
    in this section.
  prefs: []
  type: TYPE_NORMAL
- en: 'This distance is **3**. These two sum up to 5, which is less than infinity.
    Remember we are examining the adjacent node **F**. Since there are no more adjacent
    nodes to node **E**, we mark node **E** as visited. Our updated table and the
    figure will have the following values:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Node** | **Shortest distance from source** | **Previous node** |'
  prefs: []
  type: TYPE_TB
- en: '| **A*** | 0 | None |'
  prefs: []
  type: TYPE_TB
- en: '| B | 5 | A |'
  prefs: []
  type: TYPE_TB
- en: '| C | ∞ | None |'
  prefs: []
  type: TYPE_TB
- en: '| D | 9 | A |'
  prefs: []
  type: TYPE_TB
- en: '| **E*** | 2 | A |'
  prefs: []
  type: TYPE_TB
- en: '| F | 5 | E |'
  prefs: []
  type: TYPE_TB
- en: After visiting node **E**, we find the smallest value in the shortest distance
    column of the table, which is 5 for nodes **B** and **F**. Let us choose **B**
    instead of **F** purely on an alphabetical basis (we could equally have chosen
    **F**).
  prefs: []
  type: TYPE_NORMAL
- en: The adjacent nodes to **B** are **A** and **C**, but node **A** has already
    been visited. Using the rule we established earlier, the shortest distance from
    **A** to **C** is 7\. We arrive at this number because the distance from the starting
    node to node **B** is 5, while the distance from node **B** to **C** is 2.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since 7 is less than infinity, we update the shortest distance to 7 and update
    the previous node column with node **B**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/b6437e76-eaf8-4d3c-910e-cb5751588c6d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, **B** is also marked as visited. The new state of the table and the figure
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Node** | **Shortest distance from source** | **Previous node** |'
  prefs: []
  type: TYPE_TB
- en: '| **A*** | 0 | None |'
  prefs: []
  type: TYPE_TB
- en: '| **B*** | 5 | A |'
  prefs: []
  type: TYPE_TB
- en: '| C | 7 | B |'
  prefs: []
  type: TYPE_TB
- en: '| D | 9 | A |'
  prefs: []
  type: TYPE_TB
- en: '| **E*** | 2 | A |'
  prefs: []
  type: TYPE_TB
- en: '| F | 5 | E |'
  prefs: []
  type: TYPE_TB
- en: The node with the shortest distance yet unvisited is node **F**. The adjacent
    nodes to **F** are nodes **D** and **E**. But node **E** has already been visited.
    As such, we focus on finding the shortest distance from the starting node to node
    **D**.
  prefs: []
  type: TYPE_NORMAL
- en: 'We calculate this distance by adding the distance from node **A** to **F**
    to the distance from node **F** to **D**. This sums up to 7, which is less than
    **9**. Thus, we update the **9** with **7** and replace **A** with **F** in node
    **D**''s previous node column:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/d4a2d0a8-6a4e-46d5-b469-37674c0617df.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Node **F** is now marked as visited. Here is the updated table and the figure
    up to this point:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Node** | **Shortest distance from source** | **Previous node** |'
  prefs: []
  type: TYPE_TB
- en: '| **A*** | 0 | None |'
  prefs: []
  type: TYPE_TB
- en: '| **B*** | 5 | A |'
  prefs: []
  type: TYPE_TB
- en: '| C | 7 | B |'
  prefs: []
  type: TYPE_TB
- en: '| D | 7 | F |'
  prefs: []
  type: TYPE_TB
- en: '| **E*** | 2 | A |'
  prefs: []
  type: TYPE_TB
- en: '| **F*** | 5 | E |'
  prefs: []
  type: TYPE_TB
- en: 'Now, only two unvisited nodes are left, **C** and **D**, both with a distance
    cost of **7**. In alphabetical order, we choose to examine **C** because both
    nodes have the same shortest distance from the starting node **A**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/09e116c0-42d5-430f-95d8-3b10bf5f1788.png)'
  prefs: []
  type: TYPE_IMG
- en: 'However, all the adjacent nodes to **C** have been visited. Thus, we have nothing
    to do but mark node **C** as visited. The table remains unchanged at this point:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/d5ea3951-91ea-4723-8337-4d655593f431.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Lastly, we take node **D**, and find out that all its adjacent nodes have been
    visited too. We only mark it as visited. The table remains unchanged:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Node** | **Shortest distance from source** | **Previous node** |'
  prefs: []
  type: TYPE_TB
- en: '| **A*** | 0 | None |'
  prefs: []
  type: TYPE_TB
- en: '| **B*** | 5 | A |'
  prefs: []
  type: TYPE_TB
- en: '| **C*** | 7 | B |'
  prefs: []
  type: TYPE_TB
- en: '| **D*** | 7 | F |'
  prefs: []
  type: TYPE_TB
- en: '| **E*** | 2 | A |'
  prefs: []
  type: TYPE_TB
- en: '| **F*** | 5 | E |'
  prefs: []
  type: TYPE_TB
- en: 'Let''s verify this table with our initial graph. From the graph, we know that
    the shortest distance from **A** to **F** is **5**. We will need to go through
    **E** to get to node **F**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/93a5e488-c767-400f-8fcb-4a27523a7e7d.png)'
  prefs: []
  type: TYPE_IMG
- en: According to the table, the shortest distance from the source column for node
    **F** is 5\. This is true. It also tells us that to get to node **F**, we need
    to visit node **E**, and from **E** to node **A**, which is our starting node.
    This is actually the shortest path.
  prefs: []
  type: TYPE_NORMAL
- en: 'To implement Dijkstra''s algorithm to find the shortest path, we begin the
    program for finding the shortest distance by representing the table that enables
    us to track the changes in our graph. For the diagram we used, here is a dictionary
    representation of the table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The initial state of the table uses `float("inf")` to represent infinity. Each
    key in the dictionary maps to a list. At the first index of the list, the shortest
    distance from the source A is stored. At the second index, the previous node is
    stored:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: To avoid the use of magic numbers, we use the preceding constants. The shortest
    path column's index is referenced by `DISTANCE`. The previous node column's index
    is referenced by `PREVIOUS_NODE`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now all is set for the main function of the algorithm. It will take the graph,
    represented by the adjacency list, the table, and the starting node as parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: We keep the list of visited nodes in the `visited_nodes` list. The `current_node`
    and `starting_node` variables will both point to the node in the graph we choose
    to make our starting node. The `origin` value is the reference point for all other
    nodes with respect to finding the shortest path.
  prefs: []
  type: TYPE_NORMAL
- en: 'The heavy lifting of the whole process is accomplished by the use of a `while`
    loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Let's break down what the `while` loop is doing. In the body of the `while`
    loop, we obtain the current node in the graph we want to investigate with `adjacent_nodes
    = graph[current_node]`. Now, `current_node` should have been set prior. The `if`
    statement is used to find out whether all the adjacent nodes of `current_node`
    have been visited.
  prefs: []
  type: TYPE_NORMAL
- en: When the `while` loop is executed for the fir*s*t time, `current_node` will
    contain A and `adjacent_nodes` will contain nodes B, D, and E. Furthermore, `visited_nodes`
    will be empty too. If all nodes have been visited, we only move on to the statements
    further down the program. Otherwise, we begin a whole new step.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `set(adjacent_nodes).difference(set(visited_nodes))` statement returns
    the nodes that have not been visited. The loop iterates over this list of unvisited
    nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The `get_shortest_distance(table, vertex)` helper method will return the value
    stored in the shortest distance column of our table, using one of the unvisited
    nodes referenced by `vertex`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'When we are examining the adjacent nodes of the starting node, `distance_from_starting_node
    == INFINITY and current_node == starting_node` will evaluate to `True`, in which
    case we only have to find the distance between the starting node and vertex by
    referencing the graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The `get_distance` method is another helper method we use to obtain the value
    (distance) of the edge between `vertex` and `current_node`.
  prefs: []
  type: TYPE_NORMAL
- en: If the condition fails, then we assign to `total_distance` the sum of the distance
    from the starting node to `current_node`, and the distance between `current_node`
    and `vertex`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have our total distance, we need to check whether `total_distance`
    is less than the existing data in the shortest distance column of our table. If
    it is less, then we use the two helper methods to update that row:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, we add `current_node` to the list of visited nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: If all nodes have been visited, then we must exit the `while` loop. To check
    whether all the nodes have been visited, we compare the length of the `visited_nodes`
    list to the number of keys in our table. If they have become equal, we simply
    exit the `while` loop.
  prefs: []
  type: TYPE_NORMAL
- en: The `get_next_node` helper method is used to fetch the next node to visit. It
    is this method that helps us find the minimum value in the shortest distance column
    from the starting nodes using our table.
  prefs: []
  type: TYPE_NORMAL
- en: 'The whole method ends by returning the updated table. To print the table, we
    use the following statements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the output for the preceding statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'For the sake of completeness, let''s find out what the helper methods are doing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The `get_shortest_distance` function returns the value stored in index 0 of
    our table. At that index, we always store the shortest distance from the starting
    node up to `vertex`. The `set_shortest_distance` function only sets this value
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'When we update the shortest distance of a node, we update its previous node
    using the following method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Remember that the, `PREVIOUS_NODE` constant equals 1\. In the table, we store
    the value of `previous_node` at `table[vertex][PREVIOUS_NODE]`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To find the distance between any two nodes, we use the `get_distance` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The last helper method is the `get_next_node` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: The `get_next_node` function resembles a function to find the smallest item
    in a list.
  prefs: []
  type: TYPE_NORMAL
- en: The function starts off by finding the unvisited nodes in our table by using
    `visited_nodes` to obtain the difference between the two sets of lists. The very
    first item in the list of `unvisited_nodes` is assumed to be the smallest in the
    shortest distance column of `table`.
  prefs: []
  type: TYPE_NORMAL
- en: If a lesser value is found while the `for` loop runs, the `min_vertex` will
    be updated. The function then returns `min_vertex` as the unvisited vertex or
    node with the smallest shortest distance from the source.
  prefs: []
  type: TYPE_NORMAL
- en: The worst-case running time of Dijkstra's algorithm is **O**(*|E| + |V| log
    |V|*), where *|V|* is the number of vertices and *|E|* is the number of edges.
  prefs: []
  type: TYPE_NORMAL
- en: Complexity classes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Complexity classes group problems on the basis of their difficulty level, and the
    resources required in terms of time and space to solve them. In this section,
    we discuss the N, NP, NP-Complete, and NP-Hard complexity classes.
  prefs: []
  type: TYPE_NORMAL
- en: P versus NP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The advent of computers has sped up the rate at which certain tasks can be performed.
    In general, computers are good at perfecting the art of calculation and solving
    problems that can be reduced to a set of mathematical computations.
  prefs: []
  type: TYPE_NORMAL
- en: However, this assertion is not entirely true. There are some classes of problems
    that take an enormous amount of time for the computer to make a sound guess, let
    alone find the right solution.
  prefs: []
  type: TYPE_NORMAL
- en: In computer science, the class of problems that computers can solve within polynomial
    time using a step-wise process of logical steps is known as P-type, where P stands
    for polynomial. These are relatively easy to solve.
  prefs: []
  type: TYPE_NORMAL
- en: Then there is another class of problems that are considered very hard to solve.
    The word *hard problem* is used to refer to the way in which problems increase
    in difficulty when trying to find a solution. However, despite the fact that these
    problems have a high growth rate of difficulty, it is possible to determine whether a
    proposed solution solves the problem in polynomial time. These are known as NP-type
    problems. NP here stands for nondeterministic polynomial time.
  prefs: []
  type: TYPE_NORMAL
- en: Now the million dollar question is, does *P = NP*?
  prefs: []
  type: TYPE_NORMAL
- en: The proof for P* = NP* is one of the Millennium Prize Problems from the Clay
    Mathematics Institute, offering a million dollar prize for a correct solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Traveling Salesman problem is an example of an NP-type problem. The problem
    statement says: given *n* number of cities in a country, find the shortest route
    between them all, thus making the trip a cost-effective one.'
  prefs: []
  type: TYPE_NORMAL
- en: When the number of cities is small, this problem can be solved in a reasonable
    amount of time. However, when the number of cities is above any two-digit number,
    the time taken by the computer is remarkably long.
  prefs: []
  type: TYPE_NORMAL
- en: A lot of computer and cybersecurity systems are based on the RSA encryption
    algorithm. The strength of the algorithm is based on the fact that it uses the
    integer factoring problem, which is an NP-type problem.
  prefs: []
  type: TYPE_NORMAL
- en: Finding the prime factors of a prime number composed of many digits is very
    difficult. When two large prime numbers are multiplied, a large non-prime number
    is obtained. Factorization of this number is where many cryptographic algorithms
    borrow their strength.
  prefs: []
  type: TYPE_NORMAL
- en: 'All P-type problems are subsets of **NP** problems. This means that any problem
    that can be solved in polynomial time can also be verified in polynomial time:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/cfbbc0d9-bc13-467e-aeb5-982db9ad6891.png)'
  prefs: []
  type: TYPE_IMG
- en: But **P** = **NP** investigates whether problems that can be verified in polynomial
    time can also be solved in polynomial time. In particular, if they are equal,
    it means that problems that are solved by trying a number of possible solutions
    can be solved without the need to actually try all the possible solutions, invariably
    creating some sort of shortcut proof.
  prefs: []
  type: TYPE_NORMAL
- en: The proof, when finally discovered, will certainly have serious consequences
    for the fields of cryptography, game theory, mathematics, and many other fields.
  prefs: []
  type: TYPE_NORMAL
- en: NP-Hard
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A problem is NP-Hard if all other problems in NP can be polynomial-time-reducible,
    or mapped to it. It is at least as hard as the hardest problem in NP.
  prefs: []
  type: TYPE_NORMAL
- en: NP-Complete
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**NP-Complete** problems are the most difficult problems. A problem is considered an
    **NP-Complete** problem if it is an **NP-Hard** problem that is also found in
    the **NP** class.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we show the Venn diagram for various complexity groups:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/c47f7c29-0359-481b-b287-34e5ca2e9a45.png)'
  prefs: []
  type: TYPE_IMG
- en: Knowledge discovery in data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To extract useful information from the given data, we initially collect the
    raw data that is to be used to learn the patterns. Next, we apply the data preprocessing
    techniques to remove the noise from the data. Further more, we extract the important
    features from the data, which are representative of the data, to develop the model.
    Feature extraction is the most crucial step for machine learning algorithms to
    work effectively. A good feature must be informative and discriminating for the
    machine learning algorithms. Feature selection techniques are used to remove the irrelevant,
    redundant, and noisy features. Further more, the prominent features are fed to
    the machine learning algorithms to learn the patterns in the data. Finally, we
    apply the evaluation measure to judge the performance of the developed model and
    use visualization techniques to visualize the results and data. Here are the steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Data collection
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Data preprocessing
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Feature extraction
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Feature selection
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Machine learning
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluation and visualization
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed in detail algorithm design techniques, which are
    very important in the computer science field. Without too much mathematical rigor,
    we also discussed some of the main categories into which algorithms are classified.
  prefs: []
  type: TYPE_NORMAL
- en: Other design techniques in the field, such as the divide and conquer, dynamic
    programming, and greedy algorithms, were also covered, along with implementations
    of important sample algorithms.  Lastly, we presented a brief discussion on complexity
    classes. We saw how proof for P = NP will definitely be a game-changer in a number
    of fields, if such a proof is ever discovered.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will be discussing some real-world applications, tools,
    and the basics of machine learning applications.
  prefs: []
  type: TYPE_NORMAL
