- en: Abstracting Data with RDDs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover how to work with Apache Spark Resilient Distributed
    Datasets. You will learn the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Creating RDDs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reading data from files
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overview of RDD transformations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overview of RDD actions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pitfalls of using RDDs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Resilient Distributed Datasets** (**RDDs**) are collections of immutable
    JVM objects that are distributed across an Apache Spark cluster. Please note that
    if you are new to Apache Spark, you may want to initially skip this chapter as
    Spark DataFrames/Datasets are both significantly easier to develop and typically
    have faster performance. More information on Spark DataFrames can be found in
    the next chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: An RDD is the most fundamental dataset type of Apache Spark; any action on a
    Spark DataFrame eventually gets *translated* into a highly optimized execution
    of transformations and actions on RDDs (see the paragraph on catalyst optimizer
    in [Chapter 3](part0120.html#3IE3G0-dc04965c02e747b9b9a057725c821827), *Abstracting
    Data with DataFrames*, in the *Introduction* section).
  prefs: []
  type: TYPE_NORMAL
- en: Data in an RDD is split into chunks based on a key and then dispersed across
    all the executor nodes. RDDs are highly resilient, that is, there are able to
    recover quickly from any issues as the same data chunks are replicated across
    multiple executor nodes. Thus, even if one executor fails, another will still
    process the data. This allows you to perform your functional calculations against
    your dataset very quickly by harnessing the power of multiple nodes. RDDs keep
    a log of all the execution steps applied to each chunk. This, on top of the data
    replication, speeds up the computations and, if anything goes wrong, RDDs can
    still recover the portion of the data lost due to an executor error.
  prefs: []
  type: TYPE_NORMAL
- en: While it is common to lose a node in distributed environments (for example,
    due to connectivity issues, hardware problems), distribution and replication of
    the data defends against data loss, while data lineage allows the system to recover
    quickly.
  prefs: []
  type: TYPE_NORMAL
- en: Creating RDDs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For this recipe, we will start creating an RDD by generating the data within
    the PySpark. To create RDDs in Apache Spark, you will need to first install Spark
    as shown in the previous chapter. You can use the PySpark shell and/or Jupyter
    notebook to run these code samples.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We require a working installation of Spark. This means that you would have
    followed the steps outlined in the previous chapter. As a reminder, to start PySpark
    shell for your local Spark cluster, you can run this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Where `n` is the number of cores.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To quickly create an RDD, run PySpark on your machine via the bash terminal,
    or you can run the same query in a Jupyter notebook. There are two ways to create
    an RDD in PySpark: you can either use the `parallelize()` method—a collection
    (list or an array of some elements) or reference a file (or files) located either
    locally or through an external source, as noted in subsequent recipes.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'To view what is inside your RDD, you can run the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Spark context parallelize method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Under the covers, there are quite a few actions that happened when you created
    your RDD. Let''s start with the RDD creation and break down this code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Focusing first on the statement in the `sc.parallelize()` method, we first
    created a Python list (that is, `[A, B, ..., E]`) composed of a list of arrays
    (that is, `(''Mike'', 19), (''June'', 19), ..., (''Scott'', 17)`). The `sc.parallelize()`
    method is the SparkContext''s `parallelize` method to create a parallelized collection.
    This allows Spark to distribute the data across multiple nodes, instead of depending
    on a single node to process the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00020.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Now that we have created `myRDD` as a parallelized collection, Spark can operate
    against this data in parallel. Once created, the distributed dataset (`distData`)
    can be operated on in parallel. For example, we can call `myRDD.reduceByKey(add)`
    to add up the grouped by keys of the list; we have recipes for RDD operations
    in subsequent sections of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: .take(...) method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that you have created your RDD (`myRDD`), we will use the `take()` method
    to return the values to the console (or notebook cell). We will now execute an
    RDD action (more information on this in subsequent recipes), `take()`. Note that
    a common approach in PySpark is to use `collect()`, which returns all values in
    your RDD from the Spark worker nodes to the driver. There are performance implications
    when working with a large amount of data as this translates to large volumes of
    data being transferred from the Spark worker nodes to the driver. For small amounts
    of data (such as this recipe), this is perfectly fine, but, as a matter of habit,
    you should pretty much always use the `take(n)` method instead; it returns the
    first `n` elements of the RDD instead of the whole dataset. It is a more efficient
    method because it first scans one partition and uses those statistics to determine
    the number of partitions required to return the results.
  prefs: []
  type: TYPE_NORMAL
- en: Reading data from files
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For this recipe, we will create an RDD by reading a local file in PySpark.
    To create RDDs in Apache Spark, you will need to first install Spark as noted
    in the previous chapter. You can use the PySpark shell and/or Jupyter notebook
    to run these code samples. Note that while this recipe is specific to reading
    local files, a similar syntax can be applied for Hadoop, AWS S3, Azure WASBs,
    and/or Google Cloud Storage:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Storage type | Example |'
  prefs: []
  type: TYPE_TB
- en: '| Local files | `sc.textFile(''/local folder/filename.csv'')` |'
  prefs: []
  type: TYPE_TB
- en: '| Hadoop HDFS | `sc.textFile(''hdfs://folder/filename.csv'')` |'
  prefs: []
  type: TYPE_TB
- en: '| AWS S3 ([https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark-configure.html](https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark-configure.html))
    | `sc.textFile(''s3://bucket/folder/filename.csv'')` |'
  prefs: []
  type: TYPE_TB
- en: '| Azure WASBs ([https://docs.microsoft.com/en-us/azure/hdinsight/hdinsight-hadoop-use-blob-storage](https://docs.microsoft.com/en-us/azure/hdinsight/hdinsight-hadoop-use-blob-storage))
    | `sc.textFile(''wasb://bucket/folder/filename.csv'')` |'
  prefs: []
  type: TYPE_TB
- en: '| Google Cloud Storage ([https://cloud.google.com/dataproc/docs/concepts/connectors/cloud-storage#other_sparkhadoop_clusters](https://cloud.google.com/dataproc/docs/concepts/connectors/cloud-storage#other_sparkhadoop_clusters))
    | `sc.textFile(''gs://bucket/folder/filename.csv'')` |'
  prefs: []
  type: TYPE_TB
- en: '| Databricks DBFS ([https://docs.databricks.com/user-guide/dbfs-databricks-file-system.html](https://docs.databricks.com/user-guide/dbfs-databricks-file-system.html))
    | `sc.textFile(''dbfs://folder/filename.csv'')` |'
  prefs: []
  type: TYPE_TB
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will be reading a tab-delimited (or comma-delimited) file,
    so please ensure that you have a text (or CSV) file available. For your convenience, you
    can download the `airport-codes-na.txt` and `departuredelays.csv` files from [https://github.com/drabastomek/learningPySpark/tree/master/Chapter03/flight-data](https://github.com/drabastomek/learningPySpark/tree/master/Chapter03/flight-data).
    Ensure your local Spark cluster can access this file (for example, `~/data/flights/airport-codes-na.txt`).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once you start the PySpark shell via the bash terminal (or you can run the
    same query within Jupyter notebook), execute the following query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'If you are running Databricks, the same file is already included in the `/databricks-datasets`
    folder; the command is:'
  prefs: []
  type: TYPE_NORMAL
- en: '`myRDD = sc.textFile(''/databricks-datasets/flights/airport-codes-na.txt'').map(lambda
    element: element.split("\t"))`'
  prefs: []
  type: TYPE_NORMAL
- en: 'When running the query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting output is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Diving in a little deeper, let''s determine the number of rows in this RDD.
    Note that more information on RDD actions such as `count()` is included in subsequent
    recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Also, let''s find out the number of partitions that support this RDD:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: .textFile(...) method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To read the file, we are using SparkContext''s `textFile()` method via this
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Only the first parameter is required, which indicates the location of the text
    file as per `~/data/flights/airport-codes-na.txt`. There are two optional parameters
    as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '`minPartitions`: Indicates the minimum number of partitions that make up the
    RDD. The Spark engine can often determine the best number of partitions based
    on the file size, but you may want to change the number of partitions for performance
    reasons and, hence, the ability to specify the minimum number.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_unicode`: Engage this parameter if you are processing Unicode data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Note that if you were to execute this statement without the subsequent `map()` function,
    the resulting RDD would not reference the tab-delimiter—basically a list of strings
    that is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: .map(...) method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To make sense of the tab-delimiter with an RDD, we will use the `.map(...)` function
    to transform the data from a list of strings to a list of lists:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The key components of this map transformation are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`lambda`: An anonymous function (that is, a function defined without a name)
    composed of a single expression'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`split`: We''re using PySpark''s split function (within `pyspark.sql.functions`)
    to split a string around a regular expression pattern; in this case, our delimiter
    is a tab (that is, `\t`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Putting the `sc.textFile()` and `map()` functions together allows us to read
    the text file and split by the tab-delimiter to produce an RDD composed of a parallelized
    list of lists collection:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Partitions and performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Earlier in this recipe, if we had run `sc.textFile()` without specifying `minPartitions` for
    this dataset, we would only have two partitions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'But as noted, if the `minPartitions` flag is specified, then you would get
    the specified four partitions (or more):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'A key aspect of partitions for your RDD is that the more partitions you have,
    the higher the parallelism. Potentially, having more partitions will improve your
    query performance. For this portion of the recipe, let''s use a slightly larger
    file, `departuredelays.csv`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: As noted in the preceding code snippet, by default, Spark will create two partitions
    and take 3.33 seconds (on my small cluster) to count the 1.39 million rows in
    the departure delays CSV file.
  prefs: []
  type: TYPE_NORMAL
- en: 'Executing the same command, but also specifying `minPartitions` (in this case,
    eight partitions), you will notice that the `count()` method completed in 2.96
    seconds (instead of 3.33 seconds with eight partitions). Note that these values
    may be different based on your machine''s configuration, but the key takeaway
    is that modifying the number of partitions may result in faster performance due
    to parallelization. Check out the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Overview of RDD transformations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As noted in preceding sections, there are two types of operation that can be
    used to shape data in an RDD: transformations and actions. A transformation, as
    the name suggests, *transforms* one RDD into another. In other words, it takes
    an existing RDD and transforms it into one or more output RDDs. In the preceding
    recipes, we had used a `map()` function, which is an example of a transformation
    to split the data by its tab-delimiter.'
  prefs: []
  type: TYPE_NORMAL
- en: Transformations are lazy (unlike actions). They only get executed when an action
    is called on an RDD. For example, calling the `count()` function is an action;
    more information is available in the following section on actions.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This recipe will be reading a tab-delimited (or comma-delimited) file, so please
    ensure that you have a text (or CSV) file available. For your convenience, you
    can download the `airport-codes-na.txt` and `departuredelays.csv` files from [https://github.com/drabastomek/learningPySpark/tree/master/Chapter03/flight-data](https://github.com/drabastomek/learningPySpark/tree/master/Chapter03/flight-data).
    Ensure your local Spark cluster can access this file (for example, `~/data/flights/airport-codes-na.txt`).
  prefs: []
  type: TYPE_NORMAL
- en: If you are running Databricks, the same file is already included in the `/databricks-datasets`
    folder; the command is
  prefs: []
  type: TYPE_NORMAL
- en: '`myRDD = sc.textFile(''/databricks-datasets/flights/airport-codes-na.txt'').map(lambda
    line: line.split("\t"))`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Many of the transformations in the next section will use the RDDs `airports` or
    `flights`; let''s set them up using this code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we list common Apache Spark RDD transformations and code snippets.
    A more complete list can be found at [https://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations](https://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations),
    [https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD) and
    [https://training.databricks.com/visualapi.pdf](https://training.databricks.com/visualapi.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: 'The transformations include the following common tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Removing the header line from your text file: `zipWithIndex()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Selecting columns from your RDD: `map()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Running a `WHERE` (filter) clause: `filter()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Getting the distinct values: `distinct()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Getting the number of partitions: `getNumPartitions()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Determining the size of your partitions (that is, the number of elements within
    each partition): `mapPartitionsWithIndex()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: .map(...) transformation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `map(f)` transformation returns a new RDD formed by passing each element
    through a function, `f`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Look at the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'This will produce the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: .filter(...) transformation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `filter(f)`  transformation returns a new RDD based on selecting elements
    for which the `f` function returns true. Therefore, look at the following code
    snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'This will produce the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: .flatMap(...) transformation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `flatMap(f) `transformation is similar to map, but the new RDD flattens
    out all of the elements (that is, a sequence of events). Let''s look at the following
    snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code will produce the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: .distinct() transformation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `distinct()` transformation returns a new RDD containing the distinct elements
    of the source RDD. So, look at the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'This will return the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: .sample(...) transformation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `sample(withReplacement, fraction, seed)` transformation samples a fraction
    of the data, with or without replacement (the `withReplacement` parameter), based
    on a random seed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Look at the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'We can expect the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: .join(...) transformation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `join(RDD')` transformation returns an RDD of *(key, (val_left, val_right))*
    when calling RDD *(key, val_left)* and RDD *(key, val_right)*. Outer joins are
    supported through left outer join, right outer join, and full outer join.
  prefs: []
  type: TYPE_NORMAL
- en: 'Look at the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'This will give you the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: .repartition(...) transformation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `repartition(n)` transformation repartitions the RDD into `n` partitions
    by randomly reshuffling and uniformly distributing data across the network. As
    noted in the preceding recipes, this can improve performance by running more parallel
    threads concurrently. Here''s a code snippet that does precisely that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: .zipWithIndex() transformation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `zipWithIndex()` transformation appends (or ZIPs) the RDD with the element
    indices. This is very handy when wanting to remove the header row (first row)
    of a file.
  prefs: []
  type: TYPE_NORMAL
- en: 'Look at the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'This will generate this result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'To remove the header from your data, you can use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code will skip the header, as shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: .reduceByKey(...) transformation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `reduceByKey(f)` transformation reduces the elements of the RDD using `f` by
    the key. The `f` function should be commutative and associative so that it can
    be computed correctly in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: 'Look at the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'This will generate the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: .sortByKey(...) transformation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `sortByKey(asc)` transformation orders *(key, value)* RDD by *key* and
    returns an RDD in ascending or descending order. Look at the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'This will produce this output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: .union(...) transformation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `union(RDD)` transformation returns a new RDD that is the union of the
    source and argument RDDs. Look at the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'This will generate the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: .mapPartitionsWithIndex(...) transformation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `mapPartitionsWithIndex(f)` is similar to map but runs the `f` function separately
    on each partition and provides an index of the partition. It is useful to determine
    the data skew within partitions (check the following snippet):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code will produce the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Recall that a transformation takes an existing RDD and transforms it into one
    or more output RDDs. It is also a lazy process that is not initiated until an
    action is executed. In the following join example, the action is the `take()`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'To better understand what is happening when running this join, let''s review
    the Spark UI. Every Spark session launches a web-based UI, which is, by default,
    on port `4040`, for example, `http://localhost:4040`. It includes the following
    information:'
  prefs: []
  type: TYPE_NORMAL
- en: A list of scheduler stages and tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A summary of RDD sizes and memory usage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Environmental information
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Information about the running executors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For more information, please refer to the Apache Spark Monitoring documentation
    page at [https://spark.apache.org/docs/latest/monitoring.html](https://spark.apache.org/docs/latest/monitoring.html).
  prefs: []
  type: TYPE_NORMAL
- en: To dive deeper into Spark internals, a great video is Patrick Wendell's *Tuning
    and Debugging in Apache* *Spark* video, which is available at [https://www.youtube.com/watch?v=kkOG_aJ9KjQ](https://www.youtube.com/watch?v=kkOG_aJ9KjQ).
  prefs: []
  type: TYPE_NORMAL
- en: 'As can be seen in the following DAG visualization, the join statement and two
    preceding map transformations have a single job (Job 24) that created two stages
    (Stage 32 and Stage 33):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00021.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Details for Job 24
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s dive deeper into these two stages:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00022.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Details of Stage 32
  prefs: []
  type: TYPE_NORMAL
- en: 'To better understand the tasks executed in the first stage (Stage 32), we can
    dive deeper into the stage''s DAG Visualization as well as the Event Timeline:'
  prefs: []
  type: TYPE_NORMAL
- en: The two `textFile` callouts are to extract the two different files (`departuredelays.csv`
    and `airport-codes-na.txt`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once the `map` functions are complete, to support the `join`, Spark executes
    `UnionRDD` and `PairwiseRDD` to perform the basics behind the join as part of
    the `union` task
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the next stage, the `partitionBy` and `mapPartitions` tasks shuffle and
    re-map the partitions prior to providing the output via the `take()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00023.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Details of Stage 33
  prefs: []
  type: TYPE_NORMAL
- en: Note that that if you execute the same statements without the `take()` function
    (or some other *action*), only *transformation* operations will be executed with
    nothing showing up in the Spark UI denoting lazy processing.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, if you were to execute the following code snippet, note that the
    output is a pointer to a Python RDD:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: Overview of RDD actions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As noted in preceding sections, there are two types of Apache Spark RDD operations:
    transformations and actions. An *action* returns a value to the driver after running
    a computation on the dataset, typically on the workers. In the preceding recipes,
    the `take()` and `count()` RDD operations are examples of *actions*.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This recipe will be reading a tab-delimited (or comma-delimited) file, so please
    ensure that you have a text (or CSV) file available. For your convenience, you
    can download the `airport-codes-na.txt` and `departuredelays.csv` files from learning [http://bit.ly/2nroHbh](http://bit.ly/2nroHbh).
    Ensure your local Spark cluster can access this file (`~/data/flights/airport-codes-na.txt`).
  prefs: []
  type: TYPE_NORMAL
- en: If you are running Databricks, the same file is already included in the `/databricks-datasets`
    folder; the command is
  prefs: []
  type: TYPE_NORMAL
- en: '`myRDD = sc.textFile(''/databricks-datasets/flights/airport-codes-na.txt'').map(lambda
    line: line.split("\t"))`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Many of the transformations in the next section will use the RDDs `airports` or
    `flights`; let''s set them up by using the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The following list outlines common Apache Spark RDD transformations and code
    snippets. A more complete list can be found in the Apache Spark documentation, RDD
    Programing Guide | Transformations, at [https://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations](https://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations),
    the PySpark RDD API at [https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD),
    and Essential Core and Intermediate Spark Operations at [https://training.databricks.com/visualapi.pdf](https://training.databricks.com/visualapi.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: .take(...) action
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have already discussed this, but, for the sake of completeness, the `take(*n*)`
    action returns an array with the first `n` elements of the RDD. Look at the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'This will generate the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: .collect() action
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have also cautioned you about using this action; `collect()`returns all
    of the elements from the workers to the driver. Thus, look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'This will generate the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: .reduce(...) action
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `reduce(f)` action aggregates the elements of an RDD by `f`. The `f` function should
    be commutative and associative so that it can be computed correctly in parallel.
    Look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'This will produce the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: We need to make an important note here, however. When using `reduce()`, the
    reducer function needs to be associative and commutative; that is, a change in
    the order of elements and operands does not change the result.
  prefs: []
  type: TYPE_NORMAL
- en: 'Associativity rule: `(6 + 3) + 4 = 6 + (3 + 4)` Commutative rule: ` 6 + 3 +
    4 = 4 + 3 + 6`'
  prefs: []
  type: TYPE_NORMAL
- en: Error can occur if you ignore the aforementioned rules.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example, see the following RDD (with one partition only!):'
  prefs: []
  type: TYPE_NORMAL
- en: '`data_reduce = sc.parallelize([1, 2, .5, .1, 5, .2], 1)`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Reducing data to divide the current result by the subsequent one, we would
    expect a value of 10:'
  prefs: []
  type: TYPE_NORMAL
- en: '`works = data_reduce.reduce(lambda x, y: x / y)`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Partitioning the data into three partitions will produce an incorrect result:'
  prefs: []
  type: TYPE_NORMAL
- en: '`data_reduce = sc.parallelize([1, 2, .5, .1, 5, .2], 3) data_reduce.reduce(lambda
    x, y: x / y)`'
  prefs: []
  type: TYPE_NORMAL
- en: It will produce `0.004`.
  prefs: []
  type: TYPE_NORMAL
- en: .count() action
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `count()` action returns the number of elements in the RDD. See the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'This will produce the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: .saveAsTextFile(...) action
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `saveAsTextFile()` action saves your RDD into a text file; note that each
    partition is a separate file. See the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'This will actually save the following files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Recall that actions return a value to the driver after running a computation
    on the dataset, typically on the workers. Examples of some Spark actions include
    `count()` and `take()`; for this section, we will be focusing on `reduceByKey()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'To better understand what is happening when running this join, let''s review
    the Spark UI. Every Spark Session launches a web-based UI, which is, by default,
    on port `4040`, for example, `http://localhost:4040`. It includes the following
    information:'
  prefs: []
  type: TYPE_NORMAL
- en: A list of scheduler stages and tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A summary of RDD sizes and memory usage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Environmental information
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Information about the running executors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For more information, please refer to the Apache Spark Monitoring documentation
    page at [https://spark.apache.org/docs/latest/monitoring.html](https://spark.apache.org/docs/latest/monitoring.html).
  prefs: []
  type: TYPE_NORMAL
- en: To dive deeper into Spark internals, a great video is Patrick Wendell's *Tuning
    and Debugging in Apache Spark* video, which is available at [https://www.youtube.com/watch?v=kkOG_aJ9KjQ](https://www.youtube.com/watch?v=kkOG_aJ9KjQ).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00024.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Digging further into the tasks that make up each stage, notice that the bulk
    of the work is done in **Stage 18**. Note the eight parallel tasks that end up
    processing data, from extracting it from the file (`/tmp/data/departuredelays.csv`)
    to executing `reduceByKey()` in parallel:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00025.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Details of Stage 18
  prefs: []
  type: TYPE_NORMAL
- en: 'A few important callouts are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Spark's `reduceByKey(f)` assumes the `f` function is commutative and associative
    so that it can be computed correctly in parallel. As noted in the Spark UI, all
    eight tasks are processing the data extraction (`sc.textFile`) and `reduceByKey()`
    in parallel, providing faster performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As noted in the *Getting ready* section of this recipe, we executed `sc.textFile($fileLocation,
    minPartitions=8)..`. This forced the RDD to have eight partitions (at least eight
    partitions), which translated to eight tasks being executed in parallel:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/00026.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Now that you have executed `reduceByKey()`, we will run `take(5)`, which executes
    another stage that shuffles the eight partitions from the workers to the single
    driver node; that way, the data can be collected for viewing in the console.
  prefs: []
  type: TYPE_NORMAL
- en: Pitfalls of using RDDs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The key concern associated with using RDDs is that they can take a lot of time
    to master. The flexibility of running functional operators such as map, reduce,
    and shuffle allows you to perform a wide variety of transformations against your
    data. But with this power comes great responsibility, and it is potentially possible
    to write code that is inefficient, such as the use of `GroupByKey`; more information
    can be found in *Avoid GroupByKey* at [https://databricks.gitbooks.io/databricks-spark-knowledge-base/content/best_practices/prefer_reducebykey_over_groupbykey.html](https://databricks.gitbooks.io/databricks-spark-knowledge-base/content/best_practices/prefer_reducebykey_over_groupbykey.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'Generally, you will typically have slower performance when using RDDs compared
    to Spark DataFrames, as noted in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00027.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: Introducing DataFrames in Apache Spark for Large Scale Data Science
    at https://databricks.com/blog/2015/02/17/introducing-dataframes-in-spark-for-large-scale-data-science.html'
  prefs: []
  type: TYPE_NORMAL
- en: It is also important  to note that with Apache Spark 2.0+, datasets have functional
    operators (giving you flexibility similar to RDDs), yet also utilize the catalyst
    optimizer, providing faster performance. More information on datasets will be
    discussed in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: The reason RDDs are slow—especially within the context of PySpark—is because
    whenever a PySpark program is executed using RDDs, there is a potentially large
    overhead to execute the job. As noted in the following diagram, in the PySpark
    driver, the `Spark Context` uses `Py4j` to launch a JVM using `JavaSparkContext`.
    Any RDD transformations are initially mapped to `PythonRDD` objects in Java.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once these tasks are pushed out to the Spark worker(s), `PythonRDD` objects
    launch Python `subprocesses` using pipes to send both code and data to be processed
    in Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00028.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: While this approach allows PySpark to distribute the processing of the data
    to multiple Python `subprocesses` on multiple workers, as you can see, there is
    a lot of context switching and communications overhead between Python and the
    JVM.
  prefs: []
  type: TYPE_NORMAL
- en: 'An excellent resource on PySpark performance is Holden Karau’s *Improving PySpark
    Performance: Spark Performance Beyond the JVM* at [http://bit.ly/2bx89bn](http://bit.ly/2bx89bn).'
  prefs: []
  type: TYPE_NORMAL
- en: This is even more apparent when using Python UDFs, as the performance is significantly
    slower because all of the data will need to be transferred to the driver prior
    to using a Python UDF. Note that vectorized UDFs were introduced as part of Spark
    2.3 and will improve PySpark UDF performance. For more information, please refer
    to *Introducing Vectorized UDFs for PySpark* at [https://databricks.com/blog/2017/10/30/introducing-vectorized-udfs-for-pyspark.html](https://databricks.com/blog/2017/10/30/introducing-vectorized-udfs-for-pyspark.html).
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As in the previous sections, let''s make use of the `flights` dataset and create
    an RDD and a DataFrame against this dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will run the same `group by` statement—one via an RDD using
    `reduceByKey()`, and one via a DataFrame using Spark SQL `GROUP BY`. For this
    query, we will sum the time delays grouped by originating city and sort according
    to the originating city:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'For this particular configuration, it took 11.08 seconds to extract the columns,
    execute `reduceByKey()` to summarize the data, execute `sortByKey()` to order
    it, and then return the values to the driver:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'There are many advantages of Spark DataFrames, including, but not limited to
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: You can execute Spark SQL statements (not just through the Spark DataFrame API)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is a schema associated with your data so you can specify the column name
    instead of position
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this configuration and example, the query completes in 4.76 seconds, while
    RDDs complete in 11.08 seconds
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It is impossible to improve your RDD query by specifying `minPartitions` within
    `sc.textFile()` when originally loading the data to increase the number of partitions:'
  prefs: []
  type: TYPE_NORMAL
- en: '`flights = sc.textFile(''/databricks-datasets/flights/departuredelays.csv'',
    minPartitions=8), ...`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: For this configuration, the same query returned in 6.63 seconds. While this
    approach is faster, its still slower than DataFrames; in general, DataFrames are
    faster out of the box with the default configuration.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To better understand the performance of the previous RDD and DataFrame, let''s
    return to the Spark UI. For starters, when we run the `flights` RDD query, three
    separate jobs are executed, as can be seen in Databricks Community Edition in
    the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00029.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Each of these jobs spawn their own set of stages to initially read the text
    (or CSV) file, execute  `reduceByKey()`, and execute the `sortByKey()` functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00030.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'With two additional jobs to complete the `sortByKey()` execution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00031.jpeg)![](img/00032.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: As can be observed, by using RDDs directly, there can potentially be a lot of
    overhead, generating multiple jobs and stages to complete a single query.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the case of Spark DataFrames, for this query it is much simpler for it to
    consist of a single job with two stages. Note that the Spark UI has a number of
    DataFrame-specific set tasks, such as `WholeStageCodegen` and `Exchange`, that
    significantly improve the performance of Spark dataset and DataFrame queries.
    More information about the Spark SQL engine catalyst optimizer can be found in
    the next chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00033.jpeg)'
  prefs: []
  type: TYPE_IMG
