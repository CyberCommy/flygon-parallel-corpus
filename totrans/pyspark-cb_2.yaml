- en: Abstracting Data with RDDs
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover how to work with Apache Spark Resilient Distributed
    Datasets. You will learn the following recipes:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: Creating RDDs
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reading data from files
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overview of RDD transformations
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overview of RDD actions
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pitfalls of using RDDs
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Resilient Distributed Datasets** (**RDDs**) are collections of immutable
    JVM objects that are distributed across an Apache Spark cluster. Please note that
    if you are new to Apache Spark, you may want to initially skip this chapter as
    Spark DataFrames/Datasets are both significantly easier to develop and typically
    have faster performance. More information on Spark DataFrames can be found in
    the next chapter.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: An RDD is the most fundamental dataset type of Apache Spark; any action on a
    Spark DataFrame eventually gets *translated* into a highly optimized execution
    of transformations and actions on RDDs (see the paragraph on catalyst optimizer
    in [Chapter 3](part0120.html#3IE3G0-dc04965c02e747b9b9a057725c821827), *Abstracting
    Data with DataFrames*, in the *Introduction* section).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: Data in an RDD is split into chunks based on a key and then dispersed across
    all the executor nodes. RDDs are highly resilient, that is, there are able to
    recover quickly from any issues as the same data chunks are replicated across
    multiple executor nodes. Thus, even if one executor fails, another will still
    process the data. This allows you to perform your functional calculations against
    your dataset very quickly by harnessing the power of multiple nodes. RDDs keep
    a log of all the execution steps applied to each chunk. This, on top of the data
    replication, speeds up the computations and, if anything goes wrong, RDDs can
    still recover the portion of the data lost due to an executor error.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: While it is common to lose a node in distributed environments (for example,
    due to connectivity issues, hardware problems), distribution and replication of
    the data defends against data loss, while data lineage allows the system to recover
    quickly.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: Creating RDDs
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For this recipe, we will start creating an RDD by generating the data within
    the PySpark. To create RDDs in Apache Spark, you will need to first install Spark
    as shown in the previous chapter. You can use the PySpark shell and/or Jupyter
    notebook to run these code samples.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We require a working installation of Spark. This means that you would have
    followed the steps outlined in the previous chapter. As a reminder, to start PySpark
    shell for your local Spark cluster, you can run this command:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Where `n` is the number of cores.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To quickly create an RDD, run PySpark on your machine via the bash terminal,
    or you can run the same query in a Jupyter notebook. There are two ways to create
    an RDD in PySpark: you can either use the `parallelize()` method—a collection
    (list or an array of some elements) or reference a file (or files) located either
    locally or through an external source, as noted in subsequent recipes.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'To view what is inside your RDD, you can run the following code snippet:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The output is as follows:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: How it works...
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Spark context parallelize method
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Under the covers, there are quite a few actions that happened when you created
    your RDD. Let''s start with the RDD creation and break down this code snippet:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Focusing first on the statement in the `sc.parallelize()` method, we first
    created a Python list (that is, `[A, B, ..., E]`) composed of a list of arrays
    (that is, `(''Mike'', 19), (''June'', 19), ..., (''Scott'', 17)`). The `sc.parallelize()`
    method is the SparkContext''s `parallelize` method to create a parallelized collection.
    This allows Spark to distribute the data across multiple nodes, instead of depending
    on a single node to process the data:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00020.jpeg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
- en: Now that we have created `myRDD` as a parallelized collection, Spark can operate
    against this data in parallel. Once created, the distributed dataset (`distData`)
    can be operated on in parallel. For example, we can call `myRDD.reduceByKey(add)`
    to add up the grouped by keys of the list; we have recipes for RDD operations
    in subsequent sections of this chapter.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: .take(...) method
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that you have created your RDD (`myRDD`), we will use the `take()` method
    to return the values to the console (or notebook cell). We will now execute an
    RDD action (more information on this in subsequent recipes), `take()`. Note that
    a common approach in PySpark is to use `collect()`, which returns all values in
    your RDD from the Spark worker nodes to the driver. There are performance implications
    when working with a large amount of data as this translates to large volumes of
    data being transferred from the Spark worker nodes to the driver. For small amounts
    of data (such as this recipe), this is perfectly fine, but, as a matter of habit,
    you should pretty much always use the `take(n)` method instead; it returns the
    first `n` elements of the RDD instead of the whole dataset. It is a more efficient
    method because it first scans one partition and uses those statistics to determine
    the number of partitions required to return the results.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: Reading data from files
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For this recipe, we will create an RDD by reading a local file in PySpark.
    To create RDDs in Apache Spark, you will need to first install Spark as noted
    in the previous chapter. You can use the PySpark shell and/or Jupyter notebook
    to run these code samples. Note that while this recipe is specific to reading
    local files, a similar syntax can be applied for Hadoop, AWS S3, Azure WASBs,
    and/or Google Cloud Storage:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '| Storage type | Example |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
- en: '| Local files | `sc.textFile(''/local folder/filename.csv'')` |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
- en: '| Hadoop HDFS | `sc.textFile(''hdfs://folder/filename.csv'')` |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
- en: '| AWS S3 ([https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark-configure.html](https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark-configure.html))
    | `sc.textFile(''s3://bucket/folder/filename.csv'')` |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
- en: '| Azure WASBs ([https://docs.microsoft.com/en-us/azure/hdinsight/hdinsight-hadoop-use-blob-storage](https://docs.microsoft.com/en-us/azure/hdinsight/hdinsight-hadoop-use-blob-storage))
    | `sc.textFile(''wasb://bucket/folder/filename.csv'')` |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
- en: '| Google Cloud Storage ([https://cloud.google.com/dataproc/docs/concepts/connectors/cloud-storage#other_sparkhadoop_clusters](https://cloud.google.com/dataproc/docs/concepts/connectors/cloud-storage#other_sparkhadoop_clusters))
    | `sc.textFile(''gs://bucket/folder/filename.csv'')` |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
- en: '| Databricks DBFS ([https://docs.databricks.com/user-guide/dbfs-databricks-file-system.html](https://docs.databricks.com/user-guide/dbfs-databricks-file-system.html))
    | `sc.textFile(''dbfs://folder/filename.csv'')` |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
- en: Getting ready
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will be reading a tab-delimited (or comma-delimited) file,
    so please ensure that you have a text (or CSV) file available. For your convenience, you
    can download the `airport-codes-na.txt` and `departuredelays.csv` files from [https://github.com/drabastomek/learningPySpark/tree/master/Chapter03/flight-data](https://github.com/drabastomek/learningPySpark/tree/master/Chapter03/flight-data).
    Ensure your local Spark cluster can access this file (for example, `~/data/flights/airport-codes-na.txt`).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once you start the PySpark shell via the bash terminal (or you can run the
    same query within Jupyter notebook), execute the following query:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'If you are running Databricks, the same file is already included in the `/databricks-datasets`
    folder; the command is:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: '`myRDD = sc.textFile(''/databricks-datasets/flights/airport-codes-na.txt'').map(lambda
    element: element.split("\t"))`'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: 'When running the query:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The resulting output is:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Diving in a little deeper, let''s determine the number of rows in this RDD.
    Note that more information on RDD actions such as `count()` is included in subsequent
    recipes:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 深入一点，让我们确定这个RDD中的行数。请注意，有关RDD操作（如`count()`）的更多信息包含在后续的示例中：
- en: '[PRE10]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Also, let''s find out the number of partitions that support this RDD:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，让我们找出支持此RDD的分区数：
- en: '[PRE11]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: How it works...
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: '[PRE12]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: .textFile(...) method
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: .textFile(...)方法
- en: 'To read the file, we are using SparkContext''s `textFile()` method via this
    command:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 要读取文件，我们使用SparkContext的`textFile()`方法通过这个命令：
- en: '[PRE13]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Only the first parameter is required, which indicates the location of the text
    file as per `~/data/flights/airport-codes-na.txt`. There are two optional parameters
    as well:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 只有第一个参数是必需的，它指示文本文件的位置为`~/data/flights/airport-codes-na.txt`。还有两个可选参数：
- en: '`minPartitions`: Indicates the minimum number of partitions that make up the
    RDD. The Spark engine can often determine the best number of partitions based
    on the file size, but you may want to change the number of partitions for performance
    reasons and, hence, the ability to specify the minimum number.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`minPartitions`：指示组成RDD的最小分区数。Spark引擎通常可以根据文件大小确定最佳分区数，但出于性能原因，您可能希望更改分区数，因此可以指定最小数量。'
- en: '`use_unicode`: Engage this parameter if you are processing Unicode data.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_unicode`：如果处理Unicode数据，请使用此参数。'
- en: 'Note that if you were to execute this statement without the subsequent `map()` function,
    the resulting RDD would not reference the tab-delimiter—basically a list of strings
    that is:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，如果您执行此语句而没有后续的`map()`函数，生成的RDD将不引用制表符分隔符——基本上是一个字符串列表：
- en: '[PRE14]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: .map(...) method
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: .map(...)方法
- en: 'To make sense of the tab-delimiter with an RDD, we will use the `.map(...)` function
    to transform the data from a list of strings to a list of lists:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解RDD中的制表符，我们将使用`.map(...)`函数将数据从字符串列表转换为列表列表：
- en: '[PRE15]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The key components of this map transformation are:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 此映射转换的关键组件是：
- en: '`lambda`: An anonymous function (that is, a function defined without a name)
    composed of a single expression'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lambda`：一个匿名函数（即，没有名称定义的函数），由一个单一表达式组成'
- en: '`split`: We''re using PySpark''s split function (within `pyspark.sql.functions`)
    to split a string around a regular expression pattern; in this case, our delimiter
    is a tab (that is, `\t`)'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`split`：我们使用PySpark的split函数（在`pyspark.sql.functions`中）来围绕正则表达式模式分割字符串；在这种情况下，我们的分隔符是制表符（即`\t`）'
- en: 'Putting the `sc.textFile()` and `map()` functions together allows us to read
    the text file and split by the tab-delimiter to produce an RDD composed of a parallelized
    list of lists collection:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 将`sc.textFile()`和`map()`函数放在一起，可以让我们读取文本文件，并按制表符分割，生成由并行化列表集合组成的RDD：
- en: '[PRE16]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Partitions and performance
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分区和性能
- en: 'Earlier in this recipe, if we had run `sc.textFile()` without specifying `minPartitions` for
    this dataset, we would only have two partitions:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，如果我们在没有为这个数据集指定`minPartitions`的情况下运行`sc.textFile()`，我们只会有两个分区：
- en: '[PRE17]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'But as noted, if the `minPartitions` flag is specified, then you would get
    the specified four partitions (or more):'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 但是请注意，如果指定了`minPartitions`标志，那么您将获得指定的四个分区（或更多）：
- en: '[PRE18]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'A key aspect of partitions for your RDD is that the more partitions you have,
    the higher the parallelism. Potentially, having more partitions will improve your
    query performance. For this portion of the recipe, let''s use a slightly larger
    file, `departuredelays.csv`:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 对于RDD的分区的一个关键方面是，分区越多，并行性越高。潜在地，有更多的分区将提高您的查询性能。在这部分示例中，让我们使用一个稍大一点的文件，`departuredelays.csv`：
- en: '[PRE19]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: As noted in the preceding code snippet, by default, Spark will create two partitions
    and take 3.33 seconds (on my small cluster) to count the 1.39 million rows in
    the departure delays CSV file.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面的代码片段所述，默认情况下，Spark将创建两个分区，并且在我的小集群上花费3.33秒（在出发延误CSV文件中计算139万行）。
- en: 'Executing the same command, but also specifying `minPartitions` (in this case,
    eight partitions), you will notice that the `count()` method completed in 2.96
    seconds (instead of 3.33 seconds with eight partitions). Note that these values
    may be different based on your machine''s configuration, but the key takeaway
    is that modifying the number of partitions may result in faster performance due
    to parallelization. Check out the following code:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 执行相同的命令，但同时指定`minPartitions`（在这种情况下，为八个分区），您会注意到`count()`方法在2.96秒内完成（而不是使用八个分区的3.33秒）。请注意，这些值可能根据您的机器配置而有所不同，但关键是修改分区的数量可能会由于并行化而导致更快的性能。查看以下代码：
- en: '[PRE20]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Overview of RDD transformations
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RDD转换概述
- en: 'As noted in preceding sections, there are two types of operation that can be
    used to shape data in an RDD: transformations and actions. A transformation, as
    the name suggests, *transforms* one RDD into another. In other words, it takes
    an existing RDD and transforms it into one or more output RDDs. In the preceding
    recipes, we had used a `map()` function, which is an example of a transformation
    to split the data by its tab-delimiter.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面的部分所述，RDD中可以使用两种类型的操作来塑造数据：转换和操作。转换，顾名思义，*将*一个RDD转换为另一个。换句话说，它接受一个现有的RDD，并将其转换为一个或多个输出RDD。在前面的示例中，我们使用了`map()`函数，这是一个通过制表符分割数据的转换的示例。
- en: Transformations are lazy (unlike actions). They only get executed when an action
    is called on an RDD. For example, calling the `count()` function is an action;
    more information is available in the following section on actions.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 转换是懒惰的（不像操作）。它们只有在RDD上调用操作时才会执行。例如，调用`count()`函数是一个操作；有关操作的更多信息，请参阅下一节。
- en: Getting ready
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: This recipe will be reading a tab-delimited (or comma-delimited) file, so please
    ensure that you have a text (or CSV) file available. For your convenience, you
    can download the `airport-codes-na.txt` and `departuredelays.csv` files from [https://github.com/drabastomek/learningPySpark/tree/master/Chapter03/flight-data](https://github.com/drabastomek/learningPySpark/tree/master/Chapter03/flight-data).
    Ensure your local Spark cluster can access this file (for example, `~/data/flights/airport-codes-na.txt`).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: If you are running Databricks, the same file is already included in the `/databricks-datasets`
    folder; the command is
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: '`myRDD = sc.textFile(''/databricks-datasets/flights/airport-codes-na.txt'').map(lambda
    line: line.split("\t"))`'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: 'Many of the transformations in the next section will use the RDDs `airports` or
    `flights`; let''s set them up using this code snippet:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: How to do it...
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we list common Apache Spark RDD transformations and code snippets.
    A more complete list can be found at [https://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations](https://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations),
    [https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD) and
    [https://training.databricks.com/visualapi.pdf](https://training.databricks.com/visualapi.pdf).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: 'The transformations include the following common tasks:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: 'Removing the header line from your text file: `zipWithIndex()`'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Selecting columns from your RDD: `map()`'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Running a `WHERE` (filter) clause: `filter()`'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Getting the distinct values: `distinct()`'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Getting the number of partitions: `getNumPartitions()`'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Determining the size of your partitions (that is, the number of elements within
    each partition): `mapPartitionsWithIndex()`'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: .map(...) transformation
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `map(f)` transformation returns a new RDD formed by passing each element
    through a function, `f`.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: 'Look at the following code snippet:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'This will produce the following output:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: .filter(...) transformation
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `filter(f)`  transformation returns a new RDD based on selecting elements
    for which the `f` function returns true. Therefore, look at the following code
    snippet:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'This will produce the following output:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: .flatMap(...) transformation
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `flatMap(f) `transformation is similar to map, but the new RDD flattens
    out all of the elements (that is, a sequence of events). Let''s look at the following
    snippet:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The preceding code will produce the following output:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: .distinct() transformation
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `distinct()` transformation returns a new RDD containing the distinct elements
    of the source RDD. So, look at the following code snippet:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'This will return the following output:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: .sample(...) transformation
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `sample(withReplacement, fraction, seed)` transformation samples a fraction
    of the data, with or without replacement (the `withReplacement` parameter), based
    on a random seed.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: 'Look at the following code snippet:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'We can expect the following result:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: .join(...) transformation
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `join(RDD')` transformation returns an RDD of *(key, (val_left, val_right))*
    when calling RDD *(key, val_left)* and RDD *(key, val_right)*. Outer joins are
    supported through left outer join, right outer join, and full outer join.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: 'Look at the following code snippet:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'This will give you the following result:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: .repartition(...) transformation
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `repartition(n)` transformation repartitions the RDD into `n` partitions
    by randomly reshuffling and uniformly distributing data across the network. As
    noted in the preceding recipes, this can improve performance by running more parallel
    threads concurrently. Here''s a code snippet that does precisely that:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: .zipWithIndex() transformation
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `zipWithIndex()` transformation appends (or ZIPs) the RDD with the element
    indices. This is very handy when wanting to remove the header row (first row)
    of a file.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: 'Look at the following code snippet:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'This will generate this result:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'To remove the header from your data, you can use the following code:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The preceding code will skip the header, as shown as follows:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: .reduceByKey(...) transformation
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `reduceByKey(f)` transformation reduces the elements of the RDD using `f` by
    the key. The `f` function should be commutative and associative so that it can
    be computed correctly in parallel.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: 'Look at the following code snippet:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'This will generate the following output:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: .sortByKey(...) transformation
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `sortByKey(asc)` transformation orders *(key, value)* RDD by *key* and
    returns an RDD in ascending or descending order. Look at the following code snippet:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'This will produce this output:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: .union(...) transformation
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `union(RDD)` transformation returns a new RDD that is the union of the
    source and argument RDDs. Look at the following code snippet:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'This will generate the following output:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: .mapPartitionsWithIndex(...) transformation
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `mapPartitionsWithIndex(f)` is similar to map but runs the `f` function separately
    on each partition and provides an index of the partition. It is useful to determine
    the data skew within partitions (check the following snippet):'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The preceding code will produce the following result:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: How it works...
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Recall that a transformation takes an existing RDD and transforms it into one
    or more output RDDs. It is also a lazy process that is not initiated until an
    action is executed. In the following join example, the action is the `take()`
    function:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'To better understand what is happening when running this join, let''s review
    the Spark UI. Every Spark session launches a web-based UI, which is, by default,
    on port `4040`, for example, `http://localhost:4040`. It includes the following
    information:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: A list of scheduler stages and tasks
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A summary of RDD sizes and memory usage
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Environmental information
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Information about the running executors
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For more information, please refer to the Apache Spark Monitoring documentation
    page at [https://spark.apache.org/docs/latest/monitoring.html](https://spark.apache.org/docs/latest/monitoring.html).
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: To dive deeper into Spark internals, a great video is Patrick Wendell's *Tuning
    and Debugging in Apache* *Spark* video, which is available at [https://www.youtube.com/watch?v=kkOG_aJ9KjQ](https://www.youtube.com/watch?v=kkOG_aJ9KjQ).
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: 'As can be seen in the following DAG visualization, the join statement and two
    preceding map transformations have a single job (Job 24) that created two stages
    (Stage 32 and Stage 33):'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00021.jpeg)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
- en: Details for Job 24
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s dive deeper into these two stages:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00022.jpeg)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
- en: Details of Stage 32
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: 'To better understand the tasks executed in the first stage (Stage 32), we can
    dive deeper into the stage''s DAG Visualization as well as the Event Timeline:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: The two `textFile` callouts are to extract the two different files (`departuredelays.csv`
    and `airport-codes-na.txt`)
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once the `map` functions are complete, to support the `join`, Spark executes
    `UnionRDD` and `PairwiseRDD` to perform the basics behind the join as part of
    the `union` task
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the next stage, the `partitionBy` and `mapPartitions` tasks shuffle and
    re-map the partitions prior to providing the output via the `take()` function:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00023.jpeg)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
- en: Details of Stage 33
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: Note that that if you execute the same statements without the `take()` function
    (or some other *action*), only *transformation* operations will be executed with
    nothing showing up in the Spark UI denoting lazy processing.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, if you were to execute the following code snippet, note that the
    output is a pointer to a Python RDD:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Overview of RDD actions
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As noted in preceding sections, there are two types of Apache Spark RDD operations:
    transformations and actions. An *action* returns a value to the driver after running
    a computation on the dataset, typically on the workers. In the preceding recipes,
    the `take()` and `count()` RDD operations are examples of *actions*.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This recipe will be reading a tab-delimited (or comma-delimited) file, so please
    ensure that you have a text (or CSV) file available. For your convenience, you
    can download the `airport-codes-na.txt` and `departuredelays.csv` files from learning [http://bit.ly/2nroHbh](http://bit.ly/2nroHbh).
    Ensure your local Spark cluster can access this file (`~/data/flights/airport-codes-na.txt`).
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: If you are running Databricks, the same file is already included in the `/databricks-datasets`
    folder; the command is
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '`myRDD = sc.textFile(''/databricks-datasets/flights/airport-codes-na.txt'').map(lambda
    line: line.split("\t"))`'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: 'Many of the transformations in the next section will use the RDDs `airports` or
    `flights`; let''s set them up by using the following code snippet:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: How to do it...
  id: totrans-207
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The following list outlines common Apache Spark RDD transformations and code
    snippets. A more complete list can be found in the Apache Spark documentation, RDD
    Programing Guide | Transformations, at [https://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations](https://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations),
    the PySpark RDD API at [https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD),
    and Essential Core and Intermediate Spark Operations at [https://training.databricks.com/visualapi.pdf](https://training.databricks.com/visualapi.pdf).
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: .take(...) action
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have already discussed this, but, for the sake of completeness, the `take(*n*)`
    action returns an array with the first `n` elements of the RDD. Look at the following
    code:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'This will generate the following output:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: .collect() action
  id: totrans-214
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have also cautioned you about using this action; `collect()`returns all
    of the elements from the workers to the driver. Thus, look at the following code:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'This will generate the following output:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: .reduce(...) action
  id: totrans-219
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `reduce(f)` action aggregates the elements of an RDD by `f`. The `f` function should
    be commutative and associative so that it can be computed correctly in parallel.
    Look at the following code:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'This will produce the following result:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: We need to make an important note here, however. When using `reduce()`, the
    reducer function needs to be associative and commutative; that is, a change in
    the order of elements and operands does not change the result.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: 'Associativity rule: `(6 + 3) + 4 = 6 + (3 + 4)` Commutative rule: ` 6 + 3 +
    4 = 4 + 3 + 6`'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: Error can occur if you ignore the aforementioned rules.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example, see the following RDD (with one partition only!):'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '`data_reduce = sc.parallelize([1, 2, .5, .1, 5, .2], 1)`'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: 'Reducing data to divide the current result by the subsequent one, we would
    expect a value of 10:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: '`works = data_reduce.reduce(lambda x, y: x / y)`'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: 'Partitioning the data into three partitions will produce an incorrect result:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: '`data_reduce = sc.parallelize([1, 2, .5, .1, 5, .2], 3) data_reduce.reduce(lambda
    x, y: x / y)`'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: It will produce `0.004`.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: .count() action
  id: totrans-234
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `count()` action returns the number of elements in the RDD. See the following
    code:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'This will produce the following result:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: .saveAsTextFile(...) action
  id: totrans-239
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `saveAsTextFile()` action saves your RDD into a text file; note that each
    partition is a separate file. See the following snippet:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: '[PRE60]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'This will actually save the following files:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: How it works...
  id: totrans-245
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Recall that actions return a value to the driver after running a computation
    on the dataset, typically on the workers. Examples of some Spark actions include
    `count()` and `take()`; for this section, we will be focusing on `reduceByKey()`:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'To better understand what is happening when running this join, let''s review
    the Spark UI. Every Spark Session launches a web-based UI, which is, by default,
    on port `4040`, for example, `http://localhost:4040`. It includes the following
    information:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: A list of scheduler stages and tasks
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A summary of RDD sizes and memory usage
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Environmental information
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Information about the running executors
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For more information, please refer to the Apache Spark Monitoring documentation
    page at [https://spark.apache.org/docs/latest/monitoring.html](https://spark.apache.org/docs/latest/monitoring.html).
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: To dive deeper into Spark internals, a great video is Patrick Wendell's *Tuning
    and Debugging in Apache Spark* video, which is available at [https://www.youtube.com/watch?v=kkOG_aJ9KjQ](https://www.youtube.com/watch?v=kkOG_aJ9KjQ).
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '![](img/00024.jpeg)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
- en: 'Digging further into the tasks that make up each stage, notice that the bulk
    of the work is done in **Stage 18**. Note the eight parallel tasks that end up
    processing data, from extracting it from the file (`/tmp/data/departuredelays.csv`)
    to executing `reduceByKey()` in parallel:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00025.jpeg)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
- en: Details of Stage 18
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: 'A few important callouts are as follows:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: Spark's `reduceByKey(f)` assumes the `f` function is commutative and associative
    so that it can be computed correctly in parallel. As noted in the Spark UI, all
    eight tasks are processing the data extraction (`sc.textFile`) and `reduceByKey()`
    in parallel, providing faster performance.
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As noted in the *Getting ready* section of this recipe, we executed `sc.textFile($fileLocation,
    minPartitions=8)..`. This forced the RDD to have eight partitions (at least eight
    partitions), which translated to eight tasks being executed in parallel:'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/00026.jpeg)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
- en: Now that you have executed `reduceByKey()`, we will run `take(5)`, which executes
    another stage that shuffles the eight partitions from the workers to the single
    driver node; that way, the data can be collected for viewing in the console.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: Pitfalls of using RDDs
  id: totrans-265
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The key concern associated with using RDDs is that they can take a lot of time
    to master. The flexibility of running functional operators such as map, reduce,
    and shuffle allows you to perform a wide variety of transformations against your
    data. But with this power comes great responsibility, and it is potentially possible
    to write code that is inefficient, such as the use of `GroupByKey`; more information
    can be found in *Avoid GroupByKey* at [https://databricks.gitbooks.io/databricks-spark-knowledge-base/content/best_practices/prefer_reducebykey_over_groupbykey.html](https://databricks.gitbooks.io/databricks-spark-knowledge-base/content/best_practices/prefer_reducebykey_over_groupbykey.html).
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: 'Generally, you will typically have slower performance when using RDDs compared
    to Spark DataFrames, as noted in the following diagram:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00027.jpeg)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
- en: 'Source: Introducing DataFrames in Apache Spark for Large Scale Data Science
    at https://databricks.com/blog/2015/02/17/introducing-dataframes-in-spark-for-large-scale-data-science.html'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: It is also important  to note that with Apache Spark 2.0+, datasets have functional
    operators (giving you flexibility similar to RDDs), yet also utilize the catalyst
    optimizer, providing faster performance. More information on datasets will be
    discussed in the next chapter.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: The reason RDDs are slow—especially within the context of PySpark—is because
    whenever a PySpark program is executed using RDDs, there is a potentially large
    overhead to execute the job. As noted in the following diagram, in the PySpark
    driver, the `Spark Context` uses `Py4j` to launch a JVM using `JavaSparkContext`.
    Any RDD transformations are initially mapped to `PythonRDD` objects in Java.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: 'Once these tasks are pushed out to the Spark worker(s), `PythonRDD` objects
    launch Python `subprocesses` using pipes to send both code and data to be processed
    in Python:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00028.jpeg)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
- en: While this approach allows PySpark to distribute the processing of the data
    to multiple Python `subprocesses` on multiple workers, as you can see, there is
    a lot of context switching and communications overhead between Python and the
    JVM.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: 'An excellent resource on PySpark performance is Holden Karau’s *Improving PySpark
    Performance: Spark Performance Beyond the JVM* at [http://bit.ly/2bx89bn](http://bit.ly/2bx89bn).'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: This is even more apparent when using Python UDFs, as the performance is significantly
    slower because all of the data will need to be transferred to the driver prior
    to using a Python UDF. Note that vectorized UDFs were introduced as part of Spark
    2.3 and will improve PySpark UDF performance. For more information, please refer
    to *Introducing Vectorized UDFs for PySpark* at [https://databricks.com/blog/2017/10/30/introducing-vectorized-udfs-for-pyspark.html](https://databricks.com/blog/2017/10/30/introducing-vectorized-udfs-for-pyspark.html).
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-277
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As in the previous sections, let''s make use of the `flights` dataset and create
    an RDD and a DataFrame against this dataset:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: How to do it...
  id: totrans-280
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will run the same `group by` statement—one via an RDD using
    `reduceByKey()`, and one via a DataFrame using Spark SQL `GROUP BY`. For this
    query, we will sum the time delays grouped by originating city and sort according
    to the originating city:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'For this particular configuration, it took 11.08 seconds to extract the columns,
    execute `reduceByKey()` to summarize the data, execute `sortByKey()` to order
    it, and then return the values to the driver:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'There are many advantages of Spark DataFrames, including, but not limited to
    the following:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: You can execute Spark SQL statements (not just through the Spark DataFrame API)
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is a schema associated with your data so you can specify the column name
    instead of position
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this configuration and example, the query completes in 4.76 seconds, while
    RDDs complete in 11.08 seconds
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It is impossible to improve your RDD query by specifying `minPartitions` within
    `sc.textFile()` when originally loading the data to increase the number of partitions:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: '`flights = sc.textFile(''/databricks-datasets/flights/departuredelays.csv'',
    minPartitions=8), ...`'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: For this configuration, the same query returned in 6.63 seconds. While this
    approach is faster, its still slower than DataFrames; in general, DataFrames are
    faster out of the box with the default configuration.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  id: totrans-293
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To better understand the performance of the previous RDD and DataFrame, let''s
    return to the Spark UI. For starters, when we run the `flights` RDD query, three
    separate jobs are executed, as can be seen in Databricks Community Edition in
    the following screenshot:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00029.jpeg)'
  id: totrans-295
  prefs: []
  type: TYPE_IMG
- en: 'Each of these jobs spawn their own set of stages to initially read the text
    (or CSV) file, execute  `reduceByKey()`, and execute the `sortByKey()` functions:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00030.jpeg)'
  id: totrans-297
  prefs: []
  type: TYPE_IMG
- en: 'With two additional jobs to complete the `sortByKey()` execution:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00031.jpeg)![](img/00032.jpeg)'
  id: totrans-299
  prefs: []
  type: TYPE_IMG
- en: As can be observed, by using RDDs directly, there can potentially be a lot of
    overhead, generating multiple jobs and stages to complete a single query.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: 'In the case of Spark DataFrames, for this query it is much simpler for it to
    consist of a single job with two stages. Note that the Spark UI has a number of
    DataFrame-specific set tasks, such as `WholeStageCodegen` and `Exchange`, that
    significantly improve the performance of Spark dataset and DataFrame queries.
    More information about the Spark SQL engine catalyst optimizer can be found in
    the next chapter:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00033.jpeg)'
  id: totrans-302
  prefs: []
  type: TYPE_IMG
