- en: '*Chapter 6*: Concurrency and Performance'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the last chapter, we learned about the fundamental factors that affect the
    performance of concurrent programs. Now it is time to put this knowledge to practical
    use and learn about developing high-performance concurrent algorithms and data
    structures for thread-safe programs.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the one hand, to take full advantage of concurrency, one must take a high-level
    view of the problem and the solution strategy: data organization, work partitioning,
    sometimes even the definition of what constitutes a solution are the choices that
    critically affect the performance of the program. On the other hand, as we have
    seen in the last chapter, the performance is greatly impacted by low-level factors
    such as the arrangement of the data in the cache, and even the best design can
    be ruined by poor implementation. These low-level details are often difficult
    to analyze, hard to express in code, and require very careful coding. This is
    not the kind of code you want to be scattered around in your program, so the encapsulation
    of the tricky code is a necessity. We will have to give some thought to the best
    way to encapsulate this complexity.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Efficient concurrency
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use of locks, pitfalls of locking, and an introduction to lock-free programming
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thread-safe counters and accumulators
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thread-safe smart pointers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Again, you will need a C++ compiler and a micro-benchmarking tool, such as the
    Google Benchmark library we used in the previous chapter (found at [https://github.com/google/benchmark](https://github.com/google/benchmark)).
    The code accompanying this chapter can be found at [https://github.com/PacktPublishing/The-Art-of-Writing-Efficient-Programs/tree/master/Chapter06](https://github.com/PacktPublishing/The-Art-of-Writing-Efficient-Programs/tree/master/Chapter06).
  prefs: []
  type: TYPE_NORMAL
- en: What is needed to use concurrency effectively?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Fundamentally, using concurrency to improve performance is very simple: you
    really need to do just two things. The first one is to have enough work for the
    concurrent threads and processes to do so they are busy at all times. The second
    one is to reduce the use of the shared data since, as we have seen in the previous
    chapter, accessing a shared variable concurrently is very expensive. The rest
    is just a matter of the implementation.'
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, the implementation tends to be quite difficult, and the difficulty
    increases when the desired performance gains are larger and when the hardware
    becomes more powerful. This is due to Amdahl's Law, which is something every programmer
    working with concurrency has heard about, but not everyone has understood the
    full extent of its implications.
  prefs: []
  type: TYPE_NORMAL
- en: 'The law itself is simple enough. It states that, for a program that has a parallel
    (scalable) part and a single-threaded part, the maximum possible speedup *s* is
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, ![](img/Formula_2.png) is the speedup of the parallel part of the program,
    and ![](img/Formula_3.png) is the fraction of the program that is parallel. Now
    consider the consequences for a program that is running on a large multi-processor
    system: if we have 256 processors and are able to fully utilize them except for
    a measly 1/256th of the run time, the total speedup of the program is limited
    to 128, that is, it is cut in half. In other words, if only 1/256th of the program
    is single-threaded or executed under a lock, that 256-processor system will never
    be used at more than 50% of its total capacity, no matter how much we optimize
    the rest of the program.'
  prefs: []
  type: TYPE_NORMAL
- en: This is why, when it comes to developing concurrent programs, the focus of the
    design, implementation, and optimization should be on making the remaining single-threaded
    computations concurrent and on reducing the amount of time the program spends
    accessing the shared data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first objective, making the computations concurrent, starts with the choice
    of the algorithms, but many design decisions influence the outcome, so we should
    learn more about it. The second one, reducing the cost of the data sharing, is
    the continuation of the theme from the last chapter: when all threads are waiting
    to access some shared variable or a lock (which is also a shared variable in itself),
    the program is effectively single-threaded only the thread that has access at
    the moment is running. This is why global locks and globally shared data are particularly
    bad for performance. But even the data shared between several threads limit the
    performance of these threads if it is accessed concurrently.'
  prefs: []
  type: TYPE_NORMAL
- en: As we have mentioned several times earlier, the need for data sharing is driven,
    fundamentally, by the nature of the problem itself. The amount of data sharing
    for any particular problem can be greatly influenced by the algorithm, the choice
    of data structures, and other design decisions, as well as by the implementation.
    Some data sharing is the artifact of the implementation or the consequence of
    the choice of the data structures, but other shared data is inherent in the problem.
    If we need to count data elements that satisfy a certain property, at the end
    of the day, there is only one count, and all threads must update it as a shared
    variable. How much sharing actually happens and what the impact is on the total
    program speedup is, however, can depend greatly on the implementation.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two tracks we will pursue in this chapter: first, given that some
    amount of data sharing is inevitable, we will look at making this process more
    efficient. Then we will consider the design and implementation techniques that
    can be used to reduce the need for data sharing or the time spent waiting for
    access to this data. We start with the first problem, efficient data sharing.'
  prefs: []
  type: TYPE_NORMAL
- en: Locks, alternatives, and their performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once we have accepted that some data sharing is going to happen, we have to
    also accept the need for the synchronization of concurrent accesses to the shared
    data. Remember that any concurrent access to the same data without such synchronization
    leads to data races and undefined behavior.
  prefs: []
  type: TYPE_NORMAL
- en: 'The most common way to guard shared data is with a mutex:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Here, we take advantage of the C++17 template type deduction for `std::lock_guard`;
    in C++14, we would have to specify the template type argument.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using mutexes is usually fairly straightforward: any code that accesses the
    shared data should be inside a critical section, that is, sandwiched between the
    calls to lock and unlock the mutex. The mutex implementation comes with the correct
    memory barriers to ensure that the code in the critical section cannot be moved
    outside of it by the hardware or the compiler (the compilers usually don''t move
    the code across the locking operations at all, but, in theory, they could do such
    optimizations as long as they respect the memory barrier semantics).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The question that is usually asked at this point is, "How expensive is that
    mutex?" However, the question is not well defined: we can certainly give the absolute
    answer, in nanoseconds, for a particular piece of hardware and a given mutex implementation,
    but what does this value mean? It is certainly more expensive than not having
    a mutex, but without one, the program would be incorrect (and there are easier
    ways to make incorrect programs very fast). So, "expensive" can be defined only
    in comparison with the alternatives, which naturally leads to the question, what
    are the alternatives?'
  prefs: []
  type: TYPE_NORMAL
- en: 'The most obvious alternative is to make the count atomic:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We also have to consider what memory order do we really need to be associated
    with operations on the count. If the count is later used to, say, index into an
    array, we probably need the release-acquire order. But if it''s just a count,
    we just want to count some events and report the number, we have no need for any
    memory order restrictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Whether we actually get any barriers or not depends on the hardware: on X86,
    the atomic increment instruction had the bidirectional memory barrier "built-in,"
    and requesting the relaxed memory order is not going to make it any faster. Still,
    it is important to specify the requirement your code truly needs, both for portability
    and for clarity: remember that your real audience is not so much the compilers
    that have to parse your code but other programmers that need to read it later.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The program with the atomic increment has no locks and does not need any. However,
    it relies on a particular hardware capability: the processor has an atomic increment
    instruction. The set of such instructions is fairly small. What would we do if
    we needed an operation for which there are no atomic instructions? We don''t have
    to go very far for an example: in C++, there is no atomic multiplication (and
    I don''t know of any hardware that has such capability; certainly, it''s not found
    on X86 or ARM or any other common CPU architecture).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Fortunately, there is a kind of "universal" atomic operation that can be used
    to build, with varying degrees of difficulty, any read-modify-write operation.
    This operation is known as `compare_exchange`. It takes two parameters: the first
    one is the expected current value of the atomic variable, and the second one is
    the desired new value. If the actual current value does not match the expected
    one, nothing happens, there is no change to the atomic variable. However, if the
    current value does match the expected one, the desired value is written into the
    atomic variable. The C++ `compare_exchange` operation returns true or false to
    indicate whether the write did happen (true if it did). If the variable did not
    match the expected value, the actual value is returned in the first parameter.
    With compare-and-swap, we can implement our atomic increment operation in the
    following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Several notes are in order: first, the actual name of the operation in C++
    is `compare_exchange_strong.` There is also `compare_exchange_weak`; the difference
    is that the weak version can sometimes return false even when the current and
    the expected values match (on X86, it makes no difference, but on some platforms,
    the weak version can result in a faster overall operation). Second, the operation
    takes not one but two memory order arguments: the second one applies when the
    compare fails (so it is the memory order for just the comparison part of the operation).
    The first one applies when the compare succeeds and the write happens.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us analyze how this implementation works. First, we atomically read the
    current value of the count, `c`. The incremented value is, of course, `c + 1`,
    but we cannot just assign it to the count because another thread could have incremented
    the count after we read it but before we update it. So we have to do a conditional
    write: if the current value of the count is still `c`, replace it with the desired
    value `c + 1`. Otherwise, update `c` with the new current value (`compare_exchange_strong`
    does that for us) and try again. The loop exits only when we finally catch a moment
    when the atomic variable did not change between the time we last read it and the
    time we''re trying to update it. Of course, there is no reason to do any of this
    to increment the count when we have the atomic increment operation. But this approach
    can be generalized to any computation: instead of `c + 1`, we could use any other
    expression, and the program would work the same way.'
  prefs: []
  type: TYPE_NORMAL
- en: While all three versions of the code do the same operation, increment the count,
    there are fundamental differences between them that we must explore in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Lock-based, lock-free, and wait-free programs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first version, with the mutex, is the simplest to understand: one thread
    can hold the lock at any time, so that thread can increment the count without
    any further precautions. Once the lock is released, another thread can acquire
    it and increment the count, and so on. At any time, at most one thread can hold
    the lock and make any progress; all remaining threads that need the access are
    waiting on the lock. But even the thread that has the lock is not guaranteed to
    proceed forward, in general: if it needs access to another shared variable before
    it can complete its job, it may be waiting on that lock, which is held by some
    other thread. This is the common lock-based program, often not the fastest, but
    the easiest to understand and reason about.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The second program presents a very different scenario: any thread that arrives
    at the atomic increment operation executes it without delay. Of course, the hardware
    itself must lock access to the shared data to ensure the atomicity of the operations
    (as we have seen in the last chapter, this is done by granting exclusive access
    to the entire cache line to one processor at a time). From the programmer''s point
    of view, this exclusive access manifests itself as an increase in the time it
    takes to execute the atomic operation. However, in the code itself, there is no
    waiting for anything, no trying and retrying. This kind of program is called **wait-free**.
    In a wait-free program, all threads are making progress, that is, executing operations,
    at all times (although some operations may take longer if there is severe contention
    between threads for access to the same shared variable). A wait-free implementation
    is usually possible only for very simple operations (such as incrementing a count),
    but whenever it is available, it is often even simpler than the lock-based implementation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'It takes a bit more effort to understand the behavior of the last program.
    There are no locks; however, there is a loop that is repeated an unknown number
    of times. In this regard, the implementation is similar to the lock: any thread
    waiting on a lock is also stuck in a similar loop, trying and failing to acquire
    the lock. However, there is one key difference: in a lock-based program, when
    a thread has failed to acquire the lock and must try again, we can deduce that
    some other thread has the lock. We cannot be sure whether that thread is going
    to release the lock any time soon or that it, in fact, is making any progress
    toward completing its work and releasing the lock it holds (it may, for example,
    be waiting for a user to input something). In the program based on compare-and-swap,
    the only way our thread can fail to update the shared count is because some other
    thread updated it first. Therefore, we know that, of all threads trying to increment
    the count at the same time, at least one will always succeed. This kind of program
    is known as **lock-free**.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We have just seen examples of the three main types of concurrent programs:'
  prefs: []
  type: TYPE_NORMAL
- en: In a wait-free program, each thread is executing the operations it needs and
    is always making progress toward the final goal; there is no waiting for access,
    and no work needs to be redone.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In a lock-free program, multiple threads may be trying to update the same shared
    value, but only one of them will succeed. The rest will have to discard the work
    they have already done based on the original value, read the updated value, and
    do the computation again. But at least one thread is always guaranteed to commit
    its work and not have to redo it; thus, the entire program is always making progress,
    although not necessarily at full speed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, in a lock-based program, one thread is holding the lock that gives
    it access to the shared data. Just because it's holding the lock does not mean
    it's doing anything with this data, though. So, when the concurrent access happens,
    at most one thread is making progress, but even that is not guaranteed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The difference between the three programs is clear, in theory. But, I bet every
    reader wants to know the answer to the same question: which one is faster? We
    can run each version of the code inside a Google benchmark. For example, here
    is the lock-based version:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The variables that must be shared between threads are declared at the global
    scope. The initial setup, if any, can be restricted to just one thread. Other
    benchmarks are similar; only the measured code changes. Here is the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.1 – Performance of a shared count increment: mutex-based, lock-free
    (compare-and-swap, or CAS), and wait-free (atomic)'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_6.1_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.1 – Performance of a shared count increment: mutex-based, lock-free
    (compare-and-swap, or CAS), and wait-free (atomic)'
  prefs: []
  type: TYPE_NORMAL
- en: The only result that may be unexpected here is just how badly the lock-based
    version is performing. However, this is a data point, not the whole story. In
    particular, while all mutexes are locks, not all locks are mutexes. We can attempt
    to come up with a more efficient lock implementation (at least, more efficient
    for our needs).
  prefs: []
  type: TYPE_NORMAL
- en: Different locks for different problems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have just seen that a standard C++ mutex performs very poorly when it is
    used to guard access to a shared variable, especially when there are many threads
    trying to modify this variable at the same time (if all threads were reading the
    variable, we would not need to guard it at all; concurrent read-only access does
    not lead to any data races). But is the lock inefficient because of its implementation,
    or is the problem inherent in the nature of the lock? From what we learned in
    the previous chapter, we can expect any lock to be somewhat less efficient than
    the atomically incremented counter simply because a lock-based scheme uses two
    shared variables, the lock and the count, versus just one, shared variable for
    an atomic counter. However, the mutexes provided by the operating system are usually
    not particularly efficient for locking very short operations such as our count
    increment.
  prefs: []
  type: TYPE_NORMAL
- en: 'The simplest and one of the most efficient locks for this situation is a basic
    spinlock. The idea of the spinlock is this: the lock itself is just a flag that
    can have two values, let''s say 0 and 1\. If the value of the flag is 0, the lock
    is not locked. Any thread that sees this value can set the flag to 1 and proceed;
    of course, the entire operation to read the flag and set it to 1 has to be a single
    atomic operation. Any thread that sees the value of 1 must wait until the value
    changes back to 0 to indicate that the lock is available. Finally, when a thread
    that changed the flag from 0 to 1 is ready to release the lock, it changes the
    value back to 0.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The code to implement this lock looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We show only the locking and unlocking functions in the code snippet; the class
    also needs the default constructor (an atomic integer is initialized to 0 in its
    own default constructor), as well as the declarations that make it non-copyable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that locking the flag does not use a conditional exchange: we always write
    1 into the flag. The reason it works is that, if the original value of the flag
    was 0, the exchange operation sets it to 1 and returns 0 (and the loop ends),
    which is what we want. But if the original value was 1, it is replaced by 1, that
    is, does not change at all.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, note the two memory barriers: locking is accompanied by the acquire barrier,
    while unlocking is done with the release barrier. Together, these barriers delimit
    the critical section and ensure that any code written between the calls to `lock()`
    and `unlock()` stays there.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You may be expecting to see the comparison benchmark of this lock versus the
    standard mutex, but we are not going to show it: the performance of this spinlock
    is terrible. To make it useful, it needs several optimizations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'First of all, note that if the value of the flag is 1, we don''t actually need
    to replace it with 1, we can just leave it alone. Why does it matter? The exchange
    is a read-modify-write operation. Even if it changes the old value to the same
    value, it needs exclusive access to the cache line containing the flag. We don''t
    need exclusive access to just read the flag. This matters in the following scenario:
    a lock is locked, the thread that has the lock is not changing it (it is busy
    doing its work), but all other threads are checking the lock and waiting for the
    value to change to 0\. If they do not try to write into the flag, the cache line
    does not need to bounce between different CPUs: they all have the same copy of
    the memory in their caches, and this copy is current, no need to send any data
    anywhere. Only when one of the threads actually changes the value does the hardware
    need to send the new content of the memory to all CPUs. Here is the optimization
    we just described, done in code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The optimization here is that we first read the flag until we see 0, then we
    swap it with 1\. The value could have changed to 1 between the time we did the
    check and the time we did the exchange if another thread got the lock first. Also,
    note that, when pre-checking the flag, we don't care about the memory barrier
    at all since the final definitive check is always done using the exchange and
    its memory barrier.
  prefs: []
  type: TYPE_NORMAL
- en: Even with this optimization, the lock performs pretty poorly. The reason has
    to do with the way the operating systems tend to prioritize threads. In general,
    a thread that is doing heavy computing will get more CPU time on the assumption
    that it's doing something useful. Unfortunately, in our case, the most heavily
    computing thread is the one hammering on the flag while waiting for it to change.
    This can lead to an undesirable situation where one thread is trying to get the
    lock and has the CPU allocated to it, while another thread would like to release
    the lock but doesn't get scheduled for execution for some time. The solution is
    for the waiting thread to give up the CPU after several attempts, so some other
    thread can run and, hopefully, finish its work and release the lock.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several ways for a thread to release the control on the CPU; most
    are done by a system function call. There isn''t a universal best way to do so.
    Experimentally, on Linux, a call to sleep for a very short time (1 nanosecond)
    by calling `nanosleep()` seems to yield the best results, usually better than
    a call to `sched_yield()`, which is another system function to yield CPU access.
    All system calls are expensive compared to hardware instructions, so you don''t
    want to call them too often. The best balance is achieved when we try to get the
    lock several times, then yield the CPU to another thread, then try again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The optimal number of attempts to acquire the lock before releasing the CPU
    will depend on the hardware and the number of threads, but generally, values between
    8 and 16 work well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we are ready for the second round of benchmarks, and here is the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.2 – Performance of a shared count increment: spinlock-based, lock-free
    (compare-and-swap, or CAS), and wait-free (atomic)'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_6.2_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.2 – Performance of a shared count increment: spinlock-based, lock-free
    (compare-and-swap, or CAS), and wait-free (atomic)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The spinlock has done very well: it is soundly outperforming the compare-and-swap
    implementation and gives the wait-free operation tough competition.'
  prefs: []
  type: TYPE_NORMAL
- en: 'These results leave us with two questions: first, why don''t all locks use
    spinlocks if they are so much faster? Second, why do we even need the atomic operations
    if the spinlock is so good (other than for implementing the lock, of course)?'
  prefs: []
  type: TYPE_NORMAL
- en: 'The answer to the first question boils down to the title of this section: different
    locks for different problems. The downside of the spinlock is that the waiting
    thread continuously uses the CPU or is "busy waiting." On the other hand, the
    thread waiting on a system mutex is mostly idle (sleeping). Busy waiting is great
    if you need to wait for a few cycles, the duration of an increment operation:
    it''s much faster than putting the thread to sleep. On the other hand, if the
    locked computation consists of more than a handful of instructions, the threads
    waiting on the spinlock waste a lot of CPU time and deprive the other working
    threads of access to the hardware resources they need. Overall, the C++ mutex
    (`std::mutex`) or the OS mutex is usually chosen for its balance: it''s somewhat
    inefficient for locking a single instruction, it''s OK for locking a computation
    that takes dozens of nanoseconds, and it beats the alternative if we need to hold
    the lock for a long time (long is relative here, processors are fast, so 1 millisecond
    is very long). Now, we are writing about extreme performance (and the extreme
    efforts to achieve it) here, so most HPC programmers either implement their own
    fast locks for guarding short computations or use a library that provides them.'
  prefs: []
  type: TYPE_NORMAL
- en: The second question, "Is there any other downside to the locks?" takes us to
    the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Lock-based versus lock-free, what is the real difference?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When the conversation turns to the advantages of lock-free programming, the
    first argument is usually "it is faster." As we have just seen, this is not necessarily
    true: lock implementations can be very efficient if optimized for a particular
    task. However, there are other disadvantages that are inherent in the lock-based
    approach and do not depend on the implementation.'
  prefs: []
  type: TYPE_NORMAL
- en: The first and the most infamous is the possibility of the dreaded deadlock.
    The deadlock occurs when the program uses several locks, let's say lock1 and lock2\.
    Thread A has lock1 and needs to acquire lock2\. Thread B already has lock2 and
    needs to acquire lock1\. Neither thread can proceed, and both will wait forever
    because the only thread that can release the lock they need is itself blocked
    on a lock.
  prefs: []
  type: TYPE_NORMAL
- en: 'If both locks are acquired at the same time, the deadlock can be avoided if
    the locks are always acquired in the same order; C++ has a utility function for
    this purpose, `std::lock()`. However, often locks cannot be acquired at the same
    time: when thread A acquired lock1, there was no way to know that we will need
    lock2 as well since that information itself was hidden in the data that is guarded
    by lock1\. We will see examples later in the next chapter when we talk about concurrent
    data structures.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we cannot reliably acquire multiple locks, perhaps the solution is to try
    to acquire them, then, if we fail to get them all, release the locks we already
    hold so the other thread can get them? In our example, thread A holds lock1, it
    would try to get lock2 as well but without blocking: most locks have a `try_lock()`
    call that either acquires the lock or returns false. In the latter case, thread
    A releases lock1 and tries to lock them both again. This might work, especially
    in a simple test. But it has a danger of its own: the livelock, when two threads
    constantly pass locks to each other: thread A has lock1 but not lock2, thread
    B has lock2, gives it up, gets lock1, now it can''t get lock2 back because thread
    A has it. There are algorithms for acquiring multiple locks that guarantee success,
    eventually. Unfortunately, in practice, a long time may pass between now and eventually.
    These algorithms are also quite complex.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The fundamental problem of dealing with multiple locks is that the mutexes
    are not composable: there is no good way to combine two or more locks into one.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Even without the dangers of the livelock and the deadlock, lock-based programs
    suffer from other problems. One of the more frequent ones and one that is hard
    to diagnose is called **convoying**. It can happen with multiple locks or just
    one lock. Convoying looks like this: say we have a computation that is protected
    by a lock. Thread A currently has the lock and is doing its work on the shared
    data; other threads are waiting to do their part of the work. However, the work
    is not a one-shot deal: each thread has many tasks to do, and a part of each task
    requires exclusive access to the shared data. Thread A finishes one task, releases
    the lock, then zips through the next task until it gets to the point where it
    needs the lock again. The lock was released, any other thread can get it, but
    they are still waking up, whereas thread A is "hot" on the CPU. So, thread A gets
    the lock again simply because the competition is not ready for it. The tasks of
    thread A rush through the execution like trucks in a convoy, while nothing gets
    done on other threads.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Yet another problem with locks is that they do not respect any notion of priority:
    a low-priority thread that is currently holding the lock will preempt any high-priority
    thread that needs the same lock. The high-priority thread thus has to wait for
    as long as the low-priority thread determines, the situation that seems entirely
    inconsistent with the notion of high priority. For this reason, this scenario
    is sometimes called **priority inversion**.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we understand that the problems with locks are not limited to performance,
    let''s see how a lock-free program would fare with regard to the same complications.
    First of all, in a lock-free program, at least one thread is guaranteed to not
    be blocked: in the worst-case scenario, when all threads arrive at a **compare-and-swap**
    (**CAS**) operation simultaneously and with the same expected current value of
    the atomic variable, one of them is guaranteed to see the expected value (since
    the only way it can change is via a successful CAS operation). All the remaining
    threads will have to discard their computation results, reload the atomic variable,
    and repeat the computation, but the one thread that succeeded on the CAS can move
    to the next task. This prevents the possibility of a deadlock. Without the deadlock
    and the attempts to avoid it, we do not need to worry about the livelock either.
    Since all threads are busy computing their way toward the atomic operation (such
    as CAS), the high-priority thread is more likely to get there first and commit
    its results, while the low-priority thread is more likely to fail the CAS and
    have to redo its work. Similarly, a single success in committing the results does
    not position the "winning" thread for any advantage over all the other threads:
    whichever thread is ready to attempt to execute CAS first is the one that succeeds.
    This naturally eliminates the convoying.'
  prefs: []
  type: TYPE_NORMAL
- en: 'So, what''s not to like about lock-free programming, then? Just two drawbacks,
    but they are major ones. The first is the flip side of its advantages: as we said,
    even the threads that fail their CAS attempts stay busy. This solves the priority
    problem, but at a very high cost: in the case of high contention, a lot of CPU
    time is wasted doing the work only to have it redone. Worse, these threads competing
    for access to the single atomic variable are taking away the CPU resources from
    other threads that are doing some unrelated computations at the same time.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The second drawback is of an entirely different nature. While most concurrent
    programs are not easy to write or understand, lock-free programs are incredibly
    difficult to design and implement correctly. A lock-based program just has to
    guarantee that any set of operations that constitutes a single logical transaction
    is executed under a lock. It gets harder when there are multiple logical transactions
    such that some, but not all, shared data is common to several different transactions.
    That is how we arrive at the problem of multiple locks. Still, reasoning about
    the correctness of a lock-based program is not that difficult: if I see a piece
    of shared data in your code, you must show me which lock guards this data and
    prove that no thread can access this data without acquiring this lock first. If
    this is not so, you have a data race, even if you haven''t found it yet. If these
    requirements are met, you do not have data races (although you may have deadlocks
    and other problems).'
  prefs: []
  type: TYPE_NORMAL
- en: Lock-free programs, on the other hand, have an almost infinite variety of data
    synchronization schemes. Since no thread is ever paused, we have to convince ourselves
    that, no matter the order in which the threads execute the atomic operations,
    the result is correct. Moreover, without the benefit of a clearly defined critical
    section, we have to worry about the memory order and the visibility of all the
    data in the program, not just the atomic variables. We have to ask ourselves,
    is there any way one thread can change the data, and the other thread can see
    the old version of it because the memory order requirements are not strict enough?
  prefs: []
  type: TYPE_NORMAL
- en: 'The usual solution to the problem of complexity is modularization and encapsulation.
    We collect the difficult code into modules where each one has a well-defined interface
    and a clear set of requirements and guarantees. A lot of attention is paid to
    the modules that implement various concurrent algorithms. This book takes you
    in a different direction: the rest of the chapter is dedicated instead to concurrent
    data structures.'
  prefs: []
  type: TYPE_NORMAL
- en: Building blocks for concurrent programming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The development of concurrent programs is, generally, quite difficult. Several
    factors can make it even more difficult: for example, it is much harder to write
    concurrent programs that also need to be correct and efficient (in other words,
    all of them). Complex programs with many mutexes, or lock-free programs, are harder
    still.'
  prefs: []
  type: TYPE_NORMAL
- en: As was said at the conclusion of the last section, the only hope of managing
    this complexity is to corral it into small, well-defined sections of the code,
    or modules. As long as the interfaces and requirements are clear, the clients
    of these modules don't need to know whether the implementation is lock-free or
    lock-based. It does affect the performance, so the module may be too slow for
    a particular need until it's optimized, but we do these optimizations as needed,
    and they are confined to the particular module.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we focus on the modules that implement data structures for
    concurrent programming. Why data structures and not algorithms? First of all,
    there is much more literature on concurrent algorithms out there. Second, most
    programmers have a much easier time dealing with the algorithms: the code gets
    profiled, there is a function that takes an excessively long time, we find a different
    way to implement the algorithm and move on to the next high pole on the performance
    chart. Then you end up with a program where no single computation takes a large
    portion of time, but you still have this feeling that it''s nowhere as fast as
    it should be. We have said it before, but it bears repeating: when you have no
    hot code, you probably have hot data.'
  prefs: []
  type: TYPE_NORMAL
- en: The data structures play an even more important role in concurrent programs
    because they determine what guarantees the algorithms can rely on and what the
    restrictions are. Which concurrent operations can be done safely on the same data?
    How consistent is the view of the data as seen by different threads? We cannot
    write much code if we don't have answers to these questions, and the answers are
    determined by our choice of the data structures.
  prefs: []
  type: TYPE_NORMAL
- en: At the same time, the design decisions, such as the choice of interfaces and
    module boundaries, can critically impact the choices we can make when writing
    concurrent programs. Concurrency cannot be added to a design as an afterthought;
    the design has to be drawn up with the concurrency in mind from the very beginning,
    especially the organization of the data.
  prefs: []
  type: TYPE_NORMAL
- en: We begin the exploration of the concurrent data structures by defining a few
    basic terms and concepts.
  prefs: []
  type: TYPE_NORMAL
- en: The basics of concurrent data structures
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Concurrent programs that use multiple threads need thread-safe data structures.
    This seems obvious enough. But what is thread safety, and what makes a data structure
    thread-safe? At first glance, it seems simple: if a data structure can be used
    by multiple threads at the same time without any data races (shared between threads),
    then it is thread-safe.'
  prefs: []
  type: TYPE_NORMAL
- en: 'However, this definition turns out to be too simplistic:'
  prefs: []
  type: TYPE_NORMAL
- en: It raises the plank very high – for example, none of the STL containers would
    be considered thread-safe.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It carries a very high performance cost.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is often unnecessary, and so is the cost.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On top of everything else, it would be completely useless in many cases.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s tackle these considerations one at a time. Why could a thread-safe data
    structure be unnecessary even in a multi-threaded program? One trivial possibility
    is that it is used in a single-threaded portion of the program. We strive to minimize
    such portions due to their deleterious impact on the overall runtime (remember
    Amdahl''s Law?), but most programs have some, and one of the ways we make such
    code faster is by not paying unnecessary overhead. The more common scenario for
    not needing thread safety is when an object is used exclusively by one thread,
    even in a multi-threaded program. This is very common and very desirable: as we
    have said several times, shared data is the main source of inefficiency in concurrent
    programs, so we try to do as much work as possible on each thread independently,
    using only local objects and data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'But can we be certain that a class or a data structure is safe to use in a
    multi-threaded program, even if each object is never shared between threads? Not
    necessarily: just because we do not see any sharing at the interface level does
    not mean that none is going on at the implementation level. Multiple objects could
    be sharing the same data internally: static members and memory allocators are
    just some of the possibilities (we tend to think that all objects that need memory
    get it by calling `malloc()` and that `malloc()` is thread-safe, but a class could
    implement its own allocator as well).'
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, many data structures are perfectly safe to use in a multi-threaded
    code as long as none of the threads modify the object. While this may seem obvious,
    again, we have to consider the implementation: the interface may be read-only,
    but the implementation may still modify the object. If you think that it is an
    exotic possibility, consider the standard C++ shared pointer, `std::shared_ptr`:
    when you make a copy of a shared pointer, the copied object is not modified, at
    least not visibly (it is passed to the constructor of the new pointer by `const`
    reference). At the same time, you know that the reference count in the object
    has to be incremented, which means the copied-from object has changed (shared
    pointers are thread-safe in this scenario, but this did not happen by accident,
    and neither is it free, there is a performance cost).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The bottom line is, we need a more nuanced definition of thread safety. Unfortunately,
    there is no common vocabulary for this very common concept, but there are several
    popular versions. The highest level of thread safety is often called a `const`
    member functions of the class), and, second, any thread that has exclusive access
    to an object can perform any otherwise valid operations on it, no matter what
    other threads are doing at the same time. An object that does not provide any
    such guarantee cannot be used in a multi-threaded program at all: even if the
    object itself is not shared, something inside its implementation is vulnerable
    to modifications by other threads.'
  prefs: []
  type: TYPE_NORMAL
- en: In this book, we will use the language of strong and weak thread-safety guarantees.
    A class that provides a strong guarantee is sometimes called simply `const` member
    functions. Finally, the classes that do not offer any guarantees at all are called
    **thread-hostile** and, generally, cannot be used in a multi-threaded program
    at all.
  prefs: []
  type: TYPE_NORMAL
- en: 'In practice, we often encounter a mix of strong and weak guarantees: a subset
    of the interface offers a strong guarantee, but the rest of it provides only the
    weak guarantee.'
  prefs: []
  type: TYPE_NORMAL
- en: 'So, why do we not try to design every object with a strong thread safety guarantee?
    The first reason we already mentioned: there is usually performance overhead,
    the guarantee is often unnecessary because the objects are not shared between
    threads, and the key to writing an efficient program is not doing any work that
    can be avoided. The more interesting objection is the one we mentioned earlier,
    in passing: even in the case where the object is shared in a way that would require
    thread safety, the strong thread safety guarantee may be useless. Consider this
    problem: you need to develop a game where the players recruit an army and do battles.
    The names of all the units in the army are stored in a container, let''s say a
    list of strings. Another container stores the current strength of each unit. During
    the campaign, the units get killed or recruited all the time, and the gaming engine
    is multi-threaded and needs to be efficient to manage a large army. While the
    STL containers provide only the weak thread safety guarantee, let''s assume that
    we have a library of strongly thread-safe containers. It is easy to see that this
    is not enough: adding a unit requires inserting its name into one container and
    its initial strength into the other. Both operations are thread-safe by themselves.
    One thread creates a new unit and inserts it into the first container. Before
    this thread can also add its strength value, another thread sees the new unit
    and needs to look up its strength, but there is nothing in the second container
    yet. The problem is that the thread safety guarantee is offered at the wrong level:
    from the application point of view, creating a new unit is a transaction, and
    all gaming engine threads should be able to see the database either before the
    unit is added or after, but not in the intermediate state. We can accomplish that,
    for example, by using a mutex: it will be locked before the unit is added and
    unlocked only after both containers have been updated. However, in this scenario,
    we don''t care about the thread safety guarantees provided by the individual containers,
    as long as all accesses to these objects are guarded by a mutex anyway. Obviously,
    what we need instead is a unit database that itself provides the desired thread
    safety guarantees, for example, by using mutexes. This database may internally
    use several container objects, and the implementation of the database may or may
    not need any thread safety guarantees from these containers, but this should be
    invisible to the clients of the database (having thread-safe containers may make
    the implementation easier, or not).'
  prefs: []
  type: TYPE_NORMAL
- en: 'This leads us to a very important conclusion: thread safety begins at the design
    stage. The data structures and the interfaces used by the program must be chosen
    wisely, so they represent the appropriate level of abstraction and the correct
    transactions at the level where thread interaction is taking place.'
  prefs: []
  type: TYPE_NORMAL
- en: 'With this in mind, the rest of this chapter should be seen from two sides:
    on the one hand, we show how to design and implement some basic thread-safe data
    structures that can be used as building blocks for the more complex (and infinitely
    more varied) ones you will need in your programs. On the other hand, we also show
    the basic techniques for building thread-safe classes that can be used to design
    these more complex data structures.'
  prefs: []
  type: TYPE_NORMAL
- en: Counters and accumulators
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the simplest thread-safe objects is a humble counter or its more general
    form, an accumulator. The counter simply counts some events that can occur on
    any of the threads. All threads may need to increment the counter or access the
    current value, so there is potential for a race condition.
  prefs: []
  type: TYPE_NORMAL
- en: 'To be of value, we need the strong thread safety guarantee here: the weak guarantee
    is trivial; reading a value that nobody is changing is always thread-safe. We
    have already seen the available options for the implementation: a lock of some
    kind, an atomic operation (when one is available), or a lock-free CAS loop.'
  prefs: []
  type: TYPE_NORMAL
- en: The performance of a lock varies with the implementation, but a spinlock is,
    in general, preferred. The wait time for a thread that did not get immediate access
    to the counter is going to be very short. So, it does not make sense to incur
    the cost of putting the thread to sleep and waking it up later. On the other hand,
    the amount of CPU time wasted because of the busy waiting (polling the spinlock)
    is going to be negligible, most likely just a few instructions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The atomic instruction delivers good performance, but the choice of operations
    is rather limited: in C++, you can atomically add to an integer but not, for example,
    multiply it. This is enough for a basic counter but may be insufficient for a
    more general accumulator (the accumulating operation does not have to be limited
    to a sum). However, if one is available, you just cannot beat the simplicity of
    an atomic operation.'
  prefs: []
  type: TYPE_NORMAL
- en: The CAS loop can be used to implement any accumulator, regardless of the operation
    we need to use. However, on most modern hardware, it is not the fastest option
    and is outperformed by a spinlock (see *Figure 6.2*).
  prefs: []
  type: TYPE_NORMAL
- en: 'The spinlock can be further optimized for the case when it is used to access
    a single variable or a single object. Instead of a generic flag, we can make the
    lock itself be the only reference to the object it is guarding. The atomic variable
    is going to be a pointer, not an integer, but otherwise, the locking mechanism
    remains unchanged. The `lock()` function is non-standard because it returns the
    pointer to the counter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Compared to the earlier implementation of the spinlock, the meaning of the atomic
    variable is "inverted:" the lock is available if the atomic variable `p_` is not
    null, otherwise it is taken. All the optimizations we have done for the spinlock
    are applicable here as well and look exactly the same, so we are not going to
    repeat them. Also, to be complete, the class needs a set of deleted copy operations
    (locks are non-copyable). It may be movable if the ability to transfer the lock
    and the responsibility to release it to another object is desirable. If the lock
    also owns the object it is pointing to, the destructor should delete it (this
    combines the functionality of a spinlock and a unique pointer in a single class).
  prefs: []
  type: TYPE_NORMAL
- en: 'One obvious advantage of the pointer spinlock is that, as long as it provides
    the only way to access the guarded object, it is not possible to accidentally
    create a race condition and access the shared data without a lock. The second
    advantage is that this lock slightly outperforms the regular spinlock more often
    than not. Whether or not the spinlock also outperforms the atomic operation depends
    on the hardware as well. The same benchmark yields very different results on different
    processors:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.3 – Performance of a shared count increment: regular spinlock, pointer
    spinlock, lock-free (compare-and-swap, or CAS), and wait-free (atomic) for different
    hardware systems (a) and (b)'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_6.3_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.3 – Performance of a shared count increment: regular spinlock, pointer
    spinlock, lock-free (compare-and-swap, or CAS), and wait-free (atomic) for different
    hardware systems (a) and (b)'
  prefs: []
  type: TYPE_NORMAL
- en: As a rule, the more recent processors handle locks and busy waiting better,
    and it is more likely that the spinlock delivers better performance on the latest
    hardware (in *Figure 6.3*, system *b* uses Intel X86 CPUs that are one generation
    behind those in system *a*).
  prefs: []
  type: TYPE_NORMAL
- en: 'The average time it takes to execute an operation (or its inverse, the throughput)
    is the metric that we are mainly concerned with in most HPC systems. However,
    this is not the only possible metric used to gauge the performance of concurrent
    programs. For example, if the program runs on a mobile device, the power consumption
    may be of greater importance. The total CPU time used by all threads is a reasonable
    proxy for the average power consumption. The same benchmark we used to measure
    the average real time of the counter increment can be used to measure the CPU
    time as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.4 – Average CPU time used by different implementations of the thread-safe
    counter'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_6.4_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.4 – Average CPU time used by different implementations of the thread-safe
    counter
  prefs: []
  type: TYPE_NORMAL
- en: The bad news here is that no matter the implementation, the cost of accessing
    the shared data by multiple threads at once increases exponentially with the number
    of threads, at least when we have many threads (note that the *y* axis scale in
    *Figure 6.4* is logarithmic). However, the efficiency varies greatly between the
    implementations, and, at least for the most efficient implementations, the exponential
    rise does not really kick in until at least eight threads. Note that the results
    will, again, vary from one hardware system to another, so the choice must be made
    with your target platform in mind and only after the measurements have been done.
  prefs: []
  type: TYPE_NORMAL
- en: Whatever the chosen implementation, a thread-safe accumulator or a counter should
    not expose it but encapsulate it in a class. One reason is to provide the clients
    of the class with a stable interface while retaining the freedom to optimize the
    implementation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second reason is more subtle, and it has to do with the exact guarantees
    the counter provides. So far, we have focused on the counter value itself, making
    sure that it is modified and accessed by all threads without any races. Whether
    or not this is enough depends on how we use the counter. If all we want is to
    count some events, and nothing else depends on the value of the counter, then
    we only care that the value itself is correct. On the other hand, if what we are
    counting is, say, the number of elements in an array, then we are dealing with
    a data dependency. Let''s say that we have a large pre-allocated array (or a container
    that can grow without disturbing the elements already in it), and all threads
    are computing new elements to be inserted into this array. The counter counts
    the number of elements that are computed and inserted into the array and can be
    used by other threads. In other words, if a thread reads the value `N` from the
    counter, it must be assured that the first `N` elements of the array are safe
    to read (which implies that no other thread is modifying them anymore). But the
    array itself is neither atomic nor protected by a lock. To be sure, we could have
    protected the access to the entire array by a lock, but this is probably going
    to kill the performance of the program: if there are many elements already in
    the array but only one thread can read them at any time, the program might as
    well be single-threaded. On the other hand, we know that any constant, immutable
    data is safe to read from multiple threads without any locks. We just need to
    know where the boundary between the immutable and the changing data is, and that
    is exactly what the counter is supposed to provide. The key issue here is the
    memory visibility: we need a guarantee that any changes to the first `N` elements
    of the array become visible to all threads before the value of the counter changes
    from `N-1` to `N`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We studied memory visibility in the previous chapter when we discussed the
    memory model. At the time, it might have appeared to be a largely theoretical
    matter, but not anymore. From the last chapter, we know that the way we control
    the visibility is by restricting the memory order or by using memory barriers
    (two different ways to talk about the same thing). The key difference between
    a count and an index in a multi-threaded program is that the index provides an
    additional guarantee: if the thread that increments the index from `N-1` to `N`
    had completed the initialization of the array element `N` before it incremented
    the index, then any other thread that reads the index and gets the value of `N`
    (or greater) is guaranteed to see at least `N` fully initialized and safe to read
    elements in the array (assuming no other thread writes into these elements, of
    course). This is a non-trivial guarantee, do not easily dismiss it: multiple threads
    are accessing the same location in memory (the array element `N`) *without any
    locking*, and one of these threads is *writing* into this location, and yet, the
    access is safe, there is no data race. If we could not arrange for this guarantee
    using the shared index, we would have to lock all accesses to the array, and only
    one thread would be able to read it at any time. Instead, we can use this atomic
    index class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The only difference between the index at the count is in the memory visibility
    guarantees; the count offers none:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The thread safety and memory visibility guarantees should be documented for
    each of the classes, of course. Whether or not there is a performance difference
    between the two depends on the hardware. On an X86 CPU, there is none because
    the hardware instructions for atomic increment and atomic read have the "index-like"
    memory barriers whether we request them or not. On ARM CPUs, relaxed (or no-barrier)
    memory operations are noticeably faster. But, regardless of the performance, clarity
    and intent matter and should not be forgotten: if a programmer uses an index class
    that explicitly offers the memory order guarantees but does not index anything
    with it, every reader will wonder what is going on and where is that subtle and
    hidden place in the code that uses these guarantees. By using the interfaces with
    the correct set of documented guarantees, you signal to your readers what your
    intent was when writing this code.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us now return to what may be the main "hidden" accomplishment in this section.
    We learned about thread-safe counters, but along the way, we came up with an algorithm
    to seemingly violate the first rule of writing multi-threaded code: any time two
    or more threads access the same memory location and at least one of these threads
    is writing, all accesses must be locked (or atomic). We did not lock the shared
    array, we allow arbitrary data in its elements (so it''s probably not atomic),
    and we got away with it! The approach we used to avoid data races turns out to
    be the cornerstone of almost every data structure designed specifically for concurrency,
    and we will now take time to better understand and generalize it.'
  prefs: []
  type: TYPE_NORMAL
- en: Publishing protocol
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The general problem we are trying to solve is a very common one in data structure
    design and, by extension, the development of concurrent programs: one thread is
    creating new data, and the rest of the program must be able to see this data when
    it is ready, but not before. The former thread is often called the writer thread
    or the producer thread. All the other threads are reader or consumer threads.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The most obvious solution is to use a lock and follow the rule of avoiding
    the data races to the letter. If multiple threads (check) must access the same
    memory location (check) and at least one thread is writing at this location (exactly
    one thread in our case – check), then all threads must acquire a lock before accessing
    this memory location for either reading or writing. The downside of this solution
    is the performance: long after the producer is done and no more writing happens,
    all the consumer threads keep locking each other out of reading the data concurrently.
    Now, read-only access does not require any locking at all, but the problem is,
    we need to have a guaranteed point in the program such that all the writing happens
    before this point and all the reading happens after this point. Then we can say
    that all consumer threads operate in a read-only environment and do not need any
    locking. The challenge is to guarantee that boundary between reading and writing:
    remember that, unless we do some sort of synchronization, memory visibility is
    not guaranteed: just because the writer has finished modifying the memory doesn''t
    mean the reader sees the final state of that memory. The locks include the appropriate
    memory barriers, as we have seen earlier; they border the critical section and
    ensure that any operation executed after the critical section will see all the
    changes to the memory that happened before or during the critical section. But
    now we want to get the same guarantee without the locks.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The lock-free solution to this problem relies on a very specific protocol for
    passing information between the producer and the consumer threads:'
  prefs: []
  type: TYPE_NORMAL
- en: The producer thread prepares the data in a memory that is not accessible to
    other threads. It could be the memory allocated by the producer threads, or it
    could be pre-allocated memory, but the important point is that the producer is
    the only thread with a valid reference to this memory, and that valid reference
    is not shared with other threads (there may be a way for other threads to access
    this memory, but that would be a bug in the program, similar to indexing an array
    out of bounds). Since there is only one thread accessing the new data, no synchronization
    is required. As far as the other threads are concerned, the data simply does not
    exist.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'All consumer threads must use a single shared pointer for any access to the
    data, which we call the root pointer, and this pointer is initially null. It remains
    null while the producer thread is constructing the data. Again, from the point
    of view of the consumer threads, there is no data at this time. More generally,
    the "pointer" does not need to be an actual pointer: any kind of handle or reference
    can be used as long as it gives access to the memory location and can be set to
    a predetermined invalid value. For example, if all new objects are created in
    a pre-allocated array, the "pointer" could, in fact, be an index into the array,
    and the invalid value could be any value greater or equal to the array size.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The key to the protocol is that the only way for the consumer to access the
    data is through the root pointer, and this pointer remains null until the producer
    is ready to reveal, or publish, the data. The act of publishing the data is very
    simple: the producer must atomically store the correct memory location of the
    data in the root pointer, and this change must be accompanied by the release memory
    barrier.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The consumer thread can, at any time, query the root pointer, again atomically.
    If the query returns null, then there is no data (as far as the consumer is concerned),
    and the consumer thread should wait or, ideally, do some other work. If the query
    returns a non-null value, then the data is ready, and the producer will not change
    it anymore. The query must be done with the acquire memory barrier, which, in
    combination with the release barrier on the producer side, guarantees that the
    new data is visible when the change of the pointer value is observed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This process is sometimes called the **publishing protocol** because it allows
    the producer thread to publish information for other threads to consume in a way
    that guarantees no data races. As we said, the publishing protocol can be implemented
    using any handle that gives access to the memory as long as this handle can be
    changed atomically. Pointers are the most common handle, of course, followed by
    array indices.
  prefs: []
  type: TYPE_NORMAL
- en: 'The data that is being published can be simple or complex; it doesn''t matter.
    It does not even have to be a single object or a single memory location: the object
    that the root pointer points to can itself contain pointers to more data. The
    key elements of the publishing protocol are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: All consumers access a particular set of data through one root pointer. The
    only way to gain access to the data is to read a non-null value of the root pointer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The producer can prepare the data any way it wants, but the root pointer remains
    null: the producer has its own reference to the data that is local to this thread.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When the producer wants to publish the data, it sets the root pointer to the
    correct address atomically and with a release barrier. After the data is published,
    the producer cannot change it (neither can anyone else).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The consumer threads must read the root pointer atomically and with an acquire
    barrier. If they read a non-null value, they can read the data accessible through
    the root pointer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The atomic reads and writes used to implement the publishing protocol should
    not be, of course, scattered throughout the code. We should implement a publishing
    pointer class to encapsulate this functionality. In the next section, we will
    see a simple version of such a class.
  prefs: []
  type: TYPE_NORMAL
- en: Smart pointers for concurrent programming
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The challenge of concurrent (thread-safe) data structures is how to add, remove,
    and change the data in a way that maintains certain thread safety guarantees.
    The publishing protocol, which gives us a way to release new data to all threads,
    is usually the first step in adding new data to any such data structure. Thus,
    it should come as no surprise that the first class we will learn about is a pointer
    that encapsulates this protocol.
  prefs: []
  type: TYPE_NORMAL
- en: Publishing pointer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Here is a basic publishing pointer that also includes the functionality of
    a unique, or owning, pointer (so we can call it a thread-safe unique pointer):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Of course, this is a very bare-bones design; a complete implementation should
    support a custom deleter, a move constructor and assignment operator, and maybe
    a few more features, similar to `std::unique_ptr`. By the way, the standard does
    not guarantee that accessing the pointer value stored in a `std::unique_ptr` object
    is atomic or that the necessary memory barriers are used, so the standard unique
    pointer cannot be used to implement the publishing protocol.
  prefs: []
  type: TYPE_NORMAL
- en: 'By now, it should be clear to the reader what our thread-safe unique pointer
    offers: the key functions are `publish()` and `get()`, and they implement the
    publishing protocol. Note that the `publish()` method does not delete the old
    data; it is assumed that the producer thread calls `publish()` only once and only
    on a null pointer. We could add an assert for that, and it may be a good idea
    to do so in a debug build, but we are also concerned with the performance. Speaking
    of performance, a benchmark shows that the single-threaded dereferencing of our
    publishing pointer takes the same time as that of a raw pointer or of `std::unique_ptr`.
    The benchmark is not complicated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Running this benchmark gives us an idea of how fast the dereferencing of our
    lock-free publishing pointer is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.5 – The performance of the publishing pointer (consumer threads)'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_6.5_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.5 – The performance of the publishing pointer (consumer threads)
  prefs: []
  type: TYPE_NORMAL
- en: 'The result should be compared with dereferencing a raw pointer, which we can
    also do on multiple threads:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.6 – The performance of the raw pointer, for comparison with Figure
    6.5'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_6.6_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.6 – The performance of the raw pointer, for comparison with Figure
    6.5
  prefs: []
  type: TYPE_NORMAL
- en: 'The performance numbers are very close. We can also compare the speed of publishing,
    but, usually, the consumer side is more important: each object is published only
    once but then accessed many times.'
  prefs: []
  type: TYPE_NORMAL
- en: It is equally important to understand what the publishing pointer does not do.
    First of all, there is no thread safety in the construction of the pointer. We
    have assumed that both the producer and the consumer threads share access to the
    already constructed pointer, which is initialized to null. Who constructed and
    initialized the pointer? Usually, in any data structure, there is a root pointer
    through which the entire data structure can be accessed; it was initialized by
    whatever thread constructed the initial data structure. Then there are pointers
    that serve as a root for some data element and are themselves contained in another
    data element. For now, imagine a simple singly linked list where the "next" pointer
    of every list element is the root for the next element, and the head of the list
    is the root for the entire list. The thread that produces an element of the list
    must, among other things, initialize the "next" pointer to null. Then, another
    producer can add a new element and publish it. Note that this deviates from the
    general rule that the data, once published, is immutable. This is OK, however,
    because all changes to the thread-safe unique pointer are atomic. One way or another,
    it is critical that no thread can access the pointer while it is being constructed
    (this is a very common restriction, most constructions are not thread-safe, even
    the question of their thread safety is ill-posed since the object does not exist
    until it is constructed, so no guarantees can be given).
  prefs: []
  type: TYPE_NORMAL
- en: 'The next thing our pointer does not do is this: it does not offer any synchronization
    for multiple producer threads. If two threads attempt to publish their new data
    elements through the same pointer, the results are undefined, and there is a data
    race (some consumer threads will see one set of data, and others will see different
    data). If there is more than one producer thread that operates on a particular
    data structure, they must use another mechanism for synchronization.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, while our pointer implements a thread-safe publishing protocol, it
    does nothing to safely "un-publish" and delete the data. It is an owning pointer,
    so when it is deleted, so is the data it points to. However, any consumer thread
    can access the data using the value it had acquired earlier, even after the pointer
    is deleted. The issues of data ownership and lifetime must be handled in some
    other way. Ideally, we would have a point in the program where the entire data
    structure or some subset of it is known to be no longer needed; no consumer thread
    should try to access this data or even retain any pointers to it. At that point,
    the root pointer and anything accessible through it can be safely deleted. Arranging
    for such a point in the execution is a different matter entirely; it is often
    controlled by the overall algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes we want a pointer that manages both the creation and the deletion
    of the data in a thread-safe way. In this case, we need a thread-safe shared pointer.
  prefs: []
  type: TYPE_NORMAL
- en: Atomic shared pointer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If we cannot guarantee that there is a known point in the program where the
    data can be safely deleted, we have to keep track of how many consumer threads
    hold valid pointers to the data. If we want to delete this data, we have to wait
    until there is only one pointer to it in the entire program; then, it is safe
    to delete the data and the pointer itself (or at least reset it to null). This
    is a typical job for a shared pointer that does reference counting: it counts
    how many pointers to the same object are still out there in the program; the data
    is deleted by the last such pointer.'
  prefs: []
  type: TYPE_NORMAL
- en: 'When talking about thread-safe shared pointers, it is vitally important to
    understand precisely what guarantees are required from the pointer. The C++ standard
    shared pointer, `std::shared_ptr`, is often referred to as thread-safe. Specifically,
    it offers the following guarantee: if multiple threads operate on different shared
    pointers that all point to the same object, then the operations on the reference
    counter are thread safe even if two threads cause the counter to change at the
    same time. For example, if one thread is making a copy of its shared pointer while
    another thread is deleting its shared pointer and the reference count was `N`
    before these operations started, the counter will go up to `N+1`, then back to
    `N` (or down first, then up, depending on the actual order of execution) and in
    the end will have the same value `N`. The intermediate value could be either `N+1`
    or `N-1`, but there is no data race, and the behavior is well defined, including
    the final state. This guarantee implies that the operations on the reference counter
    are atomic; indeed, the reference counter is an atomic integer and the implementation
    used `fetch_add()` to atomically increment or decrement it.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This guarantee applies as long as no two threads share access to the same shared
    pointer. How to get each thread its own shared pointer is a separate issue: since
    all shared pointers pointing to the same object must be created from the very
    first such pointer, these pointers had to have been passed from one thread to
    another at some point in time. For simplicity, let us assume, for a moment, that
    the code that made copies of the shared pointer is protected by a mutex. If two
    threads access the same shared pointer, then all bets are off. For example, if
    one thread is trying to copy the shared pointer while another thread is resetting
    it at the same time, the results are undefined. In particular, the standard shared
    pointer cannot be used to implement the publishing protocol. However, once the
    copies of the shared pointer have been distributed to all threads (possibly under
    lock), the shared ownership is maintained, and the deletion of the object is handled
    in a thread-safe manner. The object will be deleted once the last shared pointer
    that points to it is deleted. Note that, since we agreed that each particular
    shared pointer is never handled by more than one thread, this is completely safe.
    If, during the execution of the program, the time comes when there is only one
    shared pointer that owns our object, then there is also only one thread that can
    access this object. Other threads cannot make copies of this pointer (we don''t
    let two threads share the same pointer object) and don''t have any other way to
    get a pointer to the same object, so the deletion will proceed effectively single-threaded.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is all well and good, but what if we cannot guarantee that two threads
    won''t try to access the same shared pointer? The first example of such access
    is our publishing protocol: the consumer threads are reading the value of the
    pointer while the producer thread may be changing it. We need the operations on
    the shared pointer itself to be atomic. In C++20, we can do just that: it lets
    us write `std::atomic<std::shared_ptr<T>>`. Note that the early proposals featured
    a new class, `std::atomic_shared_ptr<T>`, instead. In the end, this is not the
    path that was chosen.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you do not have a C++20-compliant compiler and the corresponding standard
    library or cannot use C++20 in your code, you can still do atomic operations on
    `std::shared_ptr`, but you must do so explicitly. In order to publish the object
    using the pointer `p_` that is shared between all threads, the producer thread
    must do this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'On the other hand, to acquire the pointer, the consumer thread must do this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The major downside of this approach, compared to the C++20 atomic shared pointer,
    is that there is no protection against accidental non-atomic access. It is up
    to the programmer to remember to always use atomic functions to operate on the
    shared pointer.
  prefs: []
  type: TYPE_NORMAL
- en: 'It should be noted that, while convenient, `std::shared_ptr` is not a particularly
    efficient pointer, and the atomic accesses make it even slower. We can compare
    the speed of publishing an object using the thread-safe publishing pointer from
    the last section versus the shared pointer with explicit atomic accesses:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.7 – The performance of the atomic shared publishing pointer (consumer
    threads)'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_6.7_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.7 – The performance of the atomic shared publishing pointer (consumer
    threads)
  prefs: []
  type: TYPE_NORMAL
- en: 'Again, the numbers should be compared with those from *Figure 6.5*: the publishing
    pointer is 60 times faster on one thread, and the advantage increases with the
    number of threads. Of course, the whole point of the shared pointer is that it
    provides shared resource ownership, so naturally, it takes more time to do more
    work. The point of the comparison is to show the cost of this shared ownership:
    if you can avoid it, your program will be much more efficient.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Even if you need shared ownership (and there are some concurrent data structures
    that are really hard to design without it), usually, you can do much better if
    you design your own reference-counted pointer with limited functionality and optimal
    implementation. One very common approach is to use intrusive reference counting.
    An **intrusive shared pointer** stores its reference count in the object it points
    to. When designed for a specific object, such as a list node in our particular
    data structure, the object is designed with the shared ownership in mind and contains
    a reference counter. Otherwise, we can use a wrapper class for almost any type
    and augment it with a reference counter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'When decrementing the reference count, it is important to know when it reaches
    0 (or was 1 before decrementing): the shared pointer must then delete the object.'
  prefs: []
  type: TYPE_NORMAL
- en: The implementation of even the simplest atomic shared pointer is quite lengthy;
    a very rudimentary example can be found in the sample code for this chapter. Again,
    this example contains only the bare minimum necessary for the pointer to correctly
    perform several tasks such as publishing an object and accessing the same pointer
    concurrently by multiple threads. The aim of the example is to make it easier
    to understand the essential elements of implementing such pointer (and even then,
    the code is several pages long).
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to using an intrusive reference counter, an application-specific
    shared pointer can forgo other features of `std::shared_ptr`. For example, many
    applications do not require a weak pointer, but there is an overhead for supporting
    it even if it''s never used. A minimalistic reference-counted pointer can be several
    times more efficient than the standard one:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.8 – The performance of a custom atomic shared publishing pointer
    (consumer threads)'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_6.8_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.8 – The performance of a custom atomic shared publishing pointer (consumer
    threads)
  prefs: []
  type: TYPE_NORMAL
- en: It is similarly more efficient for assignment and reassignment of the pointer,
    atomic exchange of two pointers, and other atomic operations on the pointer. Even
    this shared pointer is still much less efficient than a unique pointer, so again,
    if you can manage the data ownership explicitly, without reference-counting, do
    so.
  prefs: []
  type: TYPE_NORMAL
- en: 'We now have the two key building blocks of almost any data structure: we can
    add new data and publish it (reveal it to other threads), and we can track the
    ownership, even across threads (although it comes at a price).'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we have learned about the performance of the basic building
    blocks of any concurrent program. All accesses to the shared data must be protected
    or synchronized, but there is a wide range of options when it comes to implementing
    such synchronization. While mutex is the most commonly used and the simplest alternative,
    we have learned several other, better-performing options: spinlocks and their
    variants, as well as lock-free synchronization.'
  prefs: []
  type: TYPE_NORMAL
- en: The key to an efficient concurrent program is to make as much data as possible
    local to one thread and minimize the operations on the shared data. The requirements
    specific to each problem usually dictate that such operations cannot be eliminated
    completely, so this chapter is all about making the concurrent data accesses more
    efficient.
  prefs: []
  type: TYPE_NORMAL
- en: We studied how to count or accumulate results across multiple threads, again
    with and without locks. Understanding the data dependency issues led us to the
    discovery of the publishing protocol and its implementation in several thread-safe
    smart pointers, suitable for different applications.
  prefs: []
  type: TYPE_NORMAL
- en: We are now well prepared to take our study to the next level and put several
    of these building blocks together in the form of more complex thread-safe data
    structures. In the next chapter, you will learn how to use these techniques to
    design practical data structures for concurrent programs.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What are the defining properties of lock-based, lock-free, and wait-free programs?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If an algorithm is wait-free, does it mean that it will scale perfectly?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the drawbacks of the locks that prompt us to look for alternatives?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the difference between a shared counter and a shared index into an array
    or another container?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the key advantage of the publishing protocol?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
