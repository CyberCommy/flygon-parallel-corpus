- en: Regression and Forecasting
  prefs: []
  type: TYPE_NORMAL
- en: One of the most important tasks that a statistician or data scientist has is
    to generate a systematic understanding of the relationship between two sets of
    data. This can mean a "continuous" relationship between two sets of data, where
    one value depends directly on the value of another variable. Alternatively, it
    can mean a categorical relationship, where one value is categorized according
    to another. The tool for working with these kinds of problems is *regression*.
    In its most basic form, regression involves fitting a straight line through a
    scatter plot of the two sets of data and performing some analysis to see how well
    this line "fits" the data. Of course, we often need something more sophisticated
    to model more complex relationships that exist in the real world.
  prefs: []
  type: TYPE_NORMAL
- en: Time series represent a specialized class of these regression type problems,
    where we have a value that is evolving over a period of time. Unlike more simple
    problems, time series data usually has complex dependencies between consecutive
    values; for instance, a value may depend on both of the previous values, and perhaps
    even on the previous "noise". Time series modeling is important across science
    and economics, and there are a variety of tools for modeling time series data.
    The basic technique for working with time series data is called **ARIMA**, which
    stands for **autoregressive integrated moving average**. This model incorporates
    two underlying components, an **autoregressive** (**AR**)**component and a **moving
    average** (**MA**) component, to construct a model for the observed data.**
  prefs: []
  type: TYPE_NORMAL
- en: '**In this chapter, we will learn how to model the relationship between two
    sets of data, quantify how strong this relationship is, and generate forecasts
    about other values (the future). Then, we will learn how to use logistic regression,
    which is a variation of a simple linear model, in classification problems. Finally,
    we will build models for time series data using ARIMA and build on these models
    for different kinds of data. We will finish this chapter by using a library called
    Prophet to automatically generate a model for time series data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Using basic linear regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using multilinear regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classifying using logarithmic regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Modeling time series data with ARMA
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Forecasting from time series data using ARIMA
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Forecasting seasonal data using ARIMA
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Prophet to model time series
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's get started!
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, as usual, we will need the NumPy package imported under the
    alias `np`, the Matplotlib `pyplot` module imported as `plt`, and the Pandas package
    imported as `pd`. We can do this using the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We will also need some new packages in this chapter. The statsmodels package
    is used for regression and time series analysis, the `scikit-learn` package (`sklearn`)
    provides general data science and machine learning tools, and the Prophet package
    (`fbprophet`) is used for automatically modeling time series data. These packages
    can be installed using your favorite package manager, such as `pip`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The Prophet package can prove difficult to install on some operating systems
    because of its dependencies. If installing `fbprophet` causes a problem, you might
    want to try using the Anaconda distribution of Python and its package manager,
    `conda`, which handles the dependencies more rigorously:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Finally, we also need a small module called `tsdata` that is contained in the
    repository for this chapter. This module contains a series of utilities for producing
    sample time series data.
  prefs: []
  type: TYPE_NORMAL
- en: The code for this chapter can be found in the `Chapter 07` folder of the GitHub
    repository at [https://github.com/PacktPublishing/Applying-Math-with-Python/tree/master/Chapter%2007](https://github.com/PacktPublishing/Applying-Math-with-Python/tree/master/Chapter%2007).
  prefs: []
  type: TYPE_NORMAL
- en: 'Check out the following video to see the Code in Action: [https://bit.ly/2Ct8m0B](https://bit.ly/2Ct8m0B).'
  prefs: []
  type: TYPE_NORMAL
- en: Using basic linear regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Linear regression is a tool for modeling the dependence between two sets of
    data so that we can eventually use this model to make predictions. The name comes
    from the fact that we form a linear model (straight line) of one set of data based
    on a second. In the literature, the variable that we wish to model is frequently
    called the *response* variable, and the variable that we are using in this model
    is the *predictor* variable.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we'll learn how to use the statsmodels package to perform simple
    linear regression to model the relationship between two sets of data.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For this recipe, we will need the statsmodels `api` module imported under the
    alias `sm`, the NumPy package imported as `np`, the Matplotlib `pyplot` module
    imported as `plt`, and an instance of a NumPy default random number generator.
    All this can be achieved with the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following steps outline how to use the statsmodels package to perform a
    simple linear regression on two sets of data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we generate some example data that we can analyze. We''ll generate two
    sets of data that will illustrate a good fit and a less good fit:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'A good first step in performing regression analysis is to create a scatter
    plot of the datasets. We''ll do this on the same set of axes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to use the `sm.add_constant` utility routine so that the modeling step
    will include a constant value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can create an `OLS` model for our first set of data and use the `fit`
    method to fit the model. We then print a summary of the data using the `summary`
    method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We repeat the model fitting for the second set of data and print the summary:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we create a new range of *x* values using `linspace` that we can use to
    plot the trend lines on our scatter plot. We need to add the `constant` column
    to interact with the models that we have created:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we use the `predict` method on the model objects so that we can use the
    model to predict the response value at each of the *x* values we generated in
    the previous step:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we plot the model data computed in the previous two steps on top of
    the scatter plot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The scatter plot, along with the best fit lines (the models) we added, can
    be seen in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/8bf4175e-e73a-4f3b-b5a1-860f8d0bc8f1.png)Figure 7.1: Scatter plot
    of data with lines of best fit computed using least squares regression'
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Elementary mathematics tells us that the equation of a straight line is given
    by the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/def061a4-d8f7-482e-a3a3-78d4587c495f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *c*is the value at which the line meets the *y* axis, usually called
    the*y**intercept*, and *m*is the *gradient*of the line. In the linear regression
    context, we are trying to find a relationship between the response variable, *Y*,
    and the predictor variable, *X*, that has the form of a straight line so that
    the following occurs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/2777a0a2-21f8-41b8-97d0-0969989f1e8f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *c*and *m*are now parameters that are to be found. We can write this
    in a different way, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/f85da231-6294-45c3-9e5a-b08dd6a6ef79.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *E* is an error term, which, in general, depends on *X*. To find the
    "best" model, we need to find values for the *c* and *m* parameters, for which
    the error term, *E*, is minimized (in an appropriate sense). The basic method
    for finding the values of the parameters such that this error is minimized is
    the method of least squares, which gives its name to the type of regression used
    here: *ordinary least squares*. Once we have used this method to establish some
    relationship between a response variable and a predictor variable, our next task
    is to assess how well this model actually represents this relationship. For this,
    we form the *residuals* given by the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/c3ade12f-e352-49c2-a1f1-b07373253b4d.png)'
  prefs: []
  type: TYPE_IMG
- en: We do this for each of the data points, *X[i]* and *Y[i]*. In order to provide
    a rigorous statistical analysis of how well we have modeled the relationship between
    the data, we need the residuals to satisfy certain assumptions. First, we need
    them to be independent in the sense of probability. Second, we need them to be
    normally distributed about 0 with a common variance. (In practice, we can relax
    these slightly and still make reasonable comments about the accuracy of the model.)
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we generated response data from the predictor data using a linear
    relationship. The difference between the two response datasets we created is the
    "size" of the error at each value. For the first dataset, `y1`, the residuals
    were normally distributed with a standard deviation of 0.5, whereas for the second
    dataset, `y2`, the residuals have a standard deviation of 5.0\. We can see this
    variability in the scatter plot shown in the *Figure 7.1*, where the data for
    `y1` is generally very close to the best fit line – which closely matches the
    actual relationship that was used to generate the data – whereas the `y2` data
    is much further from the best fit line.
  prefs: []
  type: TYPE_NORMAL
- en: The `OLS` object from the statsmodels package is the main interface for ordinary
    least squares regression. We provide the response data and the predictor data
    as arrays. In order to have a constant term in the model, we need to add a column
    of ones in the predictor data. The `sm.add_constant` routine is a simple utility
    for adding this constant column. The `fit` method of the `OLS` class computes
    the parameters for the model and returns a results object (`model1` and `model2`)
    that contains the parameters for the best fit model. The `summary` method creates
    a string containing information about the model and various statistics about the
    goodness of fit. The `predict` method applies the model to new data. As the name
    suggests, it can be be used to make predictions using the model.
  prefs: []
  type: TYPE_NORMAL
- en: There are two statistics reported in the summary besides the parameter values
    themselves. The first is the *R²* value, or the adjusted version, which measures
    the variability explained by the model against the total variability. This value
    will be between 0 and 1\. A higher value indicates a better fit. The second is
    the F statistic p value, which indicates the overall significance of the model.
    As with ANOVA testing, a small F statistic indicates that the model is significant,
    meaning that the model is more likely to accurately model the data.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, the first model, `model1`, has an adjusted *R²* value of 0.986,
    indicating that the model very closely fits the data, and a p value of 6.43e-19,
    indicating high significance. The second model has an adjusted *R²* value of 0.361,
    which indicates that the model less closely fits the data, and a p value of 0.000893,
    which also indicates high significance. Even though the second model less closely
    fits the data, in terms of statistics, that is not to say that it is not useful.
    The model is still significant, though less so than the first model, but it doesn't
    account for all of the variability (or at least a significant portion of it) in
    the data. This could be indicative of additional (non-linear) structures in the
    data, or that the data is less correlated, which means there is a weaker relationship
    between the response and predictor data (due to the way we constructed the data,
    we know that the latter is true).
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Simple linear regression is a good general-purpose tool in a statistician''s
    toolkit. It is excellent for finding the nature of the relationship between two
    sets of data that are known (or suspected) to be connected in some way. The statistical
    measurement of how much one set of data depends on another is called *correlation*.
    We can measure correlation using a correlation coefficient, such as *Spearman''s
    rank correlation coefficient*. A high positive correlation coefficient indicates
    a strong positive relationship between the data, such as that seen in this recipe,
    while a high negative correlation coefficient indicates a strong negative relationship,
    where the slope of the best fit line through the data is negative. A correlation
    coefficient of 0 means that the data is not correlated: there is no relationship
    between the data.'
  prefs: []
  type: TYPE_NORMAL
- en: If the sets of data are clearly related but not in a linear (straight line)
    relationship, then it might follow a polynomial relationship where, for example,
    one value is related to the other squared. Sometimes, you can apply a transformation,
    such as a logarithm, to one set of data and then use linear regression to fit
    the transformed data. Logarithms are especially useful when there is a power-law
    relationship between the two sets of data.
  prefs: []
  type: TYPE_NORMAL
- en: Using multilinear regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Simple linear regression, as seen in the previous recipe, is excellent for producing
    simple models of a relationship between one response variable and one predictor
    variable. Unfortunately, it is far more common to have a single response variable
    that depends on many predictor variables. Moreover, we might not know which variables
    from a collection make good predictor variables. For this task, we need multilinear
    regression.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will learn how to use multilinear regression to explore the
    relationship between a response variable and several predictor variables.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For this recipe, we will need the NumPy package imported as `np`, the Matplotlib
    `pyplot` module imported as `plt`, the Pandas package imported as `pd`, and an
    instance of the NumPy default random number generator created using the following
    commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We will also need the statsmodels `api` module imported as `sm`, which can
    be imported using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following steps show you how to use multilinear regression to explore the
    relationship between several predictors and a response variable:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to create the predictor data to analyze. This will take the
    form of a Pandas `DataFrame` with four terms. We will add the constant term at
    this stage by adding a column of ones:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will generate the response data using only the first two variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we''ll produce scatter plots of the response data against each of the
    predictor variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we''ll add axis labels and titles to each scatter plot since this is
    good practice:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: model = sm.OLS(Y, p_vars).fit()
  prefs: []
  type: TYPE_NORMAL
- en: print(model.summary())
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: OLS Regression Results
  prefs: []
  type: TYPE_NORMAL
- en: ==================================================================
  prefs: []
  type: TYPE_NORMAL
- en: 'Dep. Variable: y                  R-squared: 0.770'
  prefs: []
  type: TYPE_NORMAL
- en: 'Model: OLS Adj.                   R-squared: 0.762'
  prefs: []
  type: TYPE_NORMAL
- en: 'Method: Least Squares             F-statistic: 106.8'
  prefs: []
  type: TYPE_NORMAL
- en: 'Date: Thu, 23 Apr 2020            Prob (F-statistic): 1.77e-30'
  prefs: []
  type: TYPE_NORMAL
- en: 'Time: 12:47:30                    Log-Likelihood: -389.38'
  prefs: []
  type: TYPE_NORMAL
- en: 'No. Observations: 100             AIC: 786.8'
  prefs: []
  type: TYPE_NORMAL
- en: 'Df Residuals: 96                  BIC: 797.2'
  prefs: []
  type: TYPE_NORMAL
- en: 'Df Model: 3'
  prefs: []
  type: TYPE_NORMAL
- en: 'Covariance Type: nonrobust'
  prefs: []
  type: TYPE_NORMAL
- en: ===================================================================
  prefs: []
  type: TYPE_NORMAL
- en: coef    std err     t      P>|t|      [0.025     0.975]
  prefs: []
  type: TYPE_NORMAL
- en: '-------------------------------------------------------------------'
  prefs: []
  type: TYPE_NORMAL
- en: const   -9.8676    4.028   -2.450    0.016     -17.863    -1.872
  prefs: []
  type: TYPE_NORMAL
- en: X1       4.7234    0.303    15.602   0.000       4.122     5.324
  prefs: []
  type: TYPE_NORMAL
- en: X2      -1.8945    0.166   -11.413   0.000     -2.224     -1.565
  prefs: []
  type: TYPE_NORMAL
- en: X3      -0.0910    0.206   -0.441    0.660     -0.500      0.318
  prefs: []
  type: TYPE_NORMAL
- en: ===================================================================
  prefs: []
  type: TYPE_NORMAL
- en: 'Omnibus: 0.296                Durbin-Watson: 1.881'
  prefs: []
  type: TYPE_NORMAL
- en: 'Prob(Omnibus): 0.862          Jarque-Bera (JB): 0.292'
  prefs: []
  type: TYPE_NORMAL
- en: 'Skew: 0.123                   Prob(JB): 0.864'
  prefs: []
  type: TYPE_NORMAL
- en: 'Kurtosis: 2.904               Cond. No. 72.9'
  prefs: []
  type: TYPE_NORMAL
- en: ===================================================================
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: second_model = sm.OLS(Y, p_vars.loc[:, "const":"X2"]).fit()
  prefs: []
  type: TYPE_NORMAL
- en: print(second_model.summary())
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: from numpy.random import default_rng
  prefs: []
  type: TYPE_NORMAL
- en: rng = default_rng(12345)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: from sklearn.linear_model import LogisticRegression
  prefs: []
  type: TYPE_NORMAL
- en: from sklearn.metrics import classification_report
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: df = pd.DataFrame({
  prefs: []
  type: TYPE_NORMAL
- en: '"var1": np.concatenate([rng.normal(3.0, 1.5, size=50),'
  prefs: []
  type: TYPE_NORMAL
- en: rng.normal(-4.0, 2.0, size=50)]),
  prefs: []
  type: TYPE_NORMAL
- en: '"var2": rng.uniform(size=100),'
  prefs: []
  type: TYPE_NORMAL
- en: '"var3": np.concatenate([rng.normal(-2.0, 2.0, size=50),'
  prefs: []
  type: TYPE_NORMAL
- en: rng.normal(1.5, 0.8, size=50)])
  prefs: []
  type: TYPE_NORMAL
- en: '})'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: score = 4.0 + df["var1"] - df["var3"]
  prefs: []
  type: TYPE_NORMAL
- en: Y = score >= 0
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: fig1, ax1 = plt.subplots()
  prefs: []
  type: TYPE_NORMAL
- en: ax1.plot(df.loc[Y, "var1"], df.loc[Y, "var3"], "bo", label="True
  prefs: []
  type: TYPE_NORMAL
- en: data")
  prefs: []
  type: TYPE_NORMAL
- en: ax1.plot(df.loc[~Y, "var1"], df.loc[~Y, "var3"], "rx", label="False
  prefs: []
  type: TYPE_NORMAL
- en: data")
  prefs: []
  type: TYPE_NORMAL
- en: ax1.legend()
  prefs: []
  type: TYPE_NORMAL
- en: ax1.set_xlabel("var1")
  prefs: []
  type: TYPE_NORMAL
- en: ax1.set_ylabel("var3")
  prefs: []
  type: TYPE_NORMAL
- en: ax1.set_title("Scatter plot of var3 against var1")
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: model = LogisticRegression()
  prefs: []
  type: TYPE_NORMAL
- en: model.fit(df, Y)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: test_df = pd.DataFrame({
  prefs: []
  type: TYPE_NORMAL
- en: '"var1": np.concatenate([rng.normal(3.0, 1.5, size=50),'
  prefs: []
  type: TYPE_NORMAL
- en: rng.normal(-4.0, 2.0, size=50)]),
  prefs: []
  type: TYPE_NORMAL
- en: '"var2": rng.uniform(size=100),'
  prefs: []
  type: TYPE_NORMAL
- en: '"var3": np.concatenate([rng.normal(-2.0, 2.0, size=50),'
  prefs: []
  type: TYPE_NORMAL
- en: rng.normal(1.5, 0.8, size=50)])
  prefs: []
  type: TYPE_NORMAL
- en: '})'
  prefs: []
  type: TYPE_NORMAL
- en: test_scores = 4.0 + test_df["var1"] - test_df["var3"]
  prefs: []
  type: TYPE_NORMAL
- en: test_Y = test_scores >= 0
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: test_predicts = model.predict(test_df)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: print(classification_report(test_Y, test_predicts))
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: precision     recall      f1-score      support
  prefs: []
  type: TYPE_NORMAL
- en: False       1.00        1.00         1.00          18
  prefs: []
  type: TYPE_NORMAL
- en: True       1.00        1.00         1.00          32
  prefs: []
  type: TYPE_NORMAL
- en: accuracy                                1.00          50
  prefs: []
  type: TYPE_NORMAL
- en: macro avg       1.00        1.00         1.00          50
  prefs: []
  type: TYPE_NORMAL
- en: weighted avg       1.00        1.00         1.00          50
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: from tsdata import generate_sample_data
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: sample_ts, _ = generate_sample_data()
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: ts_fig, ts_ax = plt.subplots()
  prefs: []
  type: TYPE_NORMAL
- en: sample_ts.plot(ax=ts_ax, label="Observed")
  prefs: []
  type: TYPE_NORMAL
- en: ts_ax.set_title("Time series data")
  prefs: []
  type: TYPE_NORMAL
- en: ts_ax.set_xlabel("Date")
  prefs: []
  type: TYPE_NORMAL
- en: ts_ax.set_ylabel("Value")
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: adf_results = sm.tsa.adfuller(sample_ts)
  prefs: []
  type: TYPE_NORMAL
- en: adf_pvalue = adf_results[1]
  prefs: []
  type: TYPE_NORMAL
- en: print("Augmented Dickey-Fuller test:\nP-value:", adf_pvalue)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: ap_fig, (acf_ax, pacf_ax) = plt.subplots(2, 1, sharex=True,
  prefs: []
  type: TYPE_NORMAL
- en: tight_layout=True)
  prefs: []
  type: TYPE_NORMAL
- en: sm.graphics.tsa.plot_acf(sample_ts, ax=acf_ax,
  prefs: []
  type: TYPE_NORMAL
- en: title="Observed autocorrelation")
  prefs: []
  type: TYPE_NORMAL
- en: sm.graphics.tsa.plot_pacf(sample_ts, ax=pacf_ax,
  prefs: []
  type: TYPE_NORMAL
- en: title="Observed partial autocorrelation")
  prefs: []
  type: TYPE_NORMAL
- en: pacf_ax.set_xlabel("Lags")
  prefs: []
  type: TYPE_NORMAL
- en: pacf_ax.set_ylabel("Value")
  prefs: []
  type: TYPE_NORMAL
- en: acf_ax.set_ylabel("Value")
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: ​arma_model = sm.tsa.ARMA(sample_ts, order=(1, 1))
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: arma_results = arma_model.fit()
  prefs: []
  type: TYPE_NORMAL
- en: print(arma_results.summary())
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: ARMA Model Results
  prefs: []
  type: TYPE_NORMAL
- en: ===================================================================
  prefs: []
  type: TYPE_NORMAL
- en: 'Dep. Variable: y           No. Observations: 366'
  prefs: []
  type: TYPE_NORMAL
- en: 'Model: ARMA(1, 1)          Log Likelihood -513.038'
  prefs: []
  type: TYPE_NORMAL
- en: 'Method: css-mle            S.D. of innovations 0.982'
  prefs: []
  type: TYPE_NORMAL
- en: 'Date: Fri, 01 May 2020     AIC 1034.077'
  prefs: []
  type: TYPE_NORMAL
- en: 'Time: 12:40:00             BIC 1049.687'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sample: 01-01-2020         HQIC 1040.280'
  prefs: []
  type: TYPE_NORMAL
- en: '- 12-31-2020'
  prefs: []
  type: TYPE_NORMAL
- en: ===================================================================
  prefs: []
  type: TYPE_NORMAL
- en: coef   std err     z     P>|z|    [0.025    0.975]
  prefs: []
  type: TYPE_NORMAL
- en: '-------------------------------------------------------------------'
  prefs: []
  type: TYPE_NORMAL
- en: const   -0.0242   0.143   -0.169    0.866   -0.305    0.256
  prefs: []
  type: TYPE_NORMAL
- en: ar.L1.y  0.8292   0.057    14.562   0.000    0.718    0.941
  prefs: []
  type: TYPE_NORMAL
- en: ma.L1.y -0.5189   0.090    -5.792   0.000   -0.695    -0.343
  prefs: []
  type: TYPE_NORMAL
- en: Roots
  prefs: []
  type: TYPE_NORMAL
- en: ===================================================================
  prefs: []
  type: TYPE_NORMAL
- en: Real       Imaginary       Modulus      Frequency
  prefs: []
  type: TYPE_NORMAL
- en: '-------------------------------------------------------------------'
  prefs: []
  type: TYPE_NORMAL
- en: AR.1    1.2059     +0.0000j        1.2059        0.0000
  prefs: []
  type: TYPE_NORMAL
- en: MA.1    1.9271     +0.0000j        1.9271        0.0000
  prefs: []
  type: TYPE_NORMAL
- en: '-------------------------------------------------------------------'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: residuals = arma_results.resid
  prefs: []
  type: TYPE_NORMAL
- en: rap_fig, (racf_ax, rpacf_ax) = plt.subplots(2, 1,
  prefs: []
  type: TYPE_NORMAL
- en: sharex=True, tight_layout=True)
  prefs: []
  type: TYPE_NORMAL
- en: sm.graphics.tsa.plot_acf(residuals, ax=racf_ax,
  prefs: []
  type: TYPE_NORMAL
- en: title="Residual autocorrelation")
  prefs: []
  type: TYPE_NORMAL
- en: sm.graphics.tsa.plot_pacf(residuals, ax=rpacf_ax,
  prefs: []
  type: TYPE_NORMAL
- en: title="Residual partial autocorrelation")
  prefs: []
  type: TYPE_NORMAL
- en: rpacf_ax.set_xlabel("Lags")
  prefs: []
  type: TYPE_NORMAL
- en: rpacf_ax.set_ylabel("Value")
  prefs: []
  type: TYPE_NORMAL
- en: racf_ax.set_ylabel("Value")
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: fitted = arma_results.fittedvalues
  prefs: []
  type: TYPE_NORMAL
- en: fitted.plot(c="r", ax=ts_ax, label="Fitted")
  prefs: []
  type: TYPE_NORMAL
- en: ts_ax.legend()
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: from tsdata import generate_sample_data
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: sample_ts, test_ts = generate_sample_data(trend=0.2, undiff=True)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: ts_fig, ts_ax = plt.subplots(tight_layout=True)
  prefs: []
  type: TYPE_NORMAL
- en: sample_ts.plot(ax=ts_ax, c="b", label="Observed")
  prefs: []
  type: TYPE_NORMAL
- en: ts_ax.set_title("Training time series data")
  prefs: []
  type: TYPE_NORMAL
- en: ts_ax.set_xlabel("Date")
  prefs: []
  type: TYPE_NORMAL
- en: ts_ax.set_ylabel("Value")
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: diffs = sample_ts.diff().dropna()
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: ap_fig, (acf_ax, pacf_ax) = plt.subplots(1, 2,
  prefs: []
  type: TYPE_NORMAL
- en: tight_layout=True, sharex=True)
  prefs: []
  type: TYPE_NORMAL
- en: sm.graphics.tsa.plot_acf(diffs, ax=acf_ax)
  prefs: []
  type: TYPE_NORMAL
- en: sm.graphics.tsa.plot_pacf(diffs, ax=pacf_ax)
  prefs: []
  type: TYPE_NORMAL
- en: acf_ax.set_ylabel("Value")
  prefs: []
  type: TYPE_NORMAL
- en: pacf_ax.set_xlabel("Lag")
  prefs: []
  type: TYPE_NORMAL
- en: pacf_ax.set_ylabel("Value")
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: model = sm.tsa.ARIMA(sample_ts, order=(1,1,1))
  prefs: []
  type: TYPE_NORMAL
- en: fitted = model.fit(trend="c")
  prefs: []
  type: TYPE_NORMAL
- en: print(fitted.summary())
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: ARIMA Model Results
  prefs: []
  type: TYPE_NORMAL
- en: ==================================================================
  prefs: []
  type: TYPE_NORMAL
- en: 'Dep. Variable: D.y             No. Observations: 365'
  prefs: []
  type: TYPE_NORMAL
- en: 'Model: ARIMA(1, 1, 1)          Log Likelihood -512.905'
  prefs: []
  type: TYPE_NORMAL
- en: 'Method: css-mle                S.D. of innovations 0.986'
  prefs: []
  type: TYPE_NORMAL
- en: 'Date: Sat, 02 May 2020         AIC 1033.810'
  prefs: []
  type: TYPE_NORMAL
- en: 'Time: 14:47:25                 BIC 1049.409'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sample: 01-02-2020             HQIC 1040.009'
  prefs: []
  type: TYPE_NORMAL
- en: '- 12-31-2020'
  prefs: []
  type: TYPE_NORMAL
- en: ==================================================================
  prefs: []
  type: TYPE_NORMAL
- en: coef     std err     z      P>|z|     [0.025    0.975]
  prefs: []
  type: TYPE_NORMAL
- en: '------------------------------------------------------------------'
  prefs: []
  type: TYPE_NORMAL
- en: const     0.9548    0.148     6.464    0.000     0.665    1.244
  prefs: []
  type: TYPE_NORMAL
- en: ar.L1.D.y 0.8342    0.056     14.992   0.000     0.725    0.943
  prefs: []
  type: TYPE_NORMAL
- en: ma.L1.D.y -0.5204   0.088    -5.903    0.000    -0.693   -0.348
  prefs: []
  type: TYPE_NORMAL
- en: Roots
  prefs: []
  type: TYPE_NORMAL
- en: ==================================================================
  prefs: []
  type: TYPE_NORMAL
- en: Real      Imaginary       Modulus        Frequency
  prefs: []
  type: TYPE_NORMAL
- en: '------------------------------------------------------------------'
  prefs: []
  type: TYPE_NORMAL
- en: AR.1      1.1987      +0.0000j       1.1987          0.0000
  prefs: []
  type: TYPE_NORMAL
- en: MA.1      1.9216      +0.0000j       1.9216          0.0000
  prefs: []
  type: TYPE_NORMAL
- en: '------------------------------------------------------------------'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: forecast, std_err, fc_ci = fitted.forecast(steps=50)
  prefs: []
  type: TYPE_NORMAL
- en: forecast_dates = pd.date_range("2021-01-01", periods=50)
  prefs: []
  type: TYPE_NORMAL
- en: forecast = pd.Series(forecast, index=forecast_dates)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: forecast.plot(ax=ts_ax, c="g", label="Forecast")
  prefs: []
  type: TYPE_NORMAL
- en: ts_ax.fill_between(forecast_dates, fc_ci[:, 0], fc_ci[:, 1],
  prefs: []
  type: TYPE_NORMAL
- en: color="r", alpha=0.4)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: test_ts.plot(ax=ts_ax, c="k", label="Actual")
  prefs: []
  type: TYPE_NORMAL
- en: ts_ax.legend()
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: from tsdata import generate_sample_data
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: sample_ts, test_ts = generate_sample_data(undiff=True,
  prefs: []
  type: TYPE_NORMAL
- en: seasonal=True)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: ts_fig, ts_ax = plt.subplots(tight_layout=True)
  prefs: []
  type: TYPE_NORMAL
- en: sample_ts.plot(ax=ts_ax, title="Time series", label="Observed")
  prefs: []
  type: TYPE_NORMAL
- en: ts_ax.set_xlabel("Date")
  prefs: []
  type: TYPE_NORMAL
- en: ts_ax.set_ylabel("Value")
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: ap_fig, (acf_ax, pacf_ax) = plt.subplots(2, 1,
  prefs: []
  type: TYPE_NORMAL
- en: sharex=True, tight_layout=True)
  prefs: []
  type: TYPE_NORMAL
- en: sm.graphics.tsa.plot_acf(sample_ts, ax=acf_ax)
  prefs: []
  type: TYPE_NORMAL
- en: sm.graphics.tsa.plot_pacf(sample_ts, ax=pacf_ax)
  prefs: []
  type: TYPE_NORMAL
- en: pacf_ax.set_xlabel("Lag")
  prefs: []
  type: TYPE_NORMAL
- en: acf_ax.set_ylabel("Value")
  prefs: []
  type: TYPE_NORMAL
- en: pacf_ax.set_ylabel("Value")
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: diffs = sample_ts.diff().dropna()
  prefs: []
  type: TYPE_NORMAL
- en: dap_fig, (dacf_ax, dpacf_ax) = plt.subplots(2, 1, sharex=True,
  prefs: []
  type: TYPE_NORMAL
- en: tight_layout=True)
  prefs: []
  type: TYPE_NORMAL
- en: sm.graphics.tsa.plot_acf(diffs, ax=dacf_ax,
  prefs: []
  type: TYPE_NORMAL
- en: title="Differenced ACF")
  prefs: []
  type: TYPE_NORMAL
- en: sm.graphics.tsa.plot_pacf(diffs, ax=dpacf_ax,
  prefs: []
  type: TYPE_NORMAL
- en: title="Differenced PACF")
  prefs: []
  type: TYPE_NORMAL
- en: dpacf_ax.set_xlabel("Lag")
  prefs: []
  type: TYPE_NORMAL
- en: dacf_ax.set_ylabel("Value")
  prefs: []
  type: TYPE_NORMAL
- en: dpacf_ax.set_ylabel("Value")
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: model = sm.tsa.SARIMAX(sample_ts, order=(1, 1, 1),
  prefs: []
  type: TYPE_NORMAL
- en: seasonal_order=(1, 0, 0, 7))
  prefs: []
  type: TYPE_NORMAL
- en: fitted_seasonal = model.fit()
  prefs: []
  type: TYPE_NORMAL
- en: print(fitted_seasonal.summary())
  prefs: []
  type: TYPE_NORMAL
- en: fitted_seasonal.fittedvalues.plot(ax=ts_ax, c="r",
  prefs: []
  type: TYPE_NORMAL
- en: label="Predicted")
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: SARIMAX Results
  prefs: []
  type: TYPE_NORMAL
- en: ===================================================================
  prefs: []
  type: TYPE_NORMAL
- en: 'Dep. Variable: y                      No. Observations: 366'
  prefs: []
  type: TYPE_NORMAL
- en: 'Model: SARIMAX(1, 1, 1)x(1, 0, [], 7) Log Likelihood -509.941'
  prefs: []
  type: TYPE_NORMAL
- en: 'Date: Mon, 04 May 2020                AIC 1027.881'
  prefs: []
  type: TYPE_NORMAL
- en: 'Time: 18:03:27                        BIC 1043.481'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sample: 01-01-2020                    HQIC 1034.081'
  prefs: []
  type: TYPE_NORMAL
- en: '- 12-31-2020'
  prefs: []
  type: TYPE_NORMAL
- en: 'Covariance Type:                      opg'
  prefs: []
  type: TYPE_NORMAL
- en: ===================================================================
  prefs: []
  type: TYPE_NORMAL
- en: coef     std err     z       P>|z|      [0.025     0.975]
  prefs: []
  type: TYPE_NORMAL
- en: '-------------------------------------------------------------------'
  prefs: []
  type: TYPE_NORMAL
- en: ar.L1   0.7939    0.065     12.136    0.000      0.666     0.922
  prefs: []
  type: TYPE_NORMAL
- en: ma.L1   -0.4544   0.095    -4.793     0.000     -0.640    -0.269
  prefs: []
  type: TYPE_NORMAL
- en: ar.S.L7  0.7764   0.034     22.951    0.000      0.710     0.843
  prefs: []
  type: TYPE_NORMAL
- en: sigma2   0.9388   0.073     12.783    0.000      0.795     1.083
  prefs: []
  type: TYPE_NORMAL
- en: ===================================================================
  prefs: []
  type: TYPE_NORMAL
- en: 'Ljung-Box (Q): 31.89                Jarque-Bera (JB): 0.47'
  prefs: []
  type: TYPE_NORMAL
- en: 'Prob(Q): 0.82                       Prob(JB): 0.79'
  prefs: []
  type: TYPE_NORMAL
- en: 'Heteroskedasticity (H): 1.15        Skew: -0.03'
  prefs: []
  type: TYPE_NORMAL
- en: 'Prob(H) (two-sided): 0.43           Kurtosis: 2.84'
  prefs: []
  type: TYPE_NORMAL
- en: ===================================================================
  prefs: []
  type: TYPE_NORMAL
- en: 'Warnings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[1] Covariance matrix calculated using the outer product'
  prefs: []
  type: TYPE_NORMAL
- en: of gradients (complex-step).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: forecast_result = fitted_seasonal.get_forecast(steps=50)
  prefs: []
  type: TYPE_NORMAL
- en: forecast_index = pd.date_range("2021-01-01", periods=50)
  prefs: []
  type: TYPE_NORMAL
- en: forecast = forecast_result.predicted_mean
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: forecast.plot(ax=ts_ax, c="g", label="Forecasts")
  prefs: []
  type: TYPE_NORMAL
- en: conf = forecast_result.conf_int()
  prefs: []
  type: TYPE_NORMAL
- en: ts_ax.fill_between(forecast_index, conf["lower y"],
  prefs: []
  type: TYPE_NORMAL
- en: conf["upper y"], color="r", alpha=0.4)
  prefs: []
  type: TYPE_NORMAL
- en: test_ts.plot(ax=ts_ax, color="k", label="Actual future")
  prefs: []
  type: TYPE_NORMAL
- en: ts_ax.legend()
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: from fbprophet import Prophet
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: from tsdata import generate_sample_data
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: sample_ts, test_ts = generate_sample_data(undiff=True, trend=0.2)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: df_for_prophet = pd.DataFrame({
  prefs: []
  type: TYPE_NORMAL
- en: '"ds": sample_ts.index,   # dates'
  prefs: []
  type: TYPE_NORMAL
- en: '"y": sample_ts.values    # values'
  prefs: []
  type: TYPE_NORMAL
- en: '})'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: model = Prophet()
  prefs: []
  type: TYPE_NORMAL
- en: model.fit(df_for_prophet)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: forecast_df = model.make_future_dataframe(periods=50)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: forecast = model.predict(forecast_df)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: fig, ax = plt.subplots(tight_layout=True)
  prefs: []
  type: TYPE_NORMAL
- en: sample_ts.plot(ax=ax, label="Observed", title="Forecasts")
  prefs: []
  type: TYPE_NORMAL
- en: forecast.plot(x="ds", y="yhat", ax=ax, c="r",
  prefs: []
  type: TYPE_NORMAL
- en: label="Predicted")
  prefs: []
  type: TYPE_NORMAL
- en: ax.fill_between(forecast["ds"].values, forecast["yhat_lower"].values,
  prefs: []
  type: TYPE_NORMAL
- en: forecast["yhat_upper"].values, color="r", alpha=0.4)
  prefs: []
  type: TYPE_NORMAL
- en: test_ts.plot(ax=ax, c="k", label="Future")
  prefs: []
  type: TYPE_NORMAL
- en: ax.legend()
  prefs: []
  type: TYPE_NORMAL
- en: ax.set_xlabel("Date")
  prefs: []
  type: TYPE_NORMAL
- en: ax.set_ylabel("Value")
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]**'
  prefs: []
  type: TYPE_NORMAL
