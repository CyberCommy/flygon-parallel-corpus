- en: 'Chapter 6. Next Generation Networking Stack for Docker: libnetwork'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will learn about a new networking stack for Docker: libnetwork,
    which provides a pluggable architecture with a default implementation for single
    and multi-host virtual networking:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goal
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Design
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CNM objects
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CNM attributes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CNM lifecycle
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Drivers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bridge driver
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overlay network driver
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using overlay network with Vagrant
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overlay network with Docker Machine and Docker Swarm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating an overlay network manually and using it for containers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Container network interface
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calico's libnetwork driver
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goal
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'libnetwork which is written in go language is a new way for connecting Docker
    containers. The aim is to provide a container network model that helps programmers
    and provides the abstraction of network libraries. The long-term goal of libnetwork
    is to follow the Docker and Linux philosophy to deliver modules that work independently.
    libnetwork has the aim to provide a composable need for networking in containers.
    It also aims to modularize the networking logic in Docker Engine and libcontainer
    into a single, reusable library by:'
  prefs: []
  type: TYPE_NORMAL
- en: Replacing the networking module of Docker Engine with libnetwork
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Being a model that allows local and remote drivers to provide networking to
    containers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Providing a tool dnet for managing and testing libnetwork—still a work in progress
    (reference from [https://github.com/docker/libnetwork/issues/45](https://github.com/docker/libnetwork/issues/45)).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Design
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: libnetwork implements a **container network model** (**CNM**). It formalizes
    the steps required to provide networking for containers, while providing an abstraction
    that can be used to support multiple network drivers. Its endpoint APIs are primarily
    used for managing the corresponding object and book-keeps them in order to provide
    a level of abstraction as required by the CNM model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The CNM is built on three main components. The following figure shows the network
    sandbox model of libnetwork:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Design](../images/00046.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: CNM objects
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's discuss the CNM objects in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Sandbox
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This contains the configuration of a container's network stack, which includes
    management of routing tables, the container's interface, and DNS settings. An
    implementation of a sandbox can be a Linux network namespace, a FreeBSD jail,
    or other similar concept. A sandbox may contain many endpoints from multiple networks.
    It also represents a container's network configuration such as IP-address, MAC
    address, and DNS entries. libnetwork makes use of the OS-specific parameters to
    populate the network configuration represented by sandbox. libnetwork provides
    a framework to implement sandbox in multiple operating systems. Netlink is used
    to manage the routing table in namespace, and currently two implementations of
    sandbox exist, `namespace_linux.go` and `configure_linux.go`, to uniquely identify
    the path on the host filesystem.
  prefs: []
  type: TYPE_NORMAL
- en: 'A sandbox is associated with a single Docker container. The following data
    structure shows the runtime elements of a sandbox:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'A new sandbox is instantiated from a network controller (which is explained
    in more detail later):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Endpoint
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'An endpoint joins a sandbox to the network and provides connectivity for services
    exposed by a container to the other containers deployed in the same network. It
    can be an internal port of Open vSwitch or a similar veth pair. An endpoint can
    belong to only one network but may only belong to one sandbox. An endpoint represents
    a service and provides various APIs to create and manage the endpoint. It has
    a global scope but gets attached to only one network, as shown in the following
    figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Endpoint](../images/00047.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'An endpoint is specified by the following data structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: An endpoint is associated with a unique ID and name. It is attached to a network
    and a sandbox ID. It is also associated with an IPv4 and IPv6 address space. Each
    endpoint is associated with an `endpointInterface` struct.
  prefs: []
  type: TYPE_NORMAL
- en: Network
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A network is a group of endpoints that are able to communicate with each other
    directly. It provides the required connectivity within the same host or multiple
    hosts, and whenever a network is created or updated, the corresponding driver
    is notified. An example is a VLAN or Linux bridge, which has a global scope within
    a cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Networks are controlled from a network controller, which we will discuss in
    the next section. Every network has a name, address space, ID, and network type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Network controller
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A network controller object provides APIs to create and manage a network object.
    It is an entry point in the libnetwork by binding a particular driver to a given
    network, and it supports multiple active drivers, both in-built and remote. Network
    controller allows users to bind a particular driver to a given network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Each network controller has reference to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: One or more drivers in the data structure driverTable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One or more sandboxes in the data structure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DataStore
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ipamTable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following figure shows how **Network Controller** sits between the **Docker
    Engine** and the containers and networks they are attached to:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Network controller](../images/00048.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: CNM attributes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are two types of attributes, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Options**: They are not end-user visible but are the key-value pairs of data
    to provide a flexible mechanism to pass driver-specific configuration from user
    to driver directly. libnetwork operates on the options only if the key matches
    a well-known label as a result value is picked up, which is represented by a generic
    object.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Labels**: They are a subset of options that are end-user variables represented
    in the UI using the `–labels` option. Their main function is to perform driver-specific
    operations and they are passed from the UI.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CNM lifecycle
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Consumers of the container network model interact through the CNM objects and
    its APIs to network the containers that they manage.
  prefs: []
  type: TYPE_NORMAL
- en: Drivers register with network controller. Built-in drivers register inside of
    libnetwork, while remote drivers register with libnetwork via a plugin mechanism
    (WIP). Each driver handles a particular network type.
  prefs: []
  type: TYPE_NORMAL
- en: A network controller object is created using the `libnetwork.New()` API to manage
    the allocation of networks and optionally configure a driver with driver-specific
    options.
  prefs: []
  type: TYPE_NORMAL
- en: The network is created using the controller's `NewNetwork()` API by providing
    a name and `networkType`. The `networkType` parameter helps to choose a corresponding
    driver and binds the created network to that driver. From this point, any operation
    on the network will be handled by that driver.
  prefs: []
  type: TYPE_NORMAL
- en: The `controller.NewNetwork()` API also takes in optional options parameters
    that carry driver-specific options and labels, which the drivers can make use
    for its purpose.
  prefs: []
  type: TYPE_NORMAL
- en: '`network.CreateEndpoint()` can be called to create a new endpoint in a given
    network. This API also accepts optional options parameters that vary with the
    driver.'
  prefs: []
  type: TYPE_NORMAL
- en: Drivers will be called with `driver.CreateEndpoint` and it can choose to reserve
    IPv4/IPv6 addresses when an endpoint is created in a network. The driver will
    assign these addresses using the `InterfaceInfo` interface defined in the `driver`
    API. The IPv4/IPv6 addresses are needed to complete the endpoint as a service
    definition along with the ports the endpoint exposes. A service endpoint is a
    network address and the port number that the application container is listening
    on.
  prefs: []
  type: TYPE_NORMAL
- en: '`endpoint.Join()` can be used to attach a container to an endpoint. The `Join`
    operation will create a sandbox if it doesn''t exist for that container. The drivers
    make use of the sandbox key to identify multiple endpoints attached to the same
    container.'
  prefs: []
  type: TYPE_NORMAL
- en: There is a separate API to create an endpoint and another to join the endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: An endpoint represents a service that is independent of the container. When
    an endpoint is created, it has resources reserved for the container to get attached
    to the endpoint later. It gives a consistent networking behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '`endpoint.Leave()` is invoked when a container is stopped. The driver can clean
    up the states that it allocated during the `Join()` call. libnetwork will delete
    the sandbox when the last referencing endpoint leaves the network.'
  prefs: []
  type: TYPE_NORMAL
- en: libnetwork keeps holding on to IP addresses as long as the endpoint is still
    present. These will be reused when the container (or any container) joins again.
    It ensures that the container's resources are re-used when they are stopped and
    started again.
  prefs: []
  type: TYPE_NORMAL
- en: '`endpoint.Delete()` is used to delete an endpoint from a network. This results
    in deleting the endpoint and cleaning up the cached `sandbox.Info`.'
  prefs: []
  type: TYPE_NORMAL
- en: '`network.Delete()` is used to delete a network. Delete is allowed if there
    are no endpoints attached to the network.'
  prefs: []
  type: TYPE_NORMAL
- en: Driver
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A driver owns a network and is responsible for making the network work and manages
    it. Network controller provides an API to configure the driver with specific labels/options
    that are not directly visible to the user but are transparent to libnetwork and
    can be handled by drivers directly. Drivers can be both in-built (such as bridge,
    host, or overlay) and remote (from plugin providers) to be deployed in various
    use cases and deployment scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: 'The driver owns the network implementation and is responsible for managing
    it, including **IP Address Management (IPAM)**. The following figure explains
    the process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Driver](../images/00049.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following are the in-built drivers:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Null**: In order to provide backward compatibility with old `docker --net=none`,
    this option exists primarily in the case when no networking is required.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bridge**: It provides a Linux-specific bridging implementation driver.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Overlay**: The overlay driver implements networking that can span multiple
    hosts network encapsulation such as VXLAN. We will be doing a deep-dive on two
    of its implementations: basic setup with Consul and Vagrant setup to deploy the
    overlay driver.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Remote**: It provides a means of supporting drivers over a remote transport
    and a specific driver can be written as per choice.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bridge driver
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A bridge driver represents a wrapper on a Linux bridge acting as a network
    for libcontainer. It creates a veth pair for each network created. One end is
    connected to the container and the other end is connected to the bridge. The following
    data structure represents a bridge network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Some of the actions performed in a bridge driver:'
  prefs: []
  type: TYPE_NORMAL
- en: Configuring IPTables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Managing IP forwarding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Managing Port Mapping
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enabling Bridge Net Filtering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up IPv4 and IPv6 on the bridge
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following diagram shows how the network is represented using `docker0`
    and `veth` pairs to connect endpoints with the `docker0` bridge:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Bridge driver](../images/00050.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Overlay network driver
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Overlay network in libnetwork uses VXLan along with a Linux bridge to create
    an overlaid address space. It supports multi-host networking:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Using overlay network with Vagrant
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Overlay network is created between two containers, and VXLan tunnel connects
    the containers through a bridge.
  prefs: []
  type: TYPE_NORMAL
- en: Overlay network deployment Vagrant setup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This setup has been deployed using the Docker experimental version, which keeps
    on updating regularly and might not support some of the features:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Clone the official libnetwork repository and switch to the `docs` folder:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The Vagrant script pre-exists in the repository; we will deploy the three-node
    setup for our Docker overlay network driver testing by using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We can list the deployed machine by Vagrant as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The setup is complete thanks to the Vagrant script; now, we can SSH to the
    Docker hosts and start the testing containers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We can create a new Docker container, and inside the container we can list
    the contents of the `/etc/hosts` file in order to verify that it has the overlay
    bridge specification, which was previously deployed, and it automatically connects
    to it on the launch:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Similarly, we can create the Docker container in the other host `net-2` as well
    and can verify the working of the overlay network driver as both the containers
    will be able to ping each other in spite of being deployed on different hosts.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the previous example, we started the Docker container with the default options
    and they got automatically added to a multi-host network of type overlay.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also creat a separate overlay bridge and add containers to it manually
    using the `--publish-service` option, which is part of Docker experimental:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The second host will also see this network and we can create containers added
    to the overlay network in both of these hosts by using the following option in
    the Docker command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: We will be able to verify the working of the overlay driver as both the containers
    will be able to ping each other. Also, tools such as tcpdump, wireshark, smartsniff,
    and so on can be used to capture the vXLAN package.
  prefs: []
  type: TYPE_NORMAL
- en: Overlay network with Docker Machine and Docker Swarm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This section explains the basics of creating a multi-host network. The Docker
    Engine supports multi-host networking through the overlay network driver. Overlay
    drivers need the following pre-requisites to work:'
  prefs: []
  type: TYPE_NORMAL
- en: 3.16 Linux kernel or higher
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Access to a key-value store
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Docker supports the following key-value stores: Consul, etcd, and ZooKeeper'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A cluster of hosts connected to the key-value store
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Docker Engine daemon on each host in the cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This example uses Docker Machine and Docker Swarm to create the multi-network
    host.
  prefs: []
  type: TYPE_NORMAL
- en: Docker Machine is used to create the key-value store server and the cluster.
    The cluster created is a Docker Swarm cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram explains how three VMs are set up using Docker Machine:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Overlay network with Docker Machine and Docker Swarm](../images/00051.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Prerequisites
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Vagrant
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Docker Engine
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Docker Machine
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Docker Swarm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Key-value store installation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An overlay network requires a key-value store. The key-value store stores information
    about the network state such as discovery, networks, endpoints, IP addresses,
    and so on. Docker supports various key-value stores such as Consul, etcd, and
    Zoo Keeper. This section has been implemented using Consul.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the steps to install key-value store:'
  prefs: []
  type: TYPE_NORMAL
- en: Provision a VirtualBox virtual machine called `mh-keystore`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'When a new VM is provisioned, the process adds the Docker Engine to the host.
    Consul instance will be using the consul image from the Docker Hub account ([https://hub.docker.com/r/progrium/consul/](https://hub.docker.com/r/progrium/consul/)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Start the `progrium/consul` container created previously running on the `mh-keystore`
    virtual machine:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: A bash expansion `$(docker-machine config mh-keystore)` is used to pass the
    connection configuration to the Docker `run` command. The client starts a program
    from the `progrium/consul` image running in the `mh-keystore` machine. The container
    is called `consul` (flag `–h`) and is listening on port `8500` (you can choose
    any other port as well).
  prefs: []
  type: TYPE_NORMAL
- en: 'Set the local environment to the `mh-keystore` virtual machine:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Execute the `docker ps` command to make sure the Consul container is up:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Create a Swarm cluster with two nodes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this step, we will use Docker Machine to provision two hosts for your network.
    We will create two virtual machines in VirtualBox. One of the machines will be
    Swarm master, which will be created first.
  prefs: []
  type: TYPE_NORMAL
- en: 'As each host is created, options for the overlay network driver will be passed
    to the Docker Engine using Swarm using the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a Swarm master virtual machine `mhs-demo0`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: At creation time, you supply the engine daemon with the `--cluster-store` option.
    This option tells the engine the location of the key-value store for the overlay
    network. The bash expansion `$(docker-machine ip mh-keystore)` resolves to the
    IP address of the Consul server you created in step 1 of the preceding section.
    The `--cluster-advertise` option advertises the machine on the network.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create another virtual machine `mhs-demo1` and add it to the Docker Swarm cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'List virtual machines using Docker Machine to confirm that they are all up
    and running:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: At this point, virtual machines are running. We are ready to create a multi-host
    network for containers using these virtual machines.
  prefs: []
  type: TYPE_NORMAL
- en: Creating an overlay network
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following command is used to create an overlay network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'We will only need to create the network on a single host in the Swarm cluster.
    We used the Swarm master but this command can run on any host in the Swarm cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Check that the overlay network is running using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Since we are using the Swarm master environment, we are able to see all the
    networks on all the Swarm agents: the default networks on each engine and the
    single overlay network. In this case, there are two engines running on `mhs-demo0`
    and `mhs-demo1`.'
  prefs: []
  type: TYPE_NORMAL
- en: Each `NETWORK ID` is unique.
  prefs: []
  type: TYPE_NORMAL
- en: 'Switch to each Swarm agent in turn and list the networks:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Both agents report they have the `my-net` network with the overlay driver. We
    have a multi-host overlay network running.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure shows how two containers will have containers created
    and tied together using the overlay `my-net`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Creating an overlay network](../images/00052.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Creating containers using an overlay network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are the steps for creating containers using an overlay network:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a container `c0` on `mhs-demo0` and connect to the `my-net` network:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Execute `ifconfig` to find the IPaddress of `c0`. In this case, it is `10.0.0.4`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a container, `c1` on `mhs-demo1,` and connect to the `my-net` network:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Execute `ifconfig` to find the IP address of `c1`. In this case, it is `10.0.0.3`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Ping `c1` (`10.0.0.3`) from `c0` (`10.0.0.4`) and vice versa:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Container network interface
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Container network interface** (**CNI**) is a specification that defines how
    executable plugins can be used to configure network interfaces for Linux application
    containers. The official GitHub repository of CNI explains how a go library explains
    the implementing specification.'
  prefs: []
  type: TYPE_NORMAL
- en: The container runtime first creates a new network namespace for the container
    in which it determines which network this container should belong to and which
    plugins to be executed. The network configuration is in the JSON format and defines
    on the container startup which plugin should be executed for the network. CNI
    is actually an evolving open source technology that is derived from the rkt networking
    protocol. Each CNI plugin is implemented as an executable and is invoked by a
    container management system, docker, or rkt.
  prefs: []
  type: TYPE_NORMAL
- en: After inserting the container in the network namespace, namely by attaching
    one end of a veth pair to a container and attaching the other end to a bridge,
    it then assigns an IP to the interface and sets up routes consistent with IP address
    management by invoking an appropriate IPAM plugin.
  prefs: []
  type: TYPE_NORMAL
- en: The CNI model is currently used for the networking of kubelets in the Kubernetes
    model. Kubelets are the most important components of Kubernetes nodes, which takes
    the load of running containers on top of them.
  prefs: []
  type: TYPE_NORMAL
- en: 'The package CNI for kubelet is defined in the following Kubernetes package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The following figure shows the CNI placement:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Container network interface](../images/00053.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: CNI plugin
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As per the official GitHub repository ([https://github.com/appc/cni](https://github.com/appc/cni)),
    the parameters that the CNI plugin need in order to add a container to the network
    are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Version**: The version of CNI spec that the caller is using (container call
    invoking the plugin).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Container ID**: This is optional, but recommended, and defines that there
    should be a unique ID across an administrative domain while the container is live.
    For example, the IPAM system may require that each container is allocated a unique
    ID so that it can be correlated properly to a container running in the background.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Network namespace path**: This represents the path to the network namespace
    to be added, for example, `/proc/[pid]/ns/net` or a `bind-mount/link` to it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Network configuration**: It is the JSON document that describes a network
    to which a container can be joined and is explained in the following section.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Extra arguments**: It allows granular configuration of CNI plugins on a per-container
    basis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Name of the interface inside the container**: It is the name that gets assigned
    to the container and complies with Linux restriction, which exists for interface
    names.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The results achieved are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**IPs assigned to the interface**: This is either an IPv4 address or an IPv6
    address assigned to the network as per requirements.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**List of DNS nameservers**: This is a priority-ordered address list of DNS
    name servers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Network configuration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The network configuration is in the JSON format that can be stored on disk
    or generated from other sources by container runtime. The following fields in
    the JSON have importance, as explained in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**cniVersion (string)**: It is Semantic Version 2.0 of the CNI specification
    to which this configuration meets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**name (string)**: It is the network name. It is unique across all containers
    on the host (or other administrative domain).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**type (string)**: Refers to the filename of the CNI plugin executable.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ipMasq (boolean)**: Optional, sets up an IP masquerade on the host as it
    is necessary for the host to act as a gateway to subnets that are not able to
    route to the IP assigned to the container.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ipam**: Dictionary with IPAM-specific values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**type (string)**: Refers to the filename of the IPAM plugin executable.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**routes (list)**: List of subnets (in CIDR notation) that the CNI plugin should
    make sure are reachable by routing through the network. Each entry is a dictionary
    containing:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**dst (string)**: A subnet in CIDR notation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**gw (string)**: It is the IP address of the gateway to use. If not specified,
    the default gateway for the subnet is assumed (as determined by the IPAM plugin).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'An example configuration for plugin-specific OVS is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: IP allocation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The CNI plugin assigns an IP address to the interface and installs necessary
    routes for the interface, thus it provides great flexibility for the CNI plugin
    and many CNI plugins internally have the same code to support several IP management
    schemes.
  prefs: []
  type: TYPE_NORMAL
- en: To lessen the burden on the CNI plugin, a second type of plugin, **IP address
    management plugin** (**IPAM**), is defined, which determines the interface IP/subnet,
    gateway, and routes and returns this information to the main plugin to apply.
    The IPAM plugin obtains information via a protocol, `ipam` section defined in
    the network configuration file, or data stored on the local filesystem.
  prefs: []
  type: TYPE_NORMAL
- en: IP address management interface
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The IPAM plugin is invoked by running an executable, which is searched in a
    predefined path and is indicated by a CNI plugin via `CNI_PATH`. The IPAM plugin
    receives all the system environment variables from this executable, which are
    passed to the CNI plugin.
  prefs: []
  type: TYPE_NORMAL
- en: 'IPAM receives a network configuration file via stdin. Success is indicated
    by a zero return code and the following JSON, which gets printed to stdout (in
    the case of the `ADD` command):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is an example of running Docker networking with CNI:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, install Go Lang 1.4+ and jq (command line JSON processor) to build the
    CNI plugins:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Clone the official CNI GitHub repository:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now create a `netconf` file in order to describe the network:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Build the CNI plugins:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we will execute the `priv-net-run.sh` script in order to create the private
    network with the CNI plugin:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Run a Docker container with the network namespace, which was set up previously
    using the CNI plugin:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Project Calico's libnetwork driver
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Calico provides a scalable networking solution for connecting containers, VMs,
    or bare metal. Calico provides connectivity using the scalable IP networking principle
    as a layer 3 approach. Calico can be deployed without overlays or encapsulation.
    The Calico service should be deployed as a container on each node and provides
    each container with its own IP address. It also handles all the necessary IP routing,
    security policy rules, and distribution of routes across a cluster of nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Calico architecture contains four important components in order to provide
    a better networking solution:'
  prefs: []
  type: TYPE_NORMAL
- en: Felix, the Calico worker process, is the heart of Calico networking, which primarily
    routes and provides desired connectivity to and from the workloads on host. It
    also provides the interface to kernels for outgoing endpoint traffic.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: BIRD, the route distribution open source BGP, exchanges routing information
    between hosts. The kernel endpoints, which are picked up by BIRD, are distributed
    to BGP peers in order to provide inter-host routing. Two BIRD processes run in
    the calico-node container, IPv4 (bird) and one for IPv6 (bird6).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Confd, a templating process to auto-generate configuration for BIRD, monitors
    the etcd store for any changes to BGP configuration such as log levels and IPAM
    information. Confd also dynamically generates BIRD configuration files based on
    data from etcd and triggers automatically as updates are applied to data. Confd
    triggers BIRD to load new files whenever a configuration file is changed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'calicoctl, the command line used to configure and start the Calico service,
    even allows the datastore (etcd) to define and apply security policy. The tool
    also provides the simple interface for general management of Calico configuration
    irrespective of whether Calico is running on VMs, containers, or bare metal. The
    following commands are supported at calicoctl:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'As per the official GitHub page of the Calico repository ([https://github.com/projectcalico/calico-containers](https://github.com/projectcalico/calico-containers)),
    the following integration of Calico exists:'
  prefs: []
  type: TYPE_NORMAL
- en: Calico as a Docker network plugin
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calico without Docker networking
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calico with Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calico with Mesos
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calico with Docker Swarm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following figure shows the Calico architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Project Calico''s libnetwork driver](../images/00054.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'In the following tutorial we will run the manual set up of Calico on a single
    node machine with Docker 1.9, which finally brings libnetwork out of its experimental
    version to main release, and Calico can be configured directly without the need
    of other Docker experimental versions:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Get the etcd latest release and configure it on the default port 2379:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Open the new terminal and configure the Docker daemon with the etcd key-value
    store by running the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, in the new terminal, start the Calico container in the following way:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the Calico bridge using the `docker network` command recently introduced
    in the Docker CLI:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Start the `busybox` container connected to the Calico `net1` bridge:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Inside the container we can see that the container is now connected to the Calico
    bridge and can connect to the other containers deployed on the same bridge.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we looked into some of the deeper and more conceptual aspects
    of Docker networking, one of them being libnetworking, the future Docker network
    model that is already getting into shape with the release of Docker 1.9\. While
    explaining libnetworking, we also studied the CNM model and its various objects
    and components with its implementation code snippets. Next, we looked into drivers
    of CNM, the prime one being the overlay driver, in detail, with deployment as
    part of the Vagrant setup. We also looked at the stand-alone integration of containers
    with the overlay network and as well with Docker Swarm and Docker Machine. In
    the next section, we explained about the CNI interface, its executable plugins,
    and a tutorial of configuring Docker networking with the CNI plugin.
  prefs: []
  type: TYPE_NORMAL
- en: In the last section, project Calico is explained in detail, which provides a
    scalable networking solution based out of libnetwork and provides integration
    with Docker, Kubernetes, Mesos, bare-metal, and VMs primarily.
  prefs: []
  type: TYPE_NORMAL
