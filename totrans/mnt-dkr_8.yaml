- en: Chapter 8. What Are the Next Steps?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this final chapter, we will look at the next steps you can take to monitor
    your containers, by talking about the benefits of adding alerts to your monitoring.
    Also, we will cover some different scenarios and also which type of monitoring
    is appropriate for each of them:'
  prefs: []
  type: TYPE_NORMAL
- en: Common problems (performance, availability, and so on) and which type of monitoring
    is best for your situation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are the benefits of alerting on the metrics you are collecting and what
    are the options?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some scenarios
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To look at which type of monitoring you might want to implement for your container-based
    applications, we should work through a few different example configurations that
    your container-based applications could be deploying into. First, let's remind
    ourselves about Pets, Cattle, Chickens, and Snowflakes.
  prefs: []
  type: TYPE_NORMAL
- en: Pets, Cattle, Chickens, and Snowflakes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Back in the [Chapter 1](part0014_split_000.html#DB7S1-fcf7b4d102f841bba77b823d677470e0
    "Chapter 1. Introduction to Docker Monitoring"), *Introduction to Docker Monitoring*,
    we spoke about Pets, Cattle, Chickens, and Snowflakes; in that chapter, we described
    what each term meant when it was applied to modern cloud deployments. Here, we
    will go into a little more detail about how the terms can be applied to your containers.
  prefs: []
  type: TYPE_NORMAL
- en: Pets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For your containers to be considered a Pet, you will be more than likely to
    be running either a single or a small number of fixed containers on a designated
    host.
  prefs: []
  type: TYPE_NORMAL
- en: Each one of these containers could be considered a single point of failure;
    if any one of them goes down, it will more than likely result in errors for your
    application. Worst still, if the host machine goes down for any reason, your entire
    application will be offline.
  prefs: []
  type: TYPE_NORMAL
- en: This is a typical deployment method for most of our first steps with Docker,
    and in no way should it be considered bad, frowned upon, or not recommend; as
    long as you are aware of the limitations, you will be fine.
  prefs: []
  type: TYPE_NORMAL
- en: This pattern can also be used to describe most development environments, as
    you are constantly reviewing its health and tuning as needed.
  prefs: []
  type: TYPE_NORMAL
- en: You will more than likely be hosting the machine on your local computer or on
    a hosting service such as DigitalOcean ([https://www.digitalocean.com/](https://www.digitalocean.com/)).
  prefs: []
  type: TYPE_NORMAL
- en: Cattle
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For the bulk of production or business critical deployments, you should aim
    to launch your containers in a configuration that allows them to automatically
    recover themselves after a failure, or, when more capacity is needed, additional
    containers are launched and then terminated when the scaling event is over.
  prefs: []
  type: TYPE_NORMAL
- en: 'You will more than likely be using a public cloud-based service as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Amazon EC2 Container Service: [https://aws.amazon.com/ecs/](https://aws.amazon.com/ecs/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Google Container Engine: [https://cloud.google.com/container-engine/](https://cloud.google.com/container-engine/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Joyent Triton: [https://www.joyent.com/blog/understanding-triton-containers/](https://www.joyent.com/blog/understanding-triton-containers/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Alternatively, you will be hosting on your own servers using a Docker-friendly
    and cluster-aware operating system as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'CoreOS: [https://coreos.com/](https://coreos.com/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'RancherOS: [http://rancher.com/rancher-os/](http://rancher.com/rancher-os/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You won't care so much as to where a container is launched within your cluster
    of hosts, as long as you can route traffic to it. To add more capacity to the
    cluster, you will be bringing up additional hosts when needed and removing them
    from the cluster when not needed in order to save on costs.
  prefs: []
  type: TYPE_NORMAL
- en: Chickens
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Its more than likely you will be using containers to launch, process data,
    and then terminate. This can happen anytime from once a day to several times a
    minute. You will be using a distributed scheduler as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Kubernetes by Google: [http://kubernetes.io/](http://kubernetes.io/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Apache Mesos: [http://mesos.apache.org/](http://mesos.apache.org/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Because of this, you will have a large number of containers launching and terminating
    within your cluster; you definitely won't care about where a container is launched
    or even how traffic is routed to it, as long as your data is processed correctly
    and passed back to your application.
  prefs: []
  type: TYPE_NORMAL
- en: Like the cluster described in the *Cattle* section's description, hosts will
    be added and removed automatically, probably in response to scheduled peaks such
    as end of month reporting or seasonal sales and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Snowflakes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: I hope one of the things you took away from the first chapter is that if you
    have any servers or services that you consider being Snowflakes, then you should
    do something to retire them as soon as possible.
  prefs: []
  type: TYPE_NORMAL
- en: Luckily, due to the way the containerizing of your applications works, you should
    never be able to create a snowflake using Docker, as your containerized environment
    should always be reproducible, either because you have the Docker file (everyone
    makes backups right?) or you have a working copy of the container image because
    you have exported the container as a whole using the built-in tools.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Sometimes it may not be possible to create a container using a Docker file.
    Instead, you can backup or migrate your containers by using the export command.
    For more information on exporting your containers, see the following URL:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://docs.docker.com/reference/commandline/export/](https://docs.docker.com/reference/commandline/export/)'
  prefs: []
  type: TYPE_NORMAL
- en: If you find yourself in this position, let me be the first to congratulate you
    on mitigating a future disaster by promoting your Snowflake into a Pet or even
    Cattle ahead of any problems.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Still running a Snowflake?**'
  prefs: []
  type: TYPE_NORMAL
- en: If you find yourself still running a Snowflake server or service, I cannot stress
    enough that you look at documenting, migrating, or updating the Snowflake as soon
    as possible. There is no point in monitoring a service that may be impossible
    for you to recover. Remember that there are containers for old technologies, such
    as PHP4, if you really need to run them.
  prefs: []
  type: TYPE_NORMAL
- en: Scenario one
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You are running a personal WordPress website using the official containers from
    the Docker Hub; the containers have been launched using a Docker Compose file
    like the one we have used several times throughout this book.
  prefs: []
  type: TYPE_NORMAL
- en: You have the Docker Compose file stored in a GitHub repository and you can take
    snapshots of the host machine as a backup. As it's your own blog, you are fine
    running it on a single cloud-based host.
  prefs: []
  type: TYPE_NORMAL
- en: 'A suitable monitoring will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Docker stats
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Docker top
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Docker logs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: cAdvisor
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sysdig
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As you are running a single host machine that you are treating as a backup,
    there is no real need for you to ship your log files to a central location as
    odds are your host machines; like the containers, its hosting will be online for
    months or possibly even years.
  prefs: []
  type: TYPE_NORMAL
- en: It is unlikely that you will need to dig too deeply into your containers' historical
    performance stats, as most of the tuning and troubleshooting will be done in real
    time as problems occur.
  prefs: []
  type: TYPE_NORMAL
- en: With the monitoring tools suggested, you will be able to get a good insight
    into what is happening within your containers in real time, and to get more than
    enough information on processes that are consuming too much RAM and CPU, along
    with any error messages from within the containers.
  prefs: []
  type: TYPE_NORMAL
- en: You may want to enable a service such as Pingdom ([https://www.pingdom.com/](https://www.pingdom.com/))
    or Uptime Robot ([http://uptimerobot.com/](http://uptimerobot.com/)). These services
    poll your website every few minutes to ensure that the URL you configure them
    to, check whether its loading within a certain time or at all. If they detect
    any slowdown or failures with the page loading, they can be configured to send
    an initial alert to notify you that there is a potential issue, such as both the
    services mentioned have a free tier.
  prefs: []
  type: TYPE_NORMAL
- en: Scenario two
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You are running a custom e-commerce application that needs to be highly available
    and also scale during your peak times. You are using a public cloud service and
    the toolset that comes with it to launch containers and route traffic to them.
  prefs: []
  type: TYPE_NORMAL
- en: 'A suitable monitoring will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: cAdvisor + Prometheus
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zabbix
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sysdig Cloud
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: New Relic Server Monitoring
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Datadog
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ELK + Logspout
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Log Entries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loggly
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With this scenario, there is a business need to not only be notified about container
    and host failures, but also to hold your monitoring data and logs away from your
    host servers so that you can properly review historical information. You may also
    need to keep logs for PCI compliance or internal auditing for a fixed period of
    time.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on your budget, you can achieve this by hosting your own monitoring
    (Zabbix and Prometheus) and central logging (ELK) stacks somewhere within your
    infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: You can also choose to run a few different third-party tools such as combining
    tools that monitor performance, for example, Sysdig Cloud or Datadog, with a central
    logging service, such as Log Entries or Loggly.
  prefs: []
  type: TYPE_NORMAL
- en: If appropriate, you can also run a combination of self-hosted and third-party
    tools.
  prefs: []
  type: TYPE_NORMAL
- en: 'While the self-hosted option may appear to be the most budget-friendly option,
    there are some considerations to take into account, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Your monitoring needs to be hosted away from your application. There is no point
    in having your monitoring installed on the same host as your application; what
    will alert you if the host fails?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Your monitoring needs to be highly available; do you have the infrastructure
    to do this? If your application needs to be highly available, then so does your
    monitoring.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You need to have enough capacity. Do you have the capacity to be able to store
    log files and metrics going back a month, 6 months, or a year?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you are going to have to invest in any of the preceding options, then it
    will be worth weighing up the costs of investing in both the infrastructure and
    the management of your own monitoring solution against using a third-party that
    will offer the preceding options as a service.
  prefs: []
  type: TYPE_NORMAL
- en: If you are using a container-only operating system such as CoreOS or RancherOS,
    then you will need to choose a service whose agent or collector can be executed
    from within a container, as you will not be able to install the agent binaries
    directly on the OS.
  prefs: []
  type: TYPE_NORMAL
- en: You will also need to ensure that your host machine is configured to start the
    agents/collectors on boot. This will ensure that as soon as the host machine joins
    a cluster (which is typically when containers will start to popup on the host),
    it is already sending metrics to your chosen monitoring services.
  prefs: []
  type: TYPE_NORMAL
- en: Scenario three
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Your application launches a container each time your API is called from your
    frontend application; the container takes the user input from a database, processes
    it, and then passes the results back to your front end application. Once the data
    has been successfully processed, the container is terminated. You are using a
    distributed scheduling system to launch the containers.
  prefs: []
  type: TYPE_NORMAL
- en: 'A suitable monitoring will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Zabbix
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sysdig Cloud
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Datadog
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ELK + Logspout
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Log Entries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loggly
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this scenario, you more than likely do not want to monitor things such as
    CPU and RAM utilization. These containers after all should only be around for
    a few minutes, and also your scheduler will launch the container on the host machine
    where there is enough capacity for the task to execute.
  prefs: []
  type: TYPE_NORMAL
- en: Instead, you will probably want to keep a record to verify that the container
    launched and terminated as expected. You will also want to make sure that you
    log the `STDOUT` and `STDERR` from the container while it is active, as once the
    container has been terminated, it will be impossible for you to get these messages
    back.
  prefs: []
  type: TYPE_NORMAL
- en: With the tools listed in the preceding points, you should be able to build some
    quite useful queries to get a detailed insight into how your short run processes
    are performing.
  prefs: []
  type: TYPE_NORMAL
- en: For example, you will be able to get the average lifetime of a container, as
    you know the time the container was launched and when it was terminated; knowing
    this will then allow you to set a trigger to alert you if any containers are around
    for any longer than you would expect them to be.
  prefs: []
  type: TYPE_NORMAL
- en: A little more about alerting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A lot of the tools we have looked at in this book offer at least some sort of
    basic alerting functionality; the million-dollar question is should you enable
    it?
  prefs: []
  type: TYPE_NORMAL
- en: A lot of this is dependent on the type of application you are running and how
    the containers have been deployed. As we have already mentioned a few times in
    this chapter, you should never really have a Snowflake container; this leaves
    us with Pets, Cattle, and Chickens.
  prefs: []
  type: TYPE_NORMAL
- en: Chickens
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As already discussed in the previous section, you probably don't need to worry
    about getting alerts for RAM, CPU, and hard drive performance on a cluster that
    is configured to run Chickens.
  prefs: []
  type: TYPE_NORMAL
- en: Your containers should not be up long enough to experience any real problems;
    however, should there be any unexpected spikes, your scheduler will probably have
    enough intelligence to distribute your containers to hosts that have the most
    available resources at that time.
  prefs: []
  type: TYPE_NORMAL
- en: You will need to know if any of your containers have been running longer than
    you expect them to be up; for example, a process in a container that normally
    takes no more than 60 seconds is still running after 5 minutes.
  prefs: []
  type: TYPE_NORMAL
- en: This not only means that there is a potential problem, it also means that you
    find yourself running hosts that only contain stale containers.
  prefs: []
  type: TYPE_NORMAL
- en: Cattle and Pets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When it comes to setting up alerts on Cattle or Pets, you have a few options.
  prefs: []
  type: TYPE_NORMAL
- en: You will more than likely want to receive alerts based on CPU and RAM utilization
    for both the host machine and the containers, as this could indicate a potential
    problem that could cause slow down within the application and also loss of business.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned previously, you will probably also want to be alerted if your application
    starts to serve the content that is unexpected. For example, a host and a container
    will quite happily sit there serving an application error.
  prefs: []
  type: TYPE_NORMAL
- en: You can use a service such as Pingdom, Zabbix, or New Relic to load a page and
    check for the content in the footer; if this content is missing, then an alert
    can be sent.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on how fluid your infrastructure is, in a Cattle configuration, you
    will probably want to be alerted when containers spin up and down, as this will
    indicate periods of high traffic/transactions.
  prefs: []
  type: TYPE_NORMAL
- en: Sending alerts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sending alerts differs for each tool, for example, an alert could be as simple
    as sending an email to inform you that there is an issue to the sounding of an
    audible alert in a **Network Operations Center** (**NOC**) when the CPU load of
    a container goes above five, or the load on the host goes above 10.
  prefs: []
  type: TYPE_NORMAL
- en: For those of you who require an on-call team to be alerted, most of the software
    we have covered has some level of integration alert aggregation services such
    as PagerDuty ([https://www.pagerduty.com](https://www.pagerduty.com)).
  prefs: []
  type: TYPE_NORMAL
- en: These aggregation services either intercept your alert emails or allow services
    to make API calls to them. When triggered, they can be configured to place phone
    calls, send SMS messages, and even escalate to secondary on-call technician if
    an alert has not been flagged down within a definable time.
  prefs: []
  type: TYPE_NORMAL
- en: I can't think of any cases where you shouldn't look at enabling alerting, after
    all, it's always best to know about anything that could effect your application
    before your end users do.
  prefs: []
  type: TYPE_NORMAL
- en: How much alerting you enable is really down to what you are using your containers
    for; however, I would recommend that you review all your alerts regularly and
    also actively tune your configuration.
  prefs: []
  type: TYPE_NORMAL
- en: The last thing you want is a configuration that produces too many false positives
    or one that is too twitchy, as you do not want the team who receives your alerts
    to become desensitized to the alerts that you are generating.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if a critical CPU alert is triggered every 30 minutes because of
    a scheduled job, then you will probably need to review the sensitivity of the
    alert, otherwise it is easy for the engineer to simply dismiss a critical alert
    without thinking about it, as "this alert comes every half an hour and will be
    ok in a few minutes", when your entire application could be unresponsive.
  prefs: []
  type: TYPE_NORMAL
- en: Keeping up
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While Docker has been built on top of well-established technologies such as
    **Linux Containers** (**LXC**), these have traditionally been difficult to configure
    and manage, especially for non-system administrators.
  prefs: []
  type: TYPE_NORMAL
- en: Docker removes almost all the barriers to entry, allowing everyone with a small
    amount of command-line experience to launch and manage their own container-based
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: This has forced a lot of the supporting tools to also lower their barrier to
    entry. Software that once required careful planning to deploy, such as some of
    the monitoring tools we covered in this book, can now be deployed and configured
    in minutes rather than hours.
  prefs: []
  type: TYPE_NORMAL
- en: Docker is also a very fast-moving technology; while it has been considered production-ready
    for a while, new features are being added and existing features are improved with
    regular updates.
  prefs: []
  type: TYPE_NORMAL
- en: So far, in 2015, there have been 11 releases of Docker Engine; of these, only
    six have been minor updates that fix bugs, and the rest have all been major updates.
    Details of each release can be found in the project's Changelog, which can be
    found at [https://github.com/docker/docker/blob/master/CHANGELOG.md](https://github.com/docker/docker/blob/master/CHANGELOG.md).
  prefs: []
  type: TYPE_NORMAL
- en: Because of the pace of development of Docker, it is import that you also update
    any monitoring tools you deploy. This is not only to keep up with new features,
    but also to ensure that you don't loose any functionality due to changes in the
    way in which Docker works.
  prefs: []
  type: TYPE_NORMAL
- en: This attitude of updating monitoring clients/tools can be a bit of a change
    for some administrators who maybe in the past would have configured a monitoring
    agent on a server and then not thought about it again.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As discussed in this chapter, Docker is a fast moving technology. While this
    book has been in production, there have been three major versions released from
    1.7 to 1.9; with each release Docker has become more stable and more powerful.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we have looked at different ways to implement the technologies
    that have been discussed in the previous chapters of this book. By now, you should
    have an idea of which approach is appropriate to monitor your containers and host
    machines, for both your application and for the way the application has been deployed
    using Docker.
  prefs: []
  type: TYPE_NORMAL
- en: 'No matter which approach you chose to take, it is important that you stay up-to-date
    with Docker''s development and also the new monitoring technologies as they emerge,
    the following links are good starting points to keep yourself informed:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Docker Engineering Blog: [http://blog.docker.com/category/engineering/](http://blog.docker.com/category/engineering/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Docker on Twitter: [https://twitter.com/docker](https://twitter.com/docker)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Docker on Reddit: [https://www.reddit.com/r/docker](https://www.reddit.com/r/docker)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Docker on Stack Overflow: [http://stackoverflow.com/questions/tagged/docker](http://stackoverflow.com/questions/tagged/docker)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One of the reasons why the Docker project has been embraced by developers, system
    administrators and even enterprise companies is because it is able to move at
    a quick pace, while adding more features and very impressively maintaining its
    ease of use and flexibility.
  prefs: []
  type: TYPE_NORMAL
- en: Over the next 12 months, the technology is set to be even more widespread; the
    importance of ensuring that you are capturing useful performance metrics and logs
    from your containers will become more critical and I hope that this book has helped
    you start your journey into monitoring Docker.
  prefs: []
  type: TYPE_NORMAL
