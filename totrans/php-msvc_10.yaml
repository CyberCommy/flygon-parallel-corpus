- en: Chapter 10. Strategies for Scalability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You have your application ready. Now it is time to plan for the future. In this
    chapter, we will give you a global overview of how you can check the possible
    bottlenecks of your application and how you can calculate the capacity of your
    application. At the end of the chapter, you will have the basic knowledge to create
    your own scalability plan.
  prefs: []
  type: TYPE_NORMAL
- en: Capacity planning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Capacity planning** is the process of determining the infrastructure resources
    required by your application to meet future workload demands for your app. This
    process ensures that you have adequate resources available only when they are
    needed, reducing costs to the minimum. If you know how your application is used
    and the limits of your current resources, you can extrapolate the data and know,
    more or less, the future requirements. Creating a capacity plan for your application
    has some benefits, among which we can highlight the following ones:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Minimize costs and avoid waste from over-provisioning**: Your application
    will use only the required resources, so it makes no sense, for example, to have
    a 64 GB RAM server for your database when you are only using 8 GB.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prevent bottlenecks and save time**: Your capacity plan highlights when each
    element of your infrastructure reaches its peak, giving you a hint about where
    the bottleneck can be.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Increase business productivity**: If you have a detailed plan that indicates
    the limits of each element of your infrastructure and knows when each element
    will reach its limits, it gives you spare room to dedicate your time to other
    business tasks. You will have a set of instructions to follow on the precise moment
    you need to increase the capacity of your application. No more crazy moments when
    you are suffering bottlenecks and don’t know what to do.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Used as a mapping of business objectives**: If your application is critical
    for your business, this document can be used to highlight some business objectives.
    For example, if the business wants to reach 1,000 users, your infrastructure needs
    to be allowed to support them, flagging some investments needed to fulfill this
    requirement.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Knowing the limits of your application
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The main purpose of knowing the limits of your application is to know how much
    capacity we still have at any given point of time before we start having issues.
    The first thing we need to do is create an inventory of the components of our
    application. Make the inventory as detailed as possible; it will help you know
    all the tools you have in your project. In our example application, the list of
    different components could be something similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Autodiscovery service:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hashicorp Consul
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Telemetry service:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prometheus
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Battle microservice:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Proxy: NGINX'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'App engine: PHP 7 FPM'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Data storage: Percona'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cache storage: Redis'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Location microservice:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Proxy: NGINX'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'App engine: PHP 7 FPM'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Data storage: Percona'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cache storage: Redis'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Secret microservice:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Proxy: NGINX'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'App engine: PHP 7 FPM'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Data storage: Percona'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cache storage: Redis'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'User microservice:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Proxy: NGINX'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'App engine: PHP 7 FPM'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Data storage: Percona'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cache storage: Redis'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once we have reduced our application to the base components, we need to analyze
    and determine the usage of each component over time and its maximum capacity in
    an appropriate measurement.
  prefs: []
  type: TYPE_NORMAL
- en: Some components can have multiple measurements associated, for example, the
    data storage layer (Percona in our case). For this component, we can measure the
    number of SQL transactions, amount of storage used, CPU load, and so on. In the
    previous chapters, we added a Telemetry service; you can use this service to gather
    basic stats from each component.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the basic stats you can log for each component of your application
    are as listed:'
  prefs: []
  type: TYPE_NORMAL
- en: CPU load
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Memory usage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Network usage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: IOPS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Disk utilization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In some software, you need to gather some specific measurements. For instance,
    on a database you can check the following things:'
  prefs: []
  type: TYPE_NORMAL
- en: Transactions per second
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cache hit ratio (if you have enabled query cache)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: User connections
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The next step is determining the natural growth of the application. This measurement
    can be defined as the growth performed by your application if nothing special
    has been done (such as PPC campaigns and new features). This measurement can be
    the number of new users or the amount of active users, for example. Imagine that
    you deploy your application to production and stop adding new features or doing
    marketing campaigns. If the number of new users increased 7% over the last month,
    that amount is the natural growth of your application.
  prefs: []
  type: TYPE_NORMAL
- en: Some businesses/projects have seasonal trends, which means that at specific
    dates, the usage of your application increases. Imagine that you are a gifts'
    retailer, most of your sales probably are done around Valentine’s day or at the
    end of the year (Black Friday, Xmas). If this is your case, analyze all the data
    you have to establish the seasonality data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that you have some basic stats of your application, it is time to calculate
    what is known as the headroom. The headroom can be defined as the amount of resources
    you have until you are out of resources. It can be calculated with the following
    formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Knowing the limits of your application](graphics/B06142_10_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Headroom formula
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see from the preceding formula, calculating the headroom for a specific
    component is very easy. Let’s explain each variable before we take an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '**IdealUsage:** This is a percentage that describes the amount of capacity
    for a specific component of your application that we are planning to use. This
    ideal usage should never be 100%, as strange behaviors, such as not being able
    to save data in your database, can start appearing when you approach the resource
    limits. Our recommendation is to set this amount to be between 60% and 75%, giving
    you enough extra space for peak moments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MaxCapacity:** This is the amount that indicates the maximum capacity of
    the component subject of our study. For example, a web server that can manage
    up to 250 concurrent connections.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**CurrentUsage:** This is the amount that indicates the current usage of the
    component that we are studying.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Growth:** This is the percentage that indicates the natural growth of our
    application.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Optimizations:** This is the optional variable that describes the amount
    of optimizations we can achieve in a specific amount of time. For instance, if
    your current database can manage 35 queries per second, you can achieve 50 queries
    per second after a few optimizations. In this case, the amount of optimization
    is 15.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Imagine that you are calculating the headroom of requests per second that can
    be managed by one of our NGINX. For our application, we have decided to set the
    ideal usage to 60% (0.6). From our measurements and from the data extracted from
    our load testing (explained later in the chapter), we know that the maximum number
    of **requests per second** (**RPS**) is 215\. In our current stats, our NGINX
    server served a peak of 193 RPS today and we have calculated the growth for the
    next year to be at least 11 RPS.
  prefs: []
  type: TYPE_NORMAL
- en: 'The time period we want to measure is 1 year, and we think that we can achieve
    the 250 RPS of maximum capacity in this time, so our Headroom value will be as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Headroom = 0.6 * 215 - 123 - (11 - 35) = 30 RPS*'
  prefs: []
  type: TYPE_NORMAL
- en: What does this calculation mean? Since the result is positive, it means that
    we have enough spare room for our predictions. If we divide the result by the
    sum of growth and optimizations, we can calculate how much time we have until
    we reach our limits.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since our time period is 1 year, we can calculate how much time we have until
    we reach our limits, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Headroom Time = 30 rpms / 24 = 1.25 years*'
  prefs: []
  type: TYPE_NORMAL
- en: As you have probably already deduced, we have 1.25 years until our NGINX server
    reaches the limit of RPS. In this section, we showed you how to calculate the
    headroom of a specific component; now, it is your turn to make the calculations
    for each one of your components and for the different metrics available for each
    component.
  prefs: []
  type: TYPE_NORMAL
- en: Availability math
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Availability** can be defined as how often the site is available in a specific
    period of time, for example, a week, a day, a year, and so on. Depending on how
    critical your application is for you or your business, a downtime can equal lost
    revenue. As you can suppose, the availability can become the most important metric
    on scenarios where your application is used by customers/users and they need your
    service at any time.'
  prefs: []
  type: TYPE_NORMAL
- en: We have the theoretical concept about what availability is. It is time to do
    some math, so take your calculator. As from the earlier general definition, the
    availability can be calculated as the amount of time your application can be used
    by your users/customers for divided by the time frame (the specific period of
    time we are measuring).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s imagine that we want to measure the availability of our application over
    one week. In one week, we have `10,080` minutes:'
  prefs: []
  type: TYPE_NORMAL
- en: '*7 days x 24 hours per day x 60 minutes per hour = 7 * 24 * 60 = **10,080 minutes***'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, imagine that your application had a few outages that week and the amount
    of available minutes of your application was reduced to `10,000`. To calculate
    the availability for our example, we only need to do some simple math:'
  prefs: []
  type: TYPE_NORMAL
- en: '*10,000 / 10,080 = 0.9920634921*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The availability was usually measured as a percentage (%), so we need to transform
    our result to a percentage:'
  prefs: []
  type: TYPE_NORMAL
- en: '*0.9920634921 * 100 = 99.20634921% ~ **99.21***'
  prefs: []
  type: TYPE_NORMAL
- en: 'The availability of our application over the week was `99.21%`. Not too bad,
    but far from the result we aim for, that is, the closest percentage to `100%`
    that we can get. Most of the time, the availability percentage is referred as
    the **number of nines** and, the closer they are to `100%`, the more difficult
    it is to maintain the availability of your application. To give you an overview
    of how difficult it will be to reach the `100%` availability, here are some examples
    of availabilities and the possible downtime:'
  prefs: []
  type: TYPE_NORMAL
- en: '99.21% (our example):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Weekly: 1 h 19 m 37.9 s'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Monthly: 5 h 46 m 15.0 s'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yearly: 69 h 14 m 59.9 s'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '99.5%:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Weekly: 50 m 24.0 s'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Monthly: 3 h 39 m 8.7 s'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yearly: 43 h 49 m 44.8 s'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '99.9%:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Weekly: 10 m 4.8 s'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Monthly: 43 m 49.7 s'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yearly: 8 h 45 m 57.0 s'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '99.99%:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Weekly: 1 m 0.5 s'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Monthly: 4 m 23.0 s'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yearly: 52 m 35.7 s'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '99.999%:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Weekly: 6.0 s'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Monthly: 26.3 s'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yearly: 5m 15.6 s'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '99.9999%:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Weekly: 0.6 s'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Monthly: 2.6 s'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yearly: 31.6 s'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '99.99999%:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Weekly: 0.1 s'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Monthly: 0.3 s'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yearly: 3.2 s'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As you can see, as getting close to `100%` availability becomes harder and
    harder, the downtimes are tighter. However, how can you reduce your downtime or
    at least ensure that you are doing your best to keep it low? There is no easy
    answer to this question, but we can give you a few hints of the different things
    you can do:'
  prefs: []
  type: TYPE_NORMAL
- en: The worst possible scenario will happen, so you should simulate failures frequently
    to be ready to deal with the apocalypse of your application.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Find the possible bottlenecks of your application.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tests, tests, and more tests everywhere, and, of course, keep them updated.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Log any event, any metric, anything you can measure or save as a log, and keep
    it for future references.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Know the limits of your application.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Have some good development practices in place, at least to share the knowledge
    of how your application was built. Among all of them, you can do the following
    ones:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Second approval for any hotfix or feature
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Programming in pairs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a continuous delivery pipeline.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Have a backup plan and keep your backups safe and ready to be used.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Document everything, any small change or design, and always keep the documentation
    up to date.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You now have a global overview about what **availability** means and the maximum
    downtime expected for each rate. Be careful if you give your users/customers a
    SLA (Service Level Agreement), as you will be creating a promise about the availability
    of your application that you will have to fulfill.
  prefs: []
  type: TYPE_NORMAL
- en: Load testing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Load testing can be defined as the process of putting a demand (load) in your
    application to measure its response. This process helps you identify the maximum
    capacity of your application or infrastructure and it can highlight bottlenecks
    or problematic elements of your application or infrastructure. The normal way
    of doing load testing is first doing a test on "normal" conditions, that is, with
    a normal load in your application. Having measured the response of your system
    under normal conditions allows you to have a baseline that you will use to compare
    with in future testings.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see some of the most common tools you can use for your load testing. Some
    are simple and easy to use and others are more complex and powerful.
  prefs: []
  type: TYPE_NORMAL
- en: Apache JMeter
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Apache JMeter application is an open source software built in Java and designed
    to do load testing and measure performance. At first, it was designed for web
    applications, but it was expanded to test other functions in time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the most interesting features of Apache JMeter are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Support for different applications/servers/protocols: HTTP(S), SOAP/Rest, FTP,
    LDAP, TCP, and Java objects.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Easy integration with third-party Continuous Integration tools: It has libraries
    for Maven, Gradle, and Jenkins.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Command-line mode (non GUI/headless mode): This enables you to do your test
    from any OS with Java installed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Multi-threading framework: This allows you to make concurrent samples by many
    threads and simultaneous sampling of different functions by separate thread groups.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Highly extensible: It is extensible through libraries or plugins, among others.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Full featured test IDE: It allows you to create, record, and debug your test
    plans.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As you can see, this project is an interesting tool you can use in your loading
    tests. In the following sections, we will show you how to build a simple test
    scenario. Unfortunately, we don’t have enough space in the book to cover all the
    features, but at least you will know the basics that will be the foundations of
    more complex tests in the future.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Apache JMeter
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Being developed in Java allows this application to be portable to any OS with
    Java installed. Let’s install it in our development machine.
  prefs: []
  type: TYPE_NORMAL
- en: The first step is to fulfill the main requirement--you need a JVM 6 or later
    version to make the application work. You probably have Java in your machine already,
    but if this is not the case, you can download the latest JDK from the Oracle page.
  prefs: []
  type: TYPE_NORMAL
- en: 'To check the version of your Java runtime, you only need to open a terminal
    in your OS and execute the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The preceding command will tell you the version available in your machine.
  prefs: []
  type: TYPE_NORMAL
- en: As soon as we are sure that we have the correct version, we only need to go
    to the official Apache JMeter page ([http://jmeter.apache.org](http://jmeter.apache.org))
    and download the latest binaries in ZIP or TGZ formats. Once the binaries are
    fully downloaded to your machine, you only need to uncompress the downloaded ZIP or
    TGZ, and Apache JMeter is ready to be used.
  prefs: []
  type: TYPE_NORMAL
- en: Executing load tests with Apache JMeter
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Open the folder where you have uncompressed the Apache JMeter binaries. There,
    you can find a `bin` folder and some scripts for different OSes in it. If you
    are using Linux/UNIX or Mac OS, you can execute the `jmeter.sh` script to open
    the application GUI. If you are running Windows, there is a `jmeter.bat` executable
    that you can use to open the GUI:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Executing load tests with Apache JMeter](graphics/B06142_10_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Apache JMeter GUI
  prefs: []
  type: TYPE_NORMAL
- en: The Apache JMeter GUI allows you to build your different testing plans and,
    as you can see in the preceding screenshot, the interface is very easy to understand
    even without reading the manual. Let’s build a test plan with the GUI.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A test plan can be described as a series of steps that Apache JMeter will run
    in a specific order.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our first step in order to create our test plan is adding a **Thread Group**
    under the **Test Plan** node. In Apache JMeter, a thread group can be defined
    as a simulation of concurrent users. Follow the given steps to create a new group:'
  prefs: []
  type: TYPE_NORMAL
- en: Right-click on the **Test Plan** node.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the context menu, select **Add** | **Threads (Users)** | **Thread Group**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The preceding steps will create a child element in our **Test Plan** node.
    Select it so that we can make some adjustments to our group. Refer to the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Executing load tests with Apache JMeter](graphics/B06142_10_03-1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Thread group settings
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see in the preceding screenshot, each thread group allows you to
    specify the amount of users for your tests and the duration of the test. The main
    options available are as listed:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Actions to be taken after a Sample error:** This option allows you to control
    the behavior of the test as soon as a sample error is thrown. The most used option
    is the **Continue** behavior.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Number of Threads (users):** This field allows you to specify the number
    of concurrent users you will be using to hit your application.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ramp-Up Period (in seconds):** This field is used to tell Apache JMeter how
    much time can be used to create all the threads you specified in the previous
    field. For example, if you set this field to 60 seconds and the **Number of Threads
    (users)** was set to 6, Apache JMeter will take 60 seconds to spin up all the
    6 threads, one each 10 seconds.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Loop count and Forever:** These fields allow you to stop the test after a
    specific number of executions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The remaining options are self-explanatory and in our example, we will only
    use the mentioned fields.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine that you want to use 25 threads (like users) and you set up the ramp-up
    to 100 seconds. The math will tell you that a new thread will be created every
    4 seconds until you have the 25 threads running (100/25 = 4). These two fields
    allow you to design your tests to start slowly and increase the amount of users
    hitting your application at the right time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have our threads/users defined, it is time to add a request because
    our test will do nothing without a request. To add a request, you only need to
    select the Thread Group node, right-click on the context menu, and choose **Add** | **Sampler** | **HTTP
    Request**. The previous action will add a new children node to our Thread Group.
    Selecting the new node, Apache JMeter, will show you a form similar to the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Executing load tests with Apache JMeter](graphics/B06142_10_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: HTTP Request options
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in the preceding screenshot, we can set up the host we want to
    hit our test with. In our case, we decide to hit `localhost` on port `8083` with
    a `GET` request to the `/api/v1/secret/` path. Feel free to explore the advanced
    options or add custom parameters. Apache JMeter is very flexible and covers practically
    every possible scenario.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, we have set up a basic test, now it's time to see the results.
    Let’s explore a few interesting ways to gather the information from the test.
    To see and analyze the results of each iteration of our test, we need to add a
    **Listener**. To do this, as in the previous steps, right-click on the **Thread
    Group** and navigate to **Add | Listener | View Results in Table**. This action
    will add a new node to our tests and, as soon as we start the test, the results
    will appear in the application.
  prefs: []
  type: TYPE_NORMAL
- en: If you had selected the **Forever** option in the **Thread Group**, you need
    to stop your test by hand. You can do it with the red cross icon displayed next
    to the green play. This button will stop the tests waiting for each thread to
    end their actions. If you click on the stop icon, Apache JMeter will kill all
    the threads immediately.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s give it a try and click on the green play icon to start your test. Click
    on your **View Results in Table** node and you will see all the results of the
    test appearing:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Executing load tests with Apache JMeter](graphics/B06142_10_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Apache JMeter Results in Table
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in the preceding screenshot, Apache JMeter records different
    data for each request, such as the amount of bytes sent/returned, the status,
    or the request latency, among others. All this data is interesting to analyze
    the behaviour of your application when you change the amount of load and with
    this Listener, you can even export the data so that you can use external tools
    to analyze the results.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you don’t have external tools to analyze your data with, but you want to
    have some basic stats to compare with the different loads you are exposing your
    application to, you can add another interesting Listener. As we did before, open
    the right-click context menu of the **Thread Group** and navigate to `Add` | **Listener** | `Summary
    Report`. This listener will give you some basic stats that you can use to compare
    results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Executing load tests with Apache JMeter](graphics/B06142_10_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Apache JMeter Summary Report
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from the preceding screenshot, this listener has given us some
    averages from our measurements.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using a table to show the results is fine. However, as we all know, a picture
    is worth a thousand words, so let’s add a few graph listeners so that you can
    complete your load testing report. Right-click on the **Thread Group** and, in
    the context menu, go to **Add** | **Listener** | **Response Time Graph**. You
    will see a screen similar to the following one:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Executing load tests with Apache JMeter](graphics/B06142_10_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Apache JMeter Response Time Graph
  prefs: []
  type: TYPE_NORMAL
- en: 'Feel free to make some changes to the default settings. For instance, you can
    reduce the **Interval (ms)**. If you run your tests again, all the data generated
    by the test will be used to generate a nice graph, such as the following one:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Executing load tests with Apache JMeter](graphics/B06142_10_08-1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Response time graph
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from the graph generated from our test results, the increase
    of threads (users) leads to an increase in the response times. What do you think
    this means? If you said that our testing infrastructure needs to scale up to accommodate
    the increase in the loading, your response is correct.
  prefs: []
  type: TYPE_NORMAL
- en: Apache JMeter comes with multiple options to create your loading test. We have
    only showed you how to create a basic test and how to see the results. Now, it’s
    your turn to explore all the different options available to create advanced tests
    and discover which features are more suitable for your project. Let’s see some
    other tools that you can use for your load tests.
  prefs: []
  type: TYPE_NORMAL
- en: Load testing with Artillery
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Artillery** is an open source toolkit that you can use to do load testing
    to your application, and it is similar to Apache JMeter. Among other features,
    we can highlight the following advantages of this tool:'
  prefs: []
  type: TYPE_NORMAL
- en: Support for multiple protocols, and HTTP(S) or WebSockets come out of the box
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Easy to integrate with real-time reporting software or services like DataDog
    an InfluxDB
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: High performance, so it can be used on commodity hardware/servers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Very easy to extend, so it can be adapted to your needs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Different report options with detailed performance metrics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Very flexible, so you can test practically any possible scenario
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing Artillery
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Artillery was built on node.js, so the main requirement is to have this runtime
    installed on the machine you will use to fire the tests.
  prefs: []
  type: TYPE_NORMAL
- en: We love the containerization technology but unfortunately there is no easy way
    of using artillery on Docker without a dirty hack. In any case we recommend you
    to use a dedicated VM or server were you can fire your load testings.
  prefs: []
  type: TYPE_NORMAL
- en: To use Artillery you need node.js in your VM/server and this software is very
    easy to install. We are not going to explain how to create a local VM (you can
    use VirtualBox or VMWare to create one), we are only going to show you how you
    can install it on RHEL/CentOS. For other OSes and options, you can find detailed
    information on the node.js documentation ([https://nodejs.org/en/download/](https://nodejs.org/en/download/)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Open the terminal of your RHEL/CentOS VM or server and download the setup script
    for the LTS version:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'As soon as the previous command finishes, you need to execute the next command
    as root, as shown in the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'After executing the previous commands you will have Node.js installed and ready
    in your VM/server. It is time to install Artillery with the Node.js package manager,
    the `npm` command. In your terminal, execute the following command to install
    globally Artillery:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: As soon as the previous command finishes, you will have Artillery ready to use
    it.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing we can do is to check that Artillery was correctly installed
    and that it is available. Enter the following command to do it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The preceding command will show you a lovely dinosaur, which means that Artillery
    is ready to be used.
  prefs: []
  type: TYPE_NORMAL
- en: Executing loading tests with Artillery
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Artillery is a very flexible toolkit. You can run your tests from the console
    or you can have YAML or JSON files, which describe your testing scenarios, and
    run them. Please note that in our following examples we are using `microservice_secret_nginx`
    as the host we are going to test, you need to adjust this host to the IP address
    of your local environment. Let’s give you a sneak peek of this tool; run the following
    command in our load testing VM/server:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding command will do a quick test in a duration of 30 seconds. In
    this time, Artillery will create five virtual users; each one of them will do
    one GET request to the provided URL. As soon as the preceding command is executed,
    Artillery will start the tests and print some stats every 10 seconds. At the end
    of the tests (30 seconds), this tool will show you a small report similar to the
    following one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The preceding report is very easy to understand and gives you an overview of how
    your infrastructure and app are performing.
  prefs: []
  type: TYPE_NORMAL
- en: 'A basic concept you need to understand before we start analyzing the Artillery
    report is the concept of Scenarios. In a few words, a **Scenario** is a sequence
    of tasks or actions you want to test, and they are related. Imagine that you have
    an e-commerce application; a testing scenario can be all the steps a user performs before
    they complete a purchase. Consider the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: The user loads the home.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The user searches for a product.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The user adds a product to the basket.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The user goes to the checkout.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The user makes the purchase.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: All the mentioned actions can be transformed into a request to your application
    that simulates the user action, which means that a scenario is a group of requests.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have this concept clear, we can start analyzing the report output
    from Artillery. In our sample, we only have one scenario with only one request
    (`GET`) to `http://microservice_secret_nginx/api/v1/secret/`. This testing scenario
    is executed by five virtual users who fire only one `GET` request for 30 seconds.
    A simple math calculation, `5 * 1 * 30`, gives us the total of scenarios tested
    (`150`), which is the same amount of requests in our case. The `RPS sent` field
    gives you the average requests per second our test server has made during our
    tests. It is not a very important field, but it gives you an idea about how the
    test is performed.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s check the `Request latency` and `Scenario duration` stats given by Artillery.
    The first thing you need to know is that all the measures from these groups are
    measured in milliseconds.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of `Request latency`, the data is showing us the time used by our
    application to process the requests we have sent. Two important stats are the
    95% (`p95`) and the 99% (`p99`). As you probably already know, the percentile
    is a measure used in statistics that indicates the value under which a given percentage
    of observations fall. From our example, we can see that 95% of the requests were
    processed in 1,146.5 milliseconds or less or that 99% of them were processed in
    1,191.1 milliseconds or less.
  prefs: []
  type: TYPE_NORMAL
- en: The stats shown in the `Scenario duration` in our example are pretty much the
    same as the `Request latency`, because each scenario is formed by only one request.
    If you create more complex scenarios with multiple requests on each one, the data
    of both the groups will differ.
  prefs: []
  type: TYPE_NORMAL
- en: Creating Artillery scripts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As we have told you before, Artillery allows you to create YAML or JSON files
    with the load testing scenarios. Let’s transform our quick example into a YAML file
    so that you can keep it in a repository for future executions.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do this, you only need to create a file in our testing container called,
    for example, `test-secret.yml`, with the following content:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: As you can see in the preceding code, it is similar to our `artillery quick`
    command, but now you can store them in your code repository to run it again and
    again against your application.
  prefs: []
  type: TYPE_NORMAL
- en: You can run your test with the `artillery run test-secret.yml` command and the
    results should be similar to those generated by the quick command.
  prefs: []
  type: TYPE_NORMAL
- en: The Docker container images come with the minimum software needed, so you probably
    can’t find a text editor in our load testing image. At this point of the book,
    you will be able to create a Docker volume and attach it to our testing container
    so that you can share files.
  prefs: []
  type: TYPE_NORMAL
- en: Advanced scripting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the highlighted features of this toolkit is the ability to create custom
    scripts, but you are not only attached to fire static requests. This tool allows
    you to randomize the requests using external CSV files, parsing JSON responses,
    or inline values from the script.
  prefs: []
  type: TYPE_NORMAL
- en: 'Imagine that you want to test your API endpoint responsible for the creation
    of new accounts in your application and instead of using YAML files, you are using
    a JSON script. You can use an external CSV file with the user data to be used
    in the tests with the following adjustments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The preceding `config` fields will tell Artillery where our CSV file is located
    and the different columns used in the CSV. Once we have set up our external file,
    we can use this data in our scenarios. In our example, Artillery will pick random
    rows from `test-data.csv` and generate post request to `/api/v1/user` with that
    data. The fields field from `payload` will create variables we can use, such as 
    `{{ variableName }}`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Creating this kind of scripts seems easy but, at some point of the creation
    of your scripts, you will need some debug information to know what your script
    is doing. If you want to see the details of every request, you can run your script
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'In the case of you wanting to also see the responses, you can run the load
    testing scripts as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Unfortunately, there is no space in the book to cover, in great detail, all
    the options available in Artillery. However, we wanted to show you an interesting
    tool that you can use to do load testing. If you need more information or even
    if you want to contribute to the project, you only need to go to the project’s
    page ([https://artillery.io](https://artillery.io)).
  prefs: []
  type: TYPE_NORMAL
- en: Load testing with siege
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Siege is an interesting multithread HTTP(s) load testing and benchmarking tool.
    It seems small and simple compared with other tools, but it is efficient and easy
    to use, for example, to do a quick test to your latest changes. This tool allows
    you to hit an HTTP(S) endpoint with a configurable number of concurrent virtual
    users and it can be used in three different modes: regression, Internet simulation,
    and brute force.'
  prefs: []
  type: TYPE_NORMAL
- en: Siege was developed for GNU/Linux, but it has been successfully ported to AIX,
    BSD, HP-UX, and Solaris. If you want to compile it, you shouldn’t have any problem
    on most System V UNIX variants and on most newer BSD systems.
  prefs: []
  type: TYPE_NORMAL
- en: Installing siege on RHEL, CentOS, and similar operating systems
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If you use CentOS with the extras repository enabled, you can install the EPEL repo
    with a simple command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: As soon as you have the EPEL repo available, you only need to do a `sudo yum
    install siege` to have this tool available in your OS.
  prefs: []
  type: TYPE_NORMAL
- en: 'Sometimes, the `sudo yum install epel-release` command does not work, for example,
    when you are not using Centos, your distribution is RHEL or a similar one. In
    these cases, you can install the EPEL repository by hand with the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the EPEL repository is available in your OS, you can `install siege` with
    the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Installing siege on Debian or Ubuntu
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Siege is very easy to install on Debian or Ubuntu with the official repositories.
    If you have one of the latest version of these OSs, you only need to execute the
    following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The preceding commands will update your system and install the `siege` package.
  prefs: []
  type: TYPE_NORMAL
- en: Installing siege on other OS
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If your OS wasn’t covered in the previous steps, you can do it by compiling
    the sources, there are plenty of how-tos on the Internet explaining what you need
    to do.
  prefs: []
  type: TYPE_NORMAL
- en: Quick siege example
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s create a quick text to one of our endpoints. In this case, we will test
    our endpoint in 30 seconds with 50 concurrent users. Open the terminal of the
    machine you have siege installed in and type in the following command. Feel free
    to change the command to the correct host or endpoint:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding command is explained in the following points:'
  prefs: []
  type: TYPE_NORMAL
- en: '`-c50` : Create 50 concurrent users'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`-d10` : Delay each simulated user for a random number of seconds between 1
    and 10'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`-t30s` : Time to run the test; 30 seconds in our case'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`http://localhost:8083/api/v1/secret/` : Endpoint to be tested'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As soon as you hit *Enter*, the `siege` command will start firing requests
    to your server and you will have an output similar to the following one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'After around 30 seconds, siege will stop the requests and show you some stats,
    such as the following ones:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: From the preceding results, we can conclude that all our requests were okay,
    none of our requests failed, and the average response time was 3.33 seconds. As
    you can see, this tool is simpler and it can be used on a day-to-day basis to
    check at which level of concurrent users your application starts throwing errors
    or to put the app under stress while you check other metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Scalability plan
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A **scalability plan** is a document that describes all the different components
    of your application and the necessary steps to scale the application as soon as
    it is needed. The scalability plan is a live document, so you need to review it
    and keep it updated frequently.
  prefs: []
  type: TYPE_NORMAL
- en: There is no master template ready to fill as the scalability plan is more an
    internal document with all the information you need to make the right decision
    about the scalability of your app. Our recommendation is to use the scalability
    plan as your guide, including all the contents of your capacity plan, you can
    even add how to hire new workers to this document.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the sections you can have in your scalability plan can be the following
    ones:'
  prefs: []
  type: TYPE_NORMAL
- en: An overview of the application and its components
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Comparison of cloud providers or places where you will deploy your application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Resume of your capacity plan and theoretical limits of the application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scalability phases or steps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Provisioning times and costs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Organization scalability steps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The preceding sections are only a suggestion, feel free to add or remove any
    section to fit in your business plan.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s an overview of some sections of the capacity plan. Imagine that we
    have our example microservices application ready and want to start scaling from
    the minimum resources available. First, we can describe the different elements
    we have in our application as a basic inventory from which we evolve our application:'
  prefs: []
  type: TYPE_NORMAL
- en: Battle microservice
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NGINX
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PHP 7 fpm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Location microservice
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NGINX
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PHP 7 fpm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Secret microservice
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NGNIX
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PHP 7 fpm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: User microservice
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NGINX
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PHP 7 fpm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data storage layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Database: Percona'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As you can see, we have described each component needed for our application,
    and we have started sharing the data layer between all the microservices. We didn’t
    add any cache layer; also, we didn’t add any autodiscovery and telemetry service
    (we will add extra features in the following steps).
  prefs: []
  type: TYPE_NORMAL
- en: Once we have our minimum requirements, let’s take a look at the different steps
    we can have in our scalability plan.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step #0'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this step, we will have all our requirements in one machine even though
    it is not production ready yet because your application cannot survive an issue
    in your machine. A single server with the following characteristics will be enough:'
  prefs: []
  type: TYPE_NORMAL
- en: 8 GB RAM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 500 GB disk
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The base OS will be RHEL or CentOS, with the following software installed:'
  prefs: []
  type: TYPE_NORMAL
- en: NGINX with multiple vhosts setup
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Git
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Percona
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PHP 7 fpm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this step, the provisioning time can be a few hours. We not only need to
    spin up the server, but also need to set up each one of the required services
    (NGINX, Percona, and others). Using tools such as Ansible can help us with the
    quick and repeatable provisioning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step #1'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At this point you are getting the application ready for production, choosing
    between VM or containers (in our case, we have decided to use containers for flexibility
    and performance), splitting the single server configuration into multiple servers
    dedicated to each service required, like our previous requirements, and adding
    autodiscovery and telemetry services.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find a small description of the architecture of our application at
    this step:'
  prefs: []
  type: TYPE_NORMAL
- en: Autodiscovery
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hashicorp Consul container with ContainerPilot
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Telemetry
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prometheus container with ContainerPilot
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Battle microservice
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NGINX container with ContainerPilot
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PHP 7 fpm container with ContainerPilot
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Location microservice
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NGINX container with ContainerPilot
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PHP 7 fpm container with ContainerPilot
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Secret microservice
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NGINX container with ContainerPilot
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PHP 7 fpm container with ContainerPilot
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: User microservice
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NGINX container with ContainerPilot
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PHP 7 fpm container with ContainerPilot
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data storage layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Database: Percona container with ContainerPilot'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The provisioning time in this step will be reduced from hours, like the preceding step,
    to minutes. We have an autodiscovery service (HashiCorp Consul) in place and,
    thanks to ContainerPilot, each of our different components will register itself
    in the autodiscovery register and it will auto setup. In a few minutes, we can
    have all the containers provisioned and set up.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step #2'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this step of your scalability planning, you will be adding cache layers to
    all the application microservices to reduce the number of requests and improve
    the overall performance. To improve the performance, we decided to use Redis as
    our cache engine, so you need to create a Redis container on each microservice.
    The provisioning time for this step will be like the previous one but measured
    in minutes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step #3'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this step, you will be moving the storage layer to each microservice, adding
    three Percona containers in Master-Slave mode with automatic setup with ContainerPilot
    and Consul.
  prefs: []
  type: TYPE_NORMAL
- en: The provisioning time for this step will be like the previous one, measured
    in minutes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step #4'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this step of the scalability plan you will be studying the load and usage
    patterns of the application. You will add load balancers in front of the NGINX
    containers to have more flexibility. Thanks to this new layer, we can do A/B testing
    or Blue/Green deployments, among other features. Some interesting and open source
    tools you can use in this case are Fabio Proxy and Traefik.
  prefs: []
  type: TYPE_NORMAL
- en: The provisioning time for this step will be like the previous one, measured
    in minutes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step #5'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At this final step you will be checking again the application infrastructure
    to keep it up to date and scaling up and horizontally when necessary.
  prefs: []
  type: TYPE_NORMAL
- en: The provisioning time for this step will be like the previous one, measured
    in minutes.
  prefs: []
  type: TYPE_NORMAL
- en: As we told you before, the scalability plan is a live document, so you need
    to revise it frequently. Imagine that a new database software is created in a
    few months and it is optimal for high loads; you can review your scalability plan
    and introduce this new database in your infrastructure. Feel free to add all the
    information you consider important for the scalability of your application.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we showed you how to check the limits of your application,
    which gives you an idea about the possible bottlenecks you will be fighting with.
    We also showed you the basic concepts you need to have to create your capacity
    and scalability plans. We also showed you a few options to do some load testing
    to your application. You should have enough knowledge to have your application
    ready for heavy use, or at least you know the weak points of your app.
  prefs: []
  type: TYPE_NORMAL
