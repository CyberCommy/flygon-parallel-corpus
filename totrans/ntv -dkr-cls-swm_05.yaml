- en: Chapter 5. Administer a Swarm Cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We're now going to see how to administer a running Swarm cluster. We will discuss
    in detail topics such as scaling of cluster size (adding and removing nodes),
    updating the cluster and node information; handling the node status (promotion
    and demotion), troubleshooting, and graphical interfaces (UI).
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will take a look at the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Docker Swarm standalone
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Docker Swarm Mode
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cluster management
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Swarm health
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Graphical interfaces for Swarm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Docker Swarm standalone
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In standalone mode, cluster operations need to be done directly inside the container
    `swarm`.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we are not going to cover every option in detail. Swarm v1
    will be deprecated soon, as it has already been declared obsolete by Swarm Mode.
  prefs: []
  type: TYPE_NORMAL
- en: '![Docker Swarm standalone](images/image_05_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The commands to administer a Docker Swarm standalone cluster are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create (`c`): As we saw in [Chapter 1](ch01.html "Chapter 1. Welcome to Docker
    Swarm"), *Welcome to Docker Swarm* this is how we can generate the UUID token,
    in case the token mechanism is going to be used. Typically, in production, people
    use Consul or Etcd, so this command has no relevance for production.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'List (`l`): This shows the list of cluster nodes based on an iteration through
    Consul or Etcd, that is, the Consul or Etcd must be passed as an argument.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Join (`j`): Joins the node on which the swarm container is running to the cluster.
    Here, we need to pass a discovery mechanism at the command line.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Manage (`m`): This is the core of the Standalone mode. Managing a cluster deals
    with changing cluster properties, such as Filters, Schedulers, external CA URLs,
    and timeouts. We will talk more about the application of these options to Swarm
    mode in [Chapter 6](ch06.html "Chapter 6. Deploy Real Applications on Swarm"),
    *Deploy Real Applications on Swarm* when we''ll work with a real application deployment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Docker Swarm Mode
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will continue exploring Swarm Mode commands for managing
    a cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Manually adding nodes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can choose to create new Swarm nodes, so Docker hosts, either way you prefer.
  prefs: []
  type: TYPE_NORMAL
- en: If Docker Machine is used, it will reach its limit very soon. You will have
    to be very patient while listing machines and wait for several seconds for Machine
    to get and print the information as a whole.
  prefs: []
  type: TYPE_NORMAL
- en: 'A method to add nodes manually is to use Machine with the generic driver; so,
    delegate host provisioning (Operating System installation, network and security
    groups configurations, and so on) to something else (such as Ansible), and later
    exploit Machine to install Docker in a proper manner. This is how it can be done:'
  prefs: []
  type: TYPE_NORMAL
- en: Manually configure the cloud environment (security groups, networks, and so
    on.)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Provision Ubuntu hosts with a third party tool.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run Machine with the generic driver on these hosts with the only goal to properly
    install Docker.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Manage hosts with the tool at part 2, or even others.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If you use Machine''s generic driver, it will select the latest stable Docker
    binaries. While working on this book, in order to use Docker 1.12, we sometimes
    overcame this by giving Machine the option to get the latest unstable version
    of Docker with the `--engine-install-url` option:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: At the moment of reading this book, for a production Swarm (mode), 1.12 will
    be stable; so this trick will not be necessary anymore, unless you need to use
    some of the latest Docker features.
  prefs: []
  type: TYPE_NORMAL
- en: Managers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While planning a Swarm, some considerations regarding the number of managers
    have to be kept in mind, as we saw in [Chapter 4](ch04.html "Chapter 4. Creating
    a Production-Grade Swarm"), *Creating a Production-Grade Swarm* . The theory of
    HA suggests that the number of managers must be odd and equal or more than 3\.
    To grant a quorum in high availability means that the majority of the nodes agree
    on the part of node that is leading the operations.
  prefs: []
  type: TYPE_NORMAL
- en: If there are two managers and one goes down and comes back, it's possible that
    both will be considered leaders. This causes a logical crash in the cluster organization,
    which is called a split brain.
  prefs: []
  type: TYPE_NORMAL
- en: The more managers you have, the higher is the resistance ratio to failures.
    Take a look at the following table.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Number of managers** | **Quorum (majority)** | **Maximum possible failures**
    |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 2 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 3 | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | 4 | 3 |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | 5 | 4 |'
  prefs: []
  type: TYPE_TB
- en: 'Also, in the Swarm Mode, an **ingress** overlay network is created automatically
    and associated to the nodes as ingress traffic. Its purpose is to be used with
    containers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Managers](images/image_05_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: You will want your containers to be associated to an internal overlay (VxLAN
    meshed) network to communicate with each other, rather than using public or other
    external networks. Thus, Swarm creates this for you and it is ready to use.
  prefs: []
  type: TYPE_NORMAL
- en: Workers number
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can add an arbitrary number of workers. This is the elastic part of the
    Swarm. It's totally fine to have 5, 15, 200, 2300, or 4700 running workers. This
    is the easiest part to handle; you can add and remove workers with no burdens,
    at any time, at any size.
  prefs: []
  type: TYPE_NORMAL
- en: Scripted nodes addition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The easiest way to add nodes, if you plan to not go a 100-nodes total, is to
    use basic scripting.
  prefs: []
  type: TYPE_NORMAL
- en: When executing `docker swarm init`, just copy-paste the lines printed as the
    output.
  prefs: []
  type: TYPE_NORMAL
- en: '![Scripted nodes addition](images/image_05_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Then, create a certain bunch of workers with a loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'After this, it will only be necessary to go through the list of machines, `ssh`
    into them and `join` the nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This script runs through the machines and for each, with a name starting with
    s`warm-worker-`, it will `ssh` into and join the node to the existing Swarm and
    to the leader manager, which is `172.31.10.250`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: See [https://github.com/swarm2k/swarm2k/tree/master/amazonec2](https://github.com/swarm2k/swarm2k/tree/master/amazonec2)
    for further details or to download the one liners.
  prefs: []
  type: TYPE_NORMAL
- en: Belt
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Belt is another variant for provisioning Docker Engines massively. It is basically
    a SSH wrapper on steroids and it requires you to prepare provider-specific images
    as well as provision templates before `go` massively. In this section, we'll learn
    how to do so.
  prefs: []
  type: TYPE_NORMAL
- en: You can compile Belt yourself by getting its source from Github.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Currently, Belt supports only the DigitalOcean driver. We can prepare our template
    for provisioning inside `config.yml`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Then, we can create hundreds of nodes with a couple of commands.
  prefs: []
  type: TYPE_NORMAL
- en: First, we create three manager hosts of 16 GB each, namely `mg0`, `mg1`, and
    `mg2`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we can use the `status` command to wait for all nodes being active:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll do this again for 10 worker nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Use Ansible
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can alternatively use Ansible (as I like, and it's becoming very popular)
    to make things more repeatable. We have created some Ansible modules to work with
    Machine and Swarm (Mode) directly; it is also compatible with Docker 1.12 ([https://github.com/fsoppelsa/ansible-swarm](https://github.com/fsoppelsa/ansible-swarm)).
    They require Ansible 2.2+, the very first version of Ansible that is compatible
    with binary modules.
  prefs: []
  type: TYPE_NORMAL
- en: You will need to compile the modules (written in `go`) and then pass them to
    the `ansible-playbook -M` parameter.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: There are some example plays in playbooks. Ansible's plays syntax is so easy
    to understand that it is superfluous to even explain in detail.
  prefs: []
  type: TYPE_NORMAL
- en: 'I used this play to join 10 workers to the **Swarm2k** experiment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Basically, it invokes the `docker_swarm` module after loading some host facts
    from Machine:'
  prefs: []
  type: TYPE_NORMAL
- en: The operation done is `join`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The role of the new node is `worker`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The new node joins `tcp://104.236.78.154:2377`, which was the leader manager
    at the moment of joining. This argument takes an array of managers, such as [`tcp://104.236.78.154:2377`,
    `104.236.18.183:2377`, `tcp://104.236.87.10:2377`]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It passes the password `(secret)`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It specifies some basic engine connection facts and the module will connect
    to the `dockerurl` using the certificates at `tlspath`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'After the `docker_swarm.go` is compiled in the library, joining the workers
    to the Swarm is as easy as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '![Use Ansible](images/image_05_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Cluster management
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To illustrate cluster operations better, let''s take a look at an example made
    up of three managers and ten workers. The first basic operation is listing nodes,
    with `docker node ls` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Cluster management](images/image_05_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: You can reference to the nodes by calling them either by their hostname (**manager1**)
    or by their ID (**ctv03nq6cjmbkc4v1tc644fsi**). The other columns in this list
    statement describes the properties of the cluster nodes.
  prefs: []
  type: TYPE_NORMAL
- en: '**STATUS** is about the physical reachability of the node. If the node is up,
    it''s Ready, otherwise it''s Down.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AVAILABILITY** is the node availability. A node state can either be Active
    (participating in the cluster operations), Pause (in standby, suspended, not accepting
    tasks), or Drain (waiting to be evacuated its tasks).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MANAGER STATUS** is the current status of manager. If a node is not the manager,
    this field will be empty. If a node is manager, this field can either be Reachable
    (one of the managers present to guarantee high availability) or Leader (the host
    leading all operations).![Cluster management](images/image_05_006.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nodes operations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `docker node` command comes with a few possible options.
  prefs: []
  type: TYPE_NORMAL
- en: '![Nodes operations](images/image_05_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As you see, you have all the possible commands for nodes management, but `create`.
    We are often asked when a create option will be added to the `node` command, but
    there is still no answer.
  prefs: []
  type: TYPE_NORMAL
- en: So far, create new nodes is a manual operation and the responsibility of cluster
    operators.
  prefs: []
  type: TYPE_NORMAL
- en: Demotion and promotion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Promotion is possible for worker nodes (transforming them into managers), while
    demotion is possible for manager nodes (transforming them into workers).
  prefs: []
  type: TYPE_NORMAL
- en: Always remember the table to guarantee high availability when managing the a
    lot of managers and workers (odd number, more than or equal to three).
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the following syntax to `promote worker0` and `worker1` to managers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: There is nothing magical behind the curtain. Just, Swarm attempts to change
    the node role with on-the-fly instructions.
  prefs: []
  type: TYPE_NORMAL
- en: '![Demotion and promotion](images/image_05_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Demote is the same (docker node demote **worker1**). But be careful to avoid
    accidentally demoting the node you're working from, otherwise you'll get locked
    out.
  prefs: []
  type: TYPE_NORMAL
- en: And finally, what happens if you try to demote a Leader manager? In this case,
    the Raft algorithm will start an election and a new leader will be selected among
    the active managers.
  prefs: []
  type: TYPE_NORMAL
- en: Tagging nodes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You may have noticed, in the preceding screenshot, that **worker9** is in **Drain**
    availability. This means that the node is in the process of evacuating its tasks
    (if any), which will be rescheduled somewhere else on the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can change node availability by updating its status, using `docker node
    update` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Tagging nodes](images/image_05_009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The availability option can be either `active`, `pause`, or `drain`. Here we
    just restored **worker9** to the active state.
  prefs: []
  type: TYPE_NORMAL
- en: The `active` state means that the node is running and ready to accept tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `pause` state means that the node is running, but not accepting tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `drain` state means that the node is running and not accepting tasks, but
    its currently draining its tasks that are getting rescheduled somewhere else
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another powerful update argument is about labels. There are `--label-add` and
    `--label-rm` that allow us to add labels to Swarm nodes, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Docker Swarm labels do not affect the Engine labels. It's possible to specify
    labels when starting the Docker Engine (`dockerd [...] --label "staging" --label
    "dev" [...]`). But Swarm has no power to edit or change them. Labels we see here
    only affect the Swarm behavior.
  prefs: []
  type: TYPE_NORMAL
- en: 'Labels are useful for categorizing nodes. When you start services, you can
    filter and decide where to physically spawn containers, using labels. For instance,
    if you want to dedicate a bunch of nodes with SSD to host MySQL, you can actually:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Later, when you will start a service with the replica factor, say three, you''ll
    be sure that it will start MySQL containers exactly on worker1, worker2, and worker3,
    if you filter by `node.type`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Remove nodes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Node removal is a delicate operation. It's not just about excluding a node from
    the Swarm, but also about its role and the tasks it's running.
  prefs: []
  type: TYPE_NORMAL
- en: Remove workers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If a worker has the status as Down (for example, because it was physically
    shut down), then it''s currently running nothing, so it can be safely removed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: If a worker is in has the status as Ready, instead, then the previous command
    will raise an error, refusing to remove it. The node availability (Active, Pause
    or Drain) doesn't really matter, because it can still be potentially running tasks
    at the moment, or when resumed.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, in this case an operator must manually drain the node. This means forcing
    it to release its tasks that will be rescheduled and moved to other workers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Once drained, the node can be shutdown and then removed when its status is Down.
  prefs: []
  type: TYPE_NORMAL
- en: Remove managers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Managers can''t be removed. Before removing a manager node, it must be properly
    demoted to worker, eventually drained, and then shut down:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: When a manager has to be removed, another worker node should be identified as
    a new manager and promoted later, in order to maintain an odd number of managers.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Remove with**:  `docker node rm --force`'
  prefs: []
  type: TYPE_NORMAL
- en: The `--force` flag removes a node, no matter what. This option must be used
    very carefully and it's usually the last resort in the presence of stuck nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Swarm health
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Swarm health depends, essentially, on the availability of the nodes in cluster
    and on the reliability of the managers (odd number, available, up).
  prefs: []
  type: TYPE_NORMAL
- en: 'Nodes can be listed with the usual:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'This can use the `--filter` option to filter the output. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'To get details about a specific node, use inspect as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Also, filtering options are available to extract specific data from the output
    JSON:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Outputting the number of cores (one) and the quantity of assigned memory (`1044140032`
    bytes, or 995M).
  prefs: []
  type: TYPE_NORMAL
- en: Backing up the cluster configuration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The important data on managers is stored in `/var/lib/docker/swarm`. Here we
    have:'
  prefs: []
  type: TYPE_NORMAL
- en: The certificates in `certificates/`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Raft status with Etcd logs and snapshots in `raft/`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The tasks database in `worker/`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Other less crucial information, such as the current manager status, the current
    connection socket, and so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It's a good idea to set up a periodical backup of this data, in case recovery
    is needed.
  prefs: []
  type: TYPE_NORMAL
- en: The space used by the Raft log depends on the number of tasks spawned onto the
    cluster and on how frequently their states change. For 200,000 containers, the
    Raft log can grow up to around 1GB of disk space every three hours. A log entry
    of each task occupies around 5 KB. Consequently, the log rotation policies for
    the Raft log directory, `/var/lib/docker/swarm/raft`, should be calibrated more
    or less aggressively, which depends on the available disk space.
  prefs: []
  type: TYPE_NORMAL
- en: Disaster recovery
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If the swarm directory content is lost or corrupted on a manager, it's required
    to immediately remove that manager out of the cluster using the `docker node remove
    nodeID` command (and use `--force` in case it gets stuck temporarily).
  prefs: []
  type: TYPE_NORMAL
- en: The cluster administrator should not start a manager or join it to the cluster
    with an out-of-date swarm directory. Joining the cluster with the out-of-date
    swarm directory brings the cluster to an inconsistent state, as all managers will
    try to synchronize wrong data during the process.
  prefs: []
  type: TYPE_NORMAL
- en: After bringing down the manager with the corrupted directory, it's necessary
    to delete the `/var/lib/docker/swarm/raft/wal` and `/var/lib/docker/swarm/raft/snap`
    directories. Only after this step can the manager safely re-join the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Graphical interfaces for Swarm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At the moment of writing, Swarm mode is so young, that the existing Docker graphical
    user interfaces support is yet to come or is in progress.
  prefs: []
  type: TYPE_NORMAL
- en: Shipyard
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Shipyard** ([https://shipyard-project.com/](https://shipyard-project.com/)),
    which has a nice support for Swarm (v1) operations, is now updated to use Swarm
    mode. At the of writing (August 2016), there is a 1.12 branch on Github, that
    makes this workable.'
  prefs: []
  type: TYPE_NORMAL
- en: At the time this book will be published, probably a stable version will be available
    for automated deployment already. You can take a look at the instructions at [https://shipyard-project.com/docs/deploy/automated/](https://shipyard-project.com/docs/deploy/automated/).
  prefs: []
  type: TYPE_NORMAL
- en: 'It will be something similar to going in SSH to the leader manager host and
    run a one liner, such as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: In case we still need to install a specific non-stable branch, download it from
    Github to the leader manager host and install Docker Compose.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'And finally start with `compose`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: This command will bring up a number of containers, which, in the very end, by
    default expose port `8080` so that you can connect to the public manager IP at
    port `8080` to get into the Shipyard UI.
  prefs: []
  type: TYPE_NORMAL
- en: '![Shipyard](images/image_05_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see in the following screenshot, Docker Swarm features are already
    supported in UI (there are **Services**, **Nodes**, and so on.), and operations,
    such as **Promote**, **D**emote**** , and so on, which we outlined in this chapter
    are available for each node.
  prefs: []
  type: TYPE_NORMAL
- en: '![Shipyard](images/image_05_011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Portainer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An alternative UI supporting Swarm Mode, and our preferred choice,  is **Portainer**
    ([https://github.com/portainer/portainer/](https://github.com/portainer/portainer/)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Deploying it is as easy as starting it as a container on the leader manager:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '![Portainer](images/image_005_012.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The UI has the expected options, including nice list of templates for quickly
    launching containers, such as MySQL or a private registry, Portainer supports
    Swarm services, with `-s` option when launching it.
  prefs: []
  type: TYPE_NORMAL
- en: Portainer, at time of writing, is about to launch the UI authentication feature,
    which is the first step towards full roles based access control, which is due
    in the start of 2017\. Later the RBAC will be extended to support Microsoft Active
    Directory as the directory source. Further, Portainer will also support multi-cluster
    (or multi-host) management by end of 2016\. Additional features being added at
    the start of 2017 are Docker Compose (YAML) support, and private registry management.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we went through the typical Swarm administration procedures
    and options. After showing how to add managers and workers to the cluster, we
    explained, in detail, how to update clusters and node properties, how to check
    the Swarm health and we encountered Shipyard and Portainer as UIs. After this
    we focussed on infrastructure, it's now time to use our Swarms. We'll turn the
    key and put in motion some real application in the next chapter, by creating real
    services and tasks.
  prefs: []
  type: TYPE_NORMAL
