- en: Amdahl's Law
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Often used in discussions revolving around concurrent programs, Amdahl's Law
    explains the theoretical speedup of the execution of a program that can be expected
    when using concurrency. In this chapter, we will discuss the concept of Amdahl's
    Law, and we will analyze its formula, which estimates the potential speedup of
    a program and replicates it in Python code. This chapter will also briefly cover
    the relationship between Amdahl's Law and the law of diminishing returns.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Amdahl's Law
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Amdahl''s Law: its formula and interpretation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The relationship between Amdahl's Law and the law of diminishing returns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simulation in Python, and the practical applications of Amdahl's Law
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following is a list of prerequisites for this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Ensure that you have Python 3 installed on your computer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Download the GitHub repository at [https://github.com/PacktPublishing/Mastering-Concurrency-in-Python](https://github.com/PacktPublishing/Mastering-Concurrency-in-Python)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: During this chapter, we will be working with the subfolder named `Chapter02`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Check out the following video to see the Code in Action: [http://bit.ly/2DWaOeQ](http://bit.ly/2DWaOeQ)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Amdahl's Law
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'How do you find a balance between parallelizing a sequential program (by increasing
    the number of processors) and optimizing the execution speed of the sequential
    program itself? For example, which is the better option: Having four processors
    running a given program for 40% of its execution, or using only two processors
    executing the same program, but for twice as long? This type of trade-off, which
    is commonly found in concurrent programming, can be strategically analyzed and
    answered by applying Amdahl''s Law.'
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, while concurrency and parallelism can be a powerful tool that
    provides significant improvements in program execution time, they are not a silver
    bullet that can speed up any non-sequential architecture infinitely and unconditionally.
    It is therefore important for developers and programmers to know and understand
    the limits of the speed improvements that concurrency and parallelism offer to
    their programs, and Amdahl's Law addresses those concerns.
  prefs: []
  type: TYPE_NORMAL
- en: Terminology
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Amdahl''s Law provides a mathematical formula that calculates the potential
    improvement in speed of a concurrent program by increasing its resources (specifically,
    the number of available processors). Before we can get into the theory behind
    Amdahl''s Law, first, we must clarify some terminology, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Amdahl's Law solely discusses the potential speedup in latency resulting from
    executing a task in **parallel**. While concurrency is not directly discussed
    here, the results from Amdahl's Law concerning parallelism will nonetheless give
    us an estimation regarding concurrent programs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **speed** of a program denotes the time it takes for the program to execute
    in full. This can be measured in any increment of time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Speedup** is the time that measures the benefit of executing a computation
    in parallel. It is defined as the time it takes a program to execute in serial
    (with one processor), divided by the time it takes to execute in parallel (with
    multiple processors). The formula for speedup is as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](assets/986d7d71-2d13-4c12-96ea-ee3dfe76c212.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding formula, *T(j)* is the time it takes to execute the program
    when using *j* processors.
  prefs: []
  type: TYPE_NORMAL
- en: Formula and interpretation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we get into the formula for Amdahl's Law and its implications, let's
    explore the concept of speedup, through some brief analysis. Let's assume that
    there are *N* workers working on a given job that is fully parallelizable—that
    is, the job can be perfectly divided into *N* equal sections. This means that
    *N* workers working together to complete the job will only take *1/N* of the time
    it takes one worker to complete the same job.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, most computer programs are not 100% parallelizable: some parts of
    a program might be inherently sequential, while others are broken up into parallel
    tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: The formula for Amdahl's Law
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, let *B* denote the fraction of the program that is strictly serial, and
    consider the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*B * T(1)* is the time it takes to execute the parts of the program that are
    inherently sequential.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*T(1) - B * T(1) = (1 - B) * T(1)* is the time it takes to execute the parts
    of the program that are parallelizable, with one processor:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then, *(1 - B) * T(1) / N* is the time it takes to execute these parts with
    *N* processors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, *B * T(1) + (1 - B) * T(1) / N* is the total time it takes to execute the
    whole program with *N* processors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Coming back to the formula for the speedup quantity, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/a34239f2-5c32-498d-b61d-411fbebcf3ee.png)'
  prefs: []
  type: TYPE_IMG
- en: This formula is actually a form of Amdahl's Law, used to estimate the speedup
    in a parallel program.
  prefs: []
  type: TYPE_NORMAL
- en: A quick example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s assume that we have a computer program, and the following applies to
    it:'
  prefs: []
  type: TYPE_NORMAL
- en: 40% of it is subject to parallelism, so *B = 1 - 40% = 0.6*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Its parallelizable parts will be processed by four processors, so *j = 4*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Amdahl''s Law states that the overall speedup of applying the improvement will
    be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/88f39be3-7a83-4262-a1f4-5a0c3ea01537.png)'
  prefs: []
  type: TYPE_IMG
- en: Implications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following is a quote from Gene Amdahl, in 1967:'
  prefs: []
  type: TYPE_NORMAL
- en: '"For over a decade prophets have voiced the contention that the organization
    of a single computer has reached its limits and that truly significantly advances
    can be made only by interconnection of a multiplicity of computers in such a manner
    as to permit cooperative solution... The nature of this overhead (in parallelism)
    appears to be sequential so that it is unlikely to be amenable to parallel processing
    techniques. Overhead alone would then place an upper limit on throughput of five
    to seven times the sequential processing rate, even if the housekeeping were done
    in a separate processor... At any point in time it is difficult to foresee how
    the previous bottlenecks in a sequential computer will be effectively overcome."'
  prefs: []
  type: TYPE_NORMAL
- en: 'Through the quote, Amdahl indicated that whatever concurrent and parallel techniques
    are implemented in a program, the sequential nature of the overhead portion required
    in the program always sets an upper boundary on how much speedup the program will
    gain. This is one of the implications that Amdahl''s Law further suggests. Consider
    the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/e4183f84-6169-4f5d-a160-a49a43684317.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](assets/797bcd1b-a8ef-477e-9246-1fcc3a2f64f1.png) denotes the speedup gained
    from *n* processors'
  prefs: []
  type: TYPE_NORMAL
- en: This shows that, as the number of resources (specifically, the number of available
    processors) increases, the speedup of the execution of the whole task also increases.
    However, this does not mean that we should always implement concurrency and parallelism
    with as many system processors as possible, to achieve the highest performance.
    In fact, from the formula, we can also gather that the speedup achieved from incrementing
    the number of processors decreases. In other words, as we add more processors
    for our concurrent program, we will obtain less and less improvement in execution
    time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, as mentioned previously, another implication that Amdahl''s Law
    suggests concerns the upper limit of the execution time improvement:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/03badc5e-2ea1-479c-b6f7-6b4cc6b93b2f.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](assets/a05c110e-9b5b-4c0b-8e8e-9a22a9168615.png) is the cap of how much
    improvement concurrency and parallelism can offer your program. This is to say
    that, no matter how many available resources your system has, it is impossible
    to obtain a speedup larger than ![](assets/3f0fb7e7-5719-4e70-a7a7-d541c525f98f.png) through
    concurrency, and this limit is dictated by the sequential overhead portion of
    the program (*B* is the fraction of the program that is strictly serial).'
  prefs: []
  type: TYPE_NORMAL
- en: Amdahl's Law's relationship to the law of diminishing returns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Amdahl's Law is often conflated with the law of diminishing returns, which is
    a rather popular concept in economics. However, the law of diminishing returns
    is only a special case of applying Amdahl's Law, depending on the order of improvement.
    If the order of separate tasks in the program is chosen to be improved in an **optimal**
    way, a monotonically decreasing improvement in execution time will be observed,
    demonstrating diminishing returns. An optimal method indicates first applying
    those improvements that will result in the greatest speedups, and leaving those
    improvements yielding smaller speedups for later.
  prefs: []
  type: TYPE_NORMAL
- en: Now, if we were to reverse this sequence for choosing resources, in which we
    improve less optimal components of our program before more optimal components,
    the speedup achieved through the improvement would increase throughout the process.
    Furthermore, it is actually more beneficial for us to implement system improvements
    in this **reverse-optimal** order in reality, as the more optimal components are
    usually more complex, and take more time to improve.
  prefs: []
  type: TYPE_NORMAL
- en: Another similarity between Amdahl's Law and the law of diminishing returns concerns
    the improvement in speedup obtained through adding more processors to a system.
    Specifically, as a new processor is added to the system to process a fixed-size
    task, it will offer less usable computation power than the previous processor.
    As we discussed in the last section, the improvement in this situation strictly
    decreases as the number of processors increases, and the total throughout approaches
    the upper boundary of *1/B*.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that this analysis does not take into account other
    potential bottlenecks, such as memory bandwidth and I/O bandwidth. In fact, if
    these resources do not scale with the number of processors, then simply adding
    processors results in even lower returns.
  prefs: []
  type: TYPE_NORMAL
- en: How to simulate in Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will look at the results of Amdahl's Law through a Python
    program. Still considering the task of determining whether an integer is a prime
    number, as discussed in [Chapter 1](0159c46a-c66b-4ba3-87b5-81dbeb3bcf02.xhtml),
    *Advanced Introduction to Concurrent and Parallel Programming**,* we will see
    what actual speedup is achieved through concurrency. If you already have the code
    for the book downloaded from the GitHub page, we are looking at the `Chapter02/example1.py`
    file.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a refresher, the function that checks for prime numbers is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The next part of the code is a function that takes in an integer that indicates
    the number of processors (workers) that we will be utilizing to concurrently solve
    the problem (in this case, it is used to determine which numbers in a list are
    prime numbers):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice that the variables `sub_start` and `sub_duration` measure the portion
    of the task that is being solved concurrently, which, in our earlier analysis,
    is denoted as *1 - B*. As for the input, we will be looking at numbers between
    *10^(13)* and *10^(13) + 1000*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Lastly, we will be looping from one to the maximum number of processors available
    in our system, and we will pass that number to the preceding `concurrent_solve()`
    function. As a quick tip, to obtain the number of available processors from your
    computer, call `multiprocessing.cpu_count()`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'You can run the whole program by entering the command `python example1.py`.
    Since my laptop has four cores, the following is my output after running the program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'A few things to note are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: First, in each iteration, the subsection of the task was almost as long as the
    whole program. In other words, the concurrent computation formed the majority
    of the program during each iteration. This is quite understandable, since there
    is hardly any other heavy computation in the program, aside from prime checking.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Secondly, and arguably more interestingly, we can see that, while considerable
    improvements were gained after increasing the number of processors from `1` to
    `2` (`7.6659 seconds` to `4.1153 seconds`), hardly any speedup was achieved during
    the third iteration. It took longer during the forth iteration than the third,
    but this was most likely overhead processing. This is consistent with our earlier
    discussions regarding the similarity between Amdahl's Law and the law of diminishing
    returns, when considering the number of processors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can also refer to a speedup curve to visualize this phenomenon. A speedup
    curve is simply a graph with the *x* axis showing the number of processors, compared
    to the *y* axis showing the speedup achieved. In a perfect scenario, where *S
    = j* (that is, the speedup achieved is equal to the number of processors used),
    the speedup curve would be a straight, 45-degree line. Amdahl''s Law shows that
    the speedup curve produced by any program will remain below that line, and will
    begin to flatten out as efficiency reduced. In the preceding program, this was
    during the transition from two to three processors:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](assets/7f945408-80b5-46f9-b34d-1c600d093e8f.png)'
  prefs: []
  type: TYPE_IMG
- en: Speedup curves with different parallel portions
  prefs: []
  type: TYPE_NORMAL
- en: Practical applications of Amdahl's Law
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we have discussed, by analyzing the sequential and parallelizable portion
    of a given program or system with Amdahl's Law, we can determine, or at least
    estimate, the upper limit of any potential improvements in speed resulting from
    parallel computing. Upon obtaining this estimation, we can then make an informed
    decision on whether an improved execution time is worth an increase in processing
    power.
  prefs: []
  type: TYPE_NORMAL
- en: From our examples, we can see that Amdahl's Law is applied when you have a concurrent
    program that is a mixture of both sequentially and executed-in-parallels instructions.
    By performing analysis using Amdahl's Law, we can determine the speedup through
    each incrementation of the number of cores available to perform the execution,
    as well as how close that incrementation is to helping the program achieve the
    best possible speedup from parallelization.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s come back to the initial problem that we raised at the beginning
    of the chapter: the trade-off between an increase in the number of processors
    versus an increase in how long parallelism can be applied. Let''s suppose that
    you are in charge of developing a concurrent program that currently has 40 percent
    of its instructions parallelizable. This means that multiple processors can be
    running simultaneously for 40 percent of the program execution. Now you have been
    tasked with increasing the speed of this program by implementing either of the
    following two choices:'
  prefs: []
  type: TYPE_NORMAL
- en: Having four processors implemented to execute the program instructions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Having two processors implemented, in addition to increasing the parallelizable
    portion of the program to 80 percent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'How can we analytically compare these two choices, in order to determine the
    one that will produce the best speed for our program? Luckily, Amdahl''s Law can
    assist us during this process:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For the first option, the speedup that can be obtained is as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](assets/425163cc-efba-48da-8111-4987fcf0b20f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For the second option, the speedup is as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](assets/ca967f21-ffa6-4f82-b6b4-a88bfb25fd4f.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, the second option (which has fewer processors than the first)
    is actually the better choice to speed up our specific program. This is another
    example of Amdahl's Law, illustrating that sometimes simply increasing the number
    of available processors is, in fact, undesirable in terms of improving the speed
    of a program. Similar trade-offs, with potentially different specifications, can
    also be analyzed this way.
  prefs: []
  type: TYPE_NORMAL
- en: As a final note, it is important for us to know that, while Amdahl's Law offers
    an estimation of potential speedup in an unambiguous way, the law itself makes
    a number of underlying assumptions and does not take into account some potentially
    important factors, such as the overhead of parallelism or the speed of memory.
    For this reason, the formula of Amdahl's Law simplifies various considerations
    that might be common in practice.
  prefs: []
  type: TYPE_NORMAL
- en: So, how should programmers of concurrent programs think about and use Amdahl's
    Law? We should keep in mind that the results of Amdahl's Law are simply estimates
    that can provide us with an idea about where, and by how much, we can further
    optimize a concurrent system, specifically by increasing the number of available
    processors. In the end, only actual measurements can precisely answer our questions
    about how much speedup our concurrent programs will achieve in practice. With
    that said, Amdahl's Law can still help us to effectively identify good theoretical
    strategies for improving computing speed using concurrency and parallelism.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Amdahl's Law offers us a method to estimate the potential speedup in execution
    time of a task that we can expect from a system when its resources are improved.
    It illustrates that, as the resources of the system are improved, so is the execution
    time. However, the differential speedup when incrementing the resources strictly
    decreases, and the throughput speedup is limited by the sequential overhead of
    its program.
  prefs: []
  type: TYPE_NORMAL
- en: You also saw that in specific situations (namely, when only the number of processors
    increases), Amdahl's Law resembles the law of diminishing returns. Specifically,
    as the number of processors increases, the efficiency gained through the improvement
    decreases, and the speedup curve flattens out.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, this chapter showed that improvement through concurrency and parallelism
    is not always desirable, and detailed specifications are needed for an effective
    and efficient concurrent program.
  prefs: []
  type: TYPE_NORMAL
- en: With more knowledge of the extent to which concurrency can help to speed up
    our programs, we will now start to discuss the specific tools that Python provides
    to implement concurrency. Specifically, we will consider one of the main players
    in concurrent programming, threads, in the next chapter, including their application
    in Python programming.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What is Amdahl's Law? What problem does Amdahl's Law try to solve?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explain the formula of Amdahl's Law, along with its components.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: According to Amdahl's Law, will speedup increase indefinitely as the resources
    of the system improve?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the relationship between Amdahl's Law and the law of diminishing returns?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For more information you can refer to the following links:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Amdahl''s Law* ([https://home.wlu.edu/~whaleyt/classes/parallel/topics/amdahl.html](https://home.wlu.edu/~whaleyt/classes/parallel/topics/amdahl.html)),
    by Aaron Michalove'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Uses and abuses of Amdahl''s Law*, Journal of Computing Sciences in Colleges
    17.2 (2001): 288-293, S. Krishnaprasad'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Learning Concurrency in Python: Build highly efficient, robust, and concurrent
    applications* (2017), Elliot Forbes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
