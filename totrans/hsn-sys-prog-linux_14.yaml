- en: Multithreading with Pthreads Part I - Essentials
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Pthreads进行多线程编程第一部分 - 基础知识
- en: Have you downloaded a large file using a download-accelerator type of application?
    Have you played an online game? A flight simulator program? Used word processing,
    web browsers, Java apps, and so on? (The temptation to put in a smiley emoji here
    is high!)
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 你是否使用过下载加速器类型的应用程序下载过大文件？你玩过在线游戏吗？飞行模拟器程序？使用过文字处理、网页浏览器、Java应用程序等等？（在这里放一个笑脸表情的诱惑很高！）
- en: 'It''s quite likely that you have used at least some of these; so what? All
    of these disparate applications have something in common: it''s highly likely
    that they are all designed for multithreading, meaning that their implementation
    uses multiple threads that run in parallel with each other. Multithreading has
    indeed become almost a way of life for the modern programmer.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 很可能你至少使用过其中一些；那又怎样呢？所有这些不同的应用程序有一个共同点：它们很可能都是为多线程设计的，这意味着它们的实现使用多个线程并行运行。多线程确实已经成为现代程序员几乎是一种生活方式。
- en: Explaining a topic as large as multithreading is itself a big task; hence we
    are dividing the coverage into three separate chapters. This one is the first
    of them.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 解释一个像多线程这样庞大的话题本身就是一项艰巨的任务；因此我们将其分成三个单独的章节进行覆盖。这是其中的第一章。
- en: 'This chapter is itself logically divided into two broad parts: in the first,
    we carefully consider and understand the concepts behind the threading model—the
    what and why of multithreading. What exactly is a thread, why do we want threads,
    and a quick take on how multithreading has evolved on the Linux platform.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章本身在逻辑上分为两个广泛的部分：在第一部分中，我们仔细考虑并理解线程模型背后的概念——多线程的“什么”和“为什么”。线程到底是什么，我们为什么需要线程，以及多线程在Linux平台上是如何发展的一个快速了解。
- en: In the second part, we focus on the thread management APIs—the how (to some
    extent) of multithreading on Linux. The API set required to create and manage
    threads is discussed, with, of course, a lot of practical code to be seen and
    tried out.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二部分中，我们将重点关注Linux上多线程的线程管理API，即多线程的“如何”（在某种程度上）。我们将讨论创建和管理线程所需的API集合，并且当然会有很多实际的代码可以看到和尝试。
- en: At the outset of this topic, we must also clearly point out the fact that in
    this book we are only concerned with multithreading in the context of software
    programming; particularly, the **POSIX threads** (**pthreads**) implementation
    and specifically, pthreads on the Linux platform. We do not attempt to deal with
    various other multithreaded frameworks and implementations that have sprung up
    (such as MPI, OpenMP, OpenCL, and so on) or hardware threading (hyperthreading,
    GPUs with CUDA, and so on).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个话题的开始，我们还必须明确指出这样一个事实，即在本书中，我们只关注软件编程的多线程；特别是在Linux平台上的POSIX线程（pthreads）实现，具体来说是Linux平台上的pthreads。我们不打算处理其他各种出现的多线程框架和实现（如MPI、OpenMP、OpenCL等）或硬件线程（超线程、具有CUDA的GPU等）。
- en: 'In this chapter, you will learn about programming with multiple threads on
    the Linux platform, specifically, getting started with the pthreads programming
    model or framework. This chapter is broadly divided into two parts:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将学习如何在Linux平台上使用多个线程进行编程，具体来说，是如何开始使用pthread编程模型或框架。本章大致分为两部分：
- en: In the first, key multithreading concepts—the what and the why of multithreading
    —are covered, laying the groundwork for the second part (and indeed the two subsequent
    chapters on multithreading).
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第一部分，涵盖了关键的多线程概念——多线程的“什么”和“为什么”，为第二部分（以及后面两章关于多线程的内容）奠定了基础。
- en: The second part covers the essential pthreads APIs required to build a functional
    multithreaded application on Linux (it deliberately does not cover all aspects,
    through; the next two chapters will build on this one).
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二部分涵盖了在Linux上构建功能性多线程应用程序所需的基本pthread API（它故意没有涵盖所有方面；接下来的两章将在此基础上展开）。
- en: Multithreading concepts
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多线程概念
- en: In this section, we'll learn about the what and why of multithreading on the
    Linux platform. We will begin by answering the FAQ, "what exactly is a thread?".
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习在Linux平台上多线程的“什么”和“为什么”。我们将从回答“线程到底是什么？”这个常见问题开始。
- en: What exactly is a thread?
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线程到底是什么？
- en: 'In the good (or bad?) old days, Unix programmers had a straightforward software
    model (which got inherited pretty much exactly by other OSes and vendors): there
    is a process that lives in a **virtual address space** (**VAS**); the VAS essentially
    consists of homogeneous regions (essentially collections of virtual pages) called segments: text,
    data, other mappings (libraries), and stack. The text is really the executable—in
    fact, the machine—code that is fed to the processor. We have certainly covered
    all of this earlier in this book (you can brush up on these basics in [Chapter
    2](976fc2af-8bb4-4060-96cd-3b921682ed75.xhtml), *Virtual Memory*).'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在古老的Unix程序员的好（或坏？）旧日子里，有一个简单的软件模型（其他操作系统和供应商几乎完全继承了这个模型）：有一个存在于虚拟地址空间（VAS）中的进程；VAS本质上由称为段的同质区域（基本上是虚拟页面的集合）组成：文本、数据、其他映射（库）和栈。文本实际上是可执行的——事实上是机器——代码，它被馈送到处理器。我们在本书的早期部分已经涵盖了所有这些内容（你可以在第2章《虚拟内存》中复习这些基础知识）。
- en: A thread is an independent execution (or flow) path within a process. The life
    and scope of a thread, in the familiar procedural programming paradigm we typically
    work with, is simply a function.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 线程是进程内部的独立执行（或流）路径。在线程的生命周期和范围中，在我们通常使用的熟悉的过程式编程范式中，它只是一个函数。
- en: 'So, in the traditional model we mentioned previously, we have a single thread
    of execution; that thread, in the C programming paradigm, is the `main()` function!
    Think about it: the `main()` thread is where execution begins (well, at least
    from the app developer''s viewpoint) and ends. This model is (now) called the single
    threaded software model. As opposed to what? The multithreaded one, of course.
    So, there we have it: it is possible to have more than one thread alive and executing
    concurrently (in parallel) with other independent threads within the same process.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在我们之前提到的传统模型中，我们有一个执行线程；在C编程范式中，该线程是`main()`函数！想想看：`main()`线程是执行开始（至少从应用程序开发者的角度来看）和结束的地方。这个模型现在被称为单线程软件模型。与之相对的是什么？当然是多线程模型。所以，我们可以有多个线程与同一进程中的其他独立线程同时执行（并行）。
- en: 'But, hang on, can''t processes generate parallelism too and have multiple copies
    of themselves working on different aspects of the application? Yes, of course:
    we have covered the `fork(2)` system call in all its considerable glory (and implications)
    in [Chapter 10](607ad988-406d-4736-90a4-3a318672ab6e.xhtml), *Process Creation*.
    This is known as the multiprocessing model. So, if we have multiprocessing – where
    several processes run in parallel and, hey, they get the work done—the million
    dollar question becomes: "why multithreading at all?" (Kindly deposit a million
    dollars and we shall provide the answer.) There are several good reasons; check
    out the upcoming sections (especially *Motivation – why threads?*; we do suggest
    that first-time readers follow the sequence as laid out in this book) for more
    detail.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，等等，进程难道也不能产生并行性，并且在应用程序的不同方面上有多个副本在工作吗？当然可以：我们已经在[第10章](607ad988-406d-4736-90a4-3a318672ab6e.xhtml)中以所有的荣耀（和影响）介绍了`fork(2)`系统调用。这被称为多进程模型。因此，如果我们有多进程——在这里，有几个进程并行运行，并且完成了工作——百万美元的问题就变成了：“为什么还要使用多线程？”（请存入一百万美元，我们将提供答案。）有几个很好的理由；请查看接下来的章节（特别是*动机-为什么要使用线程？*；我们建议第一次读者按照本书中所规定的顺序进行阅读）以获取更多细节。
- en: Resource sharing
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 资源共享
- en: In [Chapter 10](607ad988-406d-4736-90a4-3a318672ab6e.xhtml), *Process Creation*,
    we repeatedly pointed out that although the fork(2) system call is very powerful
    and useful, it's considered to be a heavyweight operation; performing the fork takes
    a lot of CPU cycles (and thus time) and is expensive in terms of memory (RAM),
    too. Computer scientists were looking for a way to lighten this; the result, as
    you have guessed, is the thread.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第10章](607ad988-406d-4736-90a4-3a318672ab6e.xhtml)中，*进程创建*，我们反复指出，虽然fork(2)系统调用非常强大和有用，但它被认为是一种重量级操作；执行fork需要大量的CPU周期（因此需要时间），而且在内存（RAM）方面也很昂贵。计算机科学家们正在寻找一种减轻这种情况的方法；结果，正如你所猜到的那样，就是线程。
- en: 'Hang on, though: for the convenience of the reader, we reproduce a diagram—*The
    Linux process – inheritance and non-inheritance across the fork()*—from [Chapter
    10](607ad988-406d-4736-90a4-3a318672ab6e.xhtml), *Process Creation*:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 不过，为了方便读者，我们在这里重现了一个图表——*Linux进程-在fork()中的继承和非继承*——来自[第10章](607ad988-406d-4736-90a4-3a318672ab6e.xhtml)，*进程创建*：
- en: '![](img/8beb0e4e-7a50-43a7-a15e-f07976f2cf65.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8beb0e4e-7a50-43a7-a15e-f07976f2cf65.png)'
- en: Figure 1: The Linux process – inheritance and non-inheritance across the fork()
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：Linux进程-在fork()中的继承和非继承
- en: 'This diagram is important because it shows us why the fork is a heavy weight
    operation: every time you invoke the fork(2) system call,, the complete VAS of
    the parent process and all the data structures on the right inherited across fork side
    of the diagram have to be  copied into the newly born child process. That is indeed
    a lot of work and memory usage! (Okay, we''re exaggerating a bit: as mentioned
    in [Chapter 10](607ad988-406d-4736-90a4-3a318672ab6e.xhtml), *Process Creation**, *modern
    OSes, especially Linux, do take a lot of pains to optimize the fork. Nevertheless,
    it''s heavy. Check out our example 1 demo program that follows—the creation and
    destruction of a process is much slower (and takes much more RAM) than the creation
    and destruction of a thread.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这个图表很重要，因为它向我们展示了为什么fork是一种重量级操作：每次调用fork(2)系统调用时，父进程的完整虚拟地址空间和图表右侧的所有数据结构都必须被复制到新生的子进程中。这确实是很多工作和内存使用！（好吧，我们有点夸张：正如在[第10章](607ad988-406d-4736-90a4-3a318672ab6e.xhtml)中所提到的，*进程创建*，*现代操作系统，特别是Linux，确实费了很多功夫来优化fork。尽管如此，它还是很重的。请查看我们的示例1演示程序，进程的创建和销毁比线程的创建和销毁要慢得多（并且需要更多的RAM）。
- en: 'The reality is this: when a process creates a thread, the thread shares (almost)
    everything with all other threads of the same process—all of the preceding VAS,
    thus the segments, and all the data structures—except for a stack.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 事实是这样的：当一个进程创建一个线程时，该线程与同一进程的所有其他线程（几乎）共享所有内容——包括之前的虚拟地址空间、段和所有数据结构——除了栈。
- en: 'Every thread has its own private stack segment. Where does it reside? Obviously,
    within the VAS of the creating process; where exactly it resides is really inconsequential
    to us (recall that it''s all virtual memory, in any case, not physical). The question
    that''s a lot more relevant and important to the app developer is how large the
    thread stack will be. The short answer: the same as usual (typically 8 MB on the
    Linux platform), but we shall get to the nitty-gritty details later in this chapter.
    Just think of it this way: the stack of `main()` always resides at the very top
    of the (user mode) virtual address space; the stacks of the remaining threads
    in the process can reside anywhere in this space. Realistically, they typically
    reside in the virtual memory space between the heap and the stack (of main).'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram helps us understand the memory layout of a multithreaded
    process on Linux; in the upper portion of the diagram is the process before `pthread_create(3)`; the
    lower portion shows the process after the thread has been successfully created:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8f2b1600-8814-4675-988c-45739f349fd6.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
- en: Fig 2 : The thread – everything except the stack is shared across pthread_create()
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: The blue squiggle in the process text segment represents the `main()` thread;
    its stack is also clearly seen. We use the dashed lines to indicate that all these
    memory objects (both user and kernel space) are shared across `pthread_create(3)`.
    As can clearly be seen, the only new objects after `pthread_create(3)` are the
    new thread itself (**thrd2**; shown as a red squiggle in the process text segment)
    and a new stack for the just born thread **thrd2** (in red). Contrast this diagram
    with *Fig 1*; when we fork(2), pretty much everything has to be copied into the
    newly born child process.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: 'From what we have described so far, the only difference between a process and
    a thread is that of resource sharing—processes do not share, they copy; threads
    do share everything, except for the stack. Dig a little deeper and you will realize
    that both software and hardware state have to be maintained on a per thread basis.
    The Linux OS does exactly that: it maintains a per-thread task structure within
    the OS; the task structure contains all the process/thread attributes, including
    software and hardware context (CPU register values and so on) information.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: 'Again, digging a little deeper, we realize that the OS does maintain a distinct
    copy of the following attributes per thread: the stack segment (and thus the stack
    pointer), possible alternate signal stack (covered in the [Chapter 11](99fafa09-8972-4d9f-b241-46caf9de98f3.xhtml), *Signaling
    - Part I*), both regular signal and real-time signal masks, thread ID, scheduling
    policy and priority, capability bits, CPU affinity mask, and the errno value (don''t
    worry—several of these will be explained along the way).'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: Multiprocess versus multithreaded
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To help clearly understand why and how threads can provide a performance benefit,
    let''s perform a few experiments! (the importance of being empirical - experimenting,
    trying things out - is a critical feature; our [Chapter 19](b6b41870-c02e-4379-af86-b5e501799c31.xhtml),
    *Troubleshooting and Best Practices*, covers more on such points). First, we take
    two simple example programs: one, a program that compares the creation and destruction
    of processes versus threads, and two, a program that performs matrix multiplication
    in two ways—one via the traditional single threaded process model, and two, via
    the multithreaded model.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: So, what we are really comparing here is the performance in terms of execution
    time between using the multiprocess versus multithreaded model. We will have the
    reader note that, right here and now, we will not be taking pains to detail and
    explain the thread code right now for two reasons; one, it's besides the point,
    and two, until we have covered the thread APIs in some detail, it will not really
    make sense to do so. (So in effect, dear reader, we ask that you ignore the thread
    code for now; just follow along, and build and reproduce what we do here; the
    code and APIs will become clear to you as you learn more.)
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们在这里真正比较的是使用多进程模型和多线程模型的执行时间性能。我们要请读者注意，我们现在不会费力详细解释线程代码的原因有两个：一是这不是重点，二是在我们详细介绍线程API之前，这样做没有意义。（因此，亲爱的读者，我们要求你暂时忽略线程代码；只需跟着我们，构建和重现我们在这里做的事情；随着你的学习，代码和API将变得清晰。）
- en: Example 1 – creation/destruction – process/thread
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例1 - 创建/销毁 - 进程/线程
- en: 'The process model: Here''s what we do: in a loop (that executes a total of
    60,000 times!), create and destroy a process by calling `fork(2)` and subsequently exiting.
    (We take care of details such as clearing any possible zombie by waiting in the
    parent for the child to die before proceeding in the loop.) The relevant code
    is as follows (`ch14/speed_multiprcs_vs_multithrd_simple/create_destroy/fork_test.c`):'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 进程模型：我们的做法是：在一个循环中（总共执行了60,000次！），通过调用`fork(2)`创建和销毁进程，然后退出。（我们处理了一些细节，比如在父进程中等待子进程死亡，以清除任何可能的僵尸进程，然后继续循环。）相关的代码如下（`ch14/speed_multiprcs_vs_multithrd_simple/create_destroy/fork_test.c`）：
- en: 'For readability, only the relevant parts of the code are displayed in the following
    code; to view and run it, the entire source code can be found here: [https://github.com/PacktPublishing/Hands-on-System-Programming-with-Linux](https://github.com/PacktPublishing/Hands-on-System-Programming-with-Linux).'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 为了便于阅读，以下代码中只显示了相关部分；要查看和运行完整的源代码，可以在这里找到：[https://github.com/PacktPublishing/Hands-on-System-Programming-with-Linux](https://github.com/PacktPublishing/Hands-on-System-Programming-with-Linux)。
- en: '[PRE0]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We run it prefixed with the `time(1)` utility, which gives us a rough idea
    of the time taken by the program on the processor; the time spent shows up as
    three components: `real` (total wall-clock time spent), `user` (time spent in
    user space), and `sys` (time spent in kernel space):'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在`time(1)`实用程序的前缀下运行它，这给了我们一个程序在处理器上花费的时间的大致概念；花费的时间显示为三个组成部分：`real`（总的挂钟时间），`user`（用户空间中花费的时间）和`sys`（内核空间中花费的时间）：
- en: '[PRE1]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Obviously, the precise values you get on your Linux box can, and likely will,
    vary. And, no, `user` + `sys` does not add up exactly to real, either.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，你在Linux系统上得到的确切数值可能会有所不同。而且，`user` + `sys`的总和也不会完全等于real。
- en: The multithreading model
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多线程模型
- en: 'Again, here''s what we do: it''s key to understand that the code used here
    (`ch14/speed_multiprcs_vs_multithrd_simple/create_destroy/pthread_test.c`), is
    equivalent in all respects to the previous code except that here we work with
    threads and not processes: in a loop (that executes a total of 60,000 times!),
    create and destroy a thread by calling `pthread_create(3)` and subsequently `pthread_exit(3)`. (We
    take care of details such as waiting in the calling thread for the sibling thread
    to terminate by invoking `pthread_join(3)`.) As mentioned earlier, let''s skip
    the code/API details for now and just see the execution:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，我们的做法是：关键是要理解这里使用的代码（`ch14/speed_multiprcs_vs_multithrd_simple/create_destroy/pthread_test.c`）在所有方面都与前面的代码相同，只是这里我们使用线程而不是进程：在一个循环中（总共执行了60,000次！），通过调用`pthread_create(3)`创建和销毁线程，然后通过调用`pthread_exit(3)`退出。（我们处理了一些细节，比如在调用线程中等待兄弟线程终止，通过调用`pthread_join(3)`。）如前所述，让我们暂时跳过代码/API的细节，只看执行情况：
- en: '[PRE2]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Wow, the threaded code has run approximately 3x faster than the process model
    code! The conclusion is obvious: creating and destroying a thread is much faster
    than creating and destroying a process.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 哇，线程化的代码运行速度大约比进程模型的代码快3倍！结论很明显：创建和销毁线程比创建和销毁进程要快得多。
- en: 'A technical side note: For the more curious geeks: why exactly is the `fork(2)`
    so much slower than `pthread_create(3)`? Those familiar with OS development will
    understand that Linux makes heavy use of the performance-enhancing **copy-on-write**(**COW**)
    memory techniques within its internal implementation of `fork(2)`. Thus, it begs
    the question, if COW is heavily used, then what is slowing the fork down? The
    short answer: page table creation and setup cannot be COW-ed; it takes a while
    to do. When creating threads of the same process, this work (page table setup)
    is completely skipped.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 技术方面的一点说明：对于更好奇的极客：为什么`fork(2)`比`pthread_create(3)`慢得多？熟悉操作系统开发的人会明白，Linux在`fork(2)`的内部实现中大量使用了性能增强的**写时复制**（COW）内存技术。因此，问题是，如果COW被大量使用，那么是什么使fork变慢？简短的答案是：页表的创建和设置不能进行COW；这需要一段时间。当创建同一进程的线程时，这项工作（页表设置）完全被跳过。
- en: Even so, Linux's fork is pretty much considered to be the fastest of any comparable
    OS today.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 即便如此，Linux的fork在今天任何可比较的操作系统中都被认为是最快的。
- en: 'As an aside, a far more accurate way to measure the time spent—and performance
    characteristics in general—is by using the well-known `perf(1)` utility (note
    that in this book, we do not intend to cover `perf` in any detail whatsoever;
    if interested, please look up the *Further reading* section on the GitHub repository
    for some links to perf-related materials):'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，衡量花费的时间和性能特征的一种更准确的方法是使用众所周知的`perf(1)`实用程序（请注意，在本书中，我们不打算详细介绍`perf`；如果感兴趣，请查看GitHub存储库的*进一步阅读*部分，其中有一些与`perf`相关的链接）：
- en: '[PRE3]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: As can be seen in the preceding code, on a virtual machine, current versions
    of `perf` cannot show all the counters; this does not impede us in any way here
    as all we're really after is the final time it took to execute—which is shown
    in the last line of `perf` output.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前面的代码所示，在虚拟机上，当前版本的`perf`不能显示所有的计数器；这在这里并不妨碍我们，因为我们真正关心的是执行所花费的最终时间——这显示在`perf`输出的最后一行中。
- en: 'The following code shows `perf(1)` for the multithreaded app:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码显示了多线程应用程序的`perf(1)`：
- en: '[PRE4]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: For interested readers, we have also provided a wrapper script (`ch14/speed_multiprcs_vs_multithrd_simple/create_destroy/perf_runs.sh`),
    allowing the user to perform a record and report session with `perf(1)`.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 对于感兴趣的读者，我们还提供了一个包装脚本（`ch14/speed_multiprcs_vs_multithrd_simple/create_destroy/perf_runs.sh`），允许用户使用`perf(1)`进行记录和报告会话。
- en: Example 2 – matrix multiplication – process/thread
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例2-矩阵乘法-进程/线程
- en: 'A well-known exercise is to write a program to compute the (dot) product of
    two given matrices. Essentially, we would like to perform the following:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 一个众所周知的练习是编写一个计算两个给定矩阵的（点）积的程序。基本上，我们想执行以下操作：
- en: '`matrix C = matrix A * matrix B`'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '`矩阵C = 矩阵A * 矩阵B`'
- en: 'Again, we emphasize the fact that here, we are not really concerned with the
    details of the algorithm (and code); what concerns us here is how, at a design
    level the matrix multiplication is performed. We propose (and write the corresponding
    code for) two ways:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调的是，我们在这里实际上并不关心算法（和代码）的细节；我们关心的是在设计层面上如何执行矩阵乘法。我们提出（并编写相应的代码）两种方法：
- en: Sequentially, via the single threaded model
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 按顺序，通过单线程模型
- en: In parallel, via the multithreaded model
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 同时，通过多线程模型
- en: 'Note: None of this—the algorithm or code—is purported to be original or ground-breaking
    in any manner; these are well-known programs.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：这些算法或代码都不打算是原创或突破性的；这些都是众所周知的程序。
- en: In the first model, one thread—`main()`, of course—will run and perform the
    computation; the program can be found here: `ch14/speed_multiprcs_vs_multithrd_simple/matrixmul/prcs_matrixmul.c`.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个模型中，一个线程-当然是`main()`-将运行并执行计算；程序可以在这里找到：`ch14/speed_multiprcs_vs_multithrd_simple/matrixmul/prcs_matrixmul.c`。
- en: In the second, we will create at least as many threads as there are CPU cores
    on the target system to take full advantage of the hardware (this aspect is dealt
    with in a later section of this chapter called *How many threads can you create?*);
    each thread will perform a part of the computation, in parallel with the other
    threads. The program can be found here: `ch14/speed_multiprcs_vs_multithrd_simple/matrixmul/thrd_matrixmul.c`.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，我们将在目标系统上创建至少与CPU核心数相同的线程，以充分利用硬件（这个方面在本章的后面一节中处理，名为*你可以创建多少线程？*）；每个线程将与其他线程并行执行一部分计算。程序可以在这里找到：`ch14/speed_multiprcs_vs_multithrd_simple/matrixmul/thrd_matrixmul.c`。
- en: In the multithreaded version, for now, we just hardcode the number of CPU cores
    in our code to four as it matches one of our native Linux test systems.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在多线程版本中，目前，我们只是在代码中硬编码CPU核心数为四，因为它与我们的本机Linux测试系统之一匹配。
- en: To truly appreciate how the process(es) and/or threads of our applications actually
    consume CPU bandwidth, let's use the interesting `gnome-system-monitor` GUI application
    to see resource consumption graphically! (To run it, assuming it's installed,
    just type `$ gnome-system-monitor &`  on the shell).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 为了真正了解我们的应用程序的进程和/或线程如何实际消耗CPU带宽，让我们使用有趣的`gnome-system-monitor` GUI应用程序以图形方式查看资源消耗！（要运行它，假设已安装，只需在shell上键入`$
    gnome-system-monitor＆`）。
- en: We remind you that all software and hardware requirements have been enumerated
    in some detail in the software-hardware list material available on this book's
    GitHub repository.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提醒您，所有软件和硬件要求都已在本书的GitHub存储库上提供的软件硬件清单材料中详细列出。
- en: 'We will perform the experiment as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将按以下方式进行实验：
- en: 'Run the apps on a native Linux box with four CPU cores:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在具有四个CPU核心的本机Linux系统上运行应用程序：
- en: '![](img/e3a13cd1-66d2-48a1-b3ef-527f115b79ef.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: ！[](img/e3a13cd1-66d2-48a1-b3ef-527f115b79ef.png)
- en: 'Look carefully at the preceding (annotated) screenshot (zoom in if you are
    reading the electronic version); we will notice several items of interest:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 仔细看前面的（带注释的）屏幕截图（如果您正在阅读电子版本，请放大）；我们会注意到几个有趣的项目：
- en: 'In the foreground is the terminal window app where we run the `prcs_matrixmul` and
    the `thrd_matrixmul` applications:'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在前台是我们运行`prcs_matrixmul`和`thrd_matrixmul`应用程序的终端窗口应用程序：
- en: We use `perf(1)` to accurately measure the time taken and deliberately filter
    out all output except for the final number of seconds elapsed during execution.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用`perf(1)`来准确测量所花费的时间，并故意过滤除了执行期间经过的最终秒数之外的所有输出。
- en: In the background, you can see the gnome-system-monitor GUI app running.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在背景中，您可以看到正在运行的`gnome-system-monitor` GUI应用程序。
- en: 'The (native Linux) system—the particular one that we have tested this on—has
    four CPU cores:'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: （本机Linux）系统-我们已经在其上进行了测试-有四个CPU核心：
- en: 'One way to find the number of CPU cores on your system is by using the following
    code: `getconf -a | grep _NPROCESSORS_ONLN | awk ''{print $2}''`'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 找到系统上CPU核心数量的一种方法是使用以下代码：`getconf -a | grep _NPROCESSORS_ONLN | awk '{print
    $2}'`
- en: (you can update the `NCORES` macro in the source code `thrd_matrixmul.c`to reflect
    this value)
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: （您可以在源代码`thrd_matrixmul.c`中更新`NCORES`宏以反映此值）
- en: 'The `prcs_matrixmul` app runs first; while it runs, it consumes 100% CPU bandwidth on
    exactly one CPU core out of the four available (it happens to be CPU core #2)'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prcs_matrixmul`应用程序首先运行；当它运行时，它会在四个可用的CPU核心中的一个上消耗100%的CPU带宽（它恰好是CPU核心＃2）'
- en: Notice how, on the middle-to-left of the CPU History meter, the red line representing
    CPU2 shoots up to a 100% (highlighted with a purple ellipse and labeled Process)!
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请注意，在CPU历史记录仪的中间到左侧，代表CPU2的红线飙升到100%（用紫色椭圆标出并标记为进程）！
- en: At the time the screenshot was actually taken (OS on the X-axis timeline; it
    moves from right to left), the CPUs are back to normal levels.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在实际拍摄屏幕截图时（OS在X轴时间线上；它从右向左移动），CPU恢复到正常水平。
- en: 'Next (after a gap of 10 seconds in this particular run), the `thrd_matrixmul` app
    runs; and herein lies the key point: While it runs, it consumes 100% CPU bandwidth on
    all four CPU cores!'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来（在这次运行的间隔为10秒后），`thrd_matrixmul`应用程序运行；这里的关键点在于：当它运行时，它会在所有四个CPU核心上消耗100%的CPU带宽！
- en: Notice at how, approximately just after the 15s marking (read it from right-to-left)
    on the X-axis timeline, all four CPU cores shoot to 100% – that's during the execution
    of `thrd_matrixmul` (highlighted with a red ellipsis and labeled Threads).
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'What does this tell us? Something really important: the underlying Linux OS
    CPU scheduler will try and take advantage of the hardware and, if possible, schedule
    our four application threads to run in parallel on the four CPUs available! Hence,
    we get higher throughput, higher performance, and more bang for our buck.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: Understandably, you might at this point wonder about and have a lot of questions
    on how Linux performs CPU (thread) scheduling; worry not, but please have some
    patience—we shall explore CPU scheduling in some detail in [Chapter 17](36229bac-c402-4d2f-b876-d1eb4aba8051.xhtml),
    *CPU* *Scheduling on Linux*.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: 'Restricted to exactly one CPU:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The `taskset(1)` utility allows one to run a process on a specified set of
    processor core(s). (This ability to associate a process with a given CPU(s) is
    called CPU affinity. We shall come back to this in the chapter on scheduling.)
    Using `taskset` in its basic form is easy: `taskset -c <cpu-mask> <app-to-run-on-given-cpus>`'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see from the following screenshot, we contrast performing a run
    of the `thrd_matrixmul` app on all four CPU cores on the system (in the usual
    way) with running it on exactly one CPU by specifying the CPU mask via `taskset(1)`;
    the screenshot again clearly reveals how, on the former run, all four CPUs are
    pressed into action by the OS (and it takes a total of 8.084s), whereas on the
    latter run only a single CPU (it shows up as CPU3 in green) is employed to execute
    its code (resulting in a total time of 11.189s):'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/61357669-4d7c-4bf5-8ddd-2dcfbd514e28.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
- en: 'Seeing what we have just learned in this section, you might leap to the conclusion,"hey,
    we''ve found the answer: let''s just always use multithreading." But, of course,
    experience tells us that there is no silver bullet. The reality is that although
    threading does indeed offer some real advantages, as with everything in life,
    there are also downsides to it. We shall postpone more discussion on the pros
    and cons in [Chapter 16](4df10c19-b400-4805-8e6e-51a8f43dcfa4.xhtml), *Multithreading
    with Pthreads Part III*; do keep this in mind, though.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: For now, let's do one more experiment to clearly illustrate the fact that not
    just multithreading, but multiprocessing—the use of fork to spawn multiple processes—is
    very helpful as well to gain higher throughput.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: Example 3 – kernel build
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So, one last experiment (for this section): we will build (cross-compile) a
    Linux kernel ver. 4.17 for the ARM Versatile Express platform (with its default
    configuration). The details of the kernel build and so on are out of scope of
    this book, but that''s all right: the key point here is that the kernel build
    is definitely a CPU and RAM intensive operation. Not only that, the modern `make(1)` utility
    is multiprocess capable! One can tell `make` the number of jobs—processes, really—to
    internally spawn (fork) via its `-jn` option switch, where `n` is the number of
    jobs (threads). We use a heuristic (a rule of thumb) to determine this:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '`n = number-of-CPU-cores * 2`'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: (multiply by 1.5 on very high-end systems with a lot of cores.)
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: Knowing this, check out the experiments that follow.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: On a VM with 1 GB RAM, two CPU cores and parallelized make -j4
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We configure the guest VM to have two processors, and proceed with the parallelized
    build (by specifying `make -j4` ):'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The build took a total time of approximately 684 seconds (11.5 min). Just so
    you know, the compressed kernel image for ARM—the one we boot with—is the file
    called `zImage`; the uncompressed kernel image (used only for debug purposes)
    is the `vmlinux` file.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: 'While it was running, doing a quick `ps -LA`during the build indeed reveals
    its multiprocess—not multithreaded—nature:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: On a VM with 1 GB RAM, one CPU core and sequential make -j1
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We configure the guest VM to have only one processor, clean up the build directory,
    and proceed once more, but this time with a sequential build (by specifying `make
    -j1`):'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们配置客户VM只有一个处理器，清理构建目录，然后再次进行，但这次是顺序构建（通过指定`make -j1`）：
- en: '[PRE7]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The build took a total time of approximately 1232 seconds (20.5 min), which
    is nearly twice as long as the previous build!
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 构建总共花费了大约1232秒（20.5分钟），几乎是上一次构建的两倍长！
- en: You might be asking this question: so, if the build with one process took around
    20 minutes and the same build with multiple processes took approximately half
    the time, why use multithreading at all? Multiprocessing seems to be as good!
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会问这个问题：那么，如果使用一个进程构建大约花费了20分钟，而使用多个进程进行相同的构建大约花费了一半的时间，为什么还要使用多线程？多处理似乎也很好！
- en: 'No, please think: our very first example regarding process versus thread creation/destruction taught
    us that spawning (and terminating) processes is much slower than doing the same
    with threads. That is still a key advantage that many applications exploit. After
    all, threads are far more efficient than processes in terms of creation and destruction.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 不，想一想：我们关于进程与线程创建/销毁的第一个例子告诉我们，生成（和终止）进程比使用线程慢得多。这仍然是许多应用程序利用的关键优势。毕竟，线程在创建和销毁方面比进程更有效。
- en: 'In a dynamic, unpredictable environment, where we do not know in advance how
    much work will be required, the use of multithreading to be able to quickly create
    worker threads (and quickly have them terminated) is very important. Think of
    the famous Apache web server: it''s multithreaded by default (via its `mpm_worker`
    module in order to quickly serve client requests). In a similar fashion, the modern NGINX web
    server uses thread pools (more on this for those interested can be found in the *Further
    reading* section on the GitHub repository).'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个动态、不可预测的环境中，我们事先不知道需要多少工作，使用多线程能够快速创建工作线程（并快速终止它们）非常重要。想想著名的Apache网络服务器：它默认是多线程的（通过其mpm_worker模块，以便快速响应客户端请求）。同样，现代的NGINX网络服务器使用线程池（对于感兴趣的人，更多信息可以在GitHub存储库的“进一步阅读”部分找到）。
- en: Motivation – why threads?
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 动机 - 为什么要使用线程？
- en: Threading does indeed offer a number of useful advantages; here, we attempt
    to enumerate some of the more important ones. We think of this in terms of motivation
    for the application architect to make use of multithreading because of potential
    advantages to be gained. We divide this discussion into two areas: design and performance.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 线程确实提供了许多有用的优势；在这里，我们试图列举一些更重要的优势。我们认为这是对应用架构师使用多线程的动机，因为可能获得的优势。我们将这个讨论分为两个方面：设计和性能。
- en: Design motivation
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设计动机
- en: 'In terms of design, we take into account the following:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在设计方面，我们考虑以下内容：
- en: Taking advantage of potential parallelism
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利用潜在的并行性
- en: Many real-world applications will benefit from designing them in such a manner
    that the work can be split into distinct units, and these units or work parcels
    can run in parallel—concurrently—with each other. At the implementation level,
    we can use threads to implement the work parcels.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 许多现实世界的应用程序将受益于以这样的方式设计它们，使得工作可以分成不同的单元，并且这些单元或工作包可以并行 - 与彼此同时运行。在实现层面，我们可以使用线程来实现工作包。
- en: As an example, a download accelerator program exploits the network by having
    several threads perform network I/O. Each thread is assigned work to download
    only a portion of the file; they all run in parallel, effectively gaining more
    network bandwidth than a single thread could, and when done, the destination file
    is stitched together.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，下载加速器程序通过让几个线程执行网络I/O来利用网络。每个线程被分配下载文件的一部分的工作；它们都并行运行，有效地获得比单个线程更多的网络带宽，完成后，目标文件被拼接在一起。
- en: Many such examples abound; recognizing the potential for parallelism is an important
    part of the architect's job.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多这样的例子；认识到并行性的潜力是架构师工作的重要部分。
- en: Logical separation
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 逻辑分离
- en: The threading model intuitively lends itself to letting the designer logically
    separate work. For example, a GUI frontend application might have a few threads
    managing the GUI state, waiting for and reacting to user input, and so on. Other
    threads could be used to handle the app's business logic. Not mixing the **user
    interface** (**UI**) with the business logic is a key element of good design.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 线程模型直观地适合让设计者逻辑上分离工作。例如，GUI前端应用程序可能有几个线程管理GUI状态，等待并响应用户输入等。其他线程可以用于处理应用程序的业务逻辑。不将用户界面（UI）与业务逻辑混合在一起是良好设计的关键要素。
- en: Overlapping CPU with I/O
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CPU与I/O重叠
- en: This point is similar in fashion to the previous one—the logical separation
    of tasks. In the context of what we're discussing, CPU refers to software that
    is CPU-intensive or CPU-bound (the canonical example being the `while (1)`; piece
    of C code); I/O refers to software that is in a blocked state—we say that it is
    waiting on I/O, meaning that it is waiting on some other operation to complete
    (perhaps a file or network read, or any blocking API, in fact) before it can move
    forward; this is referred to as I/O bound.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这一点与前面的类似——任务的逻辑分离。在我们讨论的背景下，CPU指的是软件是CPU密集型或CPU绑定的（经典的例子是C代码的`while（1）`）；I/O指的是软件处于阻塞状态
    - 我们说它在等待I/O，意味着它在等待某些其他操作完成（也许是文件或网络读取，或者任何阻塞API），然后它才能继续前进；这被称为I/O绑定。
- en: 'So, think of it this way: let''s say we have a series of tasks to perform (with
    no dependencies between them): task A, task B, task C, and task D.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，这样想：假设我们有一系列要执行的任务（它们之间没有依赖关系）：任务A，任务B，任务C和任务D。
- en: Let's also say that task A and task C are highly CPU-bound, whereas task B and task
    D are more I/O-bound. If we use the traditional single threaded approach, then
    of course each task has to be carried out in sequence; so, the process ends up
    waiting—for perhaps a long while—for tasks B and D, thus delaying task C. If,
    on the other hand, we use a multithreaded approach, we can separate the tasks
    as individual threads. Thus, even while the threads for tasks B and D are blocked
    on I/O, the threads for task A and C continue to make progress.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以说，任务A和任务C高度依赖CPU，而任务B和任务D更依赖I/O。如果我们使用传统的单线程方法，那么每个任务都必须按顺序执行；因此，进程最终会等待——也许要等很长时间——等待任务B和D，从而延迟任务C。另一方面，如果我们使用多线程方法，我们可以将任务分开为单独的线程。因此，即使任务B和D的线程在I/O上被阻塞，任务A和C的线程仍然可以取得进展。
- en: This is called overlapping CPU with I/O. Decoupling (and separating out) tasks
    when there is no dependency between them, by using threads, is a design approach
    that is usually worth pursuing. It leads to better application responsiveness.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这被称为CPU与I/O的重叠。在没有依赖关系的情况下，通过使用线程来解耦（和分离）任务，这是一种通常值得追求的设计方法。这会导致更好的应用程序响应能力。
- en: Manager-worker model
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 经理-工人模型
- en: Threads quite easily lend themselves to the familiar manager-worker model; a
    manager thread (often `main()`) creates worker threads on demand (or pools them);
    when work arises, a worker thread handles it. Think of busy web servers.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 线程非常容易适用于熟悉的经理-工人模型；一个经理线程（通常是`main()`）根据需要创建工作线程（或者将它们汇集在一起）；当工作出现时，工作线程处理它。想想繁忙的网络服务器。
- en: IPC becoming simple(r)
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: IPC变得更简单
- en: Performing IPC  between processes takes a learning curve, experience, and just
    a lot of work. With threads belonging to a process, IPC—communication—between
    them is as simple as writing and reading global memory (well, to be honest, it's
    not that simple, as we shall learn when we reach the topics on concurrency and
    synchronization in the next chapter; it's still less work conceptually and literally
    than processing IPC).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在进程之间执行IPC需要学习曲线、经验和大量工作。对于属于一个进程的线程，它们之间的IPC——通信——就像写入和读取全局内存一样简单（说实话，这并不那么简单，当我们在下一章中讨论并发和同步的主题时，我们将了解到，概念上和实际上，这仍然比处理IPC要少得多）。
- en: Performance motivation
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 性能动机
- en: As the two examples in the previous section quite clearly showed us, using multithreading
    can raise application performance significantly; some of the reasons for this
    are mentioned here.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前一节的两个示例清楚地向我们展示的那样，使用多线程可以显著提高应用程序的性能；这其中的一些原因在这里提到。
- en: Creation and destruction
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建和销毁
- en: Preceding example 1  clearly showed us that the time taken for the creation
    and destruction of a thread is far less than that of a process. Many applications
    require that you do this almost constantly. (We shall see that creating and destroying
    threads is programmatically much simpler to do than doing the same with processes.)
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的示例1清楚地表明，创建和销毁线程所需的时间远远少于进程。许多应用程序几乎要求您几乎不断地这样做。（我们将看到，与进程相比，创建和销毁线程在编程上要简单得多。）
- en: Automatically taking advantage of modern hardware
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自动利用现代硬件的优势
- en: 'Preceding example 2 clearly illustrated this point: when running a multithreaded
    app on modern multicore hardware (high-end enterprise class servers can have in
    excess of 700 CPU cores!), the underlying OS will take care of optimally scheduling
    threads onto available CPU cores; the app developers need not concern themselves
    with this. Effectively, the Linux kernel will try and ensure perfect SMP scalability
    whenever possible, which will result in higher throughput and, ultimately, speed
    gains. (Again, dear reader, we''re being optimistic here: the reality is that
    with heavy parallelism and CPU cores also comes the heavy downsides of concurrency
    concerns; we shall discuss all of this in more detail in upcoming chapters.)'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的示例2清楚地说明了这一点：在现代多核硬件上运行多线程应用程序时（高端企业级服务器可以拥有超过700个CPU核心！），底层操作系统将负责将线程优化地调度到可用的CPU核心上；应用程序开发人员不需要关心这一点。实际上，Linux内核将尽可能确保完美的SMP可伸缩性，这将导致更高的吞吐量，最终实现速度增益。（亲爱的读者，我们在这里是乐观的：现实是，随着并行性和CPU核心的增加，也伴随着并发问题的严重缺陷；我们将在接下来的章节中更详细地讨论所有这些。）
- en: Resource sharing
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 资源共享
- en: 'We have already covered this very point in the *Resource sharing *section earlier
    in the beginning portion of this chapter (re-read it, if required). The bottom
    line is this: thread creation is comparatively cheap as opposed to process creation (the
    same goes for destruction). Also, the memory footprint of a thread as opposed
    to a process is much lower. Thus, resource sharing, and the associated performance
    advantages, are obtained.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在本章的开始部分的*资源共享*部分中涵盖了这一点（如果需要，可以重新阅读）。最重要的是：与进程创建相比，线程创建成本较低（销毁也是如此）。此外，与进程相比，线程的内存占用要低得多。因此，可以获得资源共享和相关的性能优势。
- en: Context switching
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 上下文切换
- en: 'Context switching is an unfortunate reality on the OS—it''s meta-work that
    must be done every time the OS switches from running one process to running another
    process (we have voluntary and involuntary context switches). The actual amount
    of time it takes to context switch is highly dependent on the hardware system
    and the software quality of the OS; typically, though, it''s in the region of
    tens of microseconds for x86-based hardware systems. That sounds quite tiny: to
    get an idea of why this is considered important (and indeed wasteful), look at
    the output of running `vmstat 3` on an average Linux desktop computer (`vmstat(1)` is
    a famous utility; used this way, it gives us a nice 10,000-foot view of system
    activity; hey, also try out its modern successor, `dstat(1)`):'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: (Please look up the man page on `vmstat(1)` for a detailed explanation of all
    fields). Preceding under the `system` heading, we have two columns: `in` and `cs`
    (hardware) interrupts and context switches, respectively, that have occurred in
    the last one second. Just look at the numbers (ignore the first output line, though)!
    It's fairly high. This is why it really does matter to system designers.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: 'Context switching between the threads of the same process takes a lot less
    work (and thus time) than between processes (or threads belonging to different
    processes). This makes sense: a good amount of the kernel code can be effectively
    short-circuited when the overall process remains the same. Thus, this becomes
    another advantage of using threads.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: A brief history of threading
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Threads—a sequential flow of control—have been around for a long while now;
    only, they went under the name of processes (reports put this at the time of the
    Berkeley Timesharing System, 1965). Then, by the early 1970s, along came Unix,
    which cemented the process as the combination of a VAS and a sequential flow of
    control. As mentioned earlier, this is now called the single threaded model, as
    of course only a single thread of control—the main function—existed.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: Then, in May 1993, Sun Solaris 2.2 came out with UI threads, and a thread library
    called *libthread*, which exposed the UI API set; in effect, modern threads. Competing
    Unix vendors quickly came up with their own proprietary multithreaded solutions
    (with runtime libraries exposing APIs)—Digital with DECthreads (which was later
    absorbed by Compaq Tru64 Unix and subsequently HP-UX), IBM with AIX, Silicon Graphics
    with IRIX, and so on—each with their own proprietary solution.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: POSIX threads
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Proprietary solutions poses a major problem to the big customer who owns heterogeneous
    hardware and software from several of these vendors; being proprietary, it is
    difficult to get the differing libraries and API sets to talk to each other. It''s
    the usual problem—a lack of interoperability. The good news: in 1995, the IEEE
    formed a separate POSIX committee—IEEE 1003.1c—the **POSIX threads** (**pthreads**) committee,
    to evolve a standardized solution for an API for multithreading.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: 'POSIX: Apparently, the original name of the IEEE body is **Portable Operating
    System Interface for Computing Environments** (**POSICE**). Richard M. Stallman
    (RMS)  suggested shortening the name to **Portable Operating System Interface
    for uniX** (**POSIX**), and that name has stuck.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: So, the bottom line is that pthreads is an API standard; formally, IEEE 1003.1c-1995\.
    The upshot of all of this is that all Unix and Unix-like OS vendors gradually
    built implementations supporting pthreads; so, today (in theory, at least), you
    can write a pthreads multithreaded application and it will run unmodified on any
    pthreads-compliant platform (in practice, expect a bit of porting effort).
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: Pthreads and Linux
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Of course, Linux wanted to be compliant with the POSIX threads standard; but
    who would actually build an implementation (remember, the standard is merely a
    draft specification document; it's not code)? Back in 1996, Xavier Leroy stepped
    up and built Linux's first pthreads implementation—a threading library called Linux
    threads. All considered, it was a good effort, but was not fully compatible with
    the (then brand new) pthreads standard.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: An early effort at resolving problems was called **Next Generation Posix Threads** (**NGPT**).
    At around the same time, Red Hat threw in a team to work on this area as well;
    they called the project **Native Posix Threading Library** (**NPTL**). In the
    best traditions of open source culture, the NGPT developers worked together with
    their counterparts at NPTL and began merging the best features of NGPT into NPTL.
    NGPT development was abandoned sometime in 2003;  by then, the realistic implementation
    of pthreads on Linux—which remains to this day—is NPTL.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: 'More technically: NPTL was entrenched as the superior threading API interface,
    even as features were integrated into the 2.6 Linux kernel (December 2003 onward),
    which helped greatly improve threading performance.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: NPTL implements the 1:1 threading model; this model provides true multithreading
    (user and kernel state) and is also known as the native threads model. Here, we
    do not intend to delve into these internal details; a link has been provided for
    interested readers in the *Further reading *section on the GitHub repository.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: 'One can look up the threading implementation (since glibc 2.3.2) with the following
    code (on a Fedora 28 system):'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Clearly, it's NPTL.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: Thread management – the essential pthread APIs
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this—the second major portion of this first chapter on multithreading—we
    shall now focus on the mechanics: using the pthreads API, how exactly does the
    programmer create and manage threads in an effective fashion? We will explore
    the essential pthreads API interfaces to fulfill this key purpose; this knowledge
    is the building block for writing functional and performance-friendly pthreads
    applications.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: We will take you through the thread life cycle in terms of API sets—creating,
    terminating, joining upon (waiting for), and in general, managing the threads
    of a process. We will also cover thread stack management.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: This, of course, implies that we have a pthreads runtime library installed on
    the Linux system. On modern Linux distributions, this will certainly be the case;
    it's only if you are using a rather exotic embedded Linux that you will have to
    verify this. The name of the pthreads library on the Linux platform is libpthread.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: 'A couple of key points regarding the pthread APIs are as follows:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: All pthread APIs require the  `<pthread.h>` header file to be included in the
    source.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The API often uses the object-oriented concepts of data hiding and data abstraction;
    many data types are internal typedefs; this design is deliberate: we want portable
    code. Thus, the programmer must not assume types and must work with the provided
    helper methods where applicable to access and/or query data types. (Of course,
    the code itself is the usual procedural C; nevertheless, many concepts are modeled
    around object orientation. Interestingly, the Linux kernel also follows this approach.)'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thread creation
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The pthreads API for creating a thread is `pthread_create(3)`; its signature
    is as follows:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: When compiling pthread applications, it's very important to specify the `-pthread`
    `gcc` option switch (it enables required macros for using the libpthread library(more
    on this to follow).
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '`pthread_create` is the API to invoke to create a new thread within the calling
    process. On success, the new thread will be running concurrently (in parallel)
    with other threads that may be alive in that process at that point in time; but
    what code will it be running? It will start by running the code of the `start_routine`
    function (the third parameter to this API: a pointer to the function). Of course,
    this `thread` function can subsequently make any number of function calls.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: The new thread's thread ID will be stored in the opaque data item `thread`—the
    first parameter (it's a value-result style parameter). Its data type, `pthread_t`
    is deliberately opaque; we must not assume that it's an integer (or any such thing).
    We shall soon come across when and how we use the thread ID.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: Notice that the third parameter, the function pointer—the routine run by the
    new thread—itself receives a void* parameter—a generic pointer. This is a common
    and helpful programming technique, enabling us to pass absolutely any value(s)
    to the newly created thread. (This kind of parameter is often referred to as client
    data or tag in the literature.) How do we pass it? Via the fourth parameter to `pthread_create(3)`, `arg`.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: 'The second parameter to `pthread_create(3)` is a thread attribute structure;
    here, the programmer should pass the attributes of the thread being created (we
    shall discuss some of them shortly). There is a shortcut: passing `NULL` here
    implies that the library should use the default attributes when creating a thread.
    However, the defaults on a certain Unix might differ substantially from those
    on a different Unix or Linux; writing portable code implies one does not assume
    any defaults, but rather explicitly initializes a thread with attributes that
    are correct for the application. Thus, our recommendation would definitely be
    to not pass `NULL`, but to explicitly initialize a `pthread_attr_t` structure
    and pass it along (the code examples that follow will illustrate this).'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the return value to `pthread_create(3)` is `0` on success and non-zero
    on failure; `errno` is set to a few values as appropriate (we refer you to the
    man page on `pthread_create(3)` for these details).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: 'When a new thread is created, it inherits certain attributes from its creating
    thread; these include the following:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: The creating thread's capability sets (recall our discussion in [Chapter 8](b4538277-87f0-46f1-83fa-632fa470bfd7.xhtml),
    *Process Capabilities*); this is Linux-specific
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The creating thread's CPU affinity mask; this is Linux-specific
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The signal mask
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Any pending signals and pending timers (alarms) in the new thread are cleared. CPU
    execution times will be reset as well for the new thread.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: Just so you know, on the Linux libpthreads implementation, `pthread_create(3)` calls
    the `clone(2)` system call, which, within the kernel, actually creates the thread.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: Interestingly, modern glibc's fork implementation also invokes the `clone(2)` system
    call. Flags passed to `clone(2)` determine how resource sharing is done.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: 'It''s about time that we did some coding! We will write a really simple (and
    actually quite buggy!)  `hello, world.` for pthreads application (`ch14/pthreads1.c`):'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: As you can see, we loop three times, and on each loop iteration we create a
    thread. Notice the third parameter to the `pthread_create(3)`—a function pointer
    (just providing the name of the function is sufficient; the compiler will figure
    the rest); this is the the thread's work routine. Here, it's the function `worker`.
    We also pass the fourth parameter to `pthread_create`—recall that's it's the client
    data, any data you would like to pass to the newly created thread; here, we pass
    the loop index `i` (of course, we appropriately typecast it so that the compiler
    won't complain).
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `worker` function, worker, we gain access to the client data (received
    as the formal parameter `data`) by again type-casting the `void *` back to its
    original type, `long`:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '`long datum = (long)data;`'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: We then merely emit a couple of printf's to show that, yes, we are here indeed.
    Notice how all the worker threads run the same code—the `worker` function. This
    is entirely acceptable; recall that code (text) is read-execute in terms of page
    permissions; running text in parallel is not only all right, but it's often desirable
    (providing high throughput).
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: 'To build it, we have provided the Makefile; note, though, that all the pthreads APIs
    aren''t linked in by default, like glibc. No, they are, of course, in libpthread, which
    we shall have to both explicitly compile (to our source files) and link in to
    our binary executable via the `-pthread` directive. The following snippet from
    the Makefile shows this being done:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Building it now works, but—and please note this carefully—the program does not work
    well at all! In the following code, we perform some test runs by looping around `./pthreads1`:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: As you can see the `hello, world.` message only appears intermittently and not
    at all in trial runs 4 and 5 (of course, the output you see when you try this
    out can certainly vary due to timing issues).
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: 'Why is it like this? Simple: we have inadvertently set up a buggy situation—a race!
    Where exactly? Look at the code again, carefully: what does the `main()` function
    do once the loop is done? It calls `exit(3)`; thus the entire process terminates,
    not just the main thread! And who is to say that the worker threads completed
    their work before this occurred? Ah—that, ladies and gentlemen, is your classic
    race.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: So, how do we fix it? For now, we shall just perform a couple of quick fixes;
    the proper way to avoid racy code is via synchronization; this is a big topic
    and deserves a chapter by itself (as you shall see). Okay, first, let's fix the
    problem of the main thread prematurely exiting.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: Termination
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `exit(3)` library API causes the calling process—along with all of its
    threads – to terminate. If you would like a single thread to terminate, have it
    invoke the `pthread_exit(3)` API instead:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This parameter specifies the exit status of the calling thread; for the time
    being, we ignore it and just pass `NULL` (we shall examine using this parameter
    shortly).
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: So, back to our racy app (`ch14/pthreads1.c`); let's make a second, better version
    (`ch14/pthreads2.c`). The problem, really, with our first version was the race—the main thread
    calls `exit(3)`, causing the entire process to die, probably before the worker
    threads got a chance to complete their work. So, let's fix this by having `main()`
    call `pthread_exit(3)`! Also, why not have our thread worker function terminate
    properly by explicitly invoking the `pthread_exit(3)` as well?
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the modified code snippets for the `worker()` and `main()` functions
    (`ch14/pthreads2.c`):'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Let''s try out the preceding program:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: That's much better!
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: The return of the ghost
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There is still a hidden problem. Let''s do some more experimentation: let''s
    write a third version of this program (let''s call it `ch14/pthreads3.c`). In
    it, we say, what if the worker threads take longer to perform their work (than
    they are currently taking)? We can easily simulate this with a simple `sleep(3)` function,
    which is going to be introduced into the worker routine:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Let''s try it out:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Well? It looks just fine. Is it really? There's just one more quick and minor
    modification that has to be done; increase the sleep time from 3 seconds to, say,
    30 seconds, and rebuild and retry (the only reason we do this is to give the end
    user a chance to type a `ps(1)` command, as shown in the following screenshot,
    before the app dies). Now, run it in the background , and take a closer look!
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/142c88d2-a84e-4026-855c-0fe06c95b0a1.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
- en: 'Check out the preceding screenshot: we run the `pthreads3` app in the background;
    the app (well, the main thread of the app) creates an additional three threads.
    The threads merely block by going to sleep for thirty seconds each. As we ran
    the process in the background, we get control on the shell process; now we run `ps(1)`with
    the `-LA` option switches. From the man page on `ps(1)`:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: '`-A`: Select all processes; identical to `-e`'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`-L`: Show threads, possibly with LWP and NLWP columns'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'All right! (GNU) `ps(1)` can even show us every thread alive by making use
    of the `-L` option switch (try out `ps H` too). With the `-L` switch, the first
    column in the output of `ps` is the PID of the process (quite familiar to us);
    the second column is the thread **Light Weight Process** (**LWP**); in effect,
    this is the PID of the individual thread as seen by the kernel. Interesting. Not
    just that, look at the numbers carefully: where the PID and LWP match, it''s the `main()`
    thread of the process; where the PID and LWP differ, it tells us that this is
    a child, or more correctly just a peer thread, belonging to the process; the LWP
    is the thread PID as seen by the OS. So, in our sample run, we have the process
    PID of 3906, along with four threads: the first one is the `main()` thread (as
    its PID == its LWP value), while the remaining three have the same PID—proving
    they belong to the same overall process, but their individual thread PIDs (their
    LWPs) are unique – 3907, 3908, and 3909!'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: The problem we have been referring to, though, is that in the first line—which
    represents the main thread—of the `ps` output is that the process name is followed
    by the phrase
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: '`<defunct>` (on the extreme right). The alert reader will remember that defunct is
    another term for zombie! Yes indeed, the infamous zombie has returned to haunt
    us.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: 'The main thread, by invoking `pthread_exit(3)` (recall the code of main in
    `ch14/pthreads3.c`), has exited before the other threads in the process; the Linux
    kernel thus marks it as a zombie. As we learned in [Chapter 10](607ad988-406d-4736-90a4-3a318672ab6e.xhtml), *Process
    Creation*, zombies are undesirable entities; we really do not want a zombie hanging
    around (wasting resources). So, the question, of course, is how do we prevent
    the main thread from becoming a zombie? The answer is straightforward: do not allow
    the main thread to terminate before the other threads in the application; in other
    words, the recommendation is to always keep `main()` alive, waiting for all the
    other threads to die, before it itself terminates (and thus the process terminates).
    How? Read on.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: 'Again, it goes without saying (but we shall say it!): the process remains alive
    as long as at least one thread within it remains alive.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: As a quick aside, when will the worker threads run with respect to each other
    and main? In other words, is it guaranteed that the first thread created will
    run first, followed by the second thread, then the third, and so on?
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: 'The short answer: no, there is no such guarantee. Especially on modern **Symmetric
    Multiprocessor** (**SMP**) hardware and a modern multiprocess-and-multithreaded-capable
    OS such as Linux, the actual order at runtime is indeterminate (which is a fancy
    way of saying it can''t be known). In reality, it''s up to the OS scheduler to
    make these decisions (that is, in the absence of real-time scheduling policies
    and thread priorities; we shall tackle these topics later in this book).'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: 'Another trial run of our `./pthreads2` sample program reveals this very case:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Can you see what happened? The order shown in the preceding code is: `thread
    #0`, followed by `thread #2`, followed by `thread #1`! It''s unpredictable. Do
    not assume any specific order of execution when designing your multithreaded applications.
    (We shall cover synchronization in a later chapter, which teaches us how to achieve
    the order we require.)'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: So many ways to die
  id: totrans-219
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'How can a thread terminate? It turns out there are several ways:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: Explicitly, by invoking `pthread_exit(3)`.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implicitly, by returning from the thread function; the return value is implicitly
    passed (as though via `pthread_exit` parameter).
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implicitly, by falling off the thread function; that is, hitting the close brace
    `}`; note however that this is not recommended (a later discussion will show you
    why)
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Any thread invoking the `exit(3)` API will, of course, cause the entire process,
    along with all threads in it, to die.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The thread gets canceled (which we will cover later).
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How many threads is too many?
  id: totrans-226
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So, by now, we know how to create an application process with a few threads
    executing within it. We will repeat a code snippet from our very first demo program,
    `ch14/pthreads1.c`, as follows:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Clearly, the process—well, we really mean the main thread of the process (or
    application)—goes in a loop, and each loop iteration creates a thread. So, when
    it's done, we will have three threads in addition to the main thread, which is
    a total of four threads, alive in the process.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: 'This is obvious. The point here is this: creating threads is so much simpler
    than creating (child) processes with the `fork(2)`; with fork, we had to carefully
    code it, getting the child to run its code while the parent continues with its
    code path (recall the switch-case construct; take another quick look at our `ch10/fork4.c` code
    example, if you wish to). With `pthread_create(3)`, things have become easy for
    the application programmer – just call the API in a loop—and voila! You get as
    many threads as you like! In the preceding code snippet, imagine tweaking it,
    changing the value of `NTHREADS` from 3 to 300; and just like that, the process
    will produce 300 threads. What if we made `NTHREADS` 3,000? Or 30,000!?'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: 'Thinking about this brings up a couple of pertinent questions: one, how many
    threads can you actually create? And two, how many threads should you create?
    Please, read on.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: How many threads can you create?
  id: totrans-232
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you think about it, there must be some artificial constraint upon the number
    of threads that the underlying OS will allow an application to create; otherwise,
    system resources would get exhausted pretty quickly. In fact, this is not really
    something new; our whole discussion in [Chapter 3](4161d1ed-20ea-4fa5-8947-646055d956cb.xhtml),
    *Resource Limits*, was really about similar things.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: 'With regard to threads (and processes), there are two (direct) limits that
    impact the number of threads that can exist at any given point in time:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: 'Per process resource limits: You will recall from our [Chapter 3](4161d1ed-20ea-4fa5-8947-646055d956cb.xhtml),
    *Resource Limits*, that there are two utilities to look up the currently defined
    resource limits: `ulimit(1)` and `prlimit(1)`, the latter being the modern interface.
    Let''s take a quick look at the resource limit for max user processes; also realize
    that although the word processes is used, you should actually think of these as
    threads:'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Similarly, `prlimit()` shows us the following:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Here, we have shown you how to query the limit via the CLI; to see how to change
    it—both interactively and programmatically with API interfaces – refer to [Chapter
    3](4161d1ed-20ea-4fa5-8947-646055d956cb.xhtml), *Resource Limits.*
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: 'System-wide limits: The Linux OS maintains a system-wide (not per-process)
    limit on the total number of threads that can be alive at any given point in time.
    This value is exposed to the user space via the proc filesystem:'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: So, the thing to understand is that if either of the preceding two limits are
    breached, `pthread_create(3)` (and similarly, the `fork(2)`) will fail (typically
    setting `errno` to the value `EAGAIN` try again; the OS saying, in effect, "I
    cannot do this for you right now, please try again later").
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: 'Can you change these values? Yes, of course, but with the usual caveat—you
    require root (super user) access to do so. (Again, we have discussed these points
    in detail with respect to  in [Chapter 3](4161d1ed-20ea-4fa5-8947-646055d956cb.xhtml),
    *Resource Limits*) Regarding the system-wide limit, you can indeed change it as
    the root. But, hang on, blindly changing system parameters like this without an
    understanding of the impact is a sure way to lose grip on a system! So, let''s
    start by asking ourselves this: the OS sets the `threads-max` limit at boot time;
    what does it base the value on?'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: 'The short answer: it''s directly proportional to the amount of RAM on the system.
    This makes sense: ultimately, memory is the key limiting resource with regard
    to creating threads and processes.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: 'In more detail for our dear OS-level geek readers: kernel code at boot time
    sets the `/proc/sys/kernel/threads-max` value so that thread (task) structures
    within the OS can take a maximum of one-eighth of available RAM. (The threads-max minimum
    value is 20; the maximum value is the constant `FUTEX_TID_MASK 0x3fffffff`.)'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: Also, by default, the per-process resource limit for the maximum number of threads
    is half of the system limit.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: 'As seen from the preceding code, the value we obtained was 126,446; this was
    done on a native Linux laptop with 16 GB of RAM. Running the same commands on
    a guest VM with 1 GB of RAM yields the following results:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Setting the `threads-max`kernel tunable to too high a value – beyond `FUTEX_TID_MASK`
    – will cause it to be brought down to that value (but, of course, that is almost
    certainly too large in any case). But even within limits, you can stray too far,
    causing the system to become vulnerable (to **denial-of-service** (**DoS**) attacks,
    perhaps!). On an embedded Linux system, lowering the limit might actually help
    by constraining the system.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: Code example – creating any number of threads
  id: totrans-250
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So, let''s put it to the test: we will write a simple extension of our previous
    program, this time allowing the user to specify the number of threads to attempt
    to create within the process as the parameter (`ch14/cr8_so_many_threads.c`).
    The main function is as follows:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'It''s quite simple: we convert the string value the user passed as the first
    parameter to a numeric one with `numthrds`; we then have main loop `numthrds` times,
    invoking `pthread_create(3)` and thus creating a brand new thread upon each loop
    iteration! Once created, what do the new threads do? It''s clear – they execute
    the code of the `worker` function. Let''s take a look:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Again, this is very simple: the worker threads just emit a `printf(3)`—which
    is useful because they print out their thread number—it''s just the loop index
    of course. Then, they go to sleep via the `pause(2)` system call. (This system
    call is useful: it''s a perfect blocking call; it puts the calling thread to sleep
    until a signal arrives.)'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: 'All right, let''s try it out:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: It works (notice that we've truncated the output as there would be far too much
    to show in this book). Notice how the order in which the threads come alive and
    execute (emitting their `printf`) is random. We can see that the last thread we
    created is the one highlighted in bold—thread `# 299` (0 to 299 is 300 threads).
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s run it again, but this time ask it to create an impossibly large
    number of threads (we are currently trying this out on a guest VM with 1 GB of
    RAM):'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Obviously, again, the results that you will see will depend on your system;
    we encourage the reader to try it out on different systems. Also, it's possible
    that the actual failure message may have appeared somewhere higher up in your
    Terminal window; scroll up to find it!
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: The name of the thread, as shown by `ps(1)`, and so on, can be set via the `pthread_setname_np(3)` API;
    note that the `np` suffix implies that the API is non-portable (Linux-only).
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: How many threads should one create?
  id: totrans-264
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The number of threads you create really does depend on the nature of the application. For
    our discussion here, we will consider which the application tends to be – CPU
    versus IO bound.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: 'Earlier in this chapter (specifically within the sections on *Design Motivation*
    and* Overlapping CPU with I/O*), we mentioned the fact that a thread, in terms
    of its execution behavior, falls somewhere on a continuum, somewhere between two
    extremes: one extreme being a completely CPU-bound task and the other extreme
    being a completely I/O-bound task. The continuum may be visualized like this:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4d003f8d-0633-4386-a8b0-d06b42ae6701.png)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
- en: 'Fig 3: The CPU-bound/IO-bound continuum'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: A thread that is a 100% CPU-bound will be continually hammering away on the
    CPU; a 100% I/O-bound thread is one that is always in a blocking (or wait) state,
    never executing on CPU. Both extremes are unrealistic in real applications; however,
    it's quite easy to visualize the domains where they tend to have one of these.
    For example, domains that involve heavy mathematical processing (scientific models,
    vector graphics such as flash animations in a web browser, matrix multiplication,
    and so on), (un)compression utilities, multimedia codecs, and so on will certainly
    tend to be more CPU-bound. On the other hand, many (but not all) applications
    that us humans interact with on a daily basis (think of your email client, web
    browser, word processing, and so on) tend to wait for the human to do something;
    in effect, they tend to be I/O-bound.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore—a bit simplistically, but nevertheless—this serves as a useful design
    rule of thumb: if the application being designed is I/O-bound in nature, then
    creating even a large-ish number of threads that just wait for work is all right;
    this is because they will be asleep the majority of the time, thus not placing
    any strain on the CPU(s) (of course, create too many threads and they  do strain
    memory.)'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, if the application is determined to be highly CPU-bound,
    then creating a large number of threads will stress the system (and end up causing
    thrashing – a phenomenon wherein the meta-work takes longer than the actual work!).
    Thus, for CPU-bound workloads, the thumb rule is this:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Note, though, that there do exist CPU cores that do not provide any **hyperthreading**
    (**HT**) features; on cores like this, factor should just remain 1.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: 'Actually, our discussion has been quite simplistic: many real-world applications
    (think of powerful web servers such as Apache and NGINX) will dynamically create
    and adjust the number of threads required based on the exact circumstances, configuration
    presets, and present workload. Nevertheless, the preceding discussion serves as
    a starting point so that you can start thinking about design for multithreaded
    applications.'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: Thread attributes
  id: totrans-275
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In our initial discussion on *Thread Creation* earlier in this chapter, we saw
    the `pthread_create(3)` API; the second parameter is a pointer to the thread attribute structure: `const
    pthread_attr_t *attr`. We mentioned there that passing NULL here, in effect, has
    the library create a thread with default attributes. While that is indeed the
    case, the problem is that, for truly portable applications, this is not good enough.
    Why? Because the default thread attributes actually differ quite widely from implementation
    to implementation. The right way-specify the thread attributes explicitly at thread
    creation time.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: 'Firstly, of course, we need to learn what attributes a pthread has. The following
    table enumerates this:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: '| **Attribute** | **Meaning** | **APIs: **`pthread_attr_[...](3)` | **Values
    Possible** | ***Linux Default*** |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
- en: '| Detach state | Create threads as joinable or detached | `pthread_attr_` `[get&#124;set]detachstate`
    | PTHREAD_CREATE_JOINABLE PTHREAD_CREATE_DETACHED | PTHREAD_CREATE_JOINABLE |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
- en: '| Scheduling/contention scope | Set of threads against which we compete for
    resources (CPU) | `pthread_attr_``[get&#124;set]scope` | PTHREAD_SCOPE_SYSTEM
    PTHREAD_SCOPE_PROCESS | PTHREAD_SCOPE_SYSTEM |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
- en: '| Scheduling/inheritance | Determines whether scheduling attributes are inherited
    implicitly from calling a thread or explicitly from the attr structure | `pthread_attr_``[get&#124;set]inheritsched`
    | PTHREAD_INHERIT_SCHED PTHREAD_EXPLICIT_SCHED | PTHREAD_INHERIT_SCHED |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
- en: '| Scheduling/policy | Determines the scheduling policy of the thread being
    created | `pthread_attr_``[get&#124;set]schedpolicy` | SCHED_FIFO SCHED_RR'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: SCHED_OTHER | SCHED_OTHER |
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: '| Scheduling/priority | Determines the scheduling priority of the thread being created
    | `pthread_attr_``[get&#124;set]schedparam` | struct sched_param holds    int
    sched_priority | 0 (non real-time) |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
- en: '| Stack/guard region | A guard region for the thread''s stack | `pthread_attr_``[get&#124;set]guardsize`
    | Stack guard region size in bytes | 1 page |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
- en: '| Stack/location, size | Query or set the thread''s stack location and size
    | `pthread_attr_` `[get&#124;set]stack``pthread_attr_`'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: '`[get&#124;set]stackaddr``pthread_attr_`'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: '`[get&#124;set]stacksize` | Stack address and/or stack size, in bytes | Thread
    Stack Location: left to the OSThread Stack Size: 8 MB |'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, clearly understanding what exactly many of these attributes
    signify requires further information. Please be patient as we proceed through
    this chapter (and, in fact, this book), as several of these attributes and their
    meanings will become abundantly clear ( details on scheduling will be shown in
    [Chapter 17](36229bac-c402-4d2f-b876-d1eb4aba8051.xhtml), *CPU Scheduling on Linux*).
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: Code example – querying the default thread attributes
  id: totrans-290
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For now, a useful experiment would be to query the default attributes of a
    newly born thread whose attribute structure is specified as NULL (default). How? `pthread_default_getattr_np(3)` will
    do the trick (note though, that again, the `_np` suffix implies that it''s a Linux-only,
    non-portable API):'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Interestingly, as this function depends on the `_GNU_SOURCE` macro being defined,
    we must first define the macro (early in the source); otherwise, the compile triggers
    warnings and possibly fails. (In our code, we thus use `#include "../common.h"`
    first as our *common.h *header defines the `_GNU_SOURCE` macro.)
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: 'Our code example can be found here, within this book''s GitHub repository:
    `ch14/disp_defattr_pthread.c` *. *'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code, we display a trial run on a Fedora x86_64 box running
    the 4.17.12 Linux kernel:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: For readability, only key parts of the source code are displayed; to view the
    complete source code, build and run it, the entire tree is available for cloning
    from GitHub here: [https://github.com/PacktPublishing/Hands-on-System-Programming-with-Linux](https://github.com/PacktPublishing/Hands-on-System-Programming-with-Linux).
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: 'The key function here is shown in the following code (`ch14/disp_defattr_pthread.c`);
    we first query and display the thread attribute structure''s "detached state"
    (these terms will be explained in detail shortly):'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Next, various scheduling attributes are queried and displayed (some details
    covered later  in [Chapter 17](36229bac-c402-4d2f-b876-d1eb4aba8051.xhtml), *CPU
    Scheduling on Linux*):'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Finally, the thread stack attributes are queried and displayed:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: In the preceding code, we put in the `pthread_getattr_default_np(3)` API to
    query the default thread attributes. Its counterpart, the` pthread_setattr_default_np(3)`
    API, allows you to specify what exactly the default thread attributes should be when
    creating a thread, and the second parameter to `pthread_create(3)` is passed as
    NULL. Do see its man page for details.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: 'There is an alternate way to write a similar program: why not create a thread
    with a NULL attribute structure—thus making it default attributes—and then issue
    the `pthread_getattr_np(3)` API to query and display the actual thread attributes?
    We leave this as an exercise to the reader (in fact, the man page on `pthread_attr_init(3)` supplies
    just such a program).'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: Joining
  id: totrans-306
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Imagine an application where a thread (typically, main) has spawned off several
    other worker threads. Each worker thread has a specific job to do; once done,
    it terminates (via `pthread_exit(3)`). How will the creator thread know when a
    worker thread is done (terminated)? Ah, that is precisely where joining comes
    in. With the join, the creator thread can wait for, or block upon, the death (termination)
    of another thread within the process!
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: Does this not sound very much like the `wait(2)` system call that a parent process
    issues to wait for the death of a child? True, but as we shall see shortly, it's
    certainly not identical.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, importantly, the return value from the thread that terminated is passed
    along to the thread that issued the join upon it. This way, it comes to know whether
    the worker succeeded in its task or not (and if not, the failure value can be
    examined to pinpoint the cause of failure):'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: The first parameter to `pthread_join(3)`, `thread,` is the ID of the thread
    to wait for. The moment it terminates, the calling thread will receive, in the
    second parameter (yes, it's a value-result style parameter), the return value
    from the thread that terminated—which, of course is the value passed via its `pthread_exit(3)` call.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: Thus, the join is very helpful; using this construct, you can ensure that a thread
    can block upon the termination of any given thread. Specifically, in the case
    of the main thread, we often use this mechanism to ensure that main waits for
    all other application threads to terminate before it itself terminates (thus preventing
    the zombie we saw earlier). This is considered the right approach.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall that in the earlier section, *The return of the ghost*, we clearly saw
    how the main thread, dying before its counterparts, becomes an inadvertent zombie
    (the `ch14/pthreads3.c` program). A quick example, built upon this previous code,
    will help clarify things. So, let''s enhance that program – we shall now call
    it `ch14/pthreads_joiner1.c` – so that we have the main thread wait for all other
    threads to die by invoking the `pthread_join(3)` API on each of the worker threads,
    and only then itself terminate:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'There are a few things to notice here:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: To perform the join subsequently, we require each thread's ID; hence, we declare
    an array of `pthread_t` (the `tid` variable). Each element will store the corresponding
    thread's ID value.
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Thread attributes:'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Until now, we have not explicitly initialized and made use of a thread attribute
    structure when creating threads. Here, we rectify this shortcoming. `pthread_attr_init(3)` is
    used to initialize (to defaults) an attribute structure.
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Furthermore, we explicitly make the threads joinable by setting up this attribute
    within the structure (via the `pthread_attr_setdetachstate(3)` API).
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once the threads are created, we must destroy the thread attribute structure
    (via the `pthread_attr_destroy(3)` API).
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is key to understand that only threads that have their detach state set as
    joinable can be joined upon. Interestingly, a joinable thread can later be set
    to the detached state (by calling the `pthread_detach(3)` API upon it); there
    is no converse routine.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: 'The code continues; we now show you the thread `worker` function:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Easy: we just have the so-called worker threads sleep for 8 seconds and then
    die; the `pthread_exit(3)`, this time, passes the return status `0` as a parameter.
    In the following code snippet, we continue the code of main:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Here''s the key part: in a loop, the main thread blocks (waits) upon the death
    of each worker thread via the `pthread_join(3)` API; the second (value-result
    style) parameter, in effect, returns the status of the thread that just terminated.
    The usual zero-upon-success convention is followed, thus allowing the main thread
    to figure out whether the worker threads completed their work successfully or
    not.'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s build and run it:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: As the worker threads die, they are picked up, or joined, by the main thread
    via `pthread_join`; not only that, their termination status—return value—can be
    examined.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: Okay, we'll make a copy of the preceding program and call it `ch14/pthreads_joiner2.c`.
    The only change we make is instead of having each worker thread sleep for an identical
    8 seconds, we'll make the sleep time dynamic. We will change the code; for instance,
    this line would be changed:`sleep(slptm);`
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: The new line would read as follows: `sleep(slptm-datum);`
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, `datum` is the value passed to the thread—the loop index. This way, we
    find that the worker threads sleep as follows:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: 'Worker thread #0 sleeps for (8-0) = 8 seconds'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Worker thread #1 sleeps for (8-1) = 7 seconds'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Worker thread #2 sleeps for (8-2) = 6 seconds'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Obviously, worker thread #2 will terminate first; so what? Well, think about
    it: in the meantime, the main thread is looping around `pthread_join`, but in
    the order of thread #0, thread #1, thread #2. Now, thread #0 will die last and thread
    #2 will die first. Will this be an issue?'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s try it out and see:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'What do we notice? In spite of worker thread #2 dying first, worker thread
    #0 gets joined first because, in the code, that is the thread we wait for first!'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: The thread model join and the process model wait
  id: totrans-340
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'By now, you should have begun to realize that although the `pthread_join(3)` and `wait(2)` (and
    family) APIs seem to be very similar, they are certainly not equivalent; several
    differences between them exist and are enumerated in the following table:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: '| **Situation** | **Thread : `pthread_join(3)`** | **Process: `wait[pid](2)`**
    |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
- en: '| Condition | A thread being waited for must have its detached state attribute
    set as joinable, not detached. | None; any child process can (and in fact must)
    be waited upon (recall our *fork rule #7*) |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
- en: '| Hierarchy | None: any thread can join on any other thread; there is no requirement
    of a parent-child relationship. In fact, we do not consider threads to live within
    a strict parent-child hierarchy as processes do; all threads are peers. | A strict parent-child
    hierarchy exists; only a parent can wait for a child process. |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
- en: '| Order | With threads, one is forced to join (wait) upon the particular thread
    specified as the parameter to `pthread_join(3)`. In other words, if there are,
    say, three threads running and main issues the join within an ascending ordered
    loop, then it must wait for the death or thread #1, then thread #2, and then thread
    #3\. If thread #2 terminates earlier, there is no help for it. | With wait, a
    process can wait upon the death (or stoppage) of any child, or specify a particular
    child process to wait for with waitpid. |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
- en: '| Signaling | No signal is sent upon a thread''s death. | Upon a process''s
    death, the kernel sends the `SIGCHLD` signal to the parent process. |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
- en: 'A few other points to note regarding `pthread_join(3)` are as follows:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: You require the thread ID of a thread in order to join upon it; this is deliberately
    done so that we can, in effect, only join the threads of our application process.
    Attempting to join on other threads (like a third-party library thread) would
    be poor design.
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What if the thread we are waiting for (to die) has already died? Then `pthread_join(3)` just
    returns immediately.
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What if a thread tries to join upon itself? This results in failure (with `errno` set
    to `EDEADLK`).
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attempting to have several threads join upon one thread results in undefined
    behavior; avoid this.
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If a thread attempting to join on another thread is cancelled (covered later),
    the target thread remains as it was (joinable).
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Checking for life, timing out
  id: totrans-353
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Sometimes, we might have a situation wherein we want to check whether a particular
    thread is still alive or not; one way to do so is via the `pthread_tryjoin_np(3)` API:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: The first parameter to `pthread_tryjoin_np(3)` is the thread we are attempting
    to join to; (the second parameter, as usual, is the target thread's termination
    status). Notice the try phrase within the API – this typically specifies that
    the call is non-blocking; in other words, we perform a non-blocking join on the
    target thread. If the target thread is alive, then instead of waiting for it to
    die, the API returns immediately with an error: `errno` will be set to `EBUSY` (and
    the man page tells us that this implies the thread had not yet terminated at the
    time of the call).
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: What if we would like to wait (block) upon a target thread's death, but not
    forever? In other words, we would like to wait for a given maximum time period.
    This can be achieved via the `pthread_timedjoin_np(3)` API; the first two parameters
    are the usual ones (the same as with `pthread_join)`, while the third parameter
    specifies the timeout in terms of the absolute time (or what is often called Unix
    time – the number of seconds (and nanoseconds) elapsed since midnight 1 January
    1970—the Epoch!).
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: 'As covered in [Chapter 13](1f621f72-e067-42db-b2eb-b82e20161dec.xhtml), *Timers*, the `timespec` data
    structure is of the following format:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-359
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: That's easy; but how do we specify the time as UNIX time (or time since the
    Epoch)? We refer the reader to the man page on `pthread_timedjoin_np(3)`, which
    gives a simple example of the same (also, we ask you try this API out as an exercise).
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: Another thing I noticed when using the `pthread_timedjoin_np(3)` API: it's possible
    that the join times out and then proceeds to, say, release some resources – like
    performing `free(3)` on a heap buffer—while the worker thread is still alive and
    using it. This is a bug, of course; it also goes to show that you must carefully
    think out and test the design; usually, using a blocking join on all worker threads,
    thus ensuring they have all terminated before freeing up resources, is the right
    approach.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: Again, we remind you that the `_np` suffix to the APIs implies that they are non-portable
    (Linux-only).
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: Join or not?
  id: totrans-363
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A thread that is explicitly set to the detached state cannot be joined upon;
    so, what happens when it dies? Its resources are disposed of by the library.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: 'A thread that is explicitly set to the joinable state (or if joinable is the
    default state) must be joined upon; failure to do so results in a kind of resource
    leakage. So, be careful: if you have created threads to be joinable, then you
    must ensure that the join is performed.'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: Performing a join on other app threads by the main thread is usually considered
    a best practice, since it prevents the zombie thread behavior we saw earlier.
    Also, it's usually important for the creator thread to come to know whether its
    workers successfully performed their job or not, and if not, why not. The join
    makes all of this possible.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: However, it is possible that your application does not want to wait around for
    some worker threads; in this case, ensure that you create them as detached.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: Parameter passing
  id: totrans-368
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Recall the signature of the `pthread_create(3)` API:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: '`int pthread_create(pthread_t *thread, const pthread_attr_t *attr,`'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: '`                    void *(*start_routine) **(void *), void *arg**);`'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: 'The third parameter is the thread function—in effect, the life and scope of
    the newly born thread. It receives a single parameter of type `void *`; this parameter
    to the new born thread is passed via the fourth parameter to `pthread_create`:
    `void *arg`.'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: 'As mentioned earlier, its data type is a generic pointer, precisely so that
    we can, in effect, pass along any data type as a parameter, and then in the thread
    routine, appropriately typecast and use it. Until now, we have come across simple
    use cases of the same – typically, passing along an integer value as the parameter.
    In our very first simple multithreaded app – `ch14/pthreads1.c` – in our main function, we
    did the following:'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-374
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'And, in the thread routine worker, we performed a simple typecast-and-use:'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-376
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'That''s easy, but it does raise a fairly obvious question: in the `pthread_create(3)` API, as
    there seems to be just one placeholder for the `arg` (the parameter) how can you
    pass along more than one data item – several parameters, in effect – to the thread
    routine?'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: Passing a structure as a parameter
  id: totrans-378
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The preceding heading gives it away: we pass a data structure. But how, exactly?
    Allocate memory to a pointer to the data structure, initialize it, and pass the
    pointer typecast as `void *`. (In fact, this is a very common approach that C
    programmers use.) In the thread routine, as usual, typecast and use it.'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: 'To bring clarity, we will try this out ( `ch14/param_passing/struct_as_param.c`):'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
- en: For readability, only key parts of the source code are displayed; to view the
    complete source code, build, and run it, the entire tree is available for cloning
    from GitHub here: [https://github.com/PacktPublishing/Hands-on-System-Programming-with-Linux](https://github.com/PacktPublishing/Hands-on-System-Programming-with-Linux)*.*
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-382
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: As an example, we build our very own airport info data structure, airport, and then
    set up an array (`city_airports`), initializing a few members of it.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: 'In the main function, we declare an array of pointers to the airport structure;
    we know that a pointer by itself has no memory, so in the thread creation loop,
    we allocate memory to each pointer and then initialize it to an airport (via a
    simple `memcpy(3)`):'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-385
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Okay, so we already know that the preceding code is not really optimal; we could
    have just passed the `city_airports[i]` structure pointer as the parameter to
    the thread. For the sake of a pedantic example, making use of our just allocated
    `plocdata[i]` structures, we `memcpy` one structure into another.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, in the `pthread_create(3)` call, we pass the pointer to our data structure
    as the fourth parameter.  This will become the argument to the thread; in the
    thread routine, we declare an `arg` pointer of the same data type and equate it
    to the typecast data pointer we receive:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-388
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: We can then proceed to use `arg` as a pointer to Airport; in the preceding demo
    code, we merely print out the values in the structure. We encourage the reader
    to build and run this code.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
- en: Did you notice the `%.*s` C printf format specifier trick in the preceding code?
    This is done when we want to print a string that is not necessarily NULL-terminated;
    the `%.*s` format specifier allows one to specify the size followed by the string
    pointer. The string will be printed to only size bytes.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: Thread parameters – what not to do
  id: totrans-391
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The really key thing to keep in mind when passing a parameter to a thread routine
    is that you must guarantee that the parameter passed along is thread-safe; essentially,
    that it does not get modified in any manner while a thread (or threads) are using
    it.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
- en: (Thread safety is a crucial aspect of working with threads; we shall revisit
    this point often in upcoming chapters, too).
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
- en: 'To help understand the possible issues clearly, let''s take a couple of typical
    examples. In the first one, we shall (attempt to) pass the loop index as the parameter
    to the newly born thread such as, in main (code: `ch14/pthreads1_wrong.c`):'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-395
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Did you notice!? We have passed the parameter as `&i`. So? Dereferencing it
    correctly in the thread routine should still work, right:'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-397
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Looks okay – let's give it a try!
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-399
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Well, it works. But hang on, try it a few more times—timing coincidences can
    fool you into thinking that all''s well when it''s really not:'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-401
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'There''s a bug! The index value has evaluated to the value 2 twice; why? Think
    carefully: we have passed the loop index by reference – as the pointer to the
    loop variable. Thread 1 comes alive, and looks up its value – so does thread 2,
    as does thread 3\. But wait: isn''t it possible that we have a race here? Isn''t
    it possible that by the time thread 1 runs and looks up the value of the loop
    variable it has already changed underneath it (because, don''t forget, the loop
    is running in main)? That, of course, is precisely what happened in the preceding
    code.'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
- en: In other words, passing the variable by address is unsafe because its value
    could change while it is being read (by the worker threads) as it being simultaneously
    written to (by main); hence, it's not thread-safe and therefore will be buggy
    (racy).
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
- en: 'The solution is actually really simple: do not pass the loop index by address;
    just pass it as a literal value:'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-405
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Now, each worker thread receives a copy of the loop index, thus eliminating
    any race, thus making it safe.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
- en: Now, don't jump to the conclusion that, hey, okay, so we should never pass a
    pointer (an address) as a parameter. Of course you can! Just ensure that it's thread-safe
    – that its value cannot change underneath it while being manipulated by main and
    the other application threads.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
- en: Refer back to the `ch14/struct_as_param.c` code we demonstrated in the previous
    section; we very much pass the thread parameter as a pointer to a structure. Look
    closely: each pointer was separately allocated (via `calloc(3)`) in the main thread
    creation loop. Thus, each worker thread received its own copy of the structure;
    hence, all is safe and it works well.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
- en: An interesting exercise (that we leave to the reader) is to deliberately insert
    a defect into the `struct_as_param` application by using exactly one allocated
    structure (not three) and passing it to each of the worker threads. This time,
    it will be racy and will (eventually) fail.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
- en: Thread stacks
  id: totrans-410
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We understand that whenever a thread is created, it acquires a new, freshly
    allocated piece of memory for its stack. This leads to the understanding that
    (obviously, but we shall state it nevertheless) all local variables declared within
    a thread function will remain private to that thread; this is because they will
    reside in that thread's stack. (Refer back to *Fig 2* in this chapter – the new
    stack of the newly created thread is shown in red). Also, whenever a context switch
    occurs, the **Stack Pointer** (**SP**) register is updated to point to the current
    thread's stack.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
- en: Get and set thread stack size
  id: totrans-412
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Knowing, and being able to change, the size of thread stacks does matter (do
    see the link provided in the *Further reading* section on the GitHub repository,
    which mentions a real-world experience on how setting up a stack that's too small
    for a certain platform caused random and really hard-to-debug failures).
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
- en: 'So, what is the default thread stack size? The answer has already been provided;
    recall the `disp_defattr_pthread` program we ran earlier in this chapter (in the *Code
    example – querying the default thread attributes* section): it shows us that the
    default thread stack size on the (modern NPTL) Linux platform is 8 MB.'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
- en: 'The pthreads API set provides a few routines to set and query the thread stack
    size. One way is as follows:'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-416
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: As we have already used `pthread_attr_getstacksize(3)` in the earlier `disp_defattr_pthread` program,
    we shall refrain from showing its usage once more over here. Setting the thread
    size is easily done with the complementary `pthread_attr_setstacksize(3)` API
    – the second parameter is the required size (in bytes). Note, though, that both
    of these APIs have the phrase `_attr_` in them, implying that the stack size is
    actually set or queried from the thread attribute structure and not a live thread
    itself. This leads us to understand that we can only set or query the stack size
    at the time of creation of the thread by setting up the attribute structure (which
    is, of course, subsequently passed as the second parameter to `pthread_create(3)`).
    Once a thread is created, its stack size cannot be changed. The exception to this
    rule is the stack of the main thread.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
- en: Stack location
  id: totrans-418
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Where in memory (technically, where in the VAS of the given process) does the
    thread stack actually reside? The following points help us in this regard:'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
- en: The stack of the main thread is always situated at the very top of the process
    VAS.
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The stacks of all other threads in the process are located somewhere between
    the process heap segment and the stack of main; the precise location is not known
    in advance to the app developer; in any case, we should not need to know.
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This is not directly related, but important: recall from [Chapter 2](976fc2af-8bb4-4060-96cd-3b921682ed75.xhtml),
    *Virtual Memory*, that, for most processors, the stack(s) conform to the stack-grows-down semantic;
    that is, the direction of growth of the stack segment is toward lower virtual
    addresses.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Though we should not need to, is there a way to specify the location of the
    thread stack? Well, yes, if you insist: the `pthread_attr_[get|set]stack(3)` APIs
    can be used for this purpose, as well as to set and/or query the thread stack''s
    size:'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-424
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: Although you can use `pthread_attr_setstack` to set the stack location, it's
    recommended that this be left to the OS. Also, if you do use it, it's again recommended
    that both the stack location, `stackaddr`, and the stack size, `stacksize`, be
    a multiple of the system page size (and that the location is aligned to a page
    boundary). Aligning the thread stack to a page boundary can be easily achieved
    via the `posix_memalign(3)` API (we have covered example usage of this API in [Chapter
    4](0b4868f7-a8d0-4ced-831f-20af9929de9f.xhtml), *Dynamic Memory Allocation*).
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
- en: 'Be careful: if you are specifying the stack location within the thread attribute
    structure, and creating threads in a loop (as is the normal fashion), you must
    ensure that each thread receives a unique stack location (this is often done by
    allocating the stack memory via the aforementioned `posix_memalign(3)` and then
    passing its return value as the stack location). Also, of course, the memory pages
    that will be used for the thread stack(s) must have both read-write permission
    (recall `mprotect(2)` from [Chapter 4](0b4868f7-a8d0-4ced-831f-20af9929de9f.xhtml),
    *Dynamic Memory Allocation*).'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
- en: 'After all is said and done, the mechanics of setting and querying the thread
    stack is straightforward; the really key point is this: (stress) test your application
    to ensure that the provided thread stack memory is sufficient. As we saw in the
    [Chapters 11](99fafa09-8972-4d9f-b241-46caf9de98f3.xhtml),  *Signaling - Part
    I*, overflowing the stack is a serious defect and will cause undefined behavior.'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
- en: Stack guards
  id: totrans-428
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This neatly brings us to the next point: is there a way to have the application
    know that stack memory is in danger of being, or rather, has been, overflowed?
    Indeed: stack guards. Guard memory is a region of one or more virtual memory pages
    that has been deliberately placed, and with appropriate permissions, to ensure
    that any attempt to access that memory results in failure (or a warning of some
    sort; for example, a signal handler for `SIGSEGV` could provide just such a semantic
    - with the caveat that once we''ve received the SIGSEGV, we are in an undefined
    state and must terminate; but at least we''ll know and can fix the stack size!):'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-430
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'The guard region is an additional memory region allocated at the end of the
    thread stack for the number of bytes specified. The default (guard) size is the
    system page size. Note, again, that the guard size is an attribute of the thread
    and can thus only be specified at thread creation time (and not later). We will
    run the (code: `ch14/stack_test.c`) app like so:'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-432
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: In the preceding code, we specify 2,560 KB (2.5 MB) as the thread stack size.
    Though this is far less than the default (8 MB), it turns out to be enough (for
    x86_64 at least, a quick back-of-the-envelope calculation shows that, for the
    given program parameters, we shall require a minimum of 1,960 KB to be allocated
    for each thread stack).
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code, we run it again, but this time specify the thread stack
    size as a mere 256 KB:'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-435
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: And, as expected, it segfaults.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
- en: Examining the core dump with GDB will reveal a lot of clues regarding why the
    segfault occurred – including, very importantly, the state of the thread stacks
    (in effect, the stack `backtrace(s)`), at the time of the crash. This, however,
    goes beyond the scope of this book.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
- en: We definitely encourage you to learn about using a powerful debugger such as
    GDB (see the *Further reading *section on the GitHub repository as well).
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
- en: 'Also (on our test system at least), the kernel emits a message into the kernel
    log regarding this crash; one way to look up the kernel log messages is via the
    convenience utility `dmesg(1)`. The following output is from an Ubuntu 18.04 box:'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  id: totrans-440
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'The code for the preceding application can be found here: `ch14/stack_test.c`
    :'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
- en: For readability, only key parts of the source code are displayed; to view the
    complete source code, build it, and run it, the entire tree is available for cloning
    from GitHub here: [https://github.com/PacktPublishing/Hands-on-System-Programming-with-Linux](https://github.com/PacktPublishing/Hands-on-System-Programming-with-Linux).
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-443
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: In main, we show the thread stack size attribute being initialized to the parameter
    passed by the user (in KB). The code then goes on to create three worker threads
    and then joins (waits) on them.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
- en: 'In the thread worker routine, we have only thread #2 performing some actual
    work—you guessed it, stack-intensive work. The code for this is as follows:'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  id: totrans-446
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'The `danger` function, of course, is the one where this dangerous, potentially
    stack-overflowing work is carried out:'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  id: totrans-448
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: The preceding function uses large amounts of (thread) stack space since we have
    declared a local variable called `heavylocal` – a 2D-array of `NEL*NEL` elements
    (`NEL=500`). On an x86_64 with a long data type occupying 8 bytes, this works
    out to approximately 2 MB of space! Thus, specifying the thread stack size as
    any less than 2 MB should result in a stack overflow (the stack guard memory region
    will in fact detect this) and therefore result in a segmentation violation (or
    segfault); this is precisely what happened (as you can see in our trial run).
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
- en: Interestingly, if we merely declare the local variable but do not actually make
    use of it, modern compilers will just optimize the code out; hence, in the code,
    we strive to make some (silly) use of the `heavylocal` variable.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
- en: 'A few additional points on the stack guard memory region, to round off this
    discussion, are as follows:'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
- en: If an application has used `pthread_attr_setstack(3)`, it implies that it is
    managing thread stack memory itself, and any guard size attribute will be ignored.
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The guard region must be aligned to a page boundary.
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the size of the guard memory region is less than a page, the actual (internal)
    size will be rounded to a page; `pthread_attr_getguardsize(3)` returns the theoretical
    size.
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The man page on `pthread_attr_[get|set]guardsize(3)` does provide additional
    information, including possible glibc bugs within the implementation.
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  id: totrans-456
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter forms the first of three on the large topic of writing multithreaded
    applications on the Linux platform. Here, we have covered two key areas: the first
    was in regards to the all-important concepts regarding what exactly is a thread,
    and we contrast it to the process model (which we studied in [Chapter 9](3b2340aa-4ab7-46e3-93c0-7f7c210f834b.xhtml), *Process
    Execution* and [Chapter 10](607ad988-406d-4736-90a4-3a318672ab6e.xhtml), *Process
    Creation*). Why you would prefer a multithreaded design was covered in some detail,
    and included three examples. In this way, the motivation to use a multithreaded
    design approach was being brought out.'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
- en: The second part of this chapter focused on the actual pthread APIs (and their
    related concepts), how we create a thread—how many can and how many should be
    created was addressed as well. Thread termination basics, thread attributes, passing
    along a parameter to the newly created thread, what is joining and how to perform
    it, and finally, details on how we can manipulate the thread stack (and stack
    guard) size was covered. Many example programs were shown to help solidify the
    concepts that were taught.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we shall focus squarely on another critical aspect of writing
    powerful and safe multithreaded software – the issues of concurrency, races, critical
    sections, deadlock (and it's avoidance) and atomicity; how we deal with these
    using the mutex lock (and it's variants), as well as the condition variable.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
