- en: Multithreading with Pthreads Part I - Essentials
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Have you downloaded a large file using a download-accelerator type of application?
    Have you played an online game? A flight simulator program? Used word processing,
    web browsers, Java apps, and so on? (The temptation to put in a smiley emoji here
    is high!)
  prefs: []
  type: TYPE_NORMAL
- en: 'It''s quite likely that you have used at least some of these; so what? All
    of these disparate applications have something in common: it''s highly likely
    that they are all designed for multithreading, meaning that their implementation
    uses multiple threads that run in parallel with each other. Multithreading has
    indeed become almost a way of life for the modern programmer.'
  prefs: []
  type: TYPE_NORMAL
- en: Explaining a topic as large as multithreading is itself a big task; hence we
    are dividing the coverage into three separate chapters. This one is the first
    of them.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter is itself logically divided into two broad parts: in the first,
    we carefully consider and understand the concepts behind the threading model—the
    what and why of multithreading. What exactly is a thread, why do we want threads,
    and a quick take on how multithreading has evolved on the Linux platform.'
  prefs: []
  type: TYPE_NORMAL
- en: In the second part, we focus on the thread management APIs—the how (to some
    extent) of multithreading on Linux. The API set required to create and manage
    threads is discussed, with, of course, a lot of practical code to be seen and
    tried out.
  prefs: []
  type: TYPE_NORMAL
- en: At the outset of this topic, we must also clearly point out the fact that in
    this book we are only concerned with multithreading in the context of software
    programming; particularly, the **POSIX threads** (**pthreads**) implementation
    and specifically, pthreads on the Linux platform. We do not attempt to deal with
    various other multithreaded frameworks and implementations that have sprung up
    (such as MPI, OpenMP, OpenCL, and so on) or hardware threading (hyperthreading,
    GPUs with CUDA, and so on).
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, you will learn about programming with multiple threads on
    the Linux platform, specifically, getting started with the pthreads programming
    model or framework. This chapter is broadly divided into two parts:'
  prefs: []
  type: TYPE_NORMAL
- en: In the first, key multithreading concepts—the what and the why of multithreading
    —are covered, laying the groundwork for the second part (and indeed the two subsequent
    chapters on multithreading).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second part covers the essential pthreads APIs required to build a functional
    multithreaded application on Linux (it deliberately does not cover all aspects,
    through; the next two chapters will build on this one).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multithreading concepts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we'll learn about the what and why of multithreading on the
    Linux platform. We will begin by answering the FAQ, "what exactly is a thread?".
  prefs: []
  type: TYPE_NORMAL
- en: What exactly is a thread?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the good (or bad?) old days, Unix programmers had a straightforward software
    model (which got inherited pretty much exactly by other OSes and vendors): there
    is a process that lives in a **virtual address space** (**VAS**); the VAS essentially
    consists of homogeneous regions (essentially collections of virtual pages) called segments: text,
    data, other mappings (libraries), and stack. The text is really the executable—in
    fact, the machine—code that is fed to the processor. We have certainly covered
    all of this earlier in this book (you can brush up on these basics in [Chapter
    2](976fc2af-8bb4-4060-96cd-3b921682ed75.xhtml), *Virtual Memory*).'
  prefs: []
  type: TYPE_NORMAL
- en: A thread is an independent execution (or flow) path within a process. The life
    and scope of a thread, in the familiar procedural programming paradigm we typically
    work with, is simply a function.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, in the traditional model we mentioned previously, we have a single thread
    of execution; that thread, in the C programming paradigm, is the `main()` function!
    Think about it: the `main()` thread is where execution begins (well, at least
    from the app developer''s viewpoint) and ends. This model is (now) called the single
    threaded software model. As opposed to what? The multithreaded one, of course.
    So, there we have it: it is possible to have more than one thread alive and executing
    concurrently (in parallel) with other independent threads within the same process.'
  prefs: []
  type: TYPE_NORMAL
- en: 'But, hang on, can''t processes generate parallelism too and have multiple copies
    of themselves working on different aspects of the application? Yes, of course:
    we have covered the `fork(2)` system call in all its considerable glory (and implications)
    in [Chapter 10](607ad988-406d-4736-90a4-3a318672ab6e.xhtml), *Process Creation*.
    This is known as the multiprocessing model. So, if we have multiprocessing – where
    several processes run in parallel and, hey, they get the work done—the million
    dollar question becomes: "why multithreading at all?" (Kindly deposit a million
    dollars and we shall provide the answer.) There are several good reasons; check
    out the upcoming sections (especially *Motivation – why threads?*; we do suggest
    that first-time readers follow the sequence as laid out in this book) for more
    detail.'
  prefs: []
  type: TYPE_NORMAL
- en: Resource sharing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 10](607ad988-406d-4736-90a4-3a318672ab6e.xhtml), *Process Creation*,
    we repeatedly pointed out that although the fork(2) system call is very powerful
    and useful, it's considered to be a heavyweight operation; performing the fork takes
    a lot of CPU cycles (and thus time) and is expensive in terms of memory (RAM),
    too. Computer scientists were looking for a way to lighten this; the result, as
    you have guessed, is the thread.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hang on, though: for the convenience of the reader, we reproduce a diagram—*The
    Linux process – inheritance and non-inheritance across the fork()*—from [Chapter
    10](607ad988-406d-4736-90a4-3a318672ab6e.xhtml), *Process Creation*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8beb0e4e-7a50-43a7-a15e-f07976f2cf65.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1: The Linux process – inheritance and non-inheritance across the fork()
  prefs: []
  type: TYPE_NORMAL
- en: 'This diagram is important because it shows us why the fork is a heavy weight
    operation: every time you invoke the fork(2) system call,, the complete VAS of
    the parent process and all the data structures on the right inherited across fork side
    of the diagram have to be  copied into the newly born child process. That is indeed
    a lot of work and memory usage! (Okay, we''re exaggerating a bit: as mentioned
    in [Chapter 10](607ad988-406d-4736-90a4-3a318672ab6e.xhtml), *Process Creation**, *modern
    OSes, especially Linux, do take a lot of pains to optimize the fork. Nevertheless,
    it''s heavy. Check out our example 1 demo program that follows—the creation and
    destruction of a process is much slower (and takes much more RAM) than the creation
    and destruction of a thread.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The reality is this: when a process creates a thread, the thread shares (almost)
    everything with all other threads of the same process—all of the preceding VAS,
    thus the segments, and all the data structures—except for a stack.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Every thread has its own private stack segment. Where does it reside? Obviously,
    within the VAS of the creating process; where exactly it resides is really inconsequential
    to us (recall that it''s all virtual memory, in any case, not physical). The question
    that''s a lot more relevant and important to the app developer is how large the
    thread stack will be. The short answer: the same as usual (typically 8 MB on the
    Linux platform), but we shall get to the nitty-gritty details later in this chapter.
    Just think of it this way: the stack of `main()` always resides at the very top
    of the (user mode) virtual address space; the stacks of the remaining threads
    in the process can reside anywhere in this space. Realistically, they typically
    reside in the virtual memory space between the heap and the stack (of main).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram helps us understand the memory layout of a multithreaded
    process on Linux; in the upper portion of the diagram is the process before `pthread_create(3)`; the
    lower portion shows the process after the thread has been successfully created:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8f2b1600-8814-4675-988c-45739f349fd6.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig 2 : The thread – everything except the stack is shared across pthread_create()
  prefs: []
  type: TYPE_NORMAL
- en: The blue squiggle in the process text segment represents the `main()` thread;
    its stack is also clearly seen. We use the dashed lines to indicate that all these
    memory objects (both user and kernel space) are shared across `pthread_create(3)`.
    As can clearly be seen, the only new objects after `pthread_create(3)` are the
    new thread itself (**thrd2**; shown as a red squiggle in the process text segment)
    and a new stack for the just born thread **thrd2** (in red). Contrast this diagram
    with *Fig 1*; when we fork(2), pretty much everything has to be copied into the
    newly born child process.
  prefs: []
  type: TYPE_NORMAL
- en: 'From what we have described so far, the only difference between a process and
    a thread is that of resource sharing—processes do not share, they copy; threads
    do share everything, except for the stack. Dig a little deeper and you will realize
    that both software and hardware state have to be maintained on a per thread basis.
    The Linux OS does exactly that: it maintains a per-thread task structure within
    the OS; the task structure contains all the process/thread attributes, including
    software and hardware context (CPU register values and so on) information.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Again, digging a little deeper, we realize that the OS does maintain a distinct
    copy of the following attributes per thread: the stack segment (and thus the stack
    pointer), possible alternate signal stack (covered in the [Chapter 11](99fafa09-8972-4d9f-b241-46caf9de98f3.xhtml), *Signaling
    - Part I*), both regular signal and real-time signal masks, thread ID, scheduling
    policy and priority, capability bits, CPU affinity mask, and the errno value (don''t
    worry—several of these will be explained along the way).'
  prefs: []
  type: TYPE_NORMAL
- en: Multiprocess versus multithreaded
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To help clearly understand why and how threads can provide a performance benefit,
    let''s perform a few experiments! (the importance of being empirical - experimenting,
    trying things out - is a critical feature; our [Chapter 19](b6b41870-c02e-4379-af86-b5e501799c31.xhtml),
    *Troubleshooting and Best Practices*, covers more on such points). First, we take
    two simple example programs: one, a program that compares the creation and destruction
    of processes versus threads, and two, a program that performs matrix multiplication
    in two ways—one via the traditional single threaded process model, and two, via
    the multithreaded model.'
  prefs: []
  type: TYPE_NORMAL
- en: So, what we are really comparing here is the performance in terms of execution
    time between using the multiprocess versus multithreaded model. We will have the
    reader note that, right here and now, we will not be taking pains to detail and
    explain the thread code right now for two reasons; one, it's besides the point,
    and two, until we have covered the thread APIs in some detail, it will not really
    make sense to do so. (So in effect, dear reader, we ask that you ignore the thread
    code for now; just follow along, and build and reproduce what we do here; the
    code and APIs will become clear to you as you learn more.)
  prefs: []
  type: TYPE_NORMAL
- en: Example 1 – creation/destruction – process/thread
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The process model: Here''s what we do: in a loop (that executes a total of
    60,000 times!), create and destroy a process by calling `fork(2)` and subsequently exiting.
    (We take care of details such as clearing any possible zombie by waiting in the
    parent for the child to die before proceeding in the loop.) The relevant code
    is as follows (`ch14/speed_multiprcs_vs_multithrd_simple/create_destroy/fork_test.c`):'
  prefs: []
  type: TYPE_NORMAL
- en: 'For readability, only the relevant parts of the code are displayed in the following
    code; to view and run it, the entire source code can be found here: [https://github.com/PacktPublishing/Hands-on-System-Programming-with-Linux](https://github.com/PacktPublishing/Hands-on-System-Programming-with-Linux).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We run it prefixed with the `time(1)` utility, which gives us a rough idea
    of the time taken by the program on the processor; the time spent shows up as
    three components: `real` (total wall-clock time spent), `user` (time spent in
    user space), and `sys` (time spent in kernel space):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Obviously, the precise values you get on your Linux box can, and likely will,
    vary. And, no, `user` + `sys` does not add up exactly to real, either.
  prefs: []
  type: TYPE_NORMAL
- en: The multithreading model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Again, here''s what we do: it''s key to understand that the code used here
    (`ch14/speed_multiprcs_vs_multithrd_simple/create_destroy/pthread_test.c`), is
    equivalent in all respects to the previous code except that here we work with
    threads and not processes: in a loop (that executes a total of 60,000 times!),
    create and destroy a thread by calling `pthread_create(3)` and subsequently `pthread_exit(3)`. (We
    take care of details such as waiting in the calling thread for the sibling thread
    to terminate by invoking `pthread_join(3)`.) As mentioned earlier, let''s skip
    the code/API details for now and just see the execution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Wow, the threaded code has run approximately 3x faster than the process model
    code! The conclusion is obvious: creating and destroying a thread is much faster
    than creating and destroying a process.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A technical side note: For the more curious geeks: why exactly is the `fork(2)`
    so much slower than `pthread_create(3)`? Those familiar with OS development will
    understand that Linux makes heavy use of the performance-enhancing **copy-on-write**(**COW**)
    memory techniques within its internal implementation of `fork(2)`. Thus, it begs
    the question, if COW is heavily used, then what is slowing the fork down? The
    short answer: page table creation and setup cannot be COW-ed; it takes a while
    to do. When creating threads of the same process, this work (page table setup)
    is completely skipped.'
  prefs: []
  type: TYPE_NORMAL
- en: Even so, Linux's fork is pretty much considered to be the fastest of any comparable
    OS today.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an aside, a far more accurate way to measure the time spent—and performance
    characteristics in general—is by using the well-known `perf(1)` utility (note
    that in this book, we do not intend to cover `perf` in any detail whatsoever;
    if interested, please look up the *Further reading* section on the GitHub repository
    for some links to perf-related materials):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: As can be seen in the preceding code, on a virtual machine, current versions
    of `perf` cannot show all the counters; this does not impede us in any way here
    as all we're really after is the final time it took to execute—which is shown
    in the last line of `perf` output.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code shows `perf(1)` for the multithreaded app:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: For interested readers, we have also provided a wrapper script (`ch14/speed_multiprcs_vs_multithrd_simple/create_destroy/perf_runs.sh`),
    allowing the user to perform a record and report session with `perf(1)`.
  prefs: []
  type: TYPE_NORMAL
- en: Example 2 – matrix multiplication – process/thread
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A well-known exercise is to write a program to compute the (dot) product of
    two given matrices. Essentially, we would like to perform the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`matrix C = matrix A * matrix B`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Again, we emphasize the fact that here, we are not really concerned with the
    details of the algorithm (and code); what concerns us here is how, at a design
    level the matrix multiplication is performed. We propose (and write the corresponding
    code for) two ways:'
  prefs: []
  type: TYPE_NORMAL
- en: Sequentially, via the single threaded model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In parallel, via the multithreaded model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Note: None of this—the algorithm or code—is purported to be original or ground-breaking
    in any manner; these are well-known programs.'
  prefs: []
  type: TYPE_NORMAL
- en: In the first model, one thread—`main()`, of course—will run and perform the
    computation; the program can be found here: `ch14/speed_multiprcs_vs_multithrd_simple/matrixmul/prcs_matrixmul.c`.
  prefs: []
  type: TYPE_NORMAL
- en: In the second, we will create at least as many threads as there are CPU cores
    on the target system to take full advantage of the hardware (this aspect is dealt
    with in a later section of this chapter called *How many threads can you create?*);
    each thread will perform a part of the computation, in parallel with the other
    threads. The program can be found here: `ch14/speed_multiprcs_vs_multithrd_simple/matrixmul/thrd_matrixmul.c`.
  prefs: []
  type: TYPE_NORMAL
- en: In the multithreaded version, for now, we just hardcode the number of CPU cores
    in our code to four as it matches one of our native Linux test systems.
  prefs: []
  type: TYPE_NORMAL
- en: To truly appreciate how the process(es) and/or threads of our applications actually
    consume CPU bandwidth, let's use the interesting `gnome-system-monitor` GUI application
    to see resource consumption graphically! (To run it, assuming it's installed,
    just type `$ gnome-system-monitor &`  on the shell).
  prefs: []
  type: TYPE_NORMAL
- en: We remind you that all software and hardware requirements have been enumerated
    in some detail in the software-hardware list material available on this book's
    GitHub repository.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will perform the experiment as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the apps on a native Linux box with four CPU cores:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/e3a13cd1-66d2-48a1-b3ef-527f115b79ef.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Look carefully at the preceding (annotated) screenshot (zoom in if you are
    reading the electronic version); we will notice several items of interest:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the foreground is the terminal window app where we run the `prcs_matrixmul` and
    the `thrd_matrixmul` applications:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We use `perf(1)` to accurately measure the time taken and deliberately filter
    out all output except for the final number of seconds elapsed during execution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the background, you can see the gnome-system-monitor GUI app running.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The (native Linux) system—the particular one that we have tested this on—has
    four CPU cores:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'One way to find the number of CPU cores on your system is by using the following
    code: `getconf -a | grep _NPROCESSORS_ONLN | awk ''{print $2}''`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (you can update the `NCORES` macro in the source code `thrd_matrixmul.c`to reflect
    this value)
  prefs: []
  type: TYPE_NORMAL
- en: 'The `prcs_matrixmul` app runs first; while it runs, it consumes 100% CPU bandwidth on
    exactly one CPU core out of the four available (it happens to be CPU core #2)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Notice how, on the middle-to-left of the CPU History meter, the red line representing
    CPU2 shoots up to a 100% (highlighted with a purple ellipse and labeled Process)!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At the time the screenshot was actually taken (OS on the X-axis timeline; it
    moves from right to left), the CPUs are back to normal levels.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Next (after a gap of 10 seconds in this particular run), the `thrd_matrixmul` app
    runs; and herein lies the key point: While it runs, it consumes 100% CPU bandwidth on
    all four CPU cores!'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Notice at how, approximately just after the 15s marking (read it from right-to-left)
    on the X-axis timeline, all four CPU cores shoot to 100% – that's during the execution
    of `thrd_matrixmul` (highlighted with a red ellipsis and labeled Threads).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'What does this tell us? Something really important: the underlying Linux OS
    CPU scheduler will try and take advantage of the hardware and, if possible, schedule
    our four application threads to run in parallel on the four CPUs available! Hence,
    we get higher throughput, higher performance, and more bang for our buck.'
  prefs: []
  type: TYPE_NORMAL
- en: Understandably, you might at this point wonder about and have a lot of questions
    on how Linux performs CPU (thread) scheduling; worry not, but please have some
    patience—we shall explore CPU scheduling in some detail in [Chapter 17](36229bac-c402-4d2f-b876-d1eb4aba8051.xhtml),
    *CPU* *Scheduling on Linux*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Restricted to exactly one CPU:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The `taskset(1)` utility allows one to run a process on a specified set of
    processor core(s). (This ability to associate a process with a given CPU(s) is
    called CPU affinity. We shall come back to this in the chapter on scheduling.)
    Using `taskset` in its basic form is easy: `taskset -c <cpu-mask> <app-to-run-on-given-cpus>`'
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see from the following screenshot, we contrast performing a run
    of the `thrd_matrixmul` app on all four CPU cores on the system (in the usual
    way) with running it on exactly one CPU by specifying the CPU mask via `taskset(1)`;
    the screenshot again clearly reveals how, on the former run, all four CPUs are
    pressed into action by the OS (and it takes a total of 8.084s), whereas on the
    latter run only a single CPU (it shows up as CPU3 in green) is employed to execute
    its code (resulting in a total time of 11.189s):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/61357669-4d7c-4bf5-8ddd-2dcfbd514e28.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Seeing what we have just learned in this section, you might leap to the conclusion,"hey,
    we''ve found the answer: let''s just always use multithreading." But, of course,
    experience tells us that there is no silver bullet. The reality is that although
    threading does indeed offer some real advantages, as with everything in life,
    there are also downsides to it. We shall postpone more discussion on the pros
    and cons in [Chapter 16](4df10c19-b400-4805-8e6e-51a8f43dcfa4.xhtml), *Multithreading
    with Pthreads Part III*; do keep this in mind, though.'
  prefs: []
  type: TYPE_NORMAL
- en: For now, let's do one more experiment to clearly illustrate the fact that not
    just multithreading, but multiprocessing—the use of fork to spawn multiple processes—is
    very helpful as well to gain higher throughput.
  prefs: []
  type: TYPE_NORMAL
- en: Example 3 – kernel build
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So, one last experiment (for this section): we will build (cross-compile) a
    Linux kernel ver. 4.17 for the ARM Versatile Express platform (with its default
    configuration). The details of the kernel build and so on are out of scope of
    this book, but that''s all right: the key point here is that the kernel build
    is definitely a CPU and RAM intensive operation. Not only that, the modern `make(1)` utility
    is multiprocess capable! One can tell `make` the number of jobs—processes, really—to
    internally spawn (fork) via its `-jn` option switch, where `n` is the number of
    jobs (threads). We use a heuristic (a rule of thumb) to determine this:'
  prefs: []
  type: TYPE_NORMAL
- en: '`n = number-of-CPU-cores * 2`'
  prefs: []
  type: TYPE_NORMAL
- en: (multiply by 1.5 on very high-end systems with a lot of cores.)
  prefs: []
  type: TYPE_NORMAL
- en: Knowing this, check out the experiments that follow.
  prefs: []
  type: TYPE_NORMAL
- en: On a VM with 1 GB RAM, two CPU cores and parallelized make -j4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We configure the guest VM to have two processors, and proceed with the parallelized
    build (by specifying `make -j4` ):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The build took a total time of approximately 684 seconds (11.5 min). Just so
    you know, the compressed kernel image for ARM—the one we boot with—is the file
    called `zImage`; the uncompressed kernel image (used only for debug purposes)
    is the `vmlinux` file.
  prefs: []
  type: TYPE_NORMAL
- en: 'While it was running, doing a quick `ps -LA`during the build indeed reveals
    its multiprocess—not multithreaded—nature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: On a VM with 1 GB RAM, one CPU core and sequential make -j1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We configure the guest VM to have only one processor, clean up the build directory,
    and proceed once more, but this time with a sequential build (by specifying `make
    -j1`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The build took a total time of approximately 1232 seconds (20.5 min), which
    is nearly twice as long as the previous build!
  prefs: []
  type: TYPE_NORMAL
- en: You might be asking this question: so, if the build with one process took around
    20 minutes and the same build with multiple processes took approximately half
    the time, why use multithreading at all? Multiprocessing seems to be as good!
  prefs: []
  type: TYPE_NORMAL
- en: 'No, please think: our very first example regarding process versus thread creation/destruction taught
    us that spawning (and terminating) processes is much slower than doing the same
    with threads. That is still a key advantage that many applications exploit. After
    all, threads are far more efficient than processes in terms of creation and destruction.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In a dynamic, unpredictable environment, where we do not know in advance how
    much work will be required, the use of multithreading to be able to quickly create
    worker threads (and quickly have them terminated) is very important. Think of
    the famous Apache web server: it''s multithreaded by default (via its `mpm_worker`
    module in order to quickly serve client requests). In a similar fashion, the modern NGINX web
    server uses thread pools (more on this for those interested can be found in the *Further
    reading* section on the GitHub repository).'
  prefs: []
  type: TYPE_NORMAL
- en: Motivation – why threads?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Threading does indeed offer a number of useful advantages; here, we attempt
    to enumerate some of the more important ones. We think of this in terms of motivation
    for the application architect to make use of multithreading because of potential
    advantages to be gained. We divide this discussion into two areas: design and performance.
  prefs: []
  type: TYPE_NORMAL
- en: Design motivation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In terms of design, we take into account the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Taking advantage of potential parallelism
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many real-world applications will benefit from designing them in such a manner
    that the work can be split into distinct units, and these units or work parcels
    can run in parallel—concurrently—with each other. At the implementation level,
    we can use threads to implement the work parcels.
  prefs: []
  type: TYPE_NORMAL
- en: As an example, a download accelerator program exploits the network by having
    several threads perform network I/O. Each thread is assigned work to download
    only a portion of the file; they all run in parallel, effectively gaining more
    network bandwidth than a single thread could, and when done, the destination file
    is stitched together.
  prefs: []
  type: TYPE_NORMAL
- en: Many such examples abound; recognizing the potential for parallelism is an important
    part of the architect's job.
  prefs: []
  type: TYPE_NORMAL
- en: Logical separation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The threading model intuitively lends itself to letting the designer logically
    separate work. For example, a GUI frontend application might have a few threads
    managing the GUI state, waiting for and reacting to user input, and so on. Other
    threads could be used to handle the app's business logic. Not mixing the **user
    interface** (**UI**) with the business logic is a key element of good design.
  prefs: []
  type: TYPE_NORMAL
- en: Overlapping CPU with I/O
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This point is similar in fashion to the previous one—the logical separation
    of tasks. In the context of what we're discussing, CPU refers to software that
    is CPU-intensive or CPU-bound (the canonical example being the `while (1)`; piece
    of C code); I/O refers to software that is in a blocked state—we say that it is
    waiting on I/O, meaning that it is waiting on some other operation to complete
    (perhaps a file or network read, or any blocking API, in fact) before it can move
    forward; this is referred to as I/O bound.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, think of it this way: let''s say we have a series of tasks to perform (with
    no dependencies between them): task A, task B, task C, and task D.'
  prefs: []
  type: TYPE_NORMAL
- en: Let's also say that task A and task C are highly CPU-bound, whereas task B and task
    D are more I/O-bound. If we use the traditional single threaded approach, then
    of course each task has to be carried out in sequence; so, the process ends up
    waiting—for perhaps a long while—for tasks B and D, thus delaying task C. If,
    on the other hand, we use a multithreaded approach, we can separate the tasks
    as individual threads. Thus, even while the threads for tasks B and D are blocked
    on I/O, the threads for task A and C continue to make progress.
  prefs: []
  type: TYPE_NORMAL
- en: This is called overlapping CPU with I/O. Decoupling (and separating out) tasks
    when there is no dependency between them, by using threads, is a design approach
    that is usually worth pursuing. It leads to better application responsiveness.
  prefs: []
  type: TYPE_NORMAL
- en: Manager-worker model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Threads quite easily lend themselves to the familiar manager-worker model; a
    manager thread (often `main()`) creates worker threads on demand (or pools them);
    when work arises, a worker thread handles it. Think of busy web servers.
  prefs: []
  type: TYPE_NORMAL
- en: IPC becoming simple(r)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Performing IPC  between processes takes a learning curve, experience, and just
    a lot of work. With threads belonging to a process, IPC—communication—between
    them is as simple as writing and reading global memory (well, to be honest, it's
    not that simple, as we shall learn when we reach the topics on concurrency and
    synchronization in the next chapter; it's still less work conceptually and literally
    than processing IPC).
  prefs: []
  type: TYPE_NORMAL
- en: Performance motivation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As the two examples in the previous section quite clearly showed us, using multithreading
    can raise application performance significantly; some of the reasons for this
    are mentioned here.
  prefs: []
  type: TYPE_NORMAL
- en: Creation and destruction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Preceding example 1  clearly showed us that the time taken for the creation
    and destruction of a thread is far less than that of a process. Many applications
    require that you do this almost constantly. (We shall see that creating and destroying
    threads is programmatically much simpler to do than doing the same with processes.)
  prefs: []
  type: TYPE_NORMAL
- en: Automatically taking advantage of modern hardware
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Preceding example 2 clearly illustrated this point: when running a multithreaded
    app on modern multicore hardware (high-end enterprise class servers can have in
    excess of 700 CPU cores!), the underlying OS will take care of optimally scheduling
    threads onto available CPU cores; the app developers need not concern themselves
    with this. Effectively, the Linux kernel will try and ensure perfect SMP scalability
    whenever possible, which will result in higher throughput and, ultimately, speed
    gains. (Again, dear reader, we''re being optimistic here: the reality is that
    with heavy parallelism and CPU cores also comes the heavy downsides of concurrency
    concerns; we shall discuss all of this in more detail in upcoming chapters.)'
  prefs: []
  type: TYPE_NORMAL
- en: Resource sharing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have already covered this very point in the *Resource sharing *section earlier
    in the beginning portion of this chapter (re-read it, if required). The bottom
    line is this: thread creation is comparatively cheap as opposed to process creation (the
    same goes for destruction). Also, the memory footprint of a thread as opposed
    to a process is much lower. Thus, resource sharing, and the associated performance
    advantages, are obtained.'
  prefs: []
  type: TYPE_NORMAL
- en: Context switching
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Context switching is an unfortunate reality on the OS—it''s meta-work that
    must be done every time the OS switches from running one process to running another
    process (we have voluntary and involuntary context switches). The actual amount
    of time it takes to context switch is highly dependent on the hardware system
    and the software quality of the OS; typically, though, it''s in the region of
    tens of microseconds for x86-based hardware systems. That sounds quite tiny: to
    get an idea of why this is considered important (and indeed wasteful), look at
    the output of running `vmstat 3` on an average Linux desktop computer (`vmstat(1)` is
    a famous utility; used this way, it gives us a nice 10,000-foot view of system
    activity; hey, also try out its modern successor, `dstat(1)`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: (Please look up the man page on `vmstat(1)` for a detailed explanation of all
    fields). Preceding under the `system` heading, we have two columns: `in` and `cs`
    (hardware) interrupts and context switches, respectively, that have occurred in
    the last one second. Just look at the numbers (ignore the first output line, though)!
    It's fairly high. This is why it really does matter to system designers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Context switching between the threads of the same process takes a lot less
    work (and thus time) than between processes (or threads belonging to different
    processes). This makes sense: a good amount of the kernel code can be effectively
    short-circuited when the overall process remains the same. Thus, this becomes
    another advantage of using threads.'
  prefs: []
  type: TYPE_NORMAL
- en: A brief history of threading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Threads—a sequential flow of control—have been around for a long while now;
    only, they went under the name of processes (reports put this at the time of the
    Berkeley Timesharing System, 1965). Then, by the early 1970s, along came Unix,
    which cemented the process as the combination of a VAS and a sequential flow of
    control. As mentioned earlier, this is now called the single threaded model, as
    of course only a single thread of control—the main function—existed.
  prefs: []
  type: TYPE_NORMAL
- en: Then, in May 1993, Sun Solaris 2.2 came out with UI threads, and a thread library
    called *libthread*, which exposed the UI API set; in effect, modern threads. Competing
    Unix vendors quickly came up with their own proprietary multithreaded solutions
    (with runtime libraries exposing APIs)—Digital with DECthreads (which was later
    absorbed by Compaq Tru64 Unix and subsequently HP-UX), IBM with AIX, Silicon Graphics
    with IRIX, and so on—each with their own proprietary solution.
  prefs: []
  type: TYPE_NORMAL
- en: POSIX threads
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Proprietary solutions poses a major problem to the big customer who owns heterogeneous
    hardware and software from several of these vendors; being proprietary, it is
    difficult to get the differing libraries and API sets to talk to each other. It''s
    the usual problem—a lack of interoperability. The good news: in 1995, the IEEE
    formed a separate POSIX committee—IEEE 1003.1c—the **POSIX threads** (**pthreads**) committee,
    to evolve a standardized solution for an API for multithreading.'
  prefs: []
  type: TYPE_NORMAL
- en: 'POSIX: Apparently, the original name of the IEEE body is **Portable Operating
    System Interface for Computing Environments** (**POSICE**). Richard M. Stallman
    (RMS)  suggested shortening the name to **Portable Operating System Interface
    for uniX** (**POSIX**), and that name has stuck.'
  prefs: []
  type: TYPE_NORMAL
- en: So, the bottom line is that pthreads is an API standard; formally, IEEE 1003.1c-1995\.
    The upshot of all of this is that all Unix and Unix-like OS vendors gradually
    built implementations supporting pthreads; so, today (in theory, at least), you
    can write a pthreads multithreaded application and it will run unmodified on any
    pthreads-compliant platform (in practice, expect a bit of porting effort).
  prefs: []
  type: TYPE_NORMAL
- en: Pthreads and Linux
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Of course, Linux wanted to be compliant with the POSIX threads standard; but
    who would actually build an implementation (remember, the standard is merely a
    draft specification document; it's not code)? Back in 1996, Xavier Leroy stepped
    up and built Linux's first pthreads implementation—a threading library called Linux
    threads. All considered, it was a good effort, but was not fully compatible with
    the (then brand new) pthreads standard.
  prefs: []
  type: TYPE_NORMAL
- en: An early effort at resolving problems was called **Next Generation Posix Threads** (**NGPT**).
    At around the same time, Red Hat threw in a team to work on this area as well;
    they called the project **Native Posix Threading Library** (**NPTL**). In the
    best traditions of open source culture, the NGPT developers worked together with
    their counterparts at NPTL and began merging the best features of NGPT into NPTL.
    NGPT development was abandoned sometime in 2003;  by then, the realistic implementation
    of pthreads on Linux—which remains to this day—is NPTL.
  prefs: []
  type: TYPE_NORMAL
- en: 'More technically: NPTL was entrenched as the superior threading API interface,
    even as features were integrated into the 2.6 Linux kernel (December 2003 onward),
    which helped greatly improve threading performance.'
  prefs: []
  type: TYPE_NORMAL
- en: NPTL implements the 1:1 threading model; this model provides true multithreading
    (user and kernel state) and is also known as the native threads model. Here, we
    do not intend to delve into these internal details; a link has been provided for
    interested readers in the *Further reading *section on the GitHub repository.
  prefs: []
  type: TYPE_NORMAL
- en: 'One can look up the threading implementation (since glibc 2.3.2) with the following
    code (on a Fedora 28 system):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Clearly, it's NPTL.
  prefs: []
  type: TYPE_NORMAL
- en: Thread management – the essential pthread APIs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this—the second major portion of this first chapter on multithreading—we
    shall now focus on the mechanics: using the pthreads API, how exactly does the
    programmer create and manage threads in an effective fashion? We will explore
    the essential pthreads API interfaces to fulfill this key purpose; this knowledge
    is the building block for writing functional and performance-friendly pthreads
    applications.'
  prefs: []
  type: TYPE_NORMAL
- en: We will take you through the thread life cycle in terms of API sets—creating,
    terminating, joining upon (waiting for), and in general, managing the threads
    of a process. We will also cover thread stack management.
  prefs: []
  type: TYPE_NORMAL
- en: This, of course, implies that we have a pthreads runtime library installed on
    the Linux system. On modern Linux distributions, this will certainly be the case;
    it's only if you are using a rather exotic embedded Linux that you will have to
    verify this. The name of the pthreads library on the Linux platform is libpthread.
  prefs: []
  type: TYPE_NORMAL
- en: 'A couple of key points regarding the pthread APIs are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: All pthread APIs require the  `<pthread.h>` header file to be included in the
    source.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The API often uses the object-oriented concepts of data hiding and data abstraction;
    many data types are internal typedefs; this design is deliberate: we want portable
    code. Thus, the programmer must not assume types and must work with the provided
    helper methods where applicable to access and/or query data types. (Of course,
    the code itself is the usual procedural C; nevertheless, many concepts are modeled
    around object orientation. Interestingly, the Linux kernel also follows this approach.)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thread creation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The pthreads API for creating a thread is `pthread_create(3)`; its signature
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: When compiling pthread applications, it's very important to specify the `-pthread`
    `gcc` option switch (it enables required macros for using the libpthread library(more
    on this to follow).
  prefs: []
  type: TYPE_NORMAL
- en: '`pthread_create` is the API to invoke to create a new thread within the calling
    process. On success, the new thread will be running concurrently (in parallel)
    with other threads that may be alive in that process at that point in time; but
    what code will it be running? It will start by running the code of the `start_routine`
    function (the third parameter to this API: a pointer to the function). Of course,
    this `thread` function can subsequently make any number of function calls.'
  prefs: []
  type: TYPE_NORMAL
- en: The new thread's thread ID will be stored in the opaque data item `thread`—the
    first parameter (it's a value-result style parameter). Its data type, `pthread_t`
    is deliberately opaque; we must not assume that it's an integer (or any such thing).
    We shall soon come across when and how we use the thread ID.
  prefs: []
  type: TYPE_NORMAL
- en: Notice that the third parameter, the function pointer—the routine run by the
    new thread—itself receives a void* parameter—a generic pointer. This is a common
    and helpful programming technique, enabling us to pass absolutely any value(s)
    to the newly created thread. (This kind of parameter is often referred to as client
    data or tag in the literature.) How do we pass it? Via the fourth parameter to `pthread_create(3)`, `arg`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second parameter to `pthread_create(3)` is a thread attribute structure;
    here, the programmer should pass the attributes of the thread being created (we
    shall discuss some of them shortly). There is a shortcut: passing `NULL` here
    implies that the library should use the default attributes when creating a thread.
    However, the defaults on a certain Unix might differ substantially from those
    on a different Unix or Linux; writing portable code implies one does not assume
    any defaults, but rather explicitly initializes a thread with attributes that
    are correct for the application. Thus, our recommendation would definitely be
    to not pass `NULL`, but to explicitly initialize a `pthread_attr_t` structure
    and pass it along (the code examples that follow will illustrate this).'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the return value to `pthread_create(3)` is `0` on success and non-zero
    on failure; `errno` is set to a few values as appropriate (we refer you to the
    man page on `pthread_create(3)` for these details).
  prefs: []
  type: TYPE_NORMAL
- en: 'When a new thread is created, it inherits certain attributes from its creating
    thread; these include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The creating thread's capability sets (recall our discussion in [Chapter 8](b4538277-87f0-46f1-83fa-632fa470bfd7.xhtml),
    *Process Capabilities*); this is Linux-specific
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The creating thread's CPU affinity mask; this is Linux-specific
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The signal mask
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Any pending signals and pending timers (alarms) in the new thread are cleared. CPU
    execution times will be reset as well for the new thread.
  prefs: []
  type: TYPE_NORMAL
- en: Just so you know, on the Linux libpthreads implementation, `pthread_create(3)` calls
    the `clone(2)` system call, which, within the kernel, actually creates the thread.
  prefs: []
  type: TYPE_NORMAL
- en: Interestingly, modern glibc's fork implementation also invokes the `clone(2)` system
    call. Flags passed to `clone(2)` determine how resource sharing is done.
  prefs: []
  type: TYPE_NORMAL
- en: 'It''s about time that we did some coding! We will write a really simple (and
    actually quite buggy!)  `hello, world.` for pthreads application (`ch14/pthreads1.c`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, we loop three times, and on each loop iteration we create a
    thread. Notice the third parameter to the `pthread_create(3)`—a function pointer
    (just providing the name of the function is sufficient; the compiler will figure
    the rest); this is the the thread's work routine. Here, it's the function `worker`.
    We also pass the fourth parameter to `pthread_create`—recall that's it's the client
    data, any data you would like to pass to the newly created thread; here, we pass
    the loop index `i` (of course, we appropriately typecast it so that the compiler
    won't complain).
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `worker` function, worker, we gain access to the client data (received
    as the formal parameter `data`) by again type-casting the `void *` back to its
    original type, `long`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`long datum = (long)data;`'
  prefs: []
  type: TYPE_NORMAL
- en: We then merely emit a couple of printf's to show that, yes, we are here indeed.
    Notice how all the worker threads run the same code—the `worker` function. This
    is entirely acceptable; recall that code (text) is read-execute in terms of page
    permissions; running text in parallel is not only all right, but it's often desirable
    (providing high throughput).
  prefs: []
  type: TYPE_NORMAL
- en: 'To build it, we have provided the Makefile; note, though, that all the pthreads APIs
    aren''t linked in by default, like glibc. No, they are, of course, in libpthread, which
    we shall have to both explicitly compile (to our source files) and link in to
    our binary executable via the `-pthread` directive. The following snippet from
    the Makefile shows this being done:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Building it now works, but—and please note this carefully—the program does not work
    well at all! In the following code, we perform some test runs by looping around `./pthreads1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: As you can see the `hello, world.` message only appears intermittently and not
    at all in trial runs 4 and 5 (of course, the output you see when you try this
    out can certainly vary due to timing issues).
  prefs: []
  type: TYPE_NORMAL
- en: 'Why is it like this? Simple: we have inadvertently set up a buggy situation—a race!
    Where exactly? Look at the code again, carefully: what does the `main()` function
    do once the loop is done? It calls `exit(3)`; thus the entire process terminates,
    not just the main thread! And who is to say that the worker threads completed
    their work before this occurred? Ah—that, ladies and gentlemen, is your classic
    race.'
  prefs: []
  type: TYPE_NORMAL
- en: So, how do we fix it? For now, we shall just perform a couple of quick fixes;
    the proper way to avoid racy code is via synchronization; this is a big topic
    and deserves a chapter by itself (as you shall see). Okay, first, let's fix the
    problem of the main thread prematurely exiting.
  prefs: []
  type: TYPE_NORMAL
- en: Termination
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `exit(3)` library API causes the calling process—along with all of its
    threads – to terminate. If you would like a single thread to terminate, have it
    invoke the `pthread_exit(3)` API instead:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: This parameter specifies the exit status of the calling thread; for the time
    being, we ignore it and just pass `NULL` (we shall examine using this parameter
    shortly).
  prefs: []
  type: TYPE_NORMAL
- en: So, back to our racy app (`ch14/pthreads1.c`); let's make a second, better version
    (`ch14/pthreads2.c`). The problem, really, with our first version was the race—the main thread
    calls `exit(3)`, causing the entire process to die, probably before the worker
    threads got a chance to complete their work. So, let's fix this by having `main()`
    call `pthread_exit(3)`! Also, why not have our thread worker function terminate
    properly by explicitly invoking the `pthread_exit(3)` as well?
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the modified code snippets for the `worker()` and `main()` functions
    (`ch14/pthreads2.c`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s try out the preceding program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: That's much better!
  prefs: []
  type: TYPE_NORMAL
- en: The return of the ghost
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There is still a hidden problem. Let''s do some more experimentation: let''s
    write a third version of this program (let''s call it `ch14/pthreads3.c`). In
    it, we say, what if the worker threads take longer to perform their work (than
    they are currently taking)? We can easily simulate this with a simple `sleep(3)` function,
    which is going to be introduced into the worker routine:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s try it out:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Well? It looks just fine. Is it really? There's just one more quick and minor
    modification that has to be done; increase the sleep time from 3 seconds to, say,
    30 seconds, and rebuild and retry (the only reason we do this is to give the end
    user a chance to type a `ps(1)` command, as shown in the following screenshot,
    before the app dies). Now, run it in the background , and take a closer look!
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/142c88d2-a84e-4026-855c-0fe06c95b0a1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Check out the preceding screenshot: we run the `pthreads3` app in the background;
    the app (well, the main thread of the app) creates an additional three threads.
    The threads merely block by going to sleep for thirty seconds each. As we ran
    the process in the background, we get control on the shell process; now we run `ps(1)`with
    the `-LA` option switches. From the man page on `ps(1)`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`-A`: Select all processes; identical to `-e`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`-L`: Show threads, possibly with LWP and NLWP columns'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'All right! (GNU) `ps(1)` can even show us every thread alive by making use
    of the `-L` option switch (try out `ps H` too). With the `-L` switch, the first
    column in the output of `ps` is the PID of the process (quite familiar to us);
    the second column is the thread **Light Weight Process** (**LWP**); in effect,
    this is the PID of the individual thread as seen by the kernel. Interesting. Not
    just that, look at the numbers carefully: where the PID and LWP match, it''s the `main()`
    thread of the process; where the PID and LWP differ, it tells us that this is
    a child, or more correctly just a peer thread, belonging to the process; the LWP
    is the thread PID as seen by the OS. So, in our sample run, we have the process
    PID of 3906, along with four threads: the first one is the `main()` thread (as
    its PID == its LWP value), while the remaining three have the same PID—proving
    they belong to the same overall process, but their individual thread PIDs (their
    LWPs) are unique – 3907, 3908, and 3909!'
  prefs: []
  type: TYPE_NORMAL
- en: The problem we have been referring to, though, is that in the first line—which
    represents the main thread—of the `ps` output is that the process name is followed
    by the phrase
  prefs: []
  type: TYPE_NORMAL
- en: '`<defunct>` (on the extreme right). The alert reader will remember that defunct is
    another term for zombie! Yes indeed, the infamous zombie has returned to haunt
    us.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The main thread, by invoking `pthread_exit(3)` (recall the code of main in
    `ch14/pthreads3.c`), has exited before the other threads in the process; the Linux
    kernel thus marks it as a zombie. As we learned in [Chapter 10](607ad988-406d-4736-90a4-3a318672ab6e.xhtml), *Process
    Creation*, zombies are undesirable entities; we really do not want a zombie hanging
    around (wasting resources). So, the question, of course, is how do we prevent
    the main thread from becoming a zombie? The answer is straightforward: do not allow
    the main thread to terminate before the other threads in the application; in other
    words, the recommendation is to always keep `main()` alive, waiting for all the
    other threads to die, before it itself terminates (and thus the process terminates).
    How? Read on.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Again, it goes without saying (but we shall say it!): the process remains alive
    as long as at least one thread within it remains alive.'
  prefs: []
  type: TYPE_NORMAL
- en: As a quick aside, when will the worker threads run with respect to each other
    and main? In other words, is it guaranteed that the first thread created will
    run first, followed by the second thread, then the third, and so on?
  prefs: []
  type: TYPE_NORMAL
- en: 'The short answer: no, there is no such guarantee. Especially on modern **Symmetric
    Multiprocessor** (**SMP**) hardware and a modern multiprocess-and-multithreaded-capable
    OS such as Linux, the actual order at runtime is indeterminate (which is a fancy
    way of saying it can''t be known). In reality, it''s up to the OS scheduler to
    make these decisions (that is, in the absence of real-time scheduling policies
    and thread priorities; we shall tackle these topics later in this book).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Another trial run of our `./pthreads2` sample program reveals this very case:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Can you see what happened? The order shown in the preceding code is: `thread
    #0`, followed by `thread #2`, followed by `thread #1`! It''s unpredictable. Do
    not assume any specific order of execution when designing your multithreaded applications.
    (We shall cover synchronization in a later chapter, which teaches us how to achieve
    the order we require.)'
  prefs: []
  type: TYPE_NORMAL
- en: So many ways to die
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'How can a thread terminate? It turns out there are several ways:'
  prefs: []
  type: TYPE_NORMAL
- en: Explicitly, by invoking `pthread_exit(3)`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implicitly, by returning from the thread function; the return value is implicitly
    passed (as though via `pthread_exit` parameter).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implicitly, by falling off the thread function; that is, hitting the close brace
    `}`; note however that this is not recommended (a later discussion will show you
    why)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Any thread invoking the `exit(3)` API will, of course, cause the entire process,
    along with all threads in it, to die.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The thread gets canceled (which we will cover later).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How many threads is too many?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So, by now, we know how to create an application process with a few threads
    executing within it. We will repeat a code snippet from our very first demo program,
    `ch14/pthreads1.c`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Clearly, the process—well, we really mean the main thread of the process (or
    application)—goes in a loop, and each loop iteration creates a thread. So, when
    it's done, we will have three threads in addition to the main thread, which is
    a total of four threads, alive in the process.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is obvious. The point here is this: creating threads is so much simpler
    than creating (child) processes with the `fork(2)`; with fork, we had to carefully
    code it, getting the child to run its code while the parent continues with its
    code path (recall the switch-case construct; take another quick look at our `ch10/fork4.c` code
    example, if you wish to). With `pthread_create(3)`, things have become easy for
    the application programmer – just call the API in a loop—and voila! You get as
    many threads as you like! In the preceding code snippet, imagine tweaking it,
    changing the value of `NTHREADS` from 3 to 300; and just like that, the process
    will produce 300 threads. What if we made `NTHREADS` 3,000? Or 30,000!?'
  prefs: []
  type: TYPE_NORMAL
- en: 'Thinking about this brings up a couple of pertinent questions: one, how many
    threads can you actually create? And two, how many threads should you create?
    Please, read on.'
  prefs: []
  type: TYPE_NORMAL
- en: How many threads can you create?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you think about it, there must be some artificial constraint upon the number
    of threads that the underlying OS will allow an application to create; otherwise,
    system resources would get exhausted pretty quickly. In fact, this is not really
    something new; our whole discussion in [Chapter 3](4161d1ed-20ea-4fa5-8947-646055d956cb.xhtml),
    *Resource Limits*, was really about similar things.
  prefs: []
  type: TYPE_NORMAL
- en: 'With regard to threads (and processes), there are two (direct) limits that
    impact the number of threads that can exist at any given point in time:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Per process resource limits: You will recall from our [Chapter 3](4161d1ed-20ea-4fa5-8947-646055d956cb.xhtml),
    *Resource Limits*, that there are two utilities to look up the currently defined
    resource limits: `ulimit(1)` and `prlimit(1)`, the latter being the modern interface.
    Let''s take a quick look at the resource limit for max user processes; also realize
    that although the word processes is used, you should actually think of these as
    threads:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, `prlimit()` shows us the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Here, we have shown you how to query the limit via the CLI; to see how to change
    it—both interactively and programmatically with API interfaces – refer to [Chapter
    3](4161d1ed-20ea-4fa5-8947-646055d956cb.xhtml), *Resource Limits.*
  prefs: []
  type: TYPE_NORMAL
- en: 'System-wide limits: The Linux OS maintains a system-wide (not per-process)
    limit on the total number of threads that can be alive at any given point in time.
    This value is exposed to the user space via the proc filesystem:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: So, the thing to understand is that if either of the preceding two limits are
    breached, `pthread_create(3)` (and similarly, the `fork(2)`) will fail (typically
    setting `errno` to the value `EAGAIN` try again; the OS saying, in effect, "I
    cannot do this for you right now, please try again later").
  prefs: []
  type: TYPE_NORMAL
- en: 'Can you change these values? Yes, of course, but with the usual caveat—you
    require root (super user) access to do so. (Again, we have discussed these points
    in detail with respect to  in [Chapter 3](4161d1ed-20ea-4fa5-8947-646055d956cb.xhtml),
    *Resource Limits*) Regarding the system-wide limit, you can indeed change it as
    the root. But, hang on, blindly changing system parameters like this without an
    understanding of the impact is a sure way to lose grip on a system! So, let''s
    start by asking ourselves this: the OS sets the `threads-max` limit at boot time;
    what does it base the value on?'
  prefs: []
  type: TYPE_NORMAL
- en: 'The short answer: it''s directly proportional to the amount of RAM on the system.
    This makes sense: ultimately, memory is the key limiting resource with regard
    to creating threads and processes.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In more detail for our dear OS-level geek readers: kernel code at boot time
    sets the `/proc/sys/kernel/threads-max` value so that thread (task) structures
    within the OS can take a maximum of one-eighth of available RAM. (The threads-max minimum
    value is 20; the maximum value is the constant `FUTEX_TID_MASK 0x3fffffff`.)'
  prefs: []
  type: TYPE_NORMAL
- en: Also, by default, the per-process resource limit for the maximum number of threads
    is half of the system limit.
  prefs: []
  type: TYPE_NORMAL
- en: 'As seen from the preceding code, the value we obtained was 126,446; this was
    done on a native Linux laptop with 16 GB of RAM. Running the same commands on
    a guest VM with 1 GB of RAM yields the following results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Setting the `threads-max`kernel tunable to too high a value – beyond `FUTEX_TID_MASK`
    – will cause it to be brought down to that value (but, of course, that is almost
    certainly too large in any case). But even within limits, you can stray too far,
    causing the system to become vulnerable (to **denial-of-service** (**DoS**) attacks,
    perhaps!). On an embedded Linux system, lowering the limit might actually help
    by constraining the system.
  prefs: []
  type: TYPE_NORMAL
- en: Code example – creating any number of threads
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So, let''s put it to the test: we will write a simple extension of our previous
    program, this time allowing the user to specify the number of threads to attempt
    to create within the process as the parameter (`ch14/cr8_so_many_threads.c`).
    The main function is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'It''s quite simple: we convert the string value the user passed as the first
    parameter to a numeric one with `numthrds`; we then have main loop `numthrds` times,
    invoking `pthread_create(3)` and thus creating a brand new thread upon each loop
    iteration! Once created, what do the new threads do? It''s clear – they execute
    the code of the `worker` function. Let''s take a look:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Again, this is very simple: the worker threads just emit a `printf(3)`—which
    is useful because they print out their thread number—it''s just the loop index
    of course. Then, they go to sleep via the `pause(2)` system call. (This system
    call is useful: it''s a perfect blocking call; it puts the calling thread to sleep
    until a signal arrives.)'
  prefs: []
  type: TYPE_NORMAL
- en: 'All right, let''s try it out:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: It works (notice that we've truncated the output as there would be far too much
    to show in this book). Notice how the order in which the threads come alive and
    execute (emitting their `printf`) is random. We can see that the last thread we
    created is the one highlighted in bold—thread `# 299` (0 to 299 is 300 threads).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s run it again, but this time ask it to create an impossibly large
    number of threads (we are currently trying this out on a guest VM with 1 GB of
    RAM):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Obviously, again, the results that you will see will depend on your system;
    we encourage the reader to try it out on different systems. Also, it's possible
    that the actual failure message may have appeared somewhere higher up in your
    Terminal window; scroll up to find it!
  prefs: []
  type: TYPE_NORMAL
- en: The name of the thread, as shown by `ps(1)`, and so on, can be set via the `pthread_setname_np(3)` API;
    note that the `np` suffix implies that the API is non-portable (Linux-only).
  prefs: []
  type: TYPE_NORMAL
- en: How many threads should one create?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The number of threads you create really does depend on the nature of the application. For
    our discussion here, we will consider which the application tends to be – CPU
    versus IO bound.
  prefs: []
  type: TYPE_NORMAL
- en: 'Earlier in this chapter (specifically within the sections on *Design Motivation*
    and* Overlapping CPU with I/O*), we mentioned the fact that a thread, in terms
    of its execution behavior, falls somewhere on a continuum, somewhere between two
    extremes: one extreme being a completely CPU-bound task and the other extreme
    being a completely I/O-bound task. The continuum may be visualized like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4d003f8d-0633-4386-a8b0-d06b42ae6701.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig 3: The CPU-bound/IO-bound continuum'
  prefs: []
  type: TYPE_NORMAL
- en: A thread that is a 100% CPU-bound will be continually hammering away on the
    CPU; a 100% I/O-bound thread is one that is always in a blocking (or wait) state,
    never executing on CPU. Both extremes are unrealistic in real applications; however,
    it's quite easy to visualize the domains where they tend to have one of these.
    For example, domains that involve heavy mathematical processing (scientific models,
    vector graphics such as flash animations in a web browser, matrix multiplication,
    and so on), (un)compression utilities, multimedia codecs, and so on will certainly
    tend to be more CPU-bound. On the other hand, many (but not all) applications
    that us humans interact with on a daily basis (think of your email client, web
    browser, word processing, and so on) tend to wait for the human to do something;
    in effect, they tend to be I/O-bound.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore—a bit simplistically, but nevertheless—this serves as a useful design
    rule of thumb: if the application being designed is I/O-bound in nature, then
    creating even a large-ish number of threads that just wait for work is all right;
    this is because they will be asleep the majority of the time, thus not placing
    any strain on the CPU(s) (of course, create too many threads and they  do strain
    memory.)'
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, if the application is determined to be highly CPU-bound,
    then creating a large number of threads will stress the system (and end up causing
    thrashing – a phenomenon wherein the meta-work takes longer than the actual work!).
    Thus, for CPU-bound workloads, the thumb rule is this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Note, though, that there do exist CPU cores that do not provide any **hyperthreading**
    (**HT**) features; on cores like this, factor should just remain 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'Actually, our discussion has been quite simplistic: many real-world applications
    (think of powerful web servers such as Apache and NGINX) will dynamically create
    and adjust the number of threads required based on the exact circumstances, configuration
    presets, and present workload. Nevertheless, the preceding discussion serves as
    a starting point so that you can start thinking about design for multithreaded
    applications.'
  prefs: []
  type: TYPE_NORMAL
- en: Thread attributes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In our initial discussion on *Thread Creation* earlier in this chapter, we saw
    the `pthread_create(3)` API; the second parameter is a pointer to the thread attribute structure: `const
    pthread_attr_t *attr`. We mentioned there that passing NULL here, in effect, has
    the library create a thread with default attributes. While that is indeed the
    case, the problem is that, for truly portable applications, this is not good enough.
    Why? Because the default thread attributes actually differ quite widely from implementation
    to implementation. The right way-specify the thread attributes explicitly at thread
    creation time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Firstly, of course, we need to learn what attributes a pthread has. The following
    table enumerates this:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Attribute** | **Meaning** | **APIs: **`pthread_attr_[...](3)` | **Values
    Possible** | ***Linux Default*** |'
  prefs: []
  type: TYPE_TB
- en: '| Detach state | Create threads as joinable or detached | `pthread_attr_` `[get&#124;set]detachstate`
    | PTHREAD_CREATE_JOINABLE PTHREAD_CREATE_DETACHED | PTHREAD_CREATE_JOINABLE |'
  prefs: []
  type: TYPE_TB
- en: '| Scheduling/contention scope | Set of threads against which we compete for
    resources (CPU) | `pthread_attr_``[get&#124;set]scope` | PTHREAD_SCOPE_SYSTEM
    PTHREAD_SCOPE_PROCESS | PTHREAD_SCOPE_SYSTEM |'
  prefs: []
  type: TYPE_TB
- en: '| Scheduling/inheritance | Determines whether scheduling attributes are inherited
    implicitly from calling a thread or explicitly from the attr structure | `pthread_attr_``[get&#124;set]inheritsched`
    | PTHREAD_INHERIT_SCHED PTHREAD_EXPLICIT_SCHED | PTHREAD_INHERIT_SCHED |'
  prefs: []
  type: TYPE_TB
- en: '| Scheduling/policy | Determines the scheduling policy of the thread being
    created | `pthread_attr_``[get&#124;set]schedpolicy` | SCHED_FIFO SCHED_RR'
  prefs: []
  type: TYPE_NORMAL
- en: SCHED_OTHER | SCHED_OTHER |
  prefs: []
  type: TYPE_NORMAL
- en: '| Scheduling/priority | Determines the scheduling priority of the thread being created
    | `pthread_attr_``[get&#124;set]schedparam` | struct sched_param holds    int
    sched_priority | 0 (non real-time) |'
  prefs: []
  type: TYPE_TB
- en: '| Stack/guard region | A guard region for the thread''s stack | `pthread_attr_``[get&#124;set]guardsize`
    | Stack guard region size in bytes | 1 page |'
  prefs: []
  type: TYPE_TB
- en: '| Stack/location, size | Query or set the thread''s stack location and size
    | `pthread_attr_` `[get&#124;set]stack``pthread_attr_`'
  prefs: []
  type: TYPE_NORMAL
- en: '`[get&#124;set]stackaddr``pthread_attr_`'
  prefs: []
  type: TYPE_NORMAL
- en: '`[get&#124;set]stacksize` | Stack address and/or stack size, in bytes | Thread
    Stack Location: left to the OSThread Stack Size: 8 MB |'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, clearly understanding what exactly many of these attributes
    signify requires further information. Please be patient as we proceed through
    this chapter (and, in fact, this book), as several of these attributes and their
    meanings will become abundantly clear ( details on scheduling will be shown in
    [Chapter 17](36229bac-c402-4d2f-b876-d1eb4aba8051.xhtml), *CPU Scheduling on Linux*).
  prefs: []
  type: TYPE_NORMAL
- en: Code example – querying the default thread attributes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For now, a useful experiment would be to query the default attributes of a
    newly born thread whose attribute structure is specified as NULL (default). How? `pthread_default_getattr_np(3)` will
    do the trick (note though, that again, the `_np` suffix implies that it''s a Linux-only,
    non-portable API):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Interestingly, as this function depends on the `_GNU_SOURCE` macro being defined,
    we must first define the macro (early in the source); otherwise, the compile triggers
    warnings and possibly fails. (In our code, we thus use `#include "../common.h"`
    first as our *common.h *header defines the `_GNU_SOURCE` macro.)
  prefs: []
  type: TYPE_NORMAL
- en: 'Our code example can be found here, within this book''s GitHub repository:
    `ch14/disp_defattr_pthread.c` *. *'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code, we display a trial run on a Fedora x86_64 box running
    the 4.17.12 Linux kernel:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: For readability, only key parts of the source code are displayed; to view the
    complete source code, build and run it, the entire tree is available for cloning
    from GitHub here: [https://github.com/PacktPublishing/Hands-on-System-Programming-with-Linux](https://github.com/PacktPublishing/Hands-on-System-Programming-with-Linux).
  prefs: []
  type: TYPE_NORMAL
- en: 'The key function here is shown in the following code (`ch14/disp_defattr_pthread.c`);
    we first query and display the thread attribute structure''s "detached state"
    (these terms will be explained in detail shortly):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, various scheduling attributes are queried and displayed (some details
    covered later  in [Chapter 17](36229bac-c402-4d2f-b876-d1eb4aba8051.xhtml), *CPU
    Scheduling on Linux*):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, the thread stack attributes are queried and displayed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we put in the `pthread_getattr_default_np(3)` API to
    query the default thread attributes. Its counterpart, the` pthread_setattr_default_np(3)`
    API, allows you to specify what exactly the default thread attributes should be when
    creating a thread, and the second parameter to `pthread_create(3)` is passed as
    NULL. Do see its man page for details.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is an alternate way to write a similar program: why not create a thread
    with a NULL attribute structure—thus making it default attributes—and then issue
    the `pthread_getattr_np(3)` API to query and display the actual thread attributes?
    We leave this as an exercise to the reader (in fact, the man page on `pthread_attr_init(3)` supplies
    just such a program).'
  prefs: []
  type: TYPE_NORMAL
- en: Joining
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Imagine an application where a thread (typically, main) has spawned off several
    other worker threads. Each worker thread has a specific job to do; once done,
    it terminates (via `pthread_exit(3)`). How will the creator thread know when a
    worker thread is done (terminated)? Ah, that is precisely where joining comes
    in. With the join, the creator thread can wait for, or block upon, the death (termination)
    of another thread within the process!
  prefs: []
  type: TYPE_NORMAL
- en: Does this not sound very much like the `wait(2)` system call that a parent process
    issues to wait for the death of a child? True, but as we shall see shortly, it's
    certainly not identical.
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, importantly, the return value from the thread that terminated is passed
    along to the thread that issued the join upon it. This way, it comes to know whether
    the worker succeeded in its task or not (and if not, the failure value can be
    examined to pinpoint the cause of failure):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: The first parameter to `pthread_join(3)`, `thread,` is the ID of the thread
    to wait for. The moment it terminates, the calling thread will receive, in the
    second parameter (yes, it's a value-result style parameter), the return value
    from the thread that terminated—which, of course is the value passed via its `pthread_exit(3)` call.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, the join is very helpful; using this construct, you can ensure that a thread
    can block upon the termination of any given thread. Specifically, in the case
    of the main thread, we often use this mechanism to ensure that main waits for
    all other application threads to terminate before it itself terminates (thus preventing
    the zombie we saw earlier). This is considered the right approach.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall that in the earlier section, *The return of the ghost*, we clearly saw
    how the main thread, dying before its counterparts, becomes an inadvertent zombie
    (the `ch14/pthreads3.c` program). A quick example, built upon this previous code,
    will help clarify things. So, let''s enhance that program – we shall now call
    it `ch14/pthreads_joiner1.c` – so that we have the main thread wait for all other
    threads to die by invoking the `pthread_join(3)` API on each of the worker threads,
    and only then itself terminate:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'There are a few things to notice here:'
  prefs: []
  type: TYPE_NORMAL
- en: To perform the join subsequently, we require each thread's ID; hence, we declare
    an array of `pthread_t` (the `tid` variable). Each element will store the corresponding
    thread's ID value.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Thread attributes:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Until now, we have not explicitly initialized and made use of a thread attribute
    structure when creating threads. Here, we rectify this shortcoming. `pthread_attr_init(3)` is
    used to initialize (to defaults) an attribute structure.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Furthermore, we explicitly make the threads joinable by setting up this attribute
    within the structure (via the `pthread_attr_setdetachstate(3)` API).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once the threads are created, we must destroy the thread attribute structure
    (via the `pthread_attr_destroy(3)` API).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is key to understand that only threads that have their detach state set as
    joinable can be joined upon. Interestingly, a joinable thread can later be set
    to the detached state (by calling the `pthread_detach(3)` API upon it); there
    is no converse routine.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code continues; we now show you the thread `worker` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Easy: we just have the so-called worker threads sleep for 8 seconds and then
    die; the `pthread_exit(3)`, this time, passes the return status `0` as a parameter.
    In the following code snippet, we continue the code of main:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Here''s the key part: in a loop, the main thread blocks (waits) upon the death
    of each worker thread via the `pthread_join(3)` API; the second (value-result
    style) parameter, in effect, returns the status of the thread that just terminated.
    The usual zero-upon-success convention is followed, thus allowing the main thread
    to figure out whether the worker threads completed their work successfully or
    not.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s build and run it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: As the worker threads die, they are picked up, or joined, by the main thread
    via `pthread_join`; not only that, their termination status—return value—can be
    examined.
  prefs: []
  type: TYPE_NORMAL
- en: Okay, we'll make a copy of the preceding program and call it `ch14/pthreads_joiner2.c`.
    The only change we make is instead of having each worker thread sleep for an identical
    8 seconds, we'll make the sleep time dynamic. We will change the code; for instance,
    this line would be changed:`sleep(slptm);`
  prefs: []
  type: TYPE_NORMAL
- en: The new line would read as follows: `sleep(slptm-datum);`
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, `datum` is the value passed to the thread—the loop index. This way, we
    find that the worker threads sleep as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Worker thread #0 sleeps for (8-0) = 8 seconds'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Worker thread #1 sleeps for (8-1) = 7 seconds'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Worker thread #2 sleeps for (8-2) = 6 seconds'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Obviously, worker thread #2 will terminate first; so what? Well, think about
    it: in the meantime, the main thread is looping around `pthread_join`, but in
    the order of thread #0, thread #1, thread #2. Now, thread #0 will die last and thread
    #2 will die first. Will this be an issue?'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s try it out and see:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'What do we notice? In spite of worker thread #2 dying first, worker thread
    #0 gets joined first because, in the code, that is the thread we wait for first!'
  prefs: []
  type: TYPE_NORMAL
- en: The thread model join and the process model wait
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'By now, you should have begun to realize that although the `pthread_join(3)` and `wait(2)` (and
    family) APIs seem to be very similar, they are certainly not equivalent; several
    differences between them exist and are enumerated in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Situation** | **Thread : `pthread_join(3)`** | **Process: `wait[pid](2)`**
    |'
  prefs: []
  type: TYPE_TB
- en: '| Condition | A thread being waited for must have its detached state attribute
    set as joinable, not detached. | None; any child process can (and in fact must)
    be waited upon (recall our *fork rule #7*) |'
  prefs: []
  type: TYPE_TB
- en: '| Hierarchy | None: any thread can join on any other thread; there is no requirement
    of a parent-child relationship. In fact, we do not consider threads to live within
    a strict parent-child hierarchy as processes do; all threads are peers. | A strict parent-child
    hierarchy exists; only a parent can wait for a child process. |'
  prefs: []
  type: TYPE_TB
- en: '| Order | With threads, one is forced to join (wait) upon the particular thread
    specified as the parameter to `pthread_join(3)`. In other words, if there are,
    say, three threads running and main issues the join within an ascending ordered
    loop, then it must wait for the death or thread #1, then thread #2, and then thread
    #3\. If thread #2 terminates earlier, there is no help for it. | With wait, a
    process can wait upon the death (or stoppage) of any child, or specify a particular
    child process to wait for with waitpid. |'
  prefs: []
  type: TYPE_TB
- en: '| Signaling | No signal is sent upon a thread''s death. | Upon a process''s
    death, the kernel sends the `SIGCHLD` signal to the parent process. |'
  prefs: []
  type: TYPE_TB
- en: 'A few other points to note regarding `pthread_join(3)` are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: You require the thread ID of a thread in order to join upon it; this is deliberately
    done so that we can, in effect, only join the threads of our application process.
    Attempting to join on other threads (like a third-party library thread) would
    be poor design.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What if the thread we are waiting for (to die) has already died? Then `pthread_join(3)` just
    returns immediately.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What if a thread tries to join upon itself? This results in failure (with `errno` set
    to `EDEADLK`).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attempting to have several threads join upon one thread results in undefined
    behavior; avoid this.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If a thread attempting to join on another thread is cancelled (covered later),
    the target thread remains as it was (joinable).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Checking for life, timing out
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Sometimes, we might have a situation wherein we want to check whether a particular
    thread is still alive or not; one way to do so is via the `pthread_tryjoin_np(3)` API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: The first parameter to `pthread_tryjoin_np(3)` is the thread we are attempting
    to join to; (the second parameter, as usual, is the target thread's termination
    status). Notice the try phrase within the API – this typically specifies that
    the call is non-blocking; in other words, we perform a non-blocking join on the
    target thread. If the target thread is alive, then instead of waiting for it to
    die, the API returns immediately with an error: `errno` will be set to `EBUSY` (and
    the man page tells us that this implies the thread had not yet terminated at the
    time of the call).
  prefs: []
  type: TYPE_NORMAL
- en: What if we would like to wait (block) upon a target thread's death, but not
    forever? In other words, we would like to wait for a given maximum time period.
    This can be achieved via the `pthread_timedjoin_np(3)` API; the first two parameters
    are the usual ones (the same as with `pthread_join)`, while the third parameter
    specifies the timeout in terms of the absolute time (or what is often called Unix
    time – the number of seconds (and nanoseconds) elapsed since midnight 1 January
    1970—the Epoch!).
  prefs: []
  type: TYPE_NORMAL
- en: 'As covered in [Chapter 13](1f621f72-e067-42db-b2eb-b82e20161dec.xhtml), *Timers*, the `timespec` data
    structure is of the following format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: That's easy; but how do we specify the time as UNIX time (or time since the
    Epoch)? We refer the reader to the man page on `pthread_timedjoin_np(3)`, which
    gives a simple example of the same (also, we ask you try this API out as an exercise).
  prefs: []
  type: TYPE_NORMAL
- en: Another thing I noticed when using the `pthread_timedjoin_np(3)` API: it's possible
    that the join times out and then proceeds to, say, release some resources – like
    performing `free(3)` on a heap buffer—while the worker thread is still alive and
    using it. This is a bug, of course; it also goes to show that you must carefully
    think out and test the design; usually, using a blocking join on all worker threads,
    thus ensuring they have all terminated before freeing up resources, is the right
    approach.
  prefs: []
  type: TYPE_NORMAL
- en: Again, we remind you that the `_np` suffix to the APIs implies that they are non-portable
    (Linux-only).
  prefs: []
  type: TYPE_NORMAL
- en: Join or not?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A thread that is explicitly set to the detached state cannot be joined upon;
    so, what happens when it dies? Its resources are disposed of by the library.
  prefs: []
  type: TYPE_NORMAL
- en: 'A thread that is explicitly set to the joinable state (or if joinable is the
    default state) must be joined upon; failure to do so results in a kind of resource
    leakage. So, be careful: if you have created threads to be joinable, then you
    must ensure that the join is performed.'
  prefs: []
  type: TYPE_NORMAL
- en: Performing a join on other app threads by the main thread is usually considered
    a best practice, since it prevents the zombie thread behavior we saw earlier.
    Also, it's usually important for the creator thread to come to know whether its
    workers successfully performed their job or not, and if not, why not. The join
    makes all of this possible.
  prefs: []
  type: TYPE_NORMAL
- en: However, it is possible that your application does not want to wait around for
    some worker threads; in this case, ensure that you create them as detached.
  prefs: []
  type: TYPE_NORMAL
- en: Parameter passing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Recall the signature of the `pthread_create(3)` API:'
  prefs: []
  type: TYPE_NORMAL
- en: '`int pthread_create(pthread_t *thread, const pthread_attr_t *attr,`'
  prefs: []
  type: TYPE_NORMAL
- en: '`                    void *(*start_routine) **(void *), void *arg**);`'
  prefs: []
  type: TYPE_NORMAL
- en: 'The third parameter is the thread function—in effect, the life and scope of
    the newly born thread. It receives a single parameter of type `void *`; this parameter
    to the new born thread is passed via the fourth parameter to `pthread_create`:
    `void *arg`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As mentioned earlier, its data type is a generic pointer, precisely so that
    we can, in effect, pass along any data type as a parameter, and then in the thread
    routine, appropriately typecast and use it. Until now, we have come across simple
    use cases of the same – typically, passing along an integer value as the parameter.
    In our very first simple multithreaded app – `ch14/pthreads1.c` – in our main function, we
    did the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'And, in the thread routine worker, we performed a simple typecast-and-use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'That''s easy, but it does raise a fairly obvious question: in the `pthread_create(3)` API, as
    there seems to be just one placeholder for the `arg` (the parameter) how can you
    pass along more than one data item – several parameters, in effect – to the thread
    routine?'
  prefs: []
  type: TYPE_NORMAL
- en: Passing a structure as a parameter
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The preceding heading gives it away: we pass a data structure. But how, exactly?
    Allocate memory to a pointer to the data structure, initialize it, and pass the
    pointer typecast as `void *`. (In fact, this is a very common approach that C
    programmers use.) In the thread routine, as usual, typecast and use it.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To bring clarity, we will try this out ( `ch14/param_passing/struct_as_param.c`):'
  prefs: []
  type: TYPE_NORMAL
- en: For readability, only key parts of the source code are displayed; to view the
    complete source code, build, and run it, the entire tree is available for cloning
    from GitHub here: [https://github.com/PacktPublishing/Hands-on-System-Programming-with-Linux](https://github.com/PacktPublishing/Hands-on-System-Programming-with-Linux)*.*
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: As an example, we build our very own airport info data structure, airport, and then
    set up an array (`city_airports`), initializing a few members of it.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the main function, we declare an array of pointers to the airport structure;
    we know that a pointer by itself has no memory, so in the thread creation loop,
    we allocate memory to each pointer and then initialize it to an airport (via a
    simple `memcpy(3)`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: Okay, so we already know that the preceding code is not really optimal; we could
    have just passed the `city_airports[i]` structure pointer as the parameter to
    the thread. For the sake of a pedantic example, making use of our just allocated
    `plocdata[i]` structures, we `memcpy` one structure into another.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, in the `pthread_create(3)` call, we pass the pointer to our data structure
    as the fourth parameter.  This will become the argument to the thread; in the
    thread routine, we declare an `arg` pointer of the same data type and equate it
    to the typecast data pointer we receive:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: We can then proceed to use `arg` as a pointer to Airport; in the preceding demo
    code, we merely print out the values in the structure. We encourage the reader
    to build and run this code.
  prefs: []
  type: TYPE_NORMAL
- en: Did you notice the `%.*s` C printf format specifier trick in the preceding code?
    This is done when we want to print a string that is not necessarily NULL-terminated;
    the `%.*s` format specifier allows one to specify the size followed by the string
    pointer. The string will be printed to only size bytes.
  prefs: []
  type: TYPE_NORMAL
- en: Thread parameters – what not to do
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The really key thing to keep in mind when passing a parameter to a thread routine
    is that you must guarantee that the parameter passed along is thread-safe; essentially,
    that it does not get modified in any manner while a thread (or threads) are using
    it.
  prefs: []
  type: TYPE_NORMAL
- en: (Thread safety is a crucial aspect of working with threads; we shall revisit
    this point often in upcoming chapters, too).
  prefs: []
  type: TYPE_NORMAL
- en: 'To help understand the possible issues clearly, let''s take a couple of typical
    examples. In the first one, we shall (attempt to) pass the loop index as the parameter
    to the newly born thread such as, in main (code: `ch14/pthreads1_wrong.c`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Did you notice!? We have passed the parameter as `&i`. So? Dereferencing it
    correctly in the thread routine should still work, right:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: Looks okay – let's give it a try!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Well, it works. But hang on, try it a few more times—timing coincidences can
    fool you into thinking that all''s well when it''s really not:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'There''s a bug! The index value has evaluated to the value 2 twice; why? Think
    carefully: we have passed the loop index by reference – as the pointer to the
    loop variable. Thread 1 comes alive, and looks up its value – so does thread 2,
    as does thread 3\. But wait: isn''t it possible that we have a race here? Isn''t
    it possible that by the time thread 1 runs and looks up the value of the loop
    variable it has already changed underneath it (because, don''t forget, the loop
    is running in main)? That, of course, is precisely what happened in the preceding
    code.'
  prefs: []
  type: TYPE_NORMAL
- en: In other words, passing the variable by address is unsafe because its value
    could change while it is being read (by the worker threads) as it being simultaneously
    written to (by main); hence, it's not thread-safe and therefore will be buggy
    (racy).
  prefs: []
  type: TYPE_NORMAL
- en: 'The solution is actually really simple: do not pass the loop index by address;
    just pass it as a literal value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: Now, each worker thread receives a copy of the loop index, thus eliminating
    any race, thus making it safe.
  prefs: []
  type: TYPE_NORMAL
- en: Now, don't jump to the conclusion that, hey, okay, so we should never pass a
    pointer (an address) as a parameter. Of course you can! Just ensure that it's thread-safe
    – that its value cannot change underneath it while being manipulated by main and
    the other application threads.
  prefs: []
  type: TYPE_NORMAL
- en: Refer back to the `ch14/struct_as_param.c` code we demonstrated in the previous
    section; we very much pass the thread parameter as a pointer to a structure. Look
    closely: each pointer was separately allocated (via `calloc(3)`) in the main thread
    creation loop. Thus, each worker thread received its own copy of the structure;
    hence, all is safe and it works well.
  prefs: []
  type: TYPE_NORMAL
- en: An interesting exercise (that we leave to the reader) is to deliberately insert
    a defect into the `struct_as_param` application by using exactly one allocated
    structure (not three) and passing it to each of the worker threads. This time,
    it will be racy and will (eventually) fail.
  prefs: []
  type: TYPE_NORMAL
- en: Thread stacks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We understand that whenever a thread is created, it acquires a new, freshly
    allocated piece of memory for its stack. This leads to the understanding that
    (obviously, but we shall state it nevertheless) all local variables declared within
    a thread function will remain private to that thread; this is because they will
    reside in that thread's stack. (Refer back to *Fig 2* in this chapter – the new
    stack of the newly created thread is shown in red). Also, whenever a context switch
    occurs, the **Stack Pointer** (**SP**) register is updated to point to the current
    thread's stack.
  prefs: []
  type: TYPE_NORMAL
- en: Get and set thread stack size
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Knowing, and being able to change, the size of thread stacks does matter (do
    see the link provided in the *Further reading* section on the GitHub repository,
    which mentions a real-world experience on how setting up a stack that's too small
    for a certain platform caused random and really hard-to-debug failures).
  prefs: []
  type: TYPE_NORMAL
- en: 'So, what is the default thread stack size? The answer has already been provided;
    recall the `disp_defattr_pthread` program we ran earlier in this chapter (in the *Code
    example – querying the default thread attributes* section): it shows us that the
    default thread stack size on the (modern NPTL) Linux platform is 8 MB.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The pthreads API set provides a few routines to set and query the thread stack
    size. One way is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: As we have already used `pthread_attr_getstacksize(3)` in the earlier `disp_defattr_pthread` program,
    we shall refrain from showing its usage once more over here. Setting the thread
    size is easily done with the complementary `pthread_attr_setstacksize(3)` API
    – the second parameter is the required size (in bytes). Note, though, that both
    of these APIs have the phrase `_attr_` in them, implying that the stack size is
    actually set or queried from the thread attribute structure and not a live thread
    itself. This leads us to understand that we can only set or query the stack size
    at the time of creation of the thread by setting up the attribute structure (which
    is, of course, subsequently passed as the second parameter to `pthread_create(3)`).
    Once a thread is created, its stack size cannot be changed. The exception to this
    rule is the stack of the main thread.
  prefs: []
  type: TYPE_NORMAL
- en: Stack location
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Where in memory (technically, where in the VAS of the given process) does the
    thread stack actually reside? The following points help us in this regard:'
  prefs: []
  type: TYPE_NORMAL
- en: The stack of the main thread is always situated at the very top of the process
    VAS.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The stacks of all other threads in the process are located somewhere between
    the process heap segment and the stack of main; the precise location is not known
    in advance to the app developer; in any case, we should not need to know.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This is not directly related, but important: recall from [Chapter 2](976fc2af-8bb4-4060-96cd-3b921682ed75.xhtml),
    *Virtual Memory*, that, for most processors, the stack(s) conform to the stack-grows-down semantic;
    that is, the direction of growth of the stack segment is toward lower virtual
    addresses.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Though we should not need to, is there a way to specify the location of the
    thread stack? Well, yes, if you insist: the `pthread_attr_[get|set]stack(3)` APIs
    can be used for this purpose, as well as to set and/or query the thread stack''s
    size:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: Although you can use `pthread_attr_setstack` to set the stack location, it's
    recommended that this be left to the OS. Also, if you do use it, it's again recommended
    that both the stack location, `stackaddr`, and the stack size, `stacksize`, be
    a multiple of the system page size (and that the location is aligned to a page
    boundary). Aligning the thread stack to a page boundary can be easily achieved
    via the `posix_memalign(3)` API (we have covered example usage of this API in [Chapter
    4](0b4868f7-a8d0-4ced-831f-20af9929de9f.xhtml), *Dynamic Memory Allocation*).
  prefs: []
  type: TYPE_NORMAL
- en: 'Be careful: if you are specifying the stack location within the thread attribute
    structure, and creating threads in a loop (as is the normal fashion), you must
    ensure that each thread receives a unique stack location (this is often done by
    allocating the stack memory via the aforementioned `posix_memalign(3)` and then
    passing its return value as the stack location). Also, of course, the memory pages
    that will be used for the thread stack(s) must have both read-write permission
    (recall `mprotect(2)` from [Chapter 4](0b4868f7-a8d0-4ced-831f-20af9929de9f.xhtml),
    *Dynamic Memory Allocation*).'
  prefs: []
  type: TYPE_NORMAL
- en: 'After all is said and done, the mechanics of setting and querying the thread
    stack is straightforward; the really key point is this: (stress) test your application
    to ensure that the provided thread stack memory is sufficient. As we saw in the
    [Chapters 11](99fafa09-8972-4d9f-b241-46caf9de98f3.xhtml),  *Signaling - Part
    I*, overflowing the stack is a serious defect and will cause undefined behavior.'
  prefs: []
  type: TYPE_NORMAL
- en: Stack guards
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This neatly brings us to the next point: is there a way to have the application
    know that stack memory is in danger of being, or rather, has been, overflowed?
    Indeed: stack guards. Guard memory is a region of one or more virtual memory pages
    that has been deliberately placed, and with appropriate permissions, to ensure
    that any attempt to access that memory results in failure (or a warning of some
    sort; for example, a signal handler for `SIGSEGV` could provide just such a semantic
    - with the caveat that once we''ve received the SIGSEGV, we are in an undefined
    state and must terminate; but at least we''ll know and can fix the stack size!):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'The guard region is an additional memory region allocated at the end of the
    thread stack for the number of bytes specified. The default (guard) size is the
    system page size. Note, again, that the guard size is an attribute of the thread
    and can thus only be specified at thread creation time (and not later). We will
    run the (code: `ch14/stack_test.c`) app like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we specify 2,560 KB (2.5 MB) as the thread stack size.
    Though this is far less than the default (8 MB), it turns out to be enough (for
    x86_64 at least, a quick back-of-the-envelope calculation shows that, for the
    given program parameters, we shall require a minimum of 1,960 KB to be allocated
    for each thread stack).
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code, we run it again, but this time specify the thread stack
    size as a mere 256 KB:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: And, as expected, it segfaults.
  prefs: []
  type: TYPE_NORMAL
- en: Examining the core dump with GDB will reveal a lot of clues regarding why the
    segfault occurred – including, very importantly, the state of the thread stacks
    (in effect, the stack `backtrace(s)`), at the time of the crash. This, however,
    goes beyond the scope of this book.
  prefs: []
  type: TYPE_NORMAL
- en: We definitely encourage you to learn about using a powerful debugger such as
    GDB (see the *Further reading *section on the GitHub repository as well).
  prefs: []
  type: TYPE_NORMAL
- en: 'Also (on our test system at least), the kernel emits a message into the kernel
    log regarding this crash; one way to look up the kernel log messages is via the
    convenience utility `dmesg(1)`. The following output is from an Ubuntu 18.04 box:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'The code for the preceding application can be found here: `ch14/stack_test.c`
    :'
  prefs: []
  type: TYPE_NORMAL
- en: For readability, only key parts of the source code are displayed; to view the
    complete source code, build it, and run it, the entire tree is available for cloning
    from GitHub here: [https://github.com/PacktPublishing/Hands-on-System-Programming-with-Linux](https://github.com/PacktPublishing/Hands-on-System-Programming-with-Linux).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: In main, we show the thread stack size attribute being initialized to the parameter
    passed by the user (in KB). The code then goes on to create three worker threads
    and then joins (waits) on them.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the thread worker routine, we have only thread #2 performing some actual
    work—you guessed it, stack-intensive work. The code for this is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'The `danger` function, of course, is the one where this dangerous, potentially
    stack-overflowing work is carried out:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: The preceding function uses large amounts of (thread) stack space since we have
    declared a local variable called `heavylocal` – a 2D-array of `NEL*NEL` elements
    (`NEL=500`). On an x86_64 with a long data type occupying 8 bytes, this works
    out to approximately 2 MB of space! Thus, specifying the thread stack size as
    any less than 2 MB should result in a stack overflow (the stack guard memory region
    will in fact detect this) and therefore result in a segmentation violation (or
    segfault); this is precisely what happened (as you can see in our trial run).
  prefs: []
  type: TYPE_NORMAL
- en: Interestingly, if we merely declare the local variable but do not actually make
    use of it, modern compilers will just optimize the code out; hence, in the code,
    we strive to make some (silly) use of the `heavylocal` variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'A few additional points on the stack guard memory region, to round off this
    discussion, are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: If an application has used `pthread_attr_setstack(3)`, it implies that it is
    managing thread stack memory itself, and any guard size attribute will be ignored.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The guard region must be aligned to a page boundary.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the size of the guard memory region is less than a page, the actual (internal)
    size will be rounded to a page; `pthread_attr_getguardsize(3)` returns the theoretical
    size.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The man page on `pthread_attr_[get|set]guardsize(3)` does provide additional
    information, including possible glibc bugs within the implementation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter forms the first of three on the large topic of writing multithreaded
    applications on the Linux platform. Here, we have covered two key areas: the first
    was in regards to the all-important concepts regarding what exactly is a thread,
    and we contrast it to the process model (which we studied in [Chapter 9](3b2340aa-4ab7-46e3-93c0-7f7c210f834b.xhtml), *Process
    Execution* and [Chapter 10](607ad988-406d-4736-90a4-3a318672ab6e.xhtml), *Process
    Creation*). Why you would prefer a multithreaded design was covered in some detail,
    and included three examples. In this way, the motivation to use a multithreaded
    design approach was being brought out.'
  prefs: []
  type: TYPE_NORMAL
- en: The second part of this chapter focused on the actual pthread APIs (and their
    related concepts), how we create a thread—how many can and how many should be
    created was addressed as well. Thread termination basics, thread attributes, passing
    along a parameter to the newly created thread, what is joining and how to perform
    it, and finally, details on how we can manipulate the thread stack (and stack
    guard) size was covered. Many example programs were shown to help solidify the
    concepts that were taught.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we shall focus squarely on another critical aspect of writing
    powerful and safe multithreaded software – the issues of concurrency, races, critical
    sections, deadlock (and it's avoidance) and atomicity; how we deal with these
    using the mutex lock (and it's variants), as well as the condition variable.
  prefs: []
  type: TYPE_NORMAL
