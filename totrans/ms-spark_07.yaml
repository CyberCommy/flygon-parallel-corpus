- en: Chapter 7. Extending Spark with H2O
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'H2O is an open source system, developed in Java by [http://h2o.ai/](http://h2o.ai/)
    for machine learning. It offers a rich set of machine learning algorithms, and
    a web-based data processing user interface. It offers the ability to develop in
    a range of languages: Java, Scala, Python, and R. It also has the ability to interface
    to Spark, HDFS, Amazon S3, SQL, and NoSQL databases. This chapter will concentrate
    on H2O''s integration with Apache Spark using the **Sparkling Water** component
    of H2O. A simple example, developed in Scala, will be used, based on real data
    to create a deep-learning model. This chapter will:'
  prefs: []
  type: TYPE_NORMAL
- en: Examine the H2O functionality
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consider the necessary Spark H2O environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Examine the Sparkling Water architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduce and use the H2O Flow interface
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduce deep learning with an example
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consider performance tuning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Examine data quality
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The next step will be to provide an overview of the H2O functionality, and the
    Sparkling Water architecture that will be used in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Since it is only possible to examine, and use, a small amount of H2O''s functionality
    in this chapter, I thought that it would be useful to provide a list of all of
    the functional areas that it covers. This list is taken from [http://h2o.ai/](http://h2o.ai/)
    website at [http://h2o.ai/product/algorithms/](http://h2o.ai/product/algorithms/)
    and is based upon munging/wrangling data, modeling using the data, and scoring
    the resulting models:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Process | Model | The score tool |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Data profiling | Generalized Linear Models (GLM) | Predict |'
  prefs: []
  type: TYPE_TB
- en: '| Summary statistics | Decision trees | Confusion Matrix |'
  prefs: []
  type: TYPE_TB
- en: '| Aggregate, filter, bin, and derive columns | Gradient Boosting (GBM) | AUC
    |'
  prefs: []
  type: TYPE_TB
- en: '| Slice, log transform, and anonymize | K-Means | Hit Ratio |'
  prefs: []
  type: TYPE_TB
- en: '| Variable creation | Anomaly detection | PCA Score |'
  prefs: []
  type: TYPE_TB
- en: '| PCA | Deep learning | Multi Model Scoring |'
  prefs: []
  type: TYPE_TB
- en: '| Training and validation sampling plan | Naïve Bayes |   |'
  prefs: []
  type: TYPE_TB
- en: '|   | Grid search |   |'
  prefs: []
  type: TYPE_TB
- en: The following section will explain the environment used for the Spark and H2O
    examples in this chapter and it will also explain some of the problems encountered.
  prefs: []
  type: TYPE_NORMAL
- en: The processing environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If any of you have examined my web-based blogs, or read my first book, *Big
    Data Made Easy*, you will see that I am interested in Big Data integration, and
    how the big data tools connect. None of these systems exist in isolation. The
    data will start upstream, be processed in Spark plus H2O, and then the result
    will be stored, or moved to the next step in the ETL chain. Given this idea in
    this example, I will use Cloudera CDH HDFS for storage, and source my data from
    there. I could just as easily use S3, an SQL or NoSQL database.
  prefs: []
  type: TYPE_NORMAL
- en: 'At the point of starting the development work for this chapter, I had a Cloudera
    CDH 4.1.3 cluster installed and working. I also had various Spark versions installed,
    and available for use. They are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Spark 1.0 installed as CentOS services
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark 1.2 binary downloaded and installed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark 1.3 built from a source snapshot
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'I thought that I would experiment to see which combinations of Spark, and Hadoop
    I could get to work together. I downloaded Sparkling water at [http://h2o-release.s3.amazonaws.com/sparkling-water/master/98/index.html](http://h2o-release.s3.amazonaws.com/sparkling-water/master/98/index.html)
    and used the 0.2.12-95 version. I found that the 1.0 Spark version worked with
    H2O, but the Spark libraries were missing. Some of the functionality that was
    used in many of the Sparkling Water-based examples was available. Spark versions
    1.2 and 1.3 caused the following error to occur:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The Spark master port number, although correctly configured in Spark, was not
    being picked up, and so the H2O-based application could not connect to Spark.
    After discussing the issue with the guys at H2O, I decided to upgrade to an H2O
    certified version of both Hadoop and Spark. The recommended system versions that
    should be used are available at [http://h2o.ai/product/recommended-systems-for-h2o/](http://h2o.ai/product/recommended-systems-for-h2o/).
  prefs: []
  type: TYPE_NORMAL
- en: I upgraded my CDH cluster from version 5.1.3 to version 5.3 using the Cloudera
    Manager interface parcels page. This automatically provided Spark 1.2—the version
    that has been integrated into the CDH cluster. This solved all the H2O-related
    issues, and provided me with an H2O-certified Hadoop and Spark environment.
  prefs: []
  type: TYPE_NORMAL
- en: Installing H2O
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For completeness, I will show you how I downloaded, installed, and used H2O.
    Although, I finally settled on version 0.2.12-95, I first downloaded and used
    0.2.12-92\. This section is based on the earlier install, but the approach used
    to source the software is the same. The download link changes over time so follow
    the Sparkling Water download option at [http://h2o.ai/download/](http://h2o.ai/download/).
  prefs: []
  type: TYPE_NORMAL
- en: 'This will source the zipped Sparkling water release, as shown by the CentOS
    Linux long file listing here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'This zipped release file is unpacked using the Linux `unzip` command, and it
    results in a sparkling water release file tree:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'I have moved the release tree to the `/usr/local/` area using the root account,
    and created a simple symbolic link to the release called `h2o`. This means that
    my H2O-based build can refer to this link, and it doesn''t need to change as new
    versions of sparkling water are sourced. I have also made sure, using the Linux
    `chmod` command, that my development account, hadoop, has access to the release:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The release has been installed on all the nodes of my Hadoop CDH clusters.
  prefs: []
  type: TYPE_NORMAL
- en: The build environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'From past examples, you will know that I favor SBT as a build tool for developing
    Scala source examples. I have created a development environment on the Linux CentOS
    6.5 server called `hc2r1m2` using the hadoop development account. The development
    directory is called `h2o_spark_1_2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'My SBT build configuration file named `h2o.sbt` is located here; it contains
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: I have provided SBT configuration examples in the previous chapters, so I won't
    go into the line-by line-detail here. I have used the file-based URLs to define
    the library dependencies, and have sourced the Hadoop JAR files from the Cloudera
    parcel path for the CDH install. The Sparkling Water JAR path is defined as `/usr/local/h2o/`
    that was just created.
  prefs: []
  type: TYPE_NORMAL
- en: 'I use a Bash script called `run_h2o.bash` within this development directory
    to execute my H2O-based example code. It takes the application class name as a
    parameter, and is shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This example of Spark application submission has already been covered, so again,
    I won't get into the detail. Setting the executor memory at a correct value was
    critical to avoiding out-of-memory issues and performance problems. This will
    be examined in the *Performance Tuning* section.
  prefs: []
  type: TYPE_NORMAL
- en: As in the previous examples, the application Scala code is located in the `src/main/scala`
    subdirectory, under the `development` directory level. The next section will examine
    the Apache Spark, and the H2O architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The diagrams in this section have been sourced from the [http://h2o.ai/](http://h2o.ai/)
    web site at [http://h2o.ai/blog/2014/09/how-sparkling-water-brings-h2o-to-spark/](http://
    http://h2o.ai/blog/2014/09/how-sparkling-water-brings-h2o-to-spark/) to provide
    a clear method of describing the way in which H2O Sparkling Water can be used
    to extend the functionality of Apache Spark. Both, H2O and Spark are open source
    systems. Spark MLlib contains a great deal of functionality, while H2O extends
    this with a wide range of extra functionality, including deep learning. It offers
    tools to *munge* (transform), model, and score the data. It also offers a web-based
    user interface to interact with.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next diagram, borrowed from [http://h2o.ai/](http://h2o.ai/), shows how
    H2O integrates with Spark. As we already know, Spark has master and worker servers;
    the workers create executors to do the actual work. The following steps occur
    to run a Sparkling water-based application:'
  prefs: []
  type: TYPE_NORMAL
- en: Spark's `submit` command sends the sparkling water JAR to the Spark master.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Spark master starts the workers, and distributes the JAR file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Spark workers start the executor JVMs to carry out the work.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Spark executor starts an H2O instance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The H2O instance is embedded with the Executor JVM, and so it shares the JVM
    heap space with Spark. When all of the H2O instances have started, H2O forms a
    cluster, and then the H2O flow web interface is made available.
  prefs: []
  type: TYPE_NORMAL
- en: '![Architecture](img/B01989_07_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding diagram explains how H2O fits into the Apache Spark architecture,
    and how it starts, but what about data sharing? How does data pass between Spark
    and H2O? The following diagram explains this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Architecture](img/B01989_07_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: A new H2O RDD data structure has been created for H2O and Sparkling Water. It
    is a layer, based at the top of an H2O frame, each column of which represents
    a data item, and is independently compressed to provide the best compression ratio.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the deep learning example, Scala code presented later in this chapter you
    will see that a data frame has been created implicitly from a Spark schema RDD
    and a columnar data item, income has been enumerated. I won''t dwell on this now
    as it will be explained later but this is a practical example of the above architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'In the Scala-based example that will be tackled in this chapter, the following
    actions will take place:'
  prefs: []
  type: TYPE_NORMAL
- en: Data is being sourced from HDFS, and is being stored in a Spark RDD.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Spark SQL is used to filter data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Spark schema RDD is converted into an H2O RDD.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The H2O-based processing and modeling occurs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The results are passed back to Spark for accuracy checking.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To this point, the general architecture of H2O has been examined, and the product
    has been sourced for use. The development environment has been explained, and
    the process by which H2O and Spark integrate has been considered. Now, it is time
    to delve into a practical example of the use of H2O. First though, some real-world
    data must be sourced for modeling purposes.
  prefs: []
  type: TYPE_NORMAL
- en: Sourcing the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since I have already used the **Artificial Neural Net** (**ANN**) functionality
    in [Chapter 2](ch02.html "Chapter 2. Apache Spark MLlib"), *Apache Spark MLlib*,
    to classify images, it seems only fitting that I use H2O deep learning to classify
    data in this chapter. In order to do this, I need to source data sets that are
    suitable for classification. I need either image data with associated image labels,
    or the data containing vectors and a label that I can enumerate, so that I can
    force H2O to use its classification algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: The MNIST test and training image data was sourced from [ann.lecun.com/exdb/mnist/](http://ann.lecun.com/exdb/mnist/).
    It contains 50,000 training rows, and 10,000 rows for testing. It contains digital
    images of numbers 0 to 9 and associated labels.
  prefs: []
  type: TYPE_NORMAL
- en: 'I was not able to use this data as, at the time of writing, there was a bug
    in H2O Sparkling water that limited the record size to 128 elements. The MNIST
    data has a record size of *28 x 28 + 1* elements for the image plus the label:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'This issue should have been fixed and released by the time you read this, but
    in the short term I sourced another data set called income from [http://www.cs.toronto.edu/~delve/data/datasets.html](http://www.cs.toronto.edu/~delve/data/datasets.html),
    which contains Canadian employee income data. The following information shows
    the attributes and the data volume. It also shows the list of columns in the data,
    and a sample row of the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: I will enumerate the last column in the data—the income bracket, so `<=50k`
    will enumerate to `0`. This will allow me to force the H2O deep learning algorithm
    to carry out classification rather than regression. I will also use Spark SQL
    to limit the data columns, and filter the data.
  prefs: []
  type: TYPE_NORMAL
- en: Data quality is absolutely critical when creating an H2O-based example like
    that described in this chapter. The next section examines the steps that can be
    taken to improve the data quality, and so save time.
  prefs: []
  type: TYPE_NORMAL
- en: Data Quality
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When I import CSV data files from HDFS to my Spark Scala H2O example code,
    I can filter the incoming data. The following example code contains two filter
    lines; the first checks that a data line is not empty, while the second checks
    that the final column in each data row (income), which will be enumerated, is
    not empty:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'I also needed to clean my raw data. There are two data sets, one for training
    and one for testing. It is important that the training and testing data have the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: The same number of columns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The same data types
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The null values must be allowed for in the code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The enumerated type values must match—especially for the labels
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I encountered an error related to the enumerated label column income and the
    values that it contained. I found that my test data set rows were terminated with
    a full stop character "`.`" When processed, this caused the training and the test
    data values to mismatch when enumerated.
  prefs: []
  type: TYPE_NORMAL
- en: So, I think that time and effort should be spent safeguarding the data quality,
    as a pre-step to training, and testing machine learning functionality so that
    time is not lost, and extra cost incurred.
  prefs: []
  type: TYPE_NORMAL
- en: Performance tuning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It is important to monitor the Spark application error and the standard output
    logs in the Spark web user interface if you see errors like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'If you encounter instances where application executors seem to hang without
    response, you may need to tune your executor memory. You need to do so if you
    see an error like the following in your executor log:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: This can cause a loop, as the application requests more memory than is available,
    and so waits until the next iteration retries. The application can seem to hang
    until the executors are killed, and the tasks re-executed on alternate nodes.
    A short task's run time can extend considerably due to such problems.
  prefs: []
  type: TYPE_NORMAL
- en: Monitor the Spark logs for these types of error. In the previous example, changing
    the executor memory setting in the `spark-submit` command removes the error, and
    reduces the runtime substantially. The memory value requested has been reduced
    to a figure below that which is available.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Deep learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Neural networks were introduced in [Chapter 2](ch02.html "Chapter 2. Apache
    Spark MLlib"), *Apache Spark MLlib*. This chapter builds upon this understanding
    by introducing deep learning, which uses deep neural networks. These are neural
    networks that are feature-rich, and contain extra hidden layers, so that their
    ability to extract data features is increased. These networks are generally feed-forward
    networks, where the feature characteristics are inputs to the input layer neurons.
    These neurons then fire and spread the activation through the hidden layer neurons
    to an output layer, which should present the feature label values. Errors in the
    output are then propagated back through the network (at least in back propagation),
    adjusting the neuron connection weight matrices so that classification errors
    are reduced during training.
  prefs: []
  type: TYPE_NORMAL
- en: '![Deep learning](img/B01989_07_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The previous example image, described in the H2O booklet at [https://leanpub.com/deeplearning/read](https://leanpub.com/deeplearning/read)
    ,shows a deep learning network with four input neurons to the left, two hidden
    layers in the middle, and two output neurons. The arrows show both the connections
    between neurons and the direction that activation takes through the network.
  prefs: []
  type: TYPE_NORMAL
- en: 'These networks are feature-rich because they provide the following options:'
  prefs: []
  type: TYPE_NORMAL
- en: Multiple training algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Automated network configuration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ability to configure many options
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Structure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden layer structure
  prefs: []
  type: TYPE_NORMAL
- en: Training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning rate, annealing, and momentum
  prefs: []
  type: TYPE_NORMAL
- en: 'So, after giving this brief introduction to deep learning, it is now time to
    look at some of the sample Scala-based code. H2O provides a great deal of functionality;
    the classes that are needed to build and run the network have been developed for
    you. You just need to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Prepare the data and parameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create and train the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Validate the model with a second data set
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Score the validation data set output
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When scoring your model, you must hope for a high value in percentage terms.
    Your model must be able to accurately predict and classify your data.
  prefs: []
  type: TYPE_NORMAL
- en: Example code – income
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This section examines the Scala-based H2O Sparkling Water deep learning example
    using the previous Canadian income data source. First, the Spark (`Context`, `Conf`,
    `mllib`, and `RDD`), and H2O (`h2o`, `deeplearning`, and `water`) classes are
    imported:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Next an application class called `h2o_spark_dl2` is defined, the master URL
    is created, and then a configuration object is created, based on this URL, and
    the application name. The Spark context is then created using the configuration
    object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'An H2O context is created from the Spark context, and also an SQL context:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The H2O Flow user interface is started with the `openFlow` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The training and testing of the data files are now defined (on HDFS) using
    the server URL, path, and the file names:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The CSV based training and testing data is loaded using the Spark context''s
    `textFile` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, the schema is defined in terms of a string of attributes. Then, a schema
    variable is created by splitting the string using a series of `StructField`, based
    on each column. The data types are left as String, and the true value allows for
    the Null values in the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The raw CSV line `training` and testing data is now split by commas into columns.
    The data is filtered on empty lines to ensure that the last column (`income`)
    is not empty. The actual data rows are created from the fifteen (0-14) trimmed
    elements in the raw CSV data. Both, the training and the test data sets are processed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Spark Schema RDD variables are now created for the training and test data sets
    by applying the schema variable, created previously for the data using the Spark
    context''s `applySchema` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Temporary tables are created for the training and testing data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, SQL is run against these temporary tables, both to filter the number of
    columns, and to potentially limit the data. I could have added a `WHERE` or `LIMIT`
    clause. This is a useful approach that enables me to manipulate both the column
    and row-based data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The H2O data frames are now created from the data. The final column in each
    data set (income) is enumerated, because this is the column that will form the
    deep learning label for the data. Also, enumerating this column forces the deep
    learning model to carry out classification rather than regression:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The enumerated results data income column is now saved so that the values in
    this column can be used to score the tested model prediction values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The deep learning model parameters are now set up in terms of the number of
    epochs, or iterations—the data sets for training and validation and the label
    column income, which will be used to classify the data. Also, we chose to use
    variable importance to determine which data columns are most important in the
    data. The deep learning model is then created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The model is then scored against the test data set for predictions, and these
    income predictions are compared to the previously stored enumerated test data
    income values. Finally, an accuracy percentage is output from the test data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'In the last step, the application is stopped, the H2O functionality is terminated
    via a `shutdown` call, and then the Spark context is stopped:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Based upon a training data set of 32,000, and a test data set of 16,000 income
    records, this deep learning model is quite accurate. It reaches an accuracy level
    of `83` percent, which is impressive for a few lines of code, small data sets,
    and just 100 epochs, as the run output shows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: In the next section, I will examine some of the coding needed to process the
    MNIST data, even though that example could not be completed due to an H2O limitation
    at the time of coding.
  prefs: []
  type: TYPE_NORMAL
- en: The example code – MNIST
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Since the MNIST image data record is so big, it presents problems while creating
    a Spark SQL schema, and processing a data record. The records in this data are
    in CSV format, and are formed from a 28 x 28 digit image. Each line is then terminated
    by a label value for the image. I have created my schema by defining a function
    to create the schema string to represent the record, and then calling it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The same general approach to deep learning can be taken to data processing
    as the previous example, apart from the actual processing of the raw CSV data.
    There are too many columns to process individually, and they all need to be converted
    into integers to represent their data type. This can be done in one of two ways.
    In the first example, `var args` can be used to process all the elements in the
    row:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The second example uses the `fromSeq` method to process the row elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: In the next section, the H2O Flow user interface will be examined to see how
    it can be used to both monitor H2O and process the data.
  prefs: []
  type: TYPE_NORMAL
- en: H2O Flow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: H2O Flow is a web-based open source user interface for H2O, and given that it
    is being used with Spark, Sparkling Water. It is a fully functional H2O web interface
    for monitoring the H2O Sparkling Water cluster plus jobs, and also for manipulating
    data and training models. I have created some simple example code to start the
    H2O interface. As in the previous Scala-based code samples, all I need to do is
    create a Spark, an H2O context, and then call the `openFlow` command, which will
    start the Flow interface.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following Scala code example just imports classes for Spark context, configuration,
    and H2O. It then defines the configuration in terms of the application name and
    the Spark cluster URL. A Spark context is then created using the configuration
    object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'An H2O context is then created, and started using the Spark context. The H2O
    context classes are imported, and the Flow user interface is started with the
    `openFlow` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Note, for the purposes of this example and to enable me to use the Flow application,
    I have commented out the H2O shutdown and the Spark context stop options. I would
    not normally do this, but I wanted to make this application long-running so that
    it gives me plenty of time to use the interface:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'I use my Bash script `run_h2o.bash` with the application class name called
    `h2o_spark_ex2` as a parameter. This script contains a call to the `spark-submit`
    command, which will execute the compiled application:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'When the application runs, it lists the state of the H2O cluster and provides
    a URL by which the H2O Flow browser can be accessed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The previous example shows that I can access the H2O interface using the port
    number `54323` on the host IP address `192.168.1.108`. I can simply check my host''s
    file to confirm that the host name is `hc2r1m2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'So, I can access the interface using the `hc2r1m2:54323` URL. The following
    screenshot shows the Flow interface with no data loaded. There are data processing
    and administration menu options and buttons at the top of the page. To the right,
    there are help options to enable you to learn more about H2O:'
  prefs: []
  type: TYPE_NORMAL
- en: '![H2O Flow](img/B01989_07_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following screenshot shows the menu options and buttons in greater detail.
    In the following sections, I will use a practical example to explain some of these
    options, but there will not be enough space in this chapter to cover all the functionality.
    Check the [http://h2o.ai/](http://h2o.ai/) website to learn about the Flow application
    in detail, available at [http://h2o.ai/product/flow/](http://h2o.ai/product/flow/):'
  prefs: []
  type: TYPE_NORMAL
- en: '![H2O Flow](img/B01989_07_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In greater definition, you can see that the previous menu options and buttons
    allow you to both administer your H2O Spark cluster, and also manipulate the data
    that you wish to process. The following screenshot shows a reformatted list of
    the help options available, so that, if you get stuck, you can investigate solving
    your problem from the same interface:'
  prefs: []
  type: TYPE_NORMAL
- en: '![H2O Flow](img/B01989_07_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'If I use the menu option, **Admin** | **Cluster Status**, I will obtain the
    following screenshot, which shows me the status of each cluster server in terms
    of memory, disk, load, and cores. It''s a useful snapshot that provides me with
    a color-coded indication of the status:'
  prefs: []
  type: TYPE_NORMAL
- en: '![H2O Flow](img/B01989_07_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The menu option, **Admin** | **Jobs,** provides details of the current cluster
    jobs in terms of the start, end, and run times, as well as status. Clicking on
    the job name provides further details, as shown next, including data processing
    details, and an estimated run time, which is useful. Also, if you select the **Refresh**
    button, the display will continuously refresh until it is deselected:'
  prefs: []
  type: TYPE_NORMAL
- en: '![H2O Flow](img/B01989_07_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The **Admin** | **Water Meter** option provides a visual display of the CPU
    usage on each node in the cluster. As you can see in the following screenshot,
    my meter shows that my cluster was idle:'
  prefs: []
  type: TYPE_NORMAL
- en: '![H2O Flow](img/B01989_07_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Using the menu option, **Flow** | **Upload File**, I have uploaded some of
    the training data used in the previous deep learning Scala-based example. The
    data has been loaded into a data preview pane; I can see a sample of the data
    that has been organized into cells. Also, an accurate guess has been made of the
    data types so that I can see which columns can be enumerated. This is useful if
    I want to consider classification:'
  prefs: []
  type: TYPE_NORMAL
- en: '![H2O Flow](img/B01989_07_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Having loaded the data, I am now presented with a **Frame** display, which
    offers me the ability to view, inspect, build a model, create a prediction, or
    download the data. The data display shows information like min, max, and mean.
    It shows data types, labels, and a zero data count, as shown in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![H2O Flow](img/B01989_07_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'I thought that it would be useful to create a deep learning classification
    model, based on this data, to compare the Scala-based approach to this H2O user
    interface. Using the view and inspect options, it is possible to visually, and
    interactively check the data, as well as create plots relating to the data. For
    instance, using the previous inspect option followed by the plot columns option,
    I was able to create a plot of data labels versus zero counts in the column data.
    The following screenshot shows the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![H2O Flow](img/B01989_07_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'By selecting the build model option, a menu option is offered that lets me
    choose a model type. I will select deep learning, as I already know that this
    data is suited to this classification approach. The previous Scala-based model
    resulted in an accuracy level of 83 percent:'
  prefs: []
  type: TYPE_NORMAL
- en: '![H2O Flow](img/B01989_07_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'I have selected the deep learning option. Having chosen this option, I am then
    able to set model parameters, such as training and validation data sets, as well
    as choosing the data columns that my model should use (obviously, the two data
    sets should contain the same columns). The following screenshot displays the data
    sets, and the model columns being selected:'
  prefs: []
  type: TYPE_NORMAL
- en: '![H2O Flow](img/B01989_07_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'There are a large range of basic and advanced model options available. A selection
    of them are shown in the following screenshot. I have set the response column
    to 15 as the income column. I have also set the **VARIABLE_IMPORTANCES** option.
    Note that I don''t need to enumerate the response column, as it has been done
    automatically:'
  prefs: []
  type: TYPE_NORMAL
- en: '![H2O Flow](img/B01989_07_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Note also that the epochs or iterations option is set to **100** as before.
    Also, the figure `200,200` for the hidden layers indicates that the network has
    two hidden layers, each with 200 neurons. Selecting the build model option causes
    the model to be created from these parameters. The following screenshot shows
    the model being trained, including an estimation of training time and an indication
    of the data processed so far.
  prefs: []
  type: TYPE_NORMAL
- en: '![H2O Flow](img/B01989_07_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Viewing the model, once trained, shows training and validation metrics, as
    well as a list of the important training parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '![H2O Flow](img/B01989_07_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Selecting the **Predict** option allows an alternative validation data set
    to be specified. Choosing the **Predict** option using the new data set causes
    the already trained model to be validated against a new test dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![H2O Flow](img/B01989_07_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Selecting the **Predict** option causes the prediction details for the deep
    learning model, and dataset to be displayed as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![H2O Flow](img/B01989_07_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The preceding screenshot shows the test data frame and the model category, as
    well as the validation statistics in terms of AUC, GINI, and MSE.
  prefs: []
  type: TYPE_NORMAL
- en: 'The AUC value, or area under the curve, relates to the ROC, or the receiver
    operator characteristics curve, which is also shown in the following screenshot.
    TPR means **True Positive Rate**, and FPR means **False Positive Rate**. AUC is
    a measure of accuracy with a value of one being perfect. So, the blue line shows
    greater accuracy than that of the red line:'
  prefs: []
  type: TYPE_NORMAL
- en: '![H2O Flow](img/B01989_07_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: There is a great deal of functionality available within this interface that
    I have not explained, but I hope that I have given you a feel for its power and
    potential. You can use this interface to inspect your data, and create reports
    before attempting to develop code, or as an application in its own right to delve
    into your data.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: My continuing theme, when examining both Apache Hadoop and Spark, is that none
    of these systems stand alone. They need to be integrated to form ETL-based processing
    systems. Data needs to be sourced and processed in Spark, and then passed to the
    next link in the ETL chain, or stored. I hope that this chapter has shown you
    that Spark functionality can be extended with extra libraries, and systems such
    as H2O.
  prefs: []
  type: TYPE_NORMAL
- en: Although Apache Spark MLlib (machine learning library) has a lot of functionality,
    the combination of H2O Sparkling Water and the Flow web interface provides an
    extra wealth of data analysis modeling options. Using Flow, you can also visually,
    and interactively process your data. I hope that this chapter shows you, even
    though it cannot cover all that H2O offers, that the combination of Spark and
    H2O widens your data processing possibilities.
  prefs: []
  type: TYPE_NORMAL
- en: I hope that you have found this chapter useful. As a next step, you might consider
    checking the [http://h2o.ai/](http://h2o.ai/) website or the H2O Google group,
    which is available at [https://groups.google.com/forum/#!forum/h2ostream](https://groups.google.com/forum/#!forum/h2ostream).
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter will examine the Spark-based service [https://databricks.com/](https://databricks.com/),
    which will use Amazon AWS storage for Spark cluster creation in the cloud.
  prefs: []
  type: TYPE_NORMAL
