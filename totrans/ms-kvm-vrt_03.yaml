- en: '*Chapter 2*: KVM as a Virtualization Solution'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will discuss virtualization as a concept and its implementation
    via libvirt, **Quick Emulator** (**QEMU**), and KVM. Realistically, if we want
    to explain how virtualization works and why KVM virtualization is such a fundamental
    part of 21st-century IT, we must start by explaining the technical background
    of multi-core CPUs and virtualization; and that's impossible to do without delving
    deep into the theory of CPUs and OSes so that we can get to what we're really
    after – what hypervisors are and how virtualization actually works.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Virtualization as a concept
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The internal workings of libvirt, QEMU, and KVM
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How all these communicate with each other to provide virtualization
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Virtualization as a concept
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Virtualization is a computing approach that decouples hardware from software.
    It provides a better, more efficient, and programmatic approach to resource splitting
    and sharing between various workloads – virtual machines running OSes, and applications
    on top of them.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: If we were to compare traditional, physical computing of the past with virtualization,
    we can say that by virtualizing, we get the possibility to run multiple guest
    OSes (multiple virtual servers) on the same piece of hardware (same physical server).
    If we're using a type 1 hypervisor (explained in [*Chapter 1*](B14834_01_Final_ASB_ePub.xhtml#_idTextAnchor016),
    *Understanding Linux Virtualization*), this means that the hypervisor is going
    to be in charge of letting the virtual servers access physical hardware. This
    is because there is more than one virtual server using the same hardware as the
    other virtual servers on the same physical server. This is usually supported by
    some kind of scheduling algorithm that's implemented programmatically in hypervisors
    so that we can get more efficiency from the same physical server.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: Virtualized versus physical environments
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s try to visualize these two approaches – physical and virtual. In a physical
    server, we''re installing an OS right on top of the server hardware and running
    applications on top of that OS. The following diagram shows us how this approach
    works:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.1 – Physical server'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_02_01.jpg)'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.1 – Physical server
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: 'In a virtualized world, we''re running a hypervisor (such as KVM), and virtual
    machines on top of that hypervisor. Inside these virtual machines, we''re running
    the same OS and application, just like in the physical server. The virtualized
    approach is shown in the following diagram:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.2 – Hypervisor and two virtual machines'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_02_02.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.2 – Hypervisor and two virtual machines
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: There are still various scenarios in which the physical approach is going to
    be needed. For example, there are still thousands of applications on physical
    servers all over the world because these servers *can't* be virtualized. There
    are different reasons *why* they can't be virtualized. For example, the most common
    reason is actually the simplest reason – maybe these applications are being run
    on an OS that's not on the supported OS list by the virtualization software vendor.
    That can mean that you can't virtualize that OS/application combination because
    that OS doesn't support some virtualized hardware, most commonly a network or
    a storage adapter. The same general idea applies to the cloud as well – moving
    things to the cloud isn't always the best idea, as we will describe later in this
    book.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: Why is virtualization so important?
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A lot of applications that we run today don't scale up well (adding more CPU,
    memory, or other resources) – they just aren't programmed that way or can't be
    seriously parallelized. That means that if an application can't use all the resources
    at its disposal, a server is going to have a lot of *slack space* – and this time,
    we're not talking about disk slack space; we're actually referring to *compute*
    slack space, so slack space at the CPU and memory levels. This means that we're
    underutilizing the capabilities of the server that we paid for – with the intention
    for it to be used fully, not partially.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 今天我们运行的许多应用程序都不会很好地扩展（增加更多的CPU、内存或其他资源）-它们只是没有以这种方式编程，或者不能被严重并行化。这意味着如果一个应用程序不能充分利用其所拥有的所有资源，服务器将会有很多“空闲空间”-这一次，我们不是在谈论磁盘的空闲空间；我们实际上是在指“计算”空闲空间，即CPU和内存级别的空闲空间。这意味着我们没有充分利用我们为其付费的服务器的能力-我们的意图是让它完全使用，而不是部分使用。
- en: There are other reasons why efficiency and programmatic approaches are so important.
    The fact of the matter is that beyond their war of press releases in the 2003–2005
    timeframe when it was all about the CPU frequency bragging rights (which *equals*
    CPU speed), Intel and AMD hit a wall in terms of the development of the *single-core*
    CPU as a concept. They just couldn't cram as many additional elements on the CPU
    (be it for execution or the cache) and/or bump the single core's speed without
    seriously compromising the way CPUs were being fed with electrical current. This
    meant that, at the end of the day, this approach compromised the reliability of
    the CPU and the whole system that it was running. If you want to learn more about
    that, we suggest that you look for articles about Intel's NetBurst architecture
    CPUs (for example, the Prescott core) and their younger brother, Pentium D (the
    Smithfield core), which was basically two Prescott cores glued together so that
    the end result was a dual-core CPU. A *very, very hot* dual-core CPU.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 效率和编程方法的重要性还有其他原因。事实是，在2003年至2005年这段时间，当一切都是关于CPU频率的炫耀权利（等于CPU速度）时，英特尔和AMD在单核CPU的概念发展方面遇到了瓶颈。他们无法在CPU上塞入更多的附加元素（无论是用于执行还是缓存），或者提高单核的速度，而不严重损害CPU的电流供应方式。这意味着，最终，这种方法会损害CPU和运行它的整个系统的可靠性。如果您想了解更多信息，我们建议您搜索有关英特尔NetBurst架构CPU（例如Prescott核心）和它们的年轻兄弟奔腾D（Smithfield核心）的文章，后者基本上是将两个Prescott核心粘合在一起，以便最终结果是双核CPU。一个非常非常热的双核CPU。
- en: A couple of generations before that, there were other techniques that Intel
    and AMD tried and tested in terms of the *let's have multiple execution units
    per system* principle. For example, we had Intel Pentium Pro dual-socket systems
    and AMD Opteron dual- and quad-socket systems. We'll come back to these later
    in this book when we start discussing some very important aspects of virtualization
    (for example, **Non-Unified Memory Access** (**NUMA**)).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在那之前的几代中，英特尔和AMD尝试并测试了其他技术，例如“让系统拥有多个执行单元”的原则。例如，我们有英特尔奔腾Pro双插槽系统和AMD Opteron双插槽和四插槽系统。当我们开始讨论虚拟化的一些非常重要的方面时，我们将在本书的后面回到这些内容（例如，非统一内存访问（NUMA））。
- en: So, whichever way you look at it, when PC CPUs started getting multiple cores
    in 2005 (AMD being the first to the market with a server multi-core CPU, and Intel
    being the first to the market with a desktop multi-core CPU), it was the only
    rational way to go forward. These cores were smaller, more efficient (drawing
    less power), and were generally a better long-term approach. Of course, that meant
    that OSes and applications had to be reworked heavily if companies such as Microsoft
    and Oracle wanted to use their applications and reap the benefits of a multi-core
    server.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，无论从哪个角度来看，2005年PC CPU开始获得多个核心（AMD是第一个推出服务器多核CPU的厂商，而英特尔是第一个推出桌面多核CPU的厂商）是唯一合理的前进方式。这些核心更小，更高效（耗电更少），通常是更好的长期方法。当然，这意味着如果微软和甲骨文等公司想要使用他们的应用程序并获得多核服务器的好处，操作系统和应用程序必须进行大量重写。
- en: In conclusion, for PC-based servers, looking from the CPU perspective, switching
    to multi-core CPUs was an opportune moment to start working toward virtualization
    as the concept that we know and love today.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，对于基于PC的服务器来说，从CPU的角度来看，转向多核CPU是开始朝着我们今天所熟悉和喜爱的虚拟化概念努力的一个合适的时刻。
- en: In parallel with these developments, CPUs got other additions – for example,
    additional CPU registers that can handle specific types of operations. A lot of
    people heard about instruction sets such as MMX, SSE, SSE2, SSE3, SSE4.x, AVX,
    AVX2, AES, and so on. These are all very important today as well because they
    give us a possibility of *offloading* certain instruction types to a specific
    CPU register. This means that these instructions don't have to be run on a CPU
    as a general serial device, which executes these tasks slower. Instead, these
    instructions can be sent to a specific CPU register that's specialized for these
    instructions. Think of it as having separate mini accelerators on a CPU die that
    could run some pieces of the software stack without hogging the general CPU pipeline.
    One of these additions was **Virtual Machine Extensions** (**VMX**) for Intel,
    or **AMD Virtualization** (**AMD-V**), both of which enable us to have full, hardware-based
    virtualization support for their respective platforms.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 与这些发展并行的是，CPU还有其他增加-例如，可以处理特定类型操作的额外CPU寄存器。很多人听说过MMX、SSE、SSE2、SSE3、SSE4.x、AVX、AVX2、AES等指令集。这些今天也都非常重要，因为它们给了我们将某些指令类型“卸载”到特定CPU寄存器的可能性。这意味着这些指令不必在CPU上作为一般的串行设备运行，执行这些任务更慢。相反，这些指令可以发送到专门用于这些指令的CPU寄存器。可以将其视为在CPU芯片上拥有单独的小加速器，可以运行软件堆栈的某些部分而不会占用通用CPU管道。其中之一是英特尔的虚拟机扩展（VMX），或者AMD虚拟化（AMD-V），它们都使我们能够为其各自的平台提供全面的、基于硬件的虚拟化支持。
- en: Hardware requirements for virtualization
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 虚拟化的硬件要求
- en: After the introduction of software-based virtualization on PCs, a lot of development
    was made, both on the hardware and software sides. The end result – as we mentioned
    in the previous chapter – was a CPU that had an awful lot more features and power.
    This led to a big push toward hardware-assisted virtualization, which – on paper
    – looked like the faster and more advanced way to go. Just as an example, there
    were a whole bunch of CPUs that didn't support hardware-assisted virtualization
    in the 2003–2006 timeframe, such as the Intel Pentium 4, Pentium D, the initial
    AMD Athlons, Turions, Durons, and so on. It took both Intel and AMD until 2006
    to have hardware-assisted virtualization as a feature that's more widely available
    on their respective CPUs. Furthermore, it took some time to have 64-bit CPUs,
    and there was little or no interest in running hardware-assisted virtualization
    on 32-bit architectures. The primary reason for this was the fact that you couldn't
    allocate more than 4 GB of memory, which severely limited the scope of using virtualization
    as a concept.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在PC上引入基于软件的虚拟化后，硬件和软件方面都取得了很大的发展。最终结果——正如我们在前一章中提到的——是CPU具有了更多的功能和性能。这导致了对硬件辅助虚拟化的大力推动，这在理论上看起来是更快速和更先进的方式。举个例子，在2003年至2006年期间有很多CPU不支持硬件辅助虚拟化，比如英特尔奔腾4、奔腾D，以及AMD
    Athlon、Turion、Duron等。直到2006年，英特尔和AMD才在其各自的CPU上更广泛地提供硬件辅助虚拟化作为一项功能。此外，64位CPU也需要一些时间，而在32位架构上几乎没有兴趣运行硬件辅助虚拟化。这主要原因是您无法分配超过4GB的内存，这严重限制了虚拟化作为概念的范围。
- en: 'Keeping all of this in mind, these are the requirements that we have to comply
    with today so that we can run modern-day hypervisors with full hardware-assisted
    virtualization support:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 牢记所有这些，这些是我们今天必须遵守的要求，以便我们可以运行具有完全硬件辅助虚拟化支持的现代虚拟化监控程序：
- en: '**Second-Level Address Translation, Rapid Virtualization Indexing, Extended
    Page Tables (SLAT/RVI/EPT) support**: This is the CPU technology that a hypervisor
    uses so that it can have a map of virtual-to-physical memory addresses. Virtual
    machines operate in a virtual memory space that can be scattered all over the
    physical memory, so by using an additional map such as SLAT/EPT, (implemented
    via an additional **Translation Lookaside Buffer**, or **TLB**), you''re reducing
    latency for memory access. If we didn''t have a technology like this, we''d have
    to have physical memory access to the computer memory''s physical addresses, which
    would be messy, insecure, and latency-prone. To avoid any confusion, EPT is Intel''s
    name for SLAT technology in their CPUs (AMD uses RVI terminology, while Intel
    uses EPT terminology).'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 二级地址转换，快速虚拟化索引，扩展页表（SLAT/RVI/EPT）支持：这是一个虚拟化监控程序使用的CPU技术，以便它可以拥有虚拟到物理内存地址的映射。虚拟机在虚拟内存空间中运行，可以分散在物理内存的各个位置，因此通过使用SLAT/EPT等额外的映射（通过额外的TLB实现），可以减少内存访问的延迟。如果没有这样的技术，我们将不得不访问计算机内存的物理地址，这将是混乱、不安全和延迟敏感的。为了避免混淆，EPT是英特尔CPU中SLAT技术的名称（AMD使用RVI术语，而英特尔使用EPT术语）。
- en: '**Intel VT or AMD-V support**: If an Intel CPU has VT (or an AMD CPU has AMD-V),
    that means that it supports hardware virtualization extensions and full virtualization.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**英特尔VT或AMD-V支持**：如果英特尔CPU具有VT（或AMD CPU具有AMD-V），这意味着它支持硬件虚拟化扩展和完全虚拟化。'
- en: '**Long mode support**, which means that the CPU has 64-bit support. Without
    a 64-bit architecture, virtualization would be basically useless because you''d
    have only 4 GB of memory to give to virtual machines (which is a limitation of
    the 32-bit architecture). By using a 64-bit architecture, we can allocate much
    more memory (depending on the CPU that we''re using), which means more opportunities
    to feed virtual machines with memory, without which the whole virtualization concept
    wouldn''t make any sense in the 21st-century IT space.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**长模式支持**，这意味着CPU支持64位。没有64位架构，虚拟化基本上是无用的，因为您只能为虚拟机提供4GB的内存（这是32位架构的限制）。通过使用64位架构，我们可以分配更多的内存（取决于我们使用的CPU），这意味着更多的机会为虚拟机提供内存，否则在21世纪的IT空间中整个虚拟化概念将毫无意义。'
- en: '**The possibility of having Input/Output Memory Management Unit (IOMMU) virtualization
    (such as AMD-Vi, Intel VT-d, and stage 2 tables on ARM)**, which means that we
    allow virtual machines to access peripheral hardware directly (graphics cards,
    storage controllers, network devices, and so on). This functionality must be enabled
    both on the CPU and motherboard chipset/firmware side.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**具有输入/输出内存管理单元（IOMMU）虚拟化的可能性（例如AMD-Vi、英特尔VT-d和ARM上的第2阶段表）**，这意味着我们允许虚拟机直接访问外围硬件（显卡、存储控制器、网络设备等）。此功能必须在CPU和主板芯片组/固件方面都启用。'
- en: '**The possibility to do** **Single Root Input Output Virtualization** (**SR/IOV**),
    which allows us to directly forward a PCI Express device (for example, an Ethernet
    port) to multiple virtual machines. The key aspect of SR-IOV is its ability to
    share one physical device with multiple virtual machines via functionality called
    **Virtual Functions** (**VFs**). This functionality requires hardware and driver
    support.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**进行单根输入输出虚拟化**（SR/IOV）的可能性，这使我们能够直接将PCI Express设备（例如以太网端口）转发到多个虚拟机。SR-IOV的关键方面是其通过称为**虚拟功能**（VFs）的功能，能够将一个物理设备与多个虚拟机共享。此功能需要硬件和驱动程序支持。'
- en: '**The possibility to do PCI passthrough**, which means we can take a PCI Express
    connected card (for example, a video card) connected to a server motherboard and
    present it to a virtual machine as if that card was directly connected to the
    virtual machine via functionality called **Physical Functions** (**PFs**). This
    means bypassing various hypervisor levels that the connection would ordinarily
    take place through.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PCI passthrough的可能性，意味着我们可以将连接到服务器主板的PCI Express连接卡（例如，显卡）呈现给虚拟机，就好像该卡是通过称为“物理功能”（PFs）的功能直接连接到虚拟机一样。这意味着绕过连接通常会经过的各种Hypervisor级别。
- en: '**Trusted Platform Module (TPM) support**, which is usually implemented as
    an additional motherboard chip. Using TPM can have a lot of advantages in terms
    of security because it can be used to provide cryptographic support (that is,
    to create, save, and secure the use of cryptographic keys). There was quite a
    bit of buzz in the Linux world around the use of TPM with KVM virtualization,
    which led to Intel''s open sourcing of the TPM2 stack in the summer of 2018.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可信平台模块（TPM）支持**，通常作为额外的主板芯片实现。使用TPM在安全方面有很多优势，因为它可以用于提供加密支持（即创建、保存和保护加密密钥的使用）。在Linux世界中，围绕KVM虚拟化使用TPM引起了相当大的轰动，这导致英特尔在2018年夏天开源了TPM2堆栈。'
- en: When discussing SR-IOV and PCI passthrough, make sure that you take note of
    the core functionalities, called PF and VF. These two keywords will make it easier
    to remember *where* (on a physical or virtual level) and *how* (directly or via
    a hypervisor) devices are forwarded to their respective virtual machines. These
    capabilities are very important for the enterprise space and quite a few specific
    scenarios. Just as an example, there's literally no way to have a **virtual desktop
    infrastructure** (**VDI**) solution with workstation-grade virtual machines that
    you can use to run AutoCAD and similar applications without these capabilities.
    This is because integrated graphics on CPUs are just too slow to do that properly.
    That's when you start adding GPUs to your servers – so that you can use a hypervisor
    to forward the *whole* GPU or *parts* of it to a virtual machine or multiple virtual
    machines.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论SR-IOV和PCI passthrough时，请确保注意核心功能，称为PF和VF。这两个关键词将更容易记住设备是如何（直接或通过Hypervisor）转发到各自的虚拟机的*位置*（在物理或虚拟级别）和*方式*。这些功能对企业空间非常重要，也适用于一些特定场景。举个例子，如果没有这些功能，就无法使用工作站级虚拟机来运行AutoCAD和类似的应用程序。这是因为CPU上的集成显卡速度太慢了。这时你就需要在服务器上添加GPU，这样你就可以使用Hypervisor将整个GPU或其*部分*转发到一个虚拟机或多个虚拟机。
- en: In terms of system memory, there are also various subjects to consider. AMD
    started integrating memory controllers into CPUs in Athlon 64, which was years
    before Intel did that (Intel did that first with the Nehalem CPU core, which was
    introduced in 2008). Integrating a memory controller into a CPU meant that your
    system had less latency when CPU accessed memory for memory I/O operations. Before
    this, the memory controller was integrated into what was called a NorthBridge
    chip, which was a separate chip on a system motherboard that was in charge of
    all fast buses and memory. But that means additional latency, especially when
    you try to scale out that principle to multi-socket, multi-core CPUs. Also, with
    the introduction of Athlon 64 on Socket 939, AMD switched to a dual-channel memory
    architecture, which is now a familiar theme in the desktop and server market.
    Triple and quad-channel memory controllers are de facto standards in servers.
    Some of the latest Intel Xeon CPUs support six-channel memory controllers, and
    AMD EPYC CPUs support eight-channel memory controllers as well. This has huge
    implications for the overall memory bandwidth and latency, which – in turn – has
    huge implications for the speed of memory-sensitive applications, both on physical
    and virtual servers.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在系统内存方面，也有各种要考虑的主题。AMD在Athlon 64中开始将内存控制器集成到CPU中，这是在英特尔之前的几年（英特尔首次在2008年推出的Nehalem
    CPU核心中实现了这一点）。将内存控制器集成到CPU中意味着当CPU访问内存进行内存I/O操作时，系统的延迟更低。在此之前，内存控制器集成到了所谓的NorthBridge芯片中，这是系统主板上的一个独立芯片，负责所有快速总线和内存。但这意味着额外的延迟，特别是当您尝试将这一原则扩展到多插槽、多核CPU时。此外，随着Athlon
    64在Socket 939上的推出，AMD转向了双通道内存架构，这在桌面和服务器市场上现在是一个熟悉的主题。三通道和四通道内存控制器已成为服务器的事实标准。一些最新的英特尔至强CPU支持六通道内存控制器，AMD
    EPYC CPU也支持八通道内存控制器。这对整体内存带宽和延迟有着巨大的影响，反过来又对物理和虚拟服务器上内存敏感应用程序的速度有着巨大的影响。
- en: Why is this important? The more channels you have and the lower the latency
    is, the more bandwidth you have from CPU to memory. And that is very, very desirable
    for a lot of workloads in today's IT space (for example, databases).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么这很重要？通道越多，延迟越低，CPU到内存的带宽就越大。这对今天IT空间中许多工作负载（例如数据库）非常有吸引力。
- en: Software requirements for virtualization
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 虚拟化的软件要求
- en: 'Now that we''ve covered the basic hardware aspects of virtualization, let''s
    move on to the software aspect of virtualization. To do that, we must cover some
    jargon in computer science. That being said, let''s start with something called
    protection rings. In computer science, various hierarchical protection domains/privileged
    rings exist. These are the mechanisms that protect data or faults based on the
    security that''s enforced when accessing the resources in a computer system. These
    protection domains contribute to the security of a computer system. By imagining
    these protection rings as instruction zones, we can represent them via the following
    diagram:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经涵盖了虚拟化的基本硬件方面，让我们转向虚拟化的软件方面。为了做到这一点，我们必须涵盖计算机科学中的一些行话。话虽如此，让我们从一个叫做保护环的东西开始。在计算机科学中，存在着各种分层的保护域/特权环。这些是保护数据或故障的机制，基于在访问计算机系统资源时强制执行的安全性。这些保护域有助于计算机系统的安全。通过将这些保护环想象成指令区域，我们可以通过以下图表来表示它们：
- en: '![Figure 2.3 – Protection rings (source: https://en.wikipedia.org/wiki/Protection_ring)'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.3 – 保护环（来源：https://en.wikipedia.org/wiki/Protection_ring）'
- en: '](img/B14834_02_03.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_02_03.jpg)'
- en: 'Figure 2.3 – Protection rings (source: [https://en.wikipedia.org/wiki/Protection_ring](https://en.wikipedia.org/wiki/Protection_ring))'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.3 – 保护环（来源：[https://en.wikipedia.org/wiki/Protection_ring](https://en.wikipedia.org/wiki/Protection_ring)）
- en: 'As shown in the preceding diagram, the protection rings are numbered from the
    most privileged to the least privileged. Ring 0 is the level with the most privilege
    and interacts directly with physical hardware, such as the CPU and memory. The
    resources, such as memory, I/O ports, and CPU instructions, are protected via
    these privileged rings. Rings 1 and 2 are mostly unused. Most general-purpose
    systems use only two rings, even if the hardware they run on provides more CPU
    modes than that. The two main CPU modes are the kernel mode and the user mode,
    which are also related to the way processes are executed. You can read more about
    it at this link: [https://access.redhat.com/sites/default/files/attachments/processstates_20120831.pdf](https://access.redhat.com/sites/default/files/attachments/processstates_20120831.pdf).
    From an OS''s point of view, ring 0 is called the kernel mode/supervisor mode
    and ring 3 is the user mode. As you may have assumed, applications run in ring
    3.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图所示，保护环从最特权到最不特权的顺序编号。环0是最特权的级别，直接与物理硬件交互，比如CPU和内存。这些特权环保护了资源，比如内存、I/O端口和CPU指令。环1和环2大多数情况下是未使用的。大多数通用系统只使用两个环，即使它们运行的硬件提供了更多的CPU模式。两个主要的CPU模式是内核模式和用户模式，这也与进程执行的方式有关。您可以在此链接中了解更多信息：[https://access.redhat.com/sites/default/files/attachments/processstates_20120831.pdf](https://access.redhat.com/sites/default/files/attachments/processstates_20120831.pdf)
    从操作系统的角度来看，环0被称为内核模式/监管模式，环3是用户模式。正如您可能已经猜到的那样，应用程序在环3中运行。
- en: 'OSes such as Linux and Windows use supervisor/kernel and user mode. This mode
    can do almost nothing to the outside world without calling on the kernel or without
    its help due to its restricted access to memory, CPU, and I/O ports. The kernels
    can run in privileged mode, which means that they can run on ring 0\. To perform
    specialized functions, the user-mode code (all the applications that run in ring
    3) must perform a system call to the supervisor mode or even to the kernel space,
    where the trusted code of the OS will perform the needed task and return the execution
    back to the userspace. In short, the OS runs in ring 0 in a normal environment.
    It needs the most privileged level to do resource management and provide access
    to the hardware. The following diagram explains this:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 像Linux和Windows这样的操作系统使用监管/内核和用户模式。这种模式几乎无法在没有调用内核或没有内核帮助的情况下对外部世界做任何事情，因为它对内存、CPU和I/O端口的访问受到限制。内核可以在特权模式下运行，这意味着它可以在环0上运行。为了执行专门的功能，用户模式代码（在环3中运行的所有应用程序）必须对监管模式甚至内核空间执行系统调用，操作系统的受信任代码将执行所需的任务并将执行返回到用户空间。简而言之，在正常环境中，操作系统在环0中运行。它需要最高的特权级别来进行资源管理并提供对硬件的访问。以下图表解释了这一点：
- en: '![Figure 2.4 – System call to supervisor mode'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.4 – 系统调用到监管模式'
- en: '](img/B14834_02_04.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_02_04.jpg)'
- en: Figure 2.4 – System call to supervisor mode
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.4 – 系统调用到监管模式
- en: The rings above 0 run instructions in a processor mode called unprotected. The
    hypervisor/**Virtual Machine Monitor** (**VMM**) needs to access the memory, CPU,
    and I/O devices of the host. Since only the code running in ring 0 is allowed
    to perform these operations, it needs to run in the most privileged ring, which
    is ring 0, and has to be placed next to the kernel. Without specific hardware
    virtualization support, the hypervisor or VMM runs in ring 0; this basically blocks
    the virtual machine's OS in ring 0\. So, the virtual machine's OS must reside
    in ring 1\. An OS installed in a virtual machine is also expected to access all
    the resources as it's unaware of the virtualization layer; to achieve this, it
    has to run in ring 0, similar to the VMM. Due to the fact that only one kernel
    can run in ring 0 at a time, the guest OSes have to run in another ring with fewer
    privileges or have to be modified to run in user mode.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 环0以上的环在处理器模式下运行未受保护的指令。虚拟机监视器（VMM）需要访问主机的内存、CPU和I/O设备。由于只有在环0中运行的代码被允许执行这些操作，它需要在最特权的环，即环0中运行，并且必须放置在内核旁边。没有特定的硬件虚拟化支持，虚拟机监视器或VMM在环0中运行；这基本上阻止了虚拟机的操作系统在环0中运行。因此，虚拟机的操作系统必须驻留在环1中。安装在虚拟机中的操作系统也希望访问所有资源，因为它不知道虚拟化层；为了实现这一点，它必须在环0中运行，类似于VMM。由于一次只能运行一个内核在环0中，客户操作系统必须在另一个权限较低的环中运行，或者必须修改为在用户模式下运行。
- en: This has resulted in the introduction of a couple of virtualization methods
    called full virtualization and paravirtualization, which we mentioned earlier.
    Now, let's try to explain them in a more technical way.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致了引入了一些虚拟化方法，称为全虚拟化和半虚拟化，我们之前提到过。现在，让我们尝试以更加技术化的方式来解释它们。
- en: Full virtualization
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 全虚拟化
- en: 'In full virtualization, privileged instructions are emulated to overcome the
    limitations that arise from the guest OS running in ring 1 and the VMM running
    in ring 0\. Full virtualization was implemented in first-generation x86 VMMs.
    It relies on techniques such as binary translation to trap and virtualize the
    execution of certain sensitive and non-virtualizable instructions. This being
    said, in binary translation, some system calls are interpreted and dynamically
    rewritten. The following diagram depicts how the guest OS accesses the host computer
    hardware through ring 1 for privileged instructions and how unprivileged instructions
    are executed without the involvement of ring 1:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在全虚拟化中，特权指令被模拟以克服客户操作系统在ring 1中运行和VMM在ring 0中运行所产生的限制。全虚拟化是在第一代x86 VMM中实现的。它依赖于诸如二进制翻译之类的技术来陷阱和虚拟化某些敏感和不可虚拟化的指令的执行。也就是说，在二进制翻译中，一些系统调用被解释并动态重写。以下图表描述了客户操作系统如何通过ring
    1访问主机计算机硬件以获取特权指令，以及如何在不涉及ring 1的情况下执行非特权指令：
- en: '![Figure 2.5 – Binary translation'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.5 – 二进制翻译'
- en: '](img/B14834_02_05.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_02_05.jpg)'
- en: Figure 2.5 – Binary translation
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.5 – 二进制翻译
- en: 'With this approach, the critical instructions are discovered (statically or
    dynamically at runtime) and replaced with traps in the VMM that are to be emulated
    in software. A binary translation can incur a large performance overhead in comparison
    to a virtual machine running on natively virtualized architectures. This can be
    seen in the following diagram:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 采用这种方法，关键指令被发现（在运行时静态或动态地）并在VMM中被替换为陷阱，这些陷阱将在软件中被模拟。与在本地虚拟化架构上运行的虚拟机相比，二进制翻译可能会产生较大的性能开销。这可以从以下图表中看出：
- en: '![Figure 2.6 – Full virtualization'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.6 – 全虚拟化'
- en: '](img/B14834_02_06.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_02_06.jpg)'
- en: Figure 2.6 – Full virtualization
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.6 – 全虚拟化
- en: However, as shown in the preceding diagram, when we use full virtualization,
    we can use the unmodified guest OSes. This means that we don't have to alter the
    guest kernel so that it runs on a VMM. When the guest kernel executes privileged
    operations, the VMM provides the CPU emulation to handle and modify the protected
    CPU operations. However, as we mentioned earlier, this causes performance overhead
    compared to the other mode of virtualization, called paravirtualization.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，正如前面的图表所示，当我们使用全虚拟化时，我们可以使用未经修改的客户操作系统。这意味着我们不必修改客户内核以使其在VMM上运行。当客户内核执行特权操作时，VMM提供CPU仿真来处理和修改受保护的CPU操作。然而，正如我们之前提到的，与另一种虚拟化模式——称为半虚拟化相比，这会导致性能开销。
- en: Paravirtualization
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 半虚拟化
- en: 'In paravirtualization, the guest OS needs to be modified to allow those instructions
    to access ring 0\. In other words, the OS needs to be modified to communicate
    between the VMM/hypervisor and the guest through the *backend* (hypercalls) path:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在半虚拟化中，客户操作系统需要被修改以允许这些指令访问ring 0。换句话说，操作系统需要被修改以在VMM/虚拟机监控程序和客户之间通过*后端*（超级调用）路径进行通信：
- en: '![Figure 2.7 – Paravirtualization'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.7 – 半虚拟化'
- en: '](img/B14834_02_07.jpg)'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_02_07.jpg)'
- en: Figure 2.7 – Paravirtualization
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.7 – 半虚拟化
- en: Paravirtualization ([https://en.wikipedia.org/wiki/Paravirtualization](https://en.wikipedia.org/wiki/Paravirtualization))
    is a technique in which the hypervisor provides an API, and the OS of the guest
    virtual machine calls that API, which requires host OS modifications. Privileged
    instruction calls are exchanged with the API functions provided by the VMM. In
    this case, the modified guest OS can run in ring 0.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 半虚拟化（[https://en.wikipedia.org/wiki/Paravirtualization](https://en.wikipedia.org/wiki/Paravirtualization)）是一种技术，其中虚拟机监控程序提供一个API，而客户虚拟机的操作系统调用该API，这需要对主机操作系统进行修改。特权指令调用与VMM提供的API函数进行交换。在这种情况下，修改后的客户操作系统可以在ring
    0中运行。
- en: As you can see, under this technique, the guest kernel is modified to run on
    the VMM. In other words, the guest kernel knows that it's been virtualized. The
    privileged instructions/operations that are supposed to run in ring 0 have been
    replaced with calls known as hypercalls, which talk to the VMM. These hypercalls
    invoke the VMM so that it performs the task on behalf of the guest kernel. Since
    the guest kernel can communicate directly with the VMM via hypercalls, this technique
    results in greater performance compared to full virtualization. However, this
    requires a specialized guest kernel that is aware of paravirtualization and comes
    with needed software support.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，根据这种技术，客户内核被修改为在VMM上运行。换句话说，客户内核知道自己已被虚拟化。应该在ring 0中运行的特权指令/操作已被称为超级调用的调用所取代，这些调用与VMM进行通信。这些超级调用调用VMM，以便它代表客户内核执行任务。由于客户内核可以通过超级调用直接与VMM通信，因此与全虚拟化相比，这种技术具有更高的性能。然而，这需要一个专门的客户内核，它知道半虚拟化并具有所需的软件支持。
- en: The concepts of paravirtualization and full virtualization used to be a common
    way to do virtualization but not in the best possible, manageable way. That's
    where hardware-assisted virtualization comes into play, as we will describe in
    the following section.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 半虚拟化和全虚拟化的概念曾经是一种常见的虚拟化方式，但并不是最佳的、可管理的方式。这就是硬件辅助虚拟化发挥作用的地方，我们将在下一节中描述。
- en: Hardware-assisted virtualization
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 硬件辅助虚拟化
- en: Intel and AMD realized that full virtualization and paravirtualization are the
    major challenges of virtualization on the x86 architecture (since the scope of
    this book is limited to x86 architecture, we will mainly discuss the evolution
    of this architecture here) due to the performance overhead and complexity of designing
    and maintaining the solution. Intel and AMD independently created new processor
    extensions of the x86 architecture, called Intel VT-x and AMD-V, respectively.
    On the Itanium architecture, hardware-assisted virtualization is known as VT-i.
    Hardware-assisted virtualization is a platform virtualization method designed
    to efficiently use full virtualization with the hardware capabilities. Various
    vendors call this technology by different names, including accelerated virtualization,
    hardware virtual machine, and native virtualization.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: 'For better support for virtualization, Intel and AMD introduced **Virtualization
    Technology** (**VT**) and **Secure Virtual Machine** (**SVM**), respectively,
    as extensions of the IA-32 instruction set. These extensions allow the VMM/hypervisor
    to run a guest OS that expects to run in kernel mode, in lower privileged rings.
    Hardware-assisted virtualization not only proposes new instructions but also introduces
    a new privileged access level, called ring -1, where the hypervisor/VMM can run.
    Hence, guest virtual machines can run in ring 0\. With hardware-assisted virtualization,
    the OS has direct access to resources without any emulation or OS modification.
    The hypervisor or VMM can now run at the newly introduced privilege level, ring
    -1, with the guest OSes running on ring 0\. Also, with hardware-assisted virtualization,
    the VMM/hypervisor is relaxed and needs to perform less work compared to the other
    techniques mentioned, which reduces the performance overhead. This capability
    to run directly in ring -1 can be described with the following diagram:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.8 – Hardware-assisted virtualization'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_02_08.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.8 – Hardware-assisted virtualization
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: In simple terms, this virtualization-aware hardware provides us with support
    to build the VMM and also ensures the isolation of a guest OS. This helps us achieve
    better performance and avoid the complexity of designing a virtualization solution.
    Modern virtualization techniques make use of this feature to provide virtualization.
    One example is KVM, which we are going to discuss in detail throughout this book.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: Now that we've covered the hardware and software aspects of virtualization,
    let's see how all of this applies to KVM as a virtualization technology.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: The internal workings of libvirt, QEMU, and KVM
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The interaction of libvirt, QEMU, and KVM is something that gives us the full
    virtualization capabilities that are covered in this book. They are the most important
    pieces in the Linux virtualization puzzle, as each has a role to play. Let's describe
    what they do and how they interact with each other.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: libvirt
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When working with KVM, you're most likely to first interface with its main `virsh`.
    Keep in mind that you can manage remote hypervisors via libvirt, so you're not
    restricted to a local hypervisor only. That's why virt-manager has an additional
    parameter called `--connect`. libvirt is also part of various other KVM management
    tools, such as oVirt ([http://www.ovirt.org](http://www.ovirt.org)), which we
    will discuss in the next chapter.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: The goal of the libvirt library is to provide a common and stable layer for
    managing virtual machines running on a hypervisor. In short, as a management layer,
    it is responsible for providing the API that performs management tasks such as
    virtual machine provision, creation, modification, monitoring, control, migration,
    and so on. In Linux, you will have noticed that some of the processes are daemonized.
    The libvirt process is also daemonized, and it is called `libvirtd`. As with any
    other daemon process, `libvirtd` provides services to its clients upon request.
    Let's try to understand what exactly happens when a libvirt client such as `virsh`
    or virt-manager requests a service from `libvirtd`. Based on the connection URI
    (discussed in the following section) that's passed by the client, `libvirtd` opens
    a connection to the hypervisor. This is how the client's `virsh` or virt-manager
    asks `libvirtd` to start talking to the hypervisor. In the scope of this book,
    we are aiming to look at KVM virtualization technology. So, it would be better
    to think about it in terms of a QEMU/KVM hypervisor instead of discussing some
    other hypervisor communication from `libvirtd`. You may be a bit confused when
    you see QEMU/KVM as the underlying hypervisor name instead of either QEMU or KVM.
    But don't worry – all will become clear in due course. The connection between
    QEMU and KVM will be discussed in the following chapters. For now, just know that
    there is a hypervisor that uses both the QEMU and KVM technologies.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: libvirt库的目标是提供一个通用和稳定的层，用于管理在hypervisor上运行的虚拟机。简而言之，作为一个管理层，它负责提供执行管理任务的API，如虚拟机的提供、创建、修改、监视、控制、迁移等。在Linux中，您会注意到一些进程是守护进程。libvirt进程也是守护进程，称为`libvirtd`。与任何其他守护进程一样，`libvirtd`在请求时为其客户端提供服务。让我们试着理解当一个libvirt客户端，如`virsh`或virt-manager，从`libvirtd`请求服务时到底发生了什么。根据客户端传递的连接URI（在下一节中讨论），`libvirtd`打开到hypervisor的连接。这就是客户端的`virsh`或virt-manager要求`libvirtd`开始与hypervisor通信的方式。在本书的范围内，我们的目标是研究KVM虚拟化技术。因此，最好将其视为QEMU/KVM
    hypervisor，而不是讨论来自`libvirtd`的其他hypervisor通信。当您看到QEMU/KVM作为底层hypervisor名称而不是QEMU或KVM时，您可能会有点困惑。但不用担心-一切都会在适当的时候变得清晰。QEMU和KVM之间的连接将在接下来的章节中讨论。现在，只需知道有一个hypervisor同时使用QEMU和KVM技术。
- en: Connecting to a remote system via virsh
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通过virsh连接到远程系统
- en: 'A simple command-line example of a `virsh` binary for a remote connection would
    be as follows:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 一个远程连接的`virsh`二进制的简单命令行示例如下：
- en: '[PRE0]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Let''s take a look at the source code now. We can get the libvirt source code
    from the libvirt Git repository:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看看源代码。我们可以从libvirt Git存储库中获取libvirt源代码：
- en: '[PRE1]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Once you clone the repo, you can see the following hierarchy of files in the
    repo:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦克隆了repo，您可以在repo中看到以下文件层次结构：
- en: '![Figure 2.9 – QEMU source content, downloaded via Git'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.9 – 通过Git下载的QEMU源内容'
- en: '](img/B14834_02_09.jpg)'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_02_09.jpg)'
- en: Figure 2.9 – QEMU source content, downloaded via Git
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.9 – 通过Git下载的QEMU源内容
- en: libvirt code is based on the C programming language; however, libvirt has language
    bindings in different languages, such as `C#`, `Java`, `OCaml`, `Perl`, `PHP`,
    `Python`, `Ruby`, and so on. For more details on these bindings, please refer
    to [https://libvirt.org/bindings.html](https://libvirt.org/bindings.html). The
    main (and few) directories in the source code are `docs`, `daemon`, `src`, and
    so on. The libvirt project is well documented and the documentation is available
    in the source code repo and also at [http://libvirt.org](http://libvirt.org).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: libvirt代码基于C编程语言；然而，libvirt在不同语言中有语言绑定，如`C#`、`Java`、`OCaml`、`Perl`、`PHP`、`Python`、`Ruby`等。有关这些绑定的更多详细信息，请参考[https://libvirt.org/bindings.html](https://libvirt.org/bindings.html)。源代码中的主要（和少数）目录是`docs`、`daemon`、`src`等。libvirt项目有很好的文档，并且文档可以在源代码存储库和[http://libvirt.org](http://libvirt.org)上找到。
- en: 'libvirt uses a *driver-based architecture*, which enables libvirt to communicate
    with various external hypervisors. This means that libvirt has internal drivers
    that are used to interface with other hypervisors and solutions, such as LXC,
    Xen, QEMU, VirtualBox, Microsoft Hyper-V, bhyve (BSD hypervisor), IBM PowerVM,
    OpenVZ (open Virtuozzo container-based solution), and others, as shown in the
    following diagram:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: libvirt使用*基于驱动程序的架构*，这使得libvirt能够与各种外部hypervisors进行通信。这意味着libvirt有内部驱动程序，用于与其他hypervisors和解决方案进行接口，如LXC、Xen、QEMU、VirtualBox、Microsoft
    Hyper-V、bhyve（BSD hypervisor）、IBM PowerVM、OpenVZ（开放的基于容器的解决方案）等，如下图所示：
- en: '![Figure 2.10 – Driver-based architecture'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.10 – 基于驱动程序的架构'
- en: '](img/B14834_02_10.jpg)'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_02_10.jpg)'
- en: Figure 2.10 – Driver-based architecture
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.10 – 基于驱动程序的架构
- en: The ability to connect to various virtualization solutions gets us much more
    usability out of the `virsh` command. This might come in very handy in mixed environments,
    such as if you're connecting to both KVM and XEN hypervisors from the same system.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 通过`virsh`命令连接到各种虚拟化解决方案可以让我们更多地使用`virsh`命令。这在混合环境中可能非常有用，比如如果您从同一系统连接到KVM和XEN
    hypervisors。
- en: As in the preceding figure, there is a `virsh --connect QEMU://xxxx/system`)
    passed by the clients, when initializing the library, this public API uses internal
    drivers in the background. Yes, there are different categories of driver implementations
    in libvirt. For example, there are `hypervisor`, `interface`, `network`, `nodeDevice`,
    `nwfilter`, `secret`, `storage`, and so on. Refer to `driver.h` inside the libvirt
    source code to learn about the driver data structures and other functions associated
    with the different drivers.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 与前面的图一样，当客户端在初始化库时传递`virsh --connect QEMU://xxxx/system`时，这个公共API在后台使用内部驱动程序。是的，在libvirt中有不同类别的驱动程序实现。例如，有`hypervisor`、`interface`、`network`、`nodeDevice`、`nwfilter`、`secret`、`storage`等。请参考libvirt源代码中的`driver.h`了解与不同驱动程序相关的驱动程序数据结构和其他函数。
- en: 'Take the following example:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 以以下示例为例：
- en: '[PRE2]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The `struct` fields are self-explanatory and convey which type of driver is
    represented by each of the field members. As you might have assumed, one of the
    important or main drivers is the hypervisor driver, which is the driver implementation
    of different hypervisors supported by libvirt. The drivers are categorized as
    `README` and the libvirt source code):'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: '`bhyve`: The BSD hypervisor'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`esx/`: VMware ESX and GSX support using vSphere API over SOAP'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hyperv/`: Microsoft Hyper-V support using WinRM'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lxc/`: Linux native containers'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`openvz/`: OpenVZ containers using CLI tools'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`phyp/`: IBM Power Hypervisor using CLI tools over SSH'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`qemu/`: QEMU/KVM using the QEMU CLI/monitor'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`remote/`: Generic libvirt native RPC client'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`test/`: A *mock* driver for testing'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`uml/`: User-mode Linux'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`vbox/`: VirtualBox using the native API'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`vmware/`: VMware Workstation and Player using the `vmrun` tool'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`xen/`: Xen using hypercalls, XenD SEXPR, and XenStore'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`xenapi`: Xen using `libxenserver`'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Previously, we mentioned that there are secondary-level drivers as well. Not
    all, but some secondary drivers (see the following) are shared by several hypervisors.
    That said, currently, these secondary drivers are used by hypervisors such as
    the LXC, OpenVZ, QEMU, UML, and Xen drivers. The ESX, Hyper-V, Power Hypervisor,
    Remote, Test, and VirtualBox drivers all implement secondary drivers directly.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples of secondary-level drivers include the following:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: '`cpu/`: CPU feature management'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`interface/`: Host network interface management'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`network/`: Virtual NAT networking'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nwfilter/`: Network traffic filtering rules'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`node_device/`: Host device enumeration'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`secret/`: Secret management'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`security/`: Mandatory access control drivers'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`storage/`: Storage management drivers'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'libvirt is heavily involved in regular management operations, such as the creating
    and managing of virtual machines (guest domains). Additional secondary drivers
    are consumed to perform these operations, such as interface setup, firewall rules,
    storage management, and general provisioning of APIs. The following is from [https://libvirt.org/api.html](https://libvirt.org/api.html):'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '"OnDevice the application obtains a virConnectPtr connection to the hypervisor
    it can then use to manage the hypervisor''s available domains and related virtualization
    resources, such as storage and networking. All those are exposed as first class
    objects and connected to the hypervisor connection (and the node or cluster where
    it is available)."'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure shows the five main objects exported by the API and the
    connections between them:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.11 – Exported API objects and their communication'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_02_11.jpg)'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.11 – Exported API objects and their communication
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s give some details about the main objects available in the libvirt code.
    Most functions inside libvirt make use of these objects for their operations:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '`virConnectPtr`: As we discussed earlier, libvirt has to connect to a hypervisor
    and act. The connection to the hypervisor has been represented as this object.
    This object is one of the core objects in libvirt''s API.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`virDomainPtr`: Virtual machines or guest systems are generally referred to
    as domains in libvirt code. `virDomainPtr` represents an object to an active/defined
    domain/virtual machine.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`virStorageVolPtr`: There are different storage volumes, exposed to the domains/guest
    systems. `virStorageVolPtr` generally represents one of the volumes.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`virStoragePoolPtr`: The exported storage volumes are part of one of the storage
    pools. This object represents one of the storage pools.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`virNetworkPtr`: In libvirt, we can define different networks. A single virtual
    network (active/defined status) is represented by the `virNetworkPtr` object.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You should now have some idea about the internal structure of libvirt implementations;
    this can be expanded further:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.12 – libvirt source code'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_02_12.jpg)'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.12 – libvirt source code
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: Our area of interest is QEMU/KVM. So, let's explore it further. Inside the `src`
    directory of the libvirt source code repository, there is a directory for QEMU
    hypervisor driver implementation code. Pay some attention to the source files,
    such as `qemu_driver.c`, which carries core driver methods for managing QEMU guests.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们感兴趣的是QEMU/KVM。因此，让我们进一步探讨一下。在libvirt源代码存储库的`src`目录中，有一个用于QEMU hypervisor驱动程序实现代码的目录。请注意一些源文件，比如`qemu_driver.c`，它包含了用于管理QEMU客户端的核心驱动程序方法。
- en: 'See the following example:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅以下示例：
- en: '[PRE3]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: libvirt makes use of different driver codes to probe the underlying hypervisor/emulator.
    In the context of this book, the component of libvirt responsible for finding
    out the QEMU/KVM presence is the QEMU driver code. This driver probes for the
    `qemu-kvm` binary and the `/dev/kvm` device node to confirm that the KVM fully
    virtualized hardware-accelerated guests are available. If these are not available,
    the possibility of a QEMU emulator (without KVM) is verified with the presence
    of binaries such as `qemu`, `qemu-system-x86_64`, `qemu-system-mips`, `qemu-system-microblaze`,
    and so on.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: libvirt使用不同的驱动程序代码来探测底层的hypervisor/模拟器。在本书的背景下，libvirt负责发现QEMU/KVM存在的组件是QEMU驱动程序代码。该驱动程序探测`qemu-kvm`二进制文件和`/dev/kvm`设备节点，以确认KVM完全虚拟化的硬件加速客户端是否可用。如果这些不可用，那么通过`qemu`、`qemu-system-x86_64`、`qemu-system-mips`、`qemu-system-microblaze`等二进制文件的存在来验证QEMU模拟器（无KVM）的可能性。
- en: 'The validation can be seen in `qemu_capabilities.c`:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 验证可以在`qemu_capabilities.c`中看到：
- en: '[PRE4]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Then, KVM enablement is performed as shown in the following code snippet:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，KVM启用如下代码片段所示：
- en: '[PRE5]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Basically, libvirt''s QEMU driver is looking for different binaries in different
    distributions and different paths – for example, `qemu-kvm` in RHEL/Fedora. Also,
    it finds a suitable QEMU binary based on the architecture combination of both
    host and guest. If both the QEMU binary and KVM are found, then KVM is fully virtualized
    and hardware-accelerated guests will be available. It''s also libvirt''s responsibility
    to form the entire command-line argument for the QEMU-KVM process. Finally, after
    forming the entire command-line (`qemu_command.c`) arguments and inputs, libvirt
    calls `exec()` to create a QEMU-KVM process:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，libvirt的QEMU驱动程序正在寻找不同发行版和不同路径中的不同二进制文件 - 例如，在RHEL/Fedora中的`qemu-kvm`。此外，它根据主机和客户端的架构组合找到合适的QEMU二进制文件。如果找到了QEMU二进制文件和KVM，那么KVM将完全虚拟化，并且硬件加速的客户端将可用。形成整个QEMU-KVM进程的命令行参数也是libvirt的责任。最后，在形成整个命令行参数和输入后，libvirt调用`exec()`来创建一个QEMU-KVM进程。
- en: '[PRE6]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In KVMland, there is a misconception that libvirt directly uses the device file
    (`/dev/kvm`) exposed by KVM kernel modules, and instructs KVM to do the virtualization
    via the different `ioctl()` function calls available with KVM. This is indeed
    a misconception! As mentioned earlier, libvirt spawns the QEMU-KVM process and
    QEMU talks to the KVM kernel modules. In short, QEMU talks to KVM via different
    `ioctl()` to the `/dev/kvm` device file exposed by the KVM kernel module. To create
    a virtual machine (for example, `virsh create`), all libvirt does is spawn a QEMU
    process, which in turn creates the virtual machine. Please note that a separate
    QEMU-KVM process is launched for each virtual machine by `libvirtd`. Properties
    of virtual machines (the number of CPUs, memory size, I/O device configuration,
    and so on) are defined in separate XML files that are located in the `/etc/libvirt/qemu`
    directory. These XML files contain all of the necessary settings that QEMU-KVM
    processes need to start running virtual machines. libvirt clients issue requests
    via the `AF_UNIX socket /var/run/libvirt/libvirt-sock` that `libvirtd` is listening
    on.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在KVM领域，有一个误解，即libvirt直接使用KVM内核模块暴露的设备文件（`/dev/kvm`），并通过KVM的不同`ioctl()`函数调用来指示虚拟化。这确实是一个误解！正如前面提到的，libvirt生成QEMU-KVM进程，而QEMU与KVM内核模块进行通信。简而言之，QEMU通过不同的`ioctl()`向KVM进行通信，以便访问由KVM内核模块暴露的`/dev/kvm`设备文件。要创建一个虚拟机（例如`virsh
    create`），libvirt所做的就是生成一个QEMU进程，然后QEMU创建虚拟机。请注意，`libvirtd`通过`libvirtd`为每个虚拟机启动一个单独的QEMU-KVM进程。虚拟机的属性（CPU数量、内存大小、I/O设备配置等）在`/etc/libvirt/qemu`目录中的单独的XML文件中定义。这些XML文件包含QEMU-KVM进程启动运行虚拟机所需的所有必要设置。libvirt客户端通过`libvirtd`正在监听的`AF_UNIX
    socket /var/run/libvirt/libvirt-sock`发出请求。
- en: The next topic on our list is QEMU – what it is, how it works, and how it interacts
    with KVM.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们列表上的下一个主题是QEMU - 它是什么，它是如何工作的，以及它如何与KVM交互。
- en: QEMU
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: QEMU
- en: QEMU was written by Fabrice Bellard (creator of FFmpeg). It's a free piece of
    software and mainly licensed under GNU's **General Public License** (**GPL**).
    QEMU is a generic and open source machine emulator and virtualizer. When used
    as a machine emulator, QEMU can run OSes and programs made for one machine (such
    as an ARM board) on a different machine (such as your own PC).
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: QEMU是由FFmpeg的创始人Fabrice Bellard编写的。它是一款免费软件，主要根据GNU的**通用公共许可证**（**GPL**）许可。QEMU是一款通用的开源机器模拟器和虚拟化软件。当用作机器模拟器时，QEMU可以在不同的机器上（例如您自己的PC）运行为一台机器（如ARM板）制作的操作系统和程序。
- en: By using dynamic translation, it achieves very good performance (see [https://www.qemu.org/](https://www.qemu.org/)).
    Let me rephrase the preceding paragraph and give a more specific explanation.
    QEMU is actually a hosted hypervisor/VMM that performs hardware virtualization.
    Are you confused? If so, don't worry. You will get a better picture by the end
    of this chapter, especially when you go through each of the interrelated components
    and correlate the entire path used here to perform virtualization. QEMU can act
    as an emulator or virtualizer.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 通过动态翻译，它实现了非常好的性能（参见[https://www.qemu.org/](https://www.qemu.org/)）。让我重新表述前面的段落，并给出更具体的解释。QEMU实际上是一个托管的hypervisor/VMM，执行硬件虚拟化。你感到困惑吗？如果是这样，不要担心。当你通过本章的最后，特别是当你通过每个相关的组件并将这里使用的整个路径相关联起来执行虚拟化时，你会有一个更清晰的认识。QEMU可以充当模拟器或虚拟化器。
- en: QEMU as an emulator
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: QEMU作为一个模拟器
- en: 'In the previous chapter, we discussed binary translation. When QEMU operates
    as an emulator, it is capable of running OSes/programs made for one machine type
    on a different machine type. How is this possible? It just uses binary translation
    methods. In this mode, QEMU emulates CPUs through dynamic binary translation techniques
    and provides a set of device models. Thus, it is enabled to run different unmodified
    guest OSes with different architectures. Binary translation is needed here because
    the guest code has to be executed in the host CPU. The binary translator that
    does this job is known as a **Tiny Code Generator** (**TCG**); it''s a **Just-In-Time**
    (**JIT**) compiler. It transforms the binary code written for a given processor
    into another form of binary code (such as ARM in X86), as shown in the following
    diagram (TCG information from Wikipedia at [https://en.wikipedia.org/wiki/QEMU#Tiny_Code_Generator](https://en.wikipedia.org/wiki/QEMU#Tiny_Code_Generator)):'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '在上一章中，我们讨论了二进制翻译。当QEMU作为模拟器运行时，它能够在不同的机器类型上运行为另一种机器类型制作的操作系统/程序。这是如何可能的？它只是使用了二进制翻译方法。在这种模式下，QEMU通过动态二进制翻译技术模拟CPU，并提供一组设备模型。因此，它能够运行具有不同架构的不同未修改的客户操作系统。这里需要二进制翻译，因为客户代码必须在主机CPU上执行。执行这项工作的二进制翻译器称为**Tiny
    Code Generator**（**TCG**）；它是一个**即时**（**JIT**）编译器。它将为给定处理器编写的二进制代码转换为另一种形式的二进制代码（例如ARM在X86中），如下图所示（TCG信息来自[https://en.wikipedia.org/wiki/QEMU#Tiny_Code_Generator](https://en.wikipedia.org/wiki/QEMU#Tiny_Code_Generator)）:'
- en: '![Figure 2.13 – TCG in QEMU'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.13 - QEMU中的TCG'
- en: '](img/B14834_02_13.jpg)'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_02_13.jpg)'
- en: Figure 2.13 – TCG in QEMU
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.13 - QEMU中的TCG
- en: By using this approach, QEMU can sacrifice a bit of execution speed for much
    broader compatibility. Keeping in mind that most environments nowadays are based
    around different OSes, this seems like a sensible trade-off.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用这种方法，QEMU可以牺牲一点执行速度以获得更广泛的兼容性。要牢记的是，如今大多数环境都是基于不同的操作系统，这似乎是一个明智的折衷方案。
- en: QEMU as a virtualizer
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: QEMU作为虚拟化器
- en: 'This is the mode where QEMU executes the guest code directly on the host CPU,
    thus achieving native performance. For example, when working under Xen/KVM hypervisors,
    QEMU can operate in this mode. If KVM is the underlying hypervisor, QEMU can virtualize
    embedded guests such as Power PC, S390, x86, and so on. In short, QEMU is capable
    of running without KVM using the aforementioned binary translation method. This
    execution will be slower compared to the hardware-accelerated virtualization enabled
    by KVM. In any mode, either as a virtualizer or emulator, QEMU *not only* emulates
    the processor; it also emulates different peripherals, such as disks, networks,
    VGA, PCI, serial and parallel ports, USB, and so on. Apart from this I/O device
    emulation, when working with KVM, QEMU-KVM creates and initializes virtual machines.
    As shown in the following diagram, it also initializes different POSIX threads
    for each **virtual CPU** (**vCPU**) of a guest. It also provides a framework that''s
    used to emulate the virtual machine''s physical address space within the user-mode
    address space of QEMU-KVM:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 这是QEMU在主机CPU上直接执行客户代码，从而实现本机性能的模式。例如，在Xen/KVM hypervisors下工作时，QEMU可以以这种模式运行。如果KVM是底层hypervisor，QEMU可以虚拟化嵌入式客户，如Power
    PC、S390、x86等。简而言之，QEMU能够在不使用KVM的情况下使用上述的二进制翻译方法运行。与KVM启用的硬件加速虚拟化相比，这种执行速度会较慢。在任何模式下，无论是作为虚拟化器还是模拟器，QEMU不仅仅是模拟处理器；它还模拟不同的外围设备，如磁盘、网络、VGA、PCI、串行和并行端口、USB等。除了这种I/O设备模拟外，在与KVM一起工作时，QEMU-KVM还创建和初始化虚拟机。如下图所示，它还为每个客户的**虚拟CPU**（**vCPU**）初始化不同的POSIX线程。它还提供了一个框架，用于在QEMU-KVM的用户模式地址空间内模拟虚拟机的物理地址空间：
- en: '![Figure 2.14 – QEMU as a virtualizer'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.14 - QEMU作为虚拟化器'
- en: '](img/B14834_02_14.jpg)'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_02_14.jpg)'
- en: Figure 2.14 – QEMU as a virtualizer
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.14 - QEMU作为虚拟化器
- en: To execute the guest code in the physical CPU, QEMU makes use of POSIX threads.
    That being said, the guest vCPUs are executed in the host kernel as POSIX threads.
    This itself brings lots of advantages, as these are just some processes for the
    host kernel at a high-level view. From another angle, the user-space part of the
    KVM hypervisor is provided by QEMU. QEMU runs the guest code via the KVM kernel
    module. When working with KVM, QEMU also does I/O emulation, I/O device setup,
    live migration, and so on.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在物理CPU中执行客户代码，QEMU使用了POSIX线程。也就是说，客户vCPU在主机内核中作为POSIX线程执行。这本身带来了很多优势，因为在高层视图中，这些只是主机内核的一些进程。从另一个角度来看，QEMU提供了KVM
    hypervisor的用户空间部分。QEMU通过KVM内核模块运行客户代码。在与KVM一起工作时，QEMU还进行I/O模拟、I/O设备设置、实时迁移等。
- en: QEMU opens the device file (`/dev/kvm`) that's exposed by the KVM kernel module
    and executes `ioctl()` function calls on it. Please refer to the next section
    on KVM to find out more about these `ioctl()`function calls. To conclude, KVM
    makes use of QEMU to become a complete hypervisor. KVM is an accelerator or enabler
    of the hardware virtualization extensions (VMX or SVM) provided by the processor
    so that they're tightly coupled with the CPU architecture. Indirectly, this conveys
    that virtual systems must also use the same architecture to make use of hardware
    virtualization extensions/capabilities. Once it is enabled, it will definitely
    give better performance than other techniques, such as binary translation.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: QEMU打开了由KVM内核模块暴露的设备文件(`/dev/kvm`)，并对其执行`ioctl()`函数调用。请参考下一节关于KVM的内容，了解更多关于这些`ioctl()`函数调用的信息。总之，KVM利用QEMU成为一个完整的hypervisor。KVM是处理器提供的硬件虚拟化扩展（VMX或SVM）的加速器或启用器，使它们与CPU架构紧密耦合。间接地，这表明虚拟系统也必须使用相同的架构来利用硬件虚拟化扩展/功能。一旦启用，它肯定会比其他技术（如二进制翻译）提供更好的性能。
- en: Our next step is to check how QEMU fits into the whole KVM story.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的下一步是检查QEMU如何融入整个KVM故事中。
- en: QEMU – KVM internals
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: QEMU-KVM内部
- en: 'Before we start looking into QEMU internals, let''s clone the QEMU Git repository:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始研究QEMU内部之前，让我们克隆QEMU Git存储库：
- en: '[PRE7]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Once it''s cloned, you can see a hierarchy of files inside the repo, as shown
    in the following screenshot:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦克隆完成，您可以在存储库内看到文件的层次结构，如下面的屏幕截图所示：
- en: '![Figure 2.15 – QEMU source code'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.15 – QEMU源代码'
- en: '](img/B14834_02_15.jpg)'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_02_15.jpg)'
- en: Figure 2.15 – QEMU source code
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.15 – QEMU源代码
- en: Some important data structures and `ioctl()` function calls make up the QEMU
    userspace and KVM kernel space. Some of the important data structures are `KVMState`,
    `CPU{X86}State`, `MachineState`, and so on. Before we further explore the internals,
    I would like to point out that covering them in detail is beyond the scope of
    this book; however, I will give enough pointers to understand what is happening
    under the hood and give additional references for further explanation.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 一些重要的数据结构和`ioctl()`函数调用构成了QEMU用户空间和KVM内核空间。一些重要的数据结构是`KVMState`、`CPU{X86}State`、`MachineState`等。在我们进一步探索内部之前，我想指出，详细介绍它们超出了本书的范围；但是，我会给出足够的指针来理解发生了什么，并提供更多的参考资料以供进一步解释。
- en: Data structures
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据结构
- en: 'In this section, we will discuss some of the important data structures of QEMU.
    The `KVMState` structure contains important file descriptors of virtual machine
    representation in QEMU. For example, it contains the virtual machine file descriptor,
    as shown in the following code:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将讨论QEMU的一些重要数据结构。`KVMState`结构包含了QEMU中虚拟机表示的重要文件描述符。例如，它包含了虚拟机文件描述符，如下面的代码所示：
- en: '[PRE8]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'QEMU-KVM maintains a list of `CPUX86State` structures, one structure for each
    vCPU. The content of general-purpose registers (as well as RSP and RIP) is part
    of `CPUX86State`:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: QEMU-KVM维护着`CPUX86State`结构的列表，每个vCPU都有一个结构。通用寄存器的内容（以及RSP和RIP）是`CPUX86State`的一部分：
- en: '[PRE9]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Also, `CPUX86State` looks into the standard registers for exception and interrupt
    handling:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，`CPUX86State`还查看标准寄存器以进行异常和中断处理：
- en: '[PRE10]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Various `ioctl()` function calls exist: `kvm_ioctl()`, `kvm_vm_ioctl()`, `kvm_vcpu_ioctl()`,
    `kvm_device_ioctl()`, and so on. For function definitions, please visit `KVM-all.c`
    inside the QEMU source code repo. These `ioctl()` function calls fundamentally
    map to the system KVM, virtual machine, and vCPU levels. These `ioctl()` function
    calls are analogous to the `ioctl()`function calls categorized by KVM. We will
    discuss this when we dig further into KVM internals. To get access to these `ioctl()`
    function calls exposed by the KVM kernel module, QEMU-KVM has to open `/dev/kvm`,
    and the resulting file descriptor is stored in `KVMState->fd`:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 存在各种`ioctl()`函数调用：`kvm_ioctl()`、`kvm_vm_ioctl()`、`kvm_vcpu_ioctl()`、`kvm_device_ioctl()`等。有关函数定义，请访问QEMU源代码存储库中的`KVM-all.c`。这些`ioctl()`函数调用基本上映射到系统KVM、虚拟机和vCPU级别。这些`ioctl()`函数调用类似于由KVM分类的`ioctl()`函数调用。当我们深入研究KVM内部时，我们将讨论这一点。要访问由KVM内核模块公开的这些`ioctl()`函数调用，QEMU-KVM必须打开`/dev/kvm`，并将结果文件描述符存储在`KVMState->fd`中：
- en: '`kvm_ioctl()`: These `ioctl()` function calls mainly execute on the `KVMState->fd`
    parameter, where `KVMState->fd` carries the file descriptor obtained by opening
    `/dev/kvm` – as in the following example:'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kvm_ioctl()`：这些`ioctl()`函数调用主要在`KVMState->fd`参数上执行，其中`KVMState->fd`携带通过打开`/dev/kvm`获得的文件描述符，就像下面的例子一样：'
- en: '[PRE11]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '`kvm_vm_ioctl()`: These `ioctl()` function calls mainly execute on the `KVMState->vmfd`
    parameter – as in the following example:'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kvm_vm_ioctl()`：这些`ioctl()`函数调用主要在`KVMState->vmfd`参数上执行，就像下面的例子一样：'
- en: '[PRE12]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '`kvm_vcpu_ioctl()`: These `ioctl()` function calls mainly execute on the `CPUState->kvm_fd`
    parameter, which is a vCPU file descriptor for KVM – as in the following example:'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kvm_vcpu_ioctl()`：这些`ioctl()`函数调用主要在`CPUState->kvm_fd`参数上执行，这是KVM的vCPU文件描述符，就像下面的例子一样：'
- en: '[PRE13]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '`kvm_device_ioctl()`: These `ioctl()` function calls mainly execute on the
    device `fd` parameter – as in the following example:'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kvm_device_ioctl()`：这些`ioctl()`函数调用主要在设备`fd`参数上执行，就像下面的例子一样：'
- en: '[PRE14]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '`kvm-all.c` is one of the important source files when considering QEMU KVM
    communication.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在考虑QEMU KVM通信时，`kvm-all.c`是一个重要的源文件之一。
- en: Now, let's move on and see how a virtual machine and vCPUs are created and initialized
    by QEMU in the context of KVM virtualization.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们继续看看QEMU在KVM虚拟化环境中如何创建和初始化虚拟机和vCPU。
- en: '`kvm_init()` is the function that opens the KVM device file, as shown in the
    following code, and it also fills `fd [1]` and `vmfd [2]` of `KVMState`:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '`kvm_init()`是打开KVM设备文件的函数，就像下面的代码所示，它还填充了`KVMState`的`fd [1]`和`vmfd [2]`：'
- en: '[PRE15]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'As you can see in the preceding code, the `ioctl()` function call with the
    `KVM_CREATE_VM` argument will return `vmfd`. Once QEMU has `fd` and `vmfd`, one
    more file descriptor has to be filled, which is just `kvm_fd` or `vcpu fd`. Let''s
    see how this is filled by QEMU:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 如您在前面的代码中所看到的，带有`KVM_CREATE_VM`参数的`ioctl()`函数调用将返回`vmfd`。一旦QEMU有了`fd`和`vmfd`，还必须填充一个文件描述符，即`kvm_fd`或`vcpu
    fd`。让我们看看QEMU是如何填充这个的：
- en: '[PRE16]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Some of the memory pages are shared between the QEMU-KVM process and the KVM
    kernel modules. You can see such a mapping in the `kvm_init_vcpu()` function.
    That said, two host memory pages per vCPU make a channel for communication between
    the QEMU user-space process and the KVM kernel modules: `kvm_run` and `pio_data`.
    Also understand that, during the execution of these `ioctl()` function calls that
    return the preceding `fds`, the Linux kernel allocates a file structure and related
    anonymous nodes. We will discuss the kernel part later when discussing KVM.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 一些内存页面在QEMU-KVM进程和KVM内核模块之间共享。您可以在`kvm_init_vcpu()`函数中看到这样的映射。也要了解，在执行返回前述`fds`的这些`ioctl()`函数调用期间，Linux内核会分配文件结构和相关的匿名节点。我们将在讨论KVM时稍后讨论内核部分。
- en: 'We have seen that vCPUs are `posix` threads created by QEMU-KVM. To run guest
    code, these vCPU threads execute an `ioctl()` function call with `KVM_RUN` as
    its argument, as shown in the following code:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到vCPU是由QEMU-KVM创建的`posix`线程。为了运行客户代码，这些vCPU线程执行带有`KVM_RUN`参数的`ioctl()`函数调用，就像下面的代码所示：
- en: '[PRE17]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The same function, `kvm_cpu_exec()`, also defines the actions that need to
    be taken when the control comes back to the QEMU-KVM userspace from KVM with `VM
    exit`. Even though we will discuss later on how KVM and QEMU communicate with
    each other to perform an operation on behalf of the guest, let me touch upon this
    here. KVM is an enabler of hardware extensions provided by vendors such as Intel
    and AMD with their virtualization extensions such as SVM and VMX. These extensions
    are used by KVM to directly execute the guest code on host CPUs. However, if there
    is an event – for example, as part of an operation, guest kernel code access hardware
    device register, which is emulated by the QEMU – then KVM has to exit back to
    QEMU and pass control. Then, QEMU can emulate the outcome of the operation. There
    are different exit reasons, as shown in the following code:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Now that we know about QEMU-KVM internals, let's discuss the threading models
    in QEMU.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: Threading models in QEMU
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'QEMU-KVM is a multithreaded, event-driven (with a big lock) application. The
    important threads are as follows:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: Main thread
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Worker threads for the virtual disk I/O backend
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One thread for each vCPU
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For each and every virtual machine, there is a QEMU process running in the
    host system. If the guest system is shut down, this process will be destroyed/exited.
    Apart from vCPU threads, there are dedicated I/O threads running a select (2)
    event loop to process I/O, such as network packets and disk I/O completion. I/O
    threads are also spawned by QEMU. In short, the situation will look like this:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.16 – KVM guest'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14834_02_16.jpg)'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.16 – KVM guest
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we discuss this further, there is always a question about the physical
    memory of guest systems: where is it located? Here is the deal: the guest RAM
    is assigned inside the QEMU process''s virtual address space, as shown in the
    preceding figure. That said, the physical RAM of the guest is inside the QEMU
    process address space.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: More details about threading can be fetched from the threading model at [blog.vmsplice.net/2011/03/qemu-internals-overall-architecutre-and-html?m=1](http://blog.vmsplice.net/2011/03/qemu-internals-overall-architecutre-and-html?m=1).
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: The event loop thread is also called `iothread`. Event loops are used for timers,
    file descriptor monitoring, and so on. `main_loop_wait()` is the QEMU main event
    loop thread. This main event loop thread is responsible for main loop services,
    including file descriptor callbacks, bottom halves, and timers (defined in `qemu-timer.h`).
    Bottom halves are similar to timers that execute immediately but have lower overhead,
    and scheduling them is wait-free, thread-safe, and signal-safe.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: Before we leave the QEMU code base, I would like to point out that there are
    mainly two parts to device codes. For example, the directory block contains the
    host side of the block device code, and `hw/block/` contains the code for device
    emulation.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: KVM
  id: totrans-220
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There is a common kernel module called `kvm.ko` and also hardware-based kernel
    modules such as `kvm-intel.ko` (Intel-based systems) and `kvm-amd.ko` (AMD-based
    systems). Accordingly, KVM will load the `kvm-intel.ko` (if the `vmx` flag is
    present) or `kvm-amd.ko` (if the `svm` flag is present) modules. This turns the
    Linux kernel into a hypervisor, thus achieving virtualization.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: KVM exposes a device file called `/dev/kvm` to applications so that they can
    make use of the `ioctl()` function calls system calls provided. QEMU makes use
    of this device file to talk with KVM and create, initialize, and manage the kernel-mode
    context of virtual machines.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: 'Previously, we mentioned that the QEMU-KVM userspace hosts the virtual machine''s
    physical address space within the user-mode address space of QEMU/KVM, which includes
    memory-mapped I/O. KVM helps us achieve that. There are more things that can be
    achieved with the help of KVM. The following are some examples:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: Emulation of certain I/O devices; for example, (via *MMIO*) the per-CPU local
    APIC and the system-wide IOAPIC.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Emulation of certain *privileged* (R/W of system registers CR0, CR3, and CR4)
    instructions.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The facilitation to run guest code via `VMENTRY` and handling *intercepted events*
    at `VMEXIT`.
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Injecting* events, such as virtual interrupts and page faults, into the flow
    of the execution of the virtual machine and so on. This is also achieved with
    the help of KVM.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: KVM is not a full hypervisor; however, with the help of QEMU and emulators (a
    slightly modified QEMU for I/O device emulation and BIOS), it can become one.
    KVM needs hardware virtualization-capable processors to operate. Using these capabilities,
    KVM turns the standard Linux kernel into a hypervisor. When KVM runs virtual machines,
    every virtual machine is a normal Linux process, which can obviously be scheduled
    to run on a CPU by the host kernel, as with any other process present in the host
    kernel. In [*Chapter 1*](B14834_01_Final_ASB_ePub.xhtml#_idTextAnchor016), *Understanding
    Linux Virtualization*, we discussed different CPU modes of execution. As you may
    recall, there is mainly a user mode and a kernel/supervisor mode. KVM is a virtualization
    feature in the Linux kernel that lets a program such as QEMU safely execute guest
    code directly on the host CPU. This is only possible when the target architecture
    is supported by the host CPU.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: However, KVM introduced one more mode called guest mode. In a nutshell, guest
    mode allows us to execute guest system code. It can either run the guest user
    or the kernel code. With the support of virtualization-aware hardware, KVM virtualizes
    the process states, memory management, and so on.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: Virtualization from a CPU perspective
  id: totrans-230
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: With its hardware virtualization capabilities, the processor manages the processor
    states by using **Virtual Machine Control Structure** (**VMCS**) and **Virtual
    Machine Control Block** (**VMCB**) for the host and guest OSes, and it also manages
    the I/O and interrupts on behalf of the virtualized OS. That being said, with
    the introduction of this type of hardware, tasks such as CPU instruction interception,
    register read/write support, memory management support (**Extended Page Tables**
    (**EPTs**) and **Nested Paging Table** (**NPT**)), interrupt handling support
    (APICv), IOMMU, and so on came into the picture. KVM uses the standard Linux scheduler,
    memory management, and other services. In short, what KVM does is help the userspace
    program make use of hardware virtualization capabilities. Here, you can treat
    QEMU as a userspace program as it's well integrated for different use cases. When
    I say *hardware-accelerated virtualization*, I am mainly referring to Intel VT-X
    and AMD-Vs SVM. Introducing virtualization technology processors brought about
    an extra instruction set called **VMX**.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: With Intel's VT-X, the VMM runs in *VMX root operation mode*, while the guests
    (which are unmodified OSes) run in *VMX non-root operation mode*. This VMX brings
    additional virtualization-specific instructions to the CPU, such as `VMPTRLD`,
    `VMPTRST`, `VMCLEAR`, `VMREAD`, `VMWRITE`, `VMCALL`, `VMLAUNCH`, `VMRESUME`, `VMXOFF`,
    and `VMXON`. The `VMXON` and can be disabled by `VMXOFF`. To execute the guest
    code, we have to use `VMLAUNCH`/`VMRESUME` instructions and leave `VMEXIT`. But
    wait, leave what? It's a transition from non-root operation to root operation.
    Obviously, when we do this transition, some information needs to be saved so that
    it can be fetched later. Intel provides a structure to facilitate this transition
    called VMCS; it handles much of the virtualization management functionality. For
    example, in the case of `VMEXIT`, the exit reason will be recorded inside this
    structure. Now, how do we read or write from this structure? `VMREAD` and `VMWRITE`
    instructions are used to read or write to the respective fields.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: Previously, we discussed SLAT/EPT/AMD-Vi. Without EPT, the hypervisor must exit
    the virtual machine to perform address translations, which reduces performance.
    As we noticed in Intel's virtualization-based processors' operating modes, AMD's
    SVM also has a couple of operating modes, which are nothing but host mode and
    guest mode. As you may have assumed, the hypervisor runs in host mode and the
    guests run in guest mode. Obviously, when in guest mode, some instructions can
    cause `VMEXIT` exceptions, which are handled in a manner that is specific to the
    way guest mode is entered. There should be an equivalent structure of VMCS here,
    and it is called VMCB; as discussed earlier, it contains the reason for `VMEXIT`.
    AMD added eight new instruction opcodes to support SVMs. For example, the `VMRUN`
    instruction starts the operation of a guest OS, the `VMLOAD` instruction loads
    the processor state from the VMCB, and the `VMSAVE` instruction saves the processor
    state to the VMCB. That's why AMD introduced nested paging, which is similar to
    EPT in Intel.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: When we discussed hardware virtualization extensions, we touched upon VMCS and
    VMCB. These are important data structures when we think about hardware-accelerated
    virtualization. These control blocks especially help in `VMEXIT` scenarios. Not
    every operation can be allowed for guests; at the same time, it's also difficult
    if the hypervisor does everything on behalf of the guest. Virtual machine control
    structures, such as VMCS or VMCB, control this behavior. Some operations are allowed
    for guests, such as changing some bits in shadowed control registers, but others
    are not. This clearly provides fine-grained control over what guests are allowed
    to do and not do. VMCS control structures also provide control over interrupt
    delivery and exceptions. Previously, we said the exit reason of `VMEXIT` is recorded
    inside the VMCS; it also contains some data about it. For example, if write access
    to a control register caused the exit, information about the source and destination
    registers is recorded there.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: Please take note of the VMCS or VMCB store guest configuration specifics, such
    as machine control bits and processor register settings. I suggest that you examine
    the structure definitions from the source. These data structures are also used
    by the hypervisor to define events to monitor while the guest is executing. These
    events can be intercepted. Note that these structures are in the host memory.
    At the time of using `VMEXIT`, the guest state is saved in VMCS. As mentioned
    earlier, the `VMREAD` instruction reads the specified field from the VMCS, while
    the `VMWRITE` instruction writes the specified field to the VMCS. Also, note that
    there is one VMCS or VMCB per vCPU. These control structures are part of the host
    memory. The vCPU state is recorded in these control structures.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: KVM APIs
  id: totrans-236
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As mentioned earlier, there are three main types of `ioctl()` function calls.
    The kernel docs says the following (you can check it at [https://www.kernel.org/doc/Documentation/virtual/kvm/api.txt](https://www.kernel.org/doc/Documentation/virtual/kvm/api.txt)):'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: 'Three sets of ioctl make up the KVM API. The KVM API is a set of ioctls that
    are issued to control various aspects of a virtual machine. These ioctls belong
    to three classes:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: '- System ioctls: These query and set global attributes, which affect the whole
    KVM subsystem. In addition, a system ioctl is used to create virtual machines.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: '- Device ioctls: Used for device control, executed from the same context that
    spawned the VM creation.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: '- VM ioctls: These query and set attributes that affect an entire virtual machine—for
    example, memory layout. In addition, a VM ioctl is used to create virtual CPUs
    (vCPUs). It runs VM ioctls from the same process (address space) that was used
    to create the VM.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: '- vCPU ioctls: These query and set attributes that control the operation of
    a single virtual CPU. They run vCPU ioctls from the same thread that was used
    to create the vCPU.'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: To find out more about the `ioctl()` function calls exposed by KVM and the `ioctl()`
    function calls that belong to a particular group of `fd`, please refer to `KVM.h`.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: 'See the following example:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Let's now discuss anonymous inodes and file structures.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: Anonymous inodes and file structures
  id: totrans-247
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Previously, when we discussed QEMU, we said the Linux kernel allocates file
    structures and sets its `f_ops` and anonymous inodes. Let''s look at the `kvm_main.c`
    file:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Like `kvm_chardev_fops`, there are `kvm_vm_fops` and `kvm_vcpu_fops`:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'An inode allocation may be seen as follows:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Let's have a look at the data structures now.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: Data structures
  id: totrans-255
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'From the perspective of the KVM kernel modules, each virtual machine is represented
    by a `kvm` structure:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'As you can see in the preceding code, the `kvm` structure contains an array
    of pointers to `kvm_vcpu` structures, which are the counterparts of the `CPUX86State`
    structures in the QEMU-KVM userspace. A `kvm_vcpu` structure consists of a common
    part and an x86 architecture-specific part, which includes the register content:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The x86 architecture-specific part of the `kvm_vcpu` structure contains fields
    to which the guest register state can be saved after a virtual machine exit and
    from which the guest register state can be loaded before a virtual machine entry:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'As you can see in the preceding code, `kvm_vcpu` has an associated `kvm_run`
    structure used for the communication (with `pio_data`) between the QEMU userspace
    and the KVM kernel module, as mentioned earlier. For example, in the context of
    `VMEXIT`, to satisfy the emulation of virtual hardware access, KVM has to return
    to the QEMU userspace process; KVM stores the information in the `kvm_run` structure
    for QEMU to fetch it:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The `kvm_run` struct is an important data structure; as you can see in the preceding
    code, `union` contains many exit reasons, such as `KVM_EXIT_FAIL_ENTRY`, `KVM_EXIT_IO`,
    and so on.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: When we discussed hardware virtualization extensions, we touched upon VMCS and
    VMCB. These are important data structures when we think about hardware-accelerated
    virtualization. These control blocks especially help in `VMEXIT` scenarios. Not
    every operation can be allowed for guests; at the same time, it's also difficult
    if the hypervisor does everything on behalf of the guest. Virtual machine control
    structures, such as VMCS or VMCB, control the behavior. Some operations are allowed
    for guests, such as changing some bits in shadowed control registers, but others
    are not. This clearly provides fine-grained control over what guests are allowed
    to do and not do. VMCS control structures also provide control over interrupt
    delivery and exceptions. Previously, we said the exit reason of `VMEXIT` is recorded
    inside the VMCS; it also contains some data about it. For example, if write access
    to a control register caused the exit, information about the source and destination
    registers is recorded there.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: Let's look at some of the important data structures before we dive into the
    vCPU execution flow.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: The Intel-specific implementation is in `vmx.c` and the AMD-specific implementation
    is in `svm.c`, depending on the hardware we have. As you can see, the following
    `kvm_vcpu` is part of `vcpu_vmx`. The `kvm_vcpu` structure is mainly categorized
    as a common part and an architecture-specific part. The common part contains the
    data that is common to all supported architectures and is architecture-specific
    – for example, the x86 architecture-specific (guest's saved general-purpose registers)
    part contains the data that is specific to a particular architecture. As discussed
    earlier, `kvm_vCPUs`, `kvm_run`, and `pio_data` are shared with the userspace.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: 'The `vcpu_vmx` and `vcpu_svm` structures (mentioned next) have a `kvm_vcpu`
    structure, which consists of an x86-architecture-specific part (`struct ''kvm_vcpu_arch''`)
    and a common part and also, it points to the `vmcs` and `vmcb` structures accordingly.
    Let''s check the Intel (`vmx`) structure first:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Similarly, let''s check the AMD (`svm`) structure next:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The `vcpu_vmx` or `vcpu_svm` structures are allocated by the following code
    path:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Please note that the VMCS or VMCB store guest configuration specifics such as
    machine control bits and processor register settings. I would suggest you examine
    the structure definitions from the source. These data structures are also used
    by the hypervisor to define events to monitor while the guest is executing. These
    events can be intercepted and these structures are in the host memory. At the
    time of `VMEXIT`, the guest state is saved in VMCS. As mentioned earlier, the
    `VMREAD` instruction reads a field from the VMCS, whereas the `VMWRITE` instruction
    writes the field to it. Also, note that there is one VMCS or VMCB per vCPU. These
    control structures are part of the host memory. The vCPU state is recorded in
    these control structures.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: Execution flow of vCPU
  id: totrans-275
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Finally, we are into the vCPU execution flow, which helps us put everything
    together and understand what happens under the hood.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: I hope you didn't forget that the QEMU creates a POSIX thread for a vCPU of
    the guest and `ioctl()`, which is responsible for running a CPU and has `KVM_RUN
    arg (#define KVM_RUN _IO(KVMIO, 0x80))`. The vCPU thread executes `ioctl(.., KVM_RUN,
    ...)` to run the guest code. As these are POSIX threads, the Linux kernel can
    schedule these threads as with any other process/thread in the system.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see how it all works:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'According to the underlying architecture and hardware, different structures
    are initialized by the KVM kernel modules and one among them is `vmx_x86_ops/svm_x86_ops`
    (owned by either the `kvm-intel` or `kvm-amd` module). It defines different operations
    that need to be performed when the vCPU is in context. KVM makes use of the `kvm_x86_ops`
    vector to point either of these vectors according to the KVM module (`kvm-intel`
    or `kvm-amd`) loaded for the hardware. The `run` pointer defines the function
    that needs to be executed when the guest vCPU run is in action, and `handle_exit`
    defines the actions needed to be performed at the time of `VMEXIT`. Let''s check
    the Intel (`vmx`) structure for that:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Now, let''s see the AMD (`svm`) structure for that:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The `run` pointer points to `vmx_vcpu_run` or `svm_vcpu_run` accordingly. The
    `svm_vcpu_run` or `vmx_vcpu_run` functions do the job of saving KVM host registers,
    loading guest OS registers, and `SVM_VMLOAD` instructions. We walked through the
    QEMU KVM userspace code execution at the time of `vcpu run`, once it enters the
    kernel via `syscall`. Then, following the file operations structures, it calls
    `kvm_vcpu_ioctl()`; this defines the action to be taken according to the `ioctl()`
    function call it defines:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'We will go through `vcpu_run()` to understand how it reaches `vmx_vcpu_run`
    or `svm_vcpu_run`:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Once it''s in `vcpu_enter_guest()`, you can see some of the important calls
    happening when it enters guest mode in KVM:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: You can see a high-level picture of `VMENTRY` and `VMEXIT` from the `vcpu_enter_guest()`
    function. That said, `VMENTRY` (`[vmx_vcpu_run or svm_vcpu_run]`) is just a guest
    OS executing in the CPU; different intercepted events can occur at this stage,
    causing `VMEXIT`. If this happens, any `vmx_handle_exit` or `handle_exit` function
    call will start looking into this exit cause. We have already discussed the reasons
    for `VMEXIT` in previous sections. Once there is `VMEXIT`, the exit reason is
    analyzed and action is taken accordingly.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: '`vmx_handle_exit()` is the function responsible for handling the exit reason:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '`kvm_vmx_exit_handlers[]` is the table of virtual machine exit handlers, indexed
    by `exit reason`. Similar to Intel, the `svm` code has `handle_exit()`:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '`handle_exit()` has the `svm_exit_handler` array, as shown in the following
    section.'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: 'If needed, KVM has to fall back to the userspace (QEMU) to perform the emulation
    as some of the instructions have to be performed on the QEMU emulated devices.
    For example, to emulate I/O port access, the control goes to the userspace (QEMU):'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: This chapter was a bit source code-heavy. Sometimes, digging in and checking
    the source code is just about the only way to understand how something works.
    Hopefully, this chapter managed to do just that.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-299
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered the inner workings of KVM and its main partners
    in Linux virtualization – libvirt and QEMU. We discussed various types of virtualization
    – binary translation, full, paravirtualization, and hardware-assisted virtualization.
    We checked a bit of kernel, QEMU, and libvirt source code to learn about their
    interaction *from inside*. This gave us the necessary technical know-how to understand
    the topics that will follow in this book – everything ranging from how to create
    virtual machines and virtual networks to scaling the virtualization idea to a
    cloud concept. Understanding these concepts will also make it much easier for
    you to understand the key goal of virtualization from an enterprise company's
    perspective – how to properly design a physical and virtual infrastructure, which
    will slowly but surely be introduced as a concept throughout this book. Now that
    we've covered the basics about how virtualization works, it's time to move on
    to a more practical subject – how to deploy the KVM hypervisor, management tools,
    and oVirt. We'll do this in the next chapter.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  id: totrans-301
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What is paravirtualization?
  id: totrans-302
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is full virtualization?
  id: totrans-303
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is hardware-assisted virtualization?
  id: totrans-304
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the primary goal of libvirt?
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What does KVM do? What about QEMU?
  id: totrans-306
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  id: totrans-307
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Please refer to the following links for more information regarding what was
    covered in this chapter:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: 'Binary translation: [https://pdfs.semanticscholar.org/d6a5/1a7e73f747b309ef5d44b98318065d5267cf.pdf](https://pdfs.semanticscholar.org/d6a5/1a7e73f747b309ef5d44b98318065d5267cf.pdf)'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Virtualization basics: [http://dsc.soic.indiana.edu/publications/virtualization.pdf](http://dsc.soic.indiana.edu/publications/virtualization.pdf)'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'KVM: [https://www.redhat.com/en/topics/virtualization/what-is-KVM](https://www.redhat.com/en/topics/virtualization/what-is-KVM)'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'QEMU: [https://www.qemu.org/](https://www.qemu.org/)'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Understanding full virtualization, paravirtualization, and hardware assist:
    [https://www.vmware.com/content/dam/digitalmarketing/vmware/en/pdf/techpaper/VMware_paravirtualization.pdf](https://www.vmware.com/content/dam/digitalmarketing/vmware/en/pdf/techpaper/VMware_paravirtualization.pdf)'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
