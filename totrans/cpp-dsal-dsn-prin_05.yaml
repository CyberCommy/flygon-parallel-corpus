- en: 5\. Greedy Algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Learning Objectives
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you will be able to:'
  prefs: []
  type: TYPE_NORMAL
- en: Describe the greedy approach to algorithm design
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identify the optimal substructure and greedy choice properties of a problem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement greedy algorithms such as fractional knapsack and greedy graph coloring
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement Kruskal's Minimum Spanning Tree algorithm using a disjoint-set data
    structure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, we will look at various 'greedy' approaches to algorithm design
    and see how they can be applied in order to solve real-world problems.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the previous chapter, we discussed the divide-and-conquer algorithm design
    technique, which solves a given problem by dividing the input into smaller subproblems,
    solving each subproblem, and subsequently merging the results. Continuing our
    theme of algorithm design paradigms, we will now look at our next topic: the **greedy
    approach**.'
  prefs: []
  type: TYPE_NORMAL
- en: 'On each iteration, a greedy algorithm is one that picks the ''seemingly best''
    alternative. In other words, a greedy solution to a problem composes a globally
    optimal solution to the given problem from a series of locally optimal solutions.
    For example, the following screenshot shows the shortest path that a car can take
    from Dulles International Airport in Washington DC to an office building in East
    Riverdale. Naturally, the path shown is also the shortest for any two points on
    the path that are not the starting and ending points:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.1: A route from an airport to an office in Washington DC (Source:
    project-osrm.org)](img/C14498_05_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.1: A route from an airport to an office in Washington DC (Source:
    project-osrm.org)'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Therefore, we can infer that the whole shortest path, P, is, in effect, a concatenation
    of several shortest paths between the vertices of the road network that lie along
    P. So, if we were asked to design a shortest path algorithm, one possible strategy
    would be as follows: start from the origin vertex and draw a path to the closest
    vertex that hasn''t been explored yet, and then repeat until we reach the destination
    vertex. Congratulations – you have just solved the shortest path problem using
    Dijkstra''s algorithm, which is the same one that powers commercial software such
    as Google Maps and Bing Maps!'
  prefs: []
  type: TYPE_NORMAL
- en: Expectedly, the simple approach taken by greedy algorithms makes them applicable
    only to a small subset of algorithmic problems. However, the simplicity of the
    greedy approach often makes it an excellent tool for 'first attack', by which
    we can understand the properties and behavior of the underlying problem, which
    can then be solved using other, more complex approaches.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will study the conditions under which a given problem is
    suitable for a greedy solution – the optimal substructure and greedy choice properties.
    We will see that when a problem can be shown to have these two properties, a greedy
    solution is guaranteed to yield the correct results. We will also see a few examples
    of real-world problems for which greedy solutions are used in practice, and we
    will end this chapter with a discussion of the minimum spanning tree problem,
    which commonly arises in cases of telecommunication and water supply networks,
    electrical grids, and circuit design. But first, let's start by taking a look
    at some simpler problems that can be solved using greedy algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Basic Greedy Algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will study two standard problems that can be solved using
    the greedy approach: **shortest-job-first scheduling** and the **fractional knapsack**
    problem.'
  prefs: []
  type: TYPE_NORMAL
- en: Shortest-Job-First Scheduling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Say you are standing in a queue at your bank. It's a busy day and there are
    *N* people in the queue, but the bank has only one counter open (it's also a really
    bad day!). Let's assume that it takes a person, *p**i*, the amount of time of
    *a**i* to get served at the counter. Since the people in the queue are quite rational,
    everyone agrees to reorder their places in the queue so that the *average waiting
    time* for everyone in the queue is minimized. You are tasked with finding a way
    of reordering the people in the queue. How would you solve this problem?
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.2: The original queue](img/C14498_05_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.2: The original queue'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: To take this problem apart further, let's look at an example. The preceding
    figure shows an example of the original queue, where *A**i* shows the service
    time and *W**i* shows the waiting time for the *i**th* person. The person closest
    to the counter can start getting served immediately, so the waiting time for them
    is 0\. The person who's second in the queue must wait until the first person is
    done, so they have to wait for *a**1* *= 8* units of time before getting served.
    Continuing in a similar fashion, the *i**th* person has a waiting time equal to
    the sum of the service times for all of the *i – 1* people before them in the
    queue.
  prefs: []
  type: TYPE_NORMAL
- en: 'A clue to solving this problem is as follows: since we are looking to minimize
    the *average waiting time*, we must find a way to reduce the waiting time for
    the largest possible set of people, as much as possible. One way to reduce the
    waiting time for all people is the job that can be completed the quickest. By
    repeating this idea for all the people in the queue, our solution results in the
    following reordered queue:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.3: The reordered queue with the minimum average waiting time](img/C14498_05_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.3: The reordered queue with the minimum average waiting time'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Notice that our reordered queue has an average waiting time of 8.87 units versus
    15.25 units for the original ordering, which is an improvement by a factor of
    approximately 2.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 24: Shortest-Job-First Scheduling'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will implement a shortest-job-first scheduling solution
    by taking a similar example to the one shown in the preceding figure. We will
    consider 10 people in a queue and try to minimize the average waiting time for
    all of them. Let''s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Begin by adding the required headers and creating functions for computing the
    waiting times and input/output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Add the main solver and driver code, as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Compile and run the code! Your output should look as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.4: Output of the program to schedule the shortest job first](img/C14498_05_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.4: Output of the program to schedule the shortest job first'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The Knapsack Problem(s)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will discuss the standard **knapsack problem**, also known
    as the 0-1 knapsack problem, which is known to be NP-complete, and thereby does
    not allow us to have any polynomial-time solution. Then, we will turn our discussion
    toward a version of the knapsack problem called the **fractional knapsack problem**,
    which can be solved using a greedy approach. Our focus in this section is to demonstrate
    how even subtle differences between how a problem is defined can lead to large
    changes in the solution strategies.
  prefs: []
  type: TYPE_NORMAL
- en: The Knapsack Problem
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Suppose you are given a set of objects, *O = {O**1**, O**2**, …, O**n**}*, with
    each having a certain weight, *W**i*, and a value of *V**i**.* You are also given
    a bag (or a knapsack) that can carry only a total weight of T units. Now, say
    you are tasked with finding out about a set of objects to keep in your bag so
    that the total weight is less than or equal to T, and the total value of the objects
    is the maximum it can possibly be.
  prefs: []
  type: TYPE_NORMAL
- en: A real-world example of this problem can be understood if you imagine a traveling
    trader who earns a fixed percentage profit on all their trades. They would want
    to carry the maximum value of goods to maximize their profit, but their vehicle
    (or knapsack) can hold only up to T units of weight. The trader knows the exact
    weight and value of each object. Which set of objects should they carry so that
    the total value of the objects carried for trade is the maximum possible?
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.5: The knapsack problem](img/C14498_05_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.5: The knapsack problem'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The problem that's presented in the preceding figure is the well-known knapsack
    problem and has been proven to be NP-complete. In other words, there is no known
    polynomial-time solution to this problem. As a result, we must look at all the
    possible combinations of the objects to find the combination that has the greatest
    value while having a total weight of only *T* units. The preceding diagram shows
    two ways a knapsack with a capacity of 8 units can be filled. The objects shown
    in grey are the ones that have been chosen to be put in the knapsack. We can see
    that the first set of objects has a total value of 40, the second set of objects
    has a total value of 37, and that the total weight in both cases is 8 units. Therefore,
    the second set of objects is a better choice than the first. To find the best
    possible set of objects, we must list all possible combinations and choose the
    one with the maximum value.
  prefs: []
  type: TYPE_NORMAL
- en: The Fractional Knapsack Problem
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now, we will make a small change to the knapsack problem that was given in
    the previous subsection: let''s say we are now allowed to break each object into
    as many parts as we need, and then we can choose what fraction of each object
    we want to keep in the knapsack.'
  prefs: []
  type: TYPE_NORMAL
- en: In terms of the real-world analogy, let's say that the trader in our previous
    analogy is trading items such as oil, grains, and flour. The trader may take any
    smaller measure of weight.
  prefs: []
  type: TYPE_NORMAL
- en: 'Contrary to the NP-completeness of the standard knapsack, the fractional knapsack
    problem has a simple solution: order the elements according to their value per
    weight ratio and ''greedily'' choose as many objects as possible with the maximum
    ratio. The following figure shows the optimal selection of a given set of objects
    when the knapsack''s capacity is set to 8 units. Notice that the chosen objects
    are the ones with the highest value per weight ratio:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.6: The fractional knapsack problem](img/C14498_05_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.6: The fractional knapsack problem'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We will implement this solution in the following exercise.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 25: Fractional Knapsack Problem'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will consider 10 items and try to maximize the value in
    our knapsack, which can hold a maximum weight of 25 units. Let''s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will begin by adding the required headers and defining an `Object`
    struct that will represent one object in our solution:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Note that we have overloaded the `<` and `==` operators since we will use `std::sort()`
    over a vector of `objects`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for the fractional knapsack solver is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The preceding function sorts the objects in decreasing order of their value/weight
    ratio and then picks all the fractions of objects that can fit in the knapsack
    until the knapsack is full.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, to test our implementation, add the following test and driver code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The preceding function creates objects and initializes them with random data
    from the STL random number generator. Next, it calls our implementation of the
    fractional knapsack solver and then displays the results.
  prefs: []
  type: TYPE_NORMAL
- en: 'Compile and run this code! Your output should look as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.7: Output of Exercise 25](img/C14498_05_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.7: Output of Exercise 25'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note how the solver took a fraction, that is, only 4 of the 5 units of the last
    object by weight. This is an example of how objects can be partitioned before
    being chosen to be kept in the knapsack, which differentiates the fractional knapsack
    from the 0-1 (standard) knapsack problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 11: The Interval Scheduling Problem'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Imagine that you have a set of tasks on your to-do list (doing the dishes, going
    to the supermarket to buy groceries, working on a secret project for world domination,
    and other similar chores). Each task is identified by an ID and can be completed
    only between a particular start and end time. Let's say you wish to complete the
    maximum number of tasks. On what subset, and in what order, should you work on
    your tasks to achieve your objective? Assume that you can work on only one task
    at any point in time.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example, consider the problem instance shown in the following figure.
    We have been given four different tasks that we could possibly spend our time
    working on (the rectangular boxes represent the time interval in which the task
    can be completed):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.8: Given task schedules](img/C14498_05_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.8: Given task schedules'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The following figure shows the optimal scheduling of tasks, which maximizes
    the total number of tasks completed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.9: Optimal selection of tasks](img/C14498_05_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.9: Optimal selection of tasks'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Notice how not completing task 3 allows us to complete tasks 1 and 2 instead,
    increasing the total number of completed tasks. In this activity, you will need
    to implement this greedy interval scheduling solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'The high-level steps for solving this activity are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Assume that each task has a start time, an end time, and an ID. Create a struct
    that describes a task. We will represent different tasks with different instances
    of this struct.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Implement a function that creates an `std::list` of N tasks, set their IDs sequentially
    from 1 to N, and use the values from a random number generator for the start and
    end times.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Implement the scheduling function as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a. Sort the list of tasks in increasing order of their ending times.
  prefs: []
  type: TYPE_NORMAL
- en: b. Greedily choose to complete the task with the earliest ending time.
  prefs: []
  type: TYPE_NORMAL
- en: c. Remove all the tasks that overlap with the currently chosen task (all the
    tasks that start before the current task ends).
  prefs: []
  type: TYPE_NORMAL
- en: d. If tasks remain on the list, go to *step b*. Otherwise, return the chosen
    vector of tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Your final output should look similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.10: Expected output of Activity 11](img/C14498_05_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.10: Expected output of Activity 11'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution for this activity can be found on page 516.
  prefs: []
  type: TYPE_NORMAL
- en: Requirements for Greedy Algorithms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the previous section, we looked at examples of problems where the greedy
    approach gives optimal solutions. However, a problem can be optimally solved using
    the greedy approach if and only if it has two properties: the **optimal substructure**
    property and the **greedy choice** property. In this section, we will attempt
    to understand these properties and show you how to identify whether a problem
    exhibits them.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Optimal substructure**: When an optimal solution to a given problem, P, is
    composed of the optimal solutions to its subproblems, then P is said to have an
    optimal substructure.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Greedy choice**: When an optimal solution to a given problem, P, can be reached
    by selecting the locally optimal solution on each iteration, P is said to have
    the greedy choice property.'
  prefs: []
  type: TYPE_NORMAL
- en: To understand the optimal substructure and greedy choice properties, we will
    implement Kruskal's minimum spanning tree algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: The Minimum Spanning Tree (MST) Problem
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The minimum spanning tree problem can be stated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*"Given a graph, G = < V, E >, where V is the set of vertices and E is the
    set of edges, each associated with an edge weight, find a tree, T, that spans
    all the vertices in V and has the minimum total weight."*'
  prefs: []
  type: TYPE_NORMAL
- en: A real-life application of the MST problem is the design of water supply and
    transportation networks since the designers typically wish to minimize the total
    length of the pipeline that's used or the roads that are created and still make
    sure that the services reach all designated users. Let's try to take the problem
    apart with the following example.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s say that you are given the locations of 12 villages on a map and are
    asked to find the minimum total length of road that would need to be built so
    that all the villages are reachable from one another, and that the roads do not
    form a cycle. Assume that each road can be traversed in either direction. A natural
    representation of villages in this problem is using a graph data structure. Let''s
    assume that the vertices of the following graph, *G* represent the locations of
    the 12 given villages and that the edges of *G* represent the distances between
    the vertices:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.11: Graph G representing the villages and distances between them](img/C14498_05_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.11: Graph G representing the villages and distances between them'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'A simple greedy algorithm to construct the minimum spanning tree, T, could
    be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Add all the edges of *G* in a min-heap, *H*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: From *H*, pop an edge, *e*. Naturally, *e* has the minimum cost among all edges
    in *H*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If both vertices of *e* are already in *T*, this means that adding *e* would
    create a cycle in *T*. Therefore, discard *e* and go to step 2\. Otherwise, proceed
    to the next step.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Insert *e* in the minimum spanning tree, *T*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let''s take a moment to think about why this strategy works. At each iteration
    of the loop in steps 2 and 3, we take the edge with the lowest cost and check
    whether it adds any vertex to our solution. This is stored in the minimum spanning
    tree, *T*. If it does, we add the edge to *T*; otherwise, we discard that edge
    and choose another edge with the minimum value. Our algorithm is greedy in the
    sense that at each iteration, it chooses the minimum edge weight to add to the
    solution. The preceding algorithm was invented in 1956 and is called **Kruskal''s
    minimum spanning tree algorithm**. Applying this algorithm to the graph shown
    in *figure 5.11* gives the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.12: Graph G showing the minimum spanning tree, T (with red edges)](img/C14498_05_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.12: Graph G showing the minimum spanning tree, T (with red edges)'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The total weight of edges in the minimum spanning tree, T, is *(2 × 1) + (3
    × 2) + (2 × 3) = 14* units. Therefore, the answer to our problem is that at least
    12 units of road would need to be built.
  prefs: []
  type: TYPE_NORMAL
- en: 'How do we know that our algorithm is indeed correct? We need to return to the
    definitions of optimal substructure and greedy choice and show that the MST problem
    exhibits these two properties. While a rigorous mathematical proof of the properties
    is beyond the ambit of this book, here are the intuitive ideas behind the proofs:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Optimal substructure**: We will prove this by using contradiction. Let''s
    assume that the MST problem does not exhibit an optimal substructure; that is,
    a minimum spanning tree was not composed of a set of smaller minimum spanning
    trees:'
  prefs: []
  type: TYPE_NORMAL
- en: Let's say we are given a minimum spanning tree, *T*, over the vertices of graph
    *G* Let's remove any edge, *e*, from *T*. Removing *e* decomposes *T* into smaller
    trees, *T**1* and *T**2*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Since we assumed that the MST problem does not exhibit optimal substructure,
    there must exist a spanning tree with a lesser total weight over the vertices
    of *T**1*. Take this spanning tree and add the edges *e* and *T**2* to it. This
    new tree will be *T'*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, since the total weight of *T'* is less than that of *T*, this contradicts
    our original assumption that *T* is an MST. Therefore, the MST problem must exhibit
    the optimal substructure property.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Greedy choice**: If the MST problem exhibits greedy choice property, then
    for a vertex, *v*, the minimum weight edge connecting *v* to the rest of the graph,
    *G*, should always be a part of the minimum spanning tree, *T*. We can prove this
    hypothesis by contradiction, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Say an edge *(u, v)* is the minimum weight edge connecting *v* to any other
    vertex in *G*. Assume that *(u, v)* is not a part of *T*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If *(u, v)* is not a part of *T*, then *T* must consist of some other edge connecting
    *v* to the rest of *G*. Let this edge be *(x, v)*. Since *(u, v)* is the minimum
    weight edge, by definition, the weight of *(x, v)* is greater than the weight
    of *(u, v)*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A tree with a lesser total weight than *T* can be obtained if *(x, v)* is replaced
    with *(u, v)* in *T*. This contradicts our assumption that *T* is the minimum
    spanning tree. Therefore, the MST problem must exhibit the greedy choice property.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As we mentioned earlier, we can also take a rigorous mathematical approach
    to show that the MST problem exhibits the optimal substructure property and is
    suitable for the greedy choice property. You can find it here: [https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-046j-design-and-analysis-of-algorithms-spring-2015/lecture-notes/MIT6_046JS15_lec12.pdf](https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-046j-design-and-analysis-of-algorithms-spring-2015/lecture-notes/MIT6_046JS15_lec12.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: Let's think about how to implement Kruskal's algorithm. We covered graph and
    heap data structures in *Chapter 2*, *Trees, Heaps, and Graphs*, so we know how
    to implement steps 1 and 2\. Step 3 is somewhat more complicated. We need a data
    structure that stores the edges of the graph and tells us whether adding a new
    edge would create a cycle with any possible combination of the edges already stored
    in it. This problem can be solved using a disjoint-set data structure.
  prefs: []
  type: TYPE_NORMAL
- en: Disjoint-Set (or Union-Find) Data Structures
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A **disjoint-set data structure** consists of a forest (a set of trees) of
    elements, where each element is represented by a numerical ID, has a ''rank,''
    and contains a pointer to its parent. When the data structure is initialized,
    it starts with *N* independent elements of rank 0, each of which is a part of
    a tree that contains only the element itself. The data structure supports two
    other operations:'
  prefs: []
  type: TYPE_NORMAL
- en: A `find` operation on a tree returns the root element of that tree
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A `union` operation applied on two trees merges the smaller trees into a larger
    tree, where the size of the tree is stored as the rank of its root.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'More precisely, the disjoint-set data structure supports the following operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Make-Set`: This initializes the data structure with N elements, setting the
    rank of each element to 0, and the parent pointer to itself. The following figure
    shows an example of a disjoint-set *DS* initialized with five elements. The digits
    inside the circles show the element IDs, the digit in parentheses shows the rank,
    and the arrows represent the pointer to the root element:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 5.13: Initializing disjoint set with five elements](img/C14498_05_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.13: Initializing disjoint set with five elements'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: At this stage, the data structure consists of five trees, each consisting of
    one element.
  prefs: []
  type: TYPE_NORMAL
- en: '`Find`: Starting from a given element, *x*, the `find` operation follows the
    parent pointers of elements until the root of the tree is reached. The parent
    of a root element is the root itself. In the example in the previous set, each
    element is the root of the tree, and hence this operation will return the lone
    element in the tree.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Union`: Given two elements, *x* and *y*, the `union` operation finds the roots
    of *x* and *y*. If the two roots are the same, this means that *x* and *y* belong
    to the same tree. Therefore, it does nothing. Otherwise, it sets the root with
    a higher rank as the parent of the root with a lower rank. The following figure
    shows the result of implementing the `Union(1, 2)` and `Union(4, 5)` operations
    on *DS*:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 5.14: Merging 1,2 and 4,5](img/C14498_05_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.14: Merging 1,2 and 4,5'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'As subsequent union operations are applied, more trees merge into fewer (but
    larger) trees. The following figure shows the trees in *DS* after applying `Union(2,
    3)`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.15: Merging 2,3](img/C14498_05_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.15: Merging 2,3'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The following diagram shows the trees in *DS* after applying `Union(2, 4)`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.16: Merging 2,4](img/C14498_05_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.16: Merging 2,4'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now, let''s understand how the disjoint-set data structure helps us implement
    Kruskal''s algorithm. At the start of the algorithm, before step 1, we initialize
    a disjoint-set data structure with *N* equal to the number of vertices in our
    graph, *G*. Then, step 2 takes an edge from the min heap and step 3 checks whether
    the edge under consideration forms a cycle. Notice that this check for cycles
    can be implemented using the `union` operation on *DS*, which is applied to the
    two vertices of the edge. If the `union` operation succeeds in merging the two
    trees, then the edge is added to the MST; otherwise, the edge can safely be discarded
    as it would introduce a cycle in the MST. The following illustrated steps explain
    this logic:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we begin by initializing a disjoint-set data structure, *DS*, containing
    all of the given vertices in the graph:![Figure 5.17: Step 1 of Kruskal’s algorithm
    – initialization](img/C14498_05_17.jpg)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 5.17: Step 1 of Kruskal''s algorithm – initialization'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Let's proceed to add the edge with the lowest weight to our MST. As you can
    see from the following figure, as we add *edge (2,4)*, we also apply `Union(2,4)`
    to the elements in *DS*:![](img/C14498_05_18.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 5.18: Adding edge (2, 4) to the MST after applying Union (2, 4) to the
    disjoint-set'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As we proceed with adding edges as per the algorithm, we reach *edge (1,5)*.
    As you can see, in *DS*, the corresponding elements are in the same tree. Hence,
    we cannot add that edge. As you can see from the following graph, adding that
    would have created a cycle:![](img/C14498_05_19.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 5.19: Trying to add edge (1, 5) to MST fails because vertices 1 and
    5 are in the same tree in DS'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In the following exercise, we will implement Kruskal's minimum spanning tree
    algorithm using the disjoint-set data structure.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 26: Kruskal''s MST Algorithm'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will implement the disjoint-set data structure and Kruskal''s
    algorithm to find an MST in the graph. Let''s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Begin by adding the following headers and declaring the `Graph` data structure:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'First, we will implement the disjoint set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Add the constructor for the class and implement the `Make-set` and `Find` operations,
    as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will implement the `Union` operation between two trees in the disjoint-set,
    as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that our implementation of the disjoint set is complete, let''s start implementing
    the graph. We will use an edge-list representation. The `edge` struct is defined
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Since our implementation of an edge is templatized, the edge weights are allowed
    to be of any datatype that implements the `<` and `>` operations.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following function allows a graph to be serialized and output to streams:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The graph data structure can now be implemented with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Our implementation of the graph does not allow changing the number of vertices
    in the graph after it has been created. Also, although we can add as many edges
    as needed, the deletion of edges is not implemented since it is not needed in
    this exercise.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can implement Kruskal''s algorithm like so:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Lastly, add the driver code shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, run the program! Your output should look as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.20: Getting an MST from a given graph](img/C14498_05_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.20: Getting an MST from a given graph'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Verify that the output of our algorithm is indeed the MST that was shown in
    *figure 5.12.*
  prefs: []
  type: TYPE_NORMAL
- en: The complexity of Kruskal's algorithm without using the disjoint set is *O(E
    log E)*, where E is the number of edges in the graph. With the disjoint set, however,
    the total complexity comes down to *O(E**α**(V))*, where *α**(v)* is the inverse
    of the Ackermann function. Since the inverse Ackermann function grows much slower
    than the logarithm function, the difference in the performance of the two implementations
    is small for graphs with a few vertices but can be notably large for larger graph
    instances.
  prefs: []
  type: TYPE_NORMAL
- en: The Vertex Coloring Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The vertex coloring problem can be stated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*"Given a graph, G, assign a color to each vertex of the graph so that no two
    adjacent vertices have the same color."*'
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example, the following figure shows a valid coloring of the graph that
    was shown in *figure 5.11*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.21: Coloring an uncolored graph](img/C14498_05_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.21: Coloring an uncolored graph'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Graph coloring has applications in solving a large variety of problems in the
    real world – making schedules for taxis, solving sudoku puzzles, and creating
    timetables for exams can all be mapped to finding a valid coloring of the problem,
    modeled as a graph. However, finding the minimum number of colors required to
    produce a valid vertex coloring (also called the chromatic number) is known to
    be an NP-complete problem. Thus, a minor change in the nature of the problem can
    make a massive difference to its complexity.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example of the applications of the graph coloring problem, let''s consider
    the case of sudoku solvers. Sudoku is a number-placement puzzle where the objective
    is to fill a 9 × 9 box with numbers from 1 to 9 with no number being repeated
    in each row. Each column is a 3 × 3 block. An example of a sudoku puzzle is shown
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.22: (Left) a sudoku puzzle, (Right) its solution](img/C14498_05_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.22: (Left) a sudoku puzzle, (Right) its solution'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We can model an instance of the puzzle to the graph coloring problem as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Represent each cell in the puzzle by a vertex in graph *G*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add edges between the vertices that are in the same column, row, or are in the
    same 3 × 3 block.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A valid coloring of *G* then gives us a solution to the original sudoku puzzle.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will take a look at the implementation of graph coloring in the following
    exercise.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 27: Greedy Graph Coloring'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will implement a greedy algorithm that produces a graph
    coloring for the graph shown in *figure 5.21* when the maximum number of colors
    that can be used is six. Let''s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Begin by including the required header files and declaring the `Graph` data
    structure, which we will implement later in this exercise:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The following struct implements an edge in our graph:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The following function allows us to write the graph directly to the output
    stream:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Implement the graph as an edge list, as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The following hash map stores the list of colors that will be used by our coloring
    algorithm:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let''s implement a helper function that prints the colors that have been
    assigned to each vertex:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The following function implements our coloring algorithm:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, add the driver code, as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the implementation! Your output should look as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/C14498_05_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.23: Output of the graph coloring implementation'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Our implementation always starts coloring the vertices starting with vertex
    ID 1\. However, this choice is arbitrary, and starting the greedy coloring algorithm
    with different vertices even on the same graph is very likely to result in different
    graph colorings that require a different number of colors.
  prefs: []
  type: TYPE_NORMAL
- en: The quality of a graph coloring is usually measured by how few colors it uses
    to color the graph. While finding the optimal graph coloring that uses the least
    possible number of colors is NP-complete, greedy graph coloring often serves as
    a useful approximation. For example, when designing a compiler, graph coloring
    is used to allocate CPU registers to the variables of the program that's being
    compiled. The greedy coloring algorithm is used with a set of heuristics to arrive
    at a "good enough" solution to the problem, which is desirable in practice since
    we need compilers to be fast in order to be useful.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 12: The Welsh-Powell Algorithm'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An improvement to the simple approach of starting greedy coloring with a fixed
    vertex ID is to color the vertices in decreasing order of the number of edges
    incident on the vertices (or in decreasing order of the degree of vertices).
  prefs: []
  type: TYPE_NORMAL
- en: 'The algorithm works as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Sort all the vertices in decreasing order of degree and store them in an array.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Take the first uncolored vertex in the sorted array and assign to it the first
    color that hasn't been assigned to any of its neighbors. Let this color be *C*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Traverse the sorted array and assign the color *C* to each uncolored vertex
    that doesn't have any neighbors who have been assigned *C*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If any uncolored vertices remain in the array, go to step 2\. Else, end the
    program. The colors that have been assigned to the vertices so far is the final
    output.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following is an illustrated example of the four iterations of the algorithm
    that are required to find a valid coloring of the graph shown in *figure 5.21*:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the graph that we start with:![Figure 5.24: Starting with an uncolored
    graph](img/C14498_05_24.jpg)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 5.24: Starting with an uncolored graph'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Next, we sort by decreasing order of vertices, and start by coloring red:![Figure
    5.25: Coloring red](img/C14498_05_25.jpg)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 5.25: Coloring red'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In the next round, we start coloring blue:![](img/C14498_05_26.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 5.26: Coloring blue'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In the last round, we color green:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/C14498_05_27.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.27: Coloring green'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The high-level steps to complete this activity are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Assume that each edge of the graph holds the source vertex ID, destination vertex
    ID, and the edge weight. Implement a struct that represents an edge of the graph.
    We will use instances of this struct to create different edges in our graph representation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Implement a graph using the edge list representation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Implement a function that implements the Welsh-Powell graph coloring and returns
    a vector of colors. The color at index *i* in the vector should be the one that's
    assigned to vertex ID *i*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add the driver and input/output code as required to create the graph shown in
    *figure 5.24*. It is okay to assume that the coloring always starts with vertex
    ID *1*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Your output should look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.28: Expected output of Activity 12](img/C14498_05_28.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.28: Expected output of Activity 12'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution for this activity can be found on page 518.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The greedy approach is simple: at each iteration of the algorithm, pick the
    seemingly best alternative out of all the possible alternatives. In other words,
    greedy solutions to problems are applicable when choosing the locally ''best''
    alternative at each iteration leads to the globally optimal solution to the problem.'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we looked at examples of problems where the greedy approach
    is optimal and leads to correct solutions to the given problem; that is, shortest-job-first
    scheduling. We also discussed how slightly modified versions of NP-complete problems
    such as the 0-1 knapsack and the graph coloring problem can have simple greedy
    solutions. This makes the greedy approach an important algorithm design tool for
    difficult problems. For problems that have a greedy solution, it is likely to
    be the simplest way to solve them; and even for problems that do not have a greedy
    solution, it can often be used to solve relaxed versions of the problem that might
    be 'good enough' in practice (for example, greedy graph coloring is used while
    allocating registers to variables in programming language compilers).
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we discussed the greedy choice and optimal substructure properties and
    looked at an example of proof that a given problem exhibits these properties.
    We concluded this chapter with two solutions to the minimum spanning tree problem:
    Kruskal''s algorithm and the Welsh-Powell algorithm. Our discussion of Kruskal''s
    algorithm also introduced the disjoint-set data structure.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, we will focus on graph algorithms, starting with breadth-first
    and depth-first search, and then move on to Dijkstra''s shortest path algorithm.
    We will also look at another solution to the minimum spanning tree problem: Prim''s
    algorithm.'
  prefs: []
  type: TYPE_NORMAL
