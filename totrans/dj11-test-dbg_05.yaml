- en: 'Chapter 5. Filling in the Blanks: Integrating Django and Other Test Tools'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Previous chapters have discussed the built-in application test support that
    comes with Django 1.1\. We first learned how to use doctests to test the building
    blocks of our application, and then covered the basics of unit tests. In addition,
    we saw how functions provided by `django.test.TestCase` and `django.test.Client`
    aid in testing Django applications. Through examples, we learned how to use these
    functions to test more complete pieces of our application, such as the contents
    of pages it serves and its form handling behavior.
  prefs: []
  type: TYPE_NORMAL
- en: Django alone, however, does not provide everything one might want for test support.
    Django is, after all, a web application framework, not a test framework. It doesn't,
    for example, provide any test coverage information, which is essential for developing
    comprehensive test suites, nor does it provide any support for testing client-side
    behavior, since Django is purely a server-side framework. Other tools exist that
    fill in these gaps, but often it is desirable to integrate these other tools with
    Django rather than using several entirely different tool sets to build a full
    application test suite.
  prefs: []
  type: TYPE_NORMAL
- en: In some cases even when Django does support a function, some other tool may
    be preferred. For example, if you already have experience with a Python test framework
    such as `nose`, which provides a very flexible test discovery mechanism and a
    powerful test plugin architecture, you may find Django's test runner rather limiting.
    Similarly, if you are familiar with the `twill` web testing tool, you may find
    using Django's test `Client` cumbersome for testing form behavior in comparison
    with `twill`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will investigate integration of Django with other testing
    tools. Integration can sometimes be accomplished through the use of standard Python
    unit test extension mechanisms, but sometimes more is required. Both situations
    will be covered in this chapter. Specifically, we will:'
  prefs: []
  type: TYPE_NORMAL
- en: Discuss the issues involved in integration, and learn about the hooks Django
    provides for integrating other tools into its test structure.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Look into answering the question: How much of our code is being executed by
    our tests? We will see how we can answer this question both without making any
    changes to our Django test setup and by utilizing the hooks discussed earlier.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explore the `twill` tool, and see how to use it instead of the Django test `Client`
    in our Django application tests. For this integration, we do not need to use any
    Django hooks for integration, we simply need to use Python's unit test hooks for
    test set up and tear down.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Problems of integration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Why is integration of Django testing with other tools even an issue? Consider
    the case of wanting to use the `nose` test framework. It provides its own command,
    `nosetests`, to find and run tests in a project tree. However, attempting to run
    `nosetests`, instead of `manage.py test`, in a Django project tree quickly reveals
    a problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The problem here is that some environmental setup done by `manage.py test` is
    missing. Specifically, setting up the environment so that the appropriate settings
    are found when Django code is called, hasn't been done. This particular error
    could be fixed by setting the `DJANGO_SETTINGS_MODULE` environment variable before
    running `nosetests`, but `nosetests` would not get much farther, since there is
    more that is missing.
  prefs: []
  type: TYPE_NORMAL
- en: The next problem that would be encountered would result from tests that need
    to use the database. Creating the test database is done by support code called
    by `manage.py test` before any of the tests are run. The `nosetests` command knows
    nothing about the need for a test database, so when run under `nosetests`, Django
    test cases that require a database will fail since the database won't exist. This
    problem cannot be solved by simply setting an environment variable before running
    `nosetests`.
  prefs: []
  type: TYPE_NORMAL
- en: There are two approaches that can be taken to address integration issues like
    these. First, if the other tool provides hooks for adding functionality, they
    can be used to do things such as setting up the environment and creating the test
    database before the tests are run. This approach integrates Django tests into
    the other tool. Alternatively, hooks provided by Django can be used to integrate
    the other tool into Django testing.
  prefs: []
  type: TYPE_NORMAL
- en: The first option is outside the scope of this book, so it won't be discussed
    in any detail. However, for the particular case of `nose`, its plugin architecture
    certainly supports adding the necessary function to get Django tests running under
    `nose`. There are existing nose plugins that can be used to allow Django application
    tests to run successfully when called from `nosetests`. If this is an approach
    you want to take for your own testing, you probably want to search the web for
    existing solutions before building your own `nose` plugin to accomplish this.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second option is what we will focus on in this section: the hooks that
    Django provides to allow for pulling other functions in to the normal path of
    Django testing. There are two hooks that may be used here. First, Django allows
    specification of an alternative test runner. Details of specifying this, the responsibilities
    of the test runner, and the interface it must support will be described first.
    Second, Django allows applications to provide entirely new management commands.
    Thus, it is possible to augment `manage.py test` with another command, which might
    support different options, and does whatever is necessary to integrate another
    tool into the testing path. Details on doing this will also be discussed.'
  prefs: []
  type: TYPE_NORMAL
- en: Specifying an alternative test runner
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Django uses the `TEST_RUNNER` setting to decide what code to call in order
    to run tests. By default, the value of `TEST_RUNNER` is `''django.test.simple.run_tests''`.
    We can look at the declaration and docstring for that routine to see what interface
    it must support:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The `test_labels`, `verbosity`, and `interactive` arguments are clearly going
    to come straight from the `manage.py test` command line. The `extra_tests` argument
    is a bit mysterious, as there is no supported `manage.py test` argument that might
    correspond to that. In fact, when called from `manage.py test`, `extra_tests`
    will never be specified. This argument is used by the `runtests.py` program that
    Django uses to run its own test suite. Unless you are going to write a test runner
    that will be used to run Django's own tests, you probably don't need to worry
    about `extra_tests`. However, a custom runner should implement the defined behavior
    of including `extra_tests` among those run.
  prefs: []
  type: TYPE_NORMAL
- en: 'What exactly does a test runner need to do? This question is most easily answered
    by looking at the existing `django.test.simple.run_tests` code and seeing what
    it does. Briefly, without going through the routine line by line, it:'
  prefs: []
  type: TYPE_NORMAL
- en: Sets up the test environment by calling `django.test.utils.setup_test_environment`.
    This is a documented method that a custom test runner should call as well. It
    does things to ensure, for example, that the responses generated by the test client
    have the `context` and `templates` attributes mentioned in the previous chapter.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sets `DEBUG` to `False`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Builds a `unittest.TestSuite` containing all of the tests discovered under the
    specified `test_labels`. Django's simple test runner searches only in the `models`
    and `tests` modules for tests.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creates the test database by calling `connection.creation.create_test_db`. This
    is another routine that is documented in the Django test documentation for use
    by alternative test runners.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Runs the tests.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Destroys the test database by calling `connection.creation.destroy_test_db`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cleans up the test environment by calling `django.test.utils.teardown_test_environment`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns the sum of the test failures and errors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note that Django 1.2 adds support for a class-based approach to specifying an
    alternative test runner. While Django 1.2 continues to support the function-based
    approach used earlier and described here, using function-based alternative test
    runners will be deprecated in the future. The class-based approach simplifies
    the task of making a small change to the test running behavior. Instead of needing
    to re-implement (and often largely duplicate) the existing `django.tests.simple.run_tests`
    function, you can implement an alternative test runner class that inherits from
    the default class and simply overrides whatever specific methods are necessary
    to accomplish the desired alternative behavior.
  prefs: []
  type: TYPE_NORMAL
- en: It is reasonably straightforward, then, to write a test runner. However, in
    replacing just the test runner, we are limited by the arguments and options supported
    by the `manage.py test` command. If our runner supports some option that isn't
    supported by `manage.py test`, there is no obvious way to get that option passed
    through from the command line to our test runner. Instead, `manage.py` test will
    reject any option it doesn't know about.
  prefs: []
  type: TYPE_NORMAL
- en: There is a way to get around this. Django uses the Python `optparse` module
    to parse options from command lines. Placing a bare `–` or `–-` on the command
    line causes `optparse` to halt processing the command line, so options specified
    after a bare `–` or `–-`won't be seen by the regular Django code doing the parsing.
    They will still be accessible to our test runner in `sys.argv`, though, so they
    could be retrieved and passed on to whatever tool we are integrating with.
  prefs: []
  type: TYPE_NORMAL
- en: This method works, but the existence of such options will be well-hidden from
    users, since the standard Django help for the `test` command knows nothing of
    them. By using this technique, we extend the interface supported by `manage.py
    test` without having any way to obviously publish the extensions we have made,
    as part of the built-in help for the `test` command.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, a better alternative to specifying a custom test runner may be to supply
    an entirely new management command. When creating a new command, we can define
    it to take whatever options we like, and supply the help text that should be displayed
    for each new option when the user requests help for the command. This approach
    is discussed next.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a new management command
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Providing a new management command is simple. Django looks for management commands
    in a `management.commands` package in each installed application's directory.
    Any Python module found in an installed application's `management.commands` package
    is automatically available to specify as a command to `manage.py`.
  prefs: []
  type: TYPE_NORMAL
- en: So, to create a custom test command, say `survey_test`, for our survey application,
    we create a `management` subdirectory under survey, and a `commands` directory
    under `management`. We put `__init__.py` files in both of those directories so
    that Python will recognize them as modules. Then, we put the implementation for
    the `survey_test` command in a file named `survey_test.py`.
  prefs: []
  type: TYPE_NORMAL
- en: What would need to go in `survey_test.py`? Documentation on implementing management
    commands is scant as of Django 1.1\. All it states is that the file must define
    a class named `Command` that extends `django.core.management.base.BaseCommand`.
    Beyond that, it recommends consulting some of the existing management commands
    to see what to do. Since we are looking to provide an enhanced test command, the
    easiest thing to do is probably copy the implementation of the `test` command
    (found in `django/core/management/commands/test.py`) to our `survey_test.py` file.
  prefs: []
  type: TYPE_NORMAL
- en: 'Looking at that file, we see that a management command implementation contains
    two main parts. First, after the necessary imports and class declaration, some
    attributes are defined for the class. These control things such as what options
    it supports and what help should be displayed for the command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that while `BaseCommand` is not documented in the official Django 1.1
    documentation, it does have an extensive docstring, so the exact purpose of each
    of these attributes (`option_list`, `help`, `args`, `requires_model_validation`)
    can be found by consulting the source or using the Python shell''s help function.
    Even without checking the docstring, we can see that Python''s standard `optparse`
    module is used to build the option string, so extending `option_list` to include
    additional arguments is straightforward. For example, if we wanted to add a `–-cover`
    option to turn on generation of test coverage data, we could change the `option_list`
    specification to be:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Here we have added support for specifying `–-cover` on the command line. If
    specified, it will cause the value of the `coverage` option to be `True`. If not
    specified, this new option will default to `False`. Along with adding support
    for the option, we have the ability to add help text for it.
  prefs: []
  type: TYPE_NORMAL
- en: 'The declaration section of the `Command` implementation is followed by a `handle`
    function definition. This is the code that will be called to implement our `survey_test`
    command. The existing code from the `test` command is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, this performs a very straightforward retrieval of passed options,
    uses a utility function to find the correct test runner to call, and simply calls
    the runner with the passed options. When the runner returns, if there were any
    failures, the program exits with a system exit code set to the number of failures.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can replace the last four lines with code that retrieves our new option
    and prints out whether it has been specified:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can try running our `survey_test` command to verify that it is found
    and can accept our new option:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also verify that if we do not pass `--cover` on the command line, it
    defaults to `False`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can see that help for our option is included in the help response
    for the new command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Note that all of the other options displayed in the help message that were not
    specified in our `option_list` are inherited from `BaseCommand`. In some cases,
    (for example, the `settings` and `pythonpath` arguments) appropriate handling
    of the argument is done for us before they are called; in others (`verbosity`,
    for example) we are expected to honor the documented behavior of the option in
    our implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Adding a new management command was easy! Of course, we didn't actually implement
    running tests and generating coverage data, since we do not know any way to do
    that yet. There are existing packages that provide this support, and we will see
    in the next section how they can be used to do exactly this.
  prefs: []
  type: TYPE_NORMAL
- en: For now, we might as well delete the `survey/management` tree created here.
    It was a useful exercise to experiment with seeing how to add management commands.
    However in reality, if we were to provide a customized test command to add function
    such as recording coverage data, it would be a bad approach to tie that function
    directly to our survey application. A test command that records coverage data
    would be better implemented in an independent application.
  prefs: []
  type: TYPE_NORMAL
- en: How much of the code are we testing?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When writing tests, the goal is to test everything. Although we can try to be
    vigilant and manually ensure that we have a test for every line of our code, that's
    a very hard goal to meet without some automated analysis to verify what lines
    of code are executed by our tests. For Python code, Ned Batchelder's `coverage`
    module is an excellent tool for determining what lines of code are being executed.
    In this section, we see how to use `coverage`, first as a standalone utility and
    then integrated into our Django project.
  prefs: []
  type: TYPE_NORMAL
- en: Using coverage standalone
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before using `coverage`, it must first be installed, since it's neither included
    with Python nor Django 1.1\. If you are using Linux, your distribution package
    manager may have `coverage` available to be installed on your system. Alternatively,
    the latest version of `coverage` can always be found at its web page on the Python
    Package Index (PyPI), [http://pypi.python.org/pypi/coverage](http://pypi.python.org/pypi/coverage).
    The version of `coverage` used here is 3.2.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once installed, we can use the `coverage` command with the `run` subcommand
    to run our tests and record coverage data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: As you see, the output from the test runner looks completely normal. The coverage
    module does not affect the program's output; it simply stores the coverage data
    in a file named `.coverage`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The data stored in `.coverage` can be formatted as a report using the `report`
    subcommand of `coverage`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'That''s a bit more than we actually want. We only care about coverage of our
    own code, so for a start, everything reported for modules located under `/usr`
    is not interesting. The `--omit` option to `coverage report` can be used to omit
    modules that start with particular paths. Additionally, the `-m` option can be
    used to get `coverage` to report on the lines that were not executed (missing)
    during the run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: That's much more manageable. Not surprisingly, since we have been developing
    tests for each bit of code discussed, just about everything is showing as covered.
    What's missing? If you look at lines 5 to 8 of `manage.py`, they handle the case
    where the `import` of `settings.py` raises an `ImportError`. Since that leg of
    code is not taken for a successful run, they were not executed and come up missing
    in the coverage report.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, the lines mentioned in `model_tests` (35 to 42, 47 to 51) are from
    alternative execution paths of the `testClosesReset` method, which contains this
    code starting at line 34:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Lines 35 to 42 were not executed because the database used for this run was
    SQLite, not MySQL. Then, in any single test run, only one leg of the `if strict/elif
    debug/else` block will execute, so the other legs will show up as not covered.
    (In this particular case, the `if strict` leg was the one taken.)
  prefs: []
  type: TYPE_NORMAL
- en: The remaining line noted as missing is line 66 in `survey/models.py`. This is
    the `__unicode__` method implementation for the `Question` model, which we neglected
    to write a test for. We can put doing that on our to-do list.
  prefs: []
  type: TYPE_NORMAL
- en: Although this last one is a valid indication of a missing test, neither the
    missing lines in `manage.py` nor the missing lines in our test code are really
    things we care about, as they are not reporting missing coverage for our application
    code. (Actually, if we are thorough, we would probably want to ensure that several
    runs of our test code with different settings did result in full test code execution,
    but let's assume we are only interested in coverage of our application code for
    now.) The `coverage` module supports a couple of different ways of excluding code
    from reports. One possibility is to annotate source lines with a `# pgrama no
    cover` directive to tell `coverage` to exclude them from `coverage` consideration.
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, `coverage` provides a Python API that supports specifying regular
    expressions for code constructs that should be automatically excluded, and also
    for limiting the modules included in reports. This Python API is more powerful
    than what is available through the command line, and more convenient than manually
    annotating source with `# pragma` directives. We might, then, start looking into
    how to write some `coverage` utility scripts to easily generate coverage reports
    for the tests of our application code.
  prefs: []
  type: TYPE_NORMAL
- en: Before embarking on that task, though, we might wonder if anyone has already
    done the same and provided a ready-to-use utility that integrates `coverage` with
    the Django test support. Some searching on the Web shows that the answer is yes—there
    are several blog postings discussing the subject, and at least one project packaged
    as a Django application. Use of this package is discussed next.
  prefs: []
  type: TYPE_NORMAL
- en: Integrating coverage into a Django project
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'George Song and Mikhail Korobov provide a Django application named `django_coverage`
    that supports integrating `coverage` into testing for a Django project. Like the
    base `coverage` package, `django_coverage` can be found on PyPI: [http://pypi.python.org/pypi/django-coverage](http://pypi.python.org/pypi/django-coverage).
    The version used here is 1.0.1.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `django_coverage` package offers integration of `coverage` with Django
    using both of the methods previously discussed. First, it provides a test runner
    that can be specified in `settings.py`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Using this option, every time you run `manage.py test`, coverage information
    will be generated.
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, `django_coverage` can be included in `INSTALLED_APPS`. When this
    approach is used, the `django_coverage` application provides a new management
    command, named `test_coverage`. The `test_coverage` command can be used instead
    of `test` to run tests and generate coverage information. Since generating coverage
    information does make the tests run a bit more slowly, the second option is what
    we will use here. That way, we can choose to run tests without coverage data when
    we are interested in fast execution and not concerned with checking on coverage.
  prefs: []
  type: TYPE_NORMAL
- en: Beyond listing `django_coverage` in `INSTALLED_APPS`, nothing needs to be done
    to set up `django_coverage` to run with our project. It comes with a sample `settings.py`
    file that shows the settings it supports, all with default options and comments
    describing what they do. We can override any of the default settings provided
    in `django_coverage/settings.py` by specifying our preferred value in our own
    settings file.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start, though, by using all the default settings values provided. When
    we run `python manage.py test_coverage survey`, we will get coverage information
    displayed at the end of the test output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: That is a bit curious. Recall that the `coverage` package reported in the previous
    section that one line of code in `survey.models` was not exercised by tests—the
    `__unicode__` method of the `Question` model. This report, though, shows 100%
    coverage for `survey.models`. Looking closely at the two reports, we can see that
    the statements that count for the listed modules are all lower in the `django_coverage`
    report than they were in the `coverage` report.
  prefs: []
  type: TYPE_NORMAL
- en: This difference is due to the default value of the `COVERAGE_CODE_EXCLUDES`
    setting used by `django_coverage`. The default value of this setting causes all
    `import` lines, all `__unicode__` method definitions, and all `get_absolute_url`
    method definitions to be excluded from consideration. These default exclusions
    account for the differences seen here between the two reports. If we don't like
    this default behavior, we can supply our own alternate setting, but for now, we
    will leave it as it is.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, some modules listed by `coverage` are completely missing from the
    `django_coverage` report. These too are the result of a default setting value
    (in this case, `COVERAGE_MODULE_EXCLUDES`) and there is a message in the output
    noting which modules have been excluded due to this setting. As you can see, the
    `__init__`, `tests`, and `urls` modules inside `survey` were all automatically
    excluded from coverage consideration.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, `templates` and `fixtures` are not excluded by default, and that caused
    a problem as they are not actually Python modules, so they cannot be imported.
    To get rid of the message about problems loading these, we can specify a value
    for `COVERAGE_MODULE_EXCLUDES` in our own `settings.py` file and include these
    two. Adding them to the default list, we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: If we run the `test_coverage` command again after making this change, we will
    see that the message about problems loading some modules is gone.
  prefs: []
  type: TYPE_NORMAL
- en: 'The summary information displayed with the test output is useful, but even
    better are the HTML reports `django_coverage` can generate. To get these, we must
    specify a value for the `COVERAGE_REPORT_HTML_OUTPUT_DIR` setting, which is `None`
    by default. So, we can create a `coverage_html` directory in `/dj_projects/marketr`
    and specify it in `settings.py`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The HTML reports are not particularly interesting when code coverage comes
    in at 100 percent. Hence, to see the full usefulness of the reports, let''s run
    just a single test, say the admin test for trying to add a `Survey` with a `closes`
    date that is earlier than its `opens` date:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'This time, since we have specified a directory for HTML coverage reports, instead
    of getting the summary coverage information at the end of the test run, we see:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we can use a web browser to load the `index.html` file that has been
    placed in the `coverage_html` directory. It will look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Integrating coverage into a Django project](img/7566_05_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Since we ran just a single test, we only got partial coverage of our code.
    The **% covered** values in the HTML report are color-coded to reflect how well
    covered each module is. Green is good, yellow is fair, and red is poor. In this
    case, since we ran one of the admin tests, only **survey.admin** is colored green,
    and it is not 100 percent. To see what was missed in that module, we can click
    on the **survey.admin** link:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Integrating coverage into a Django project](img/7566_05_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Reports like this provide a very convenient way to determine the parts of our
    application code that are covered by testing and the parts that are not. Lines
    not executed are highlighted in red. Here, we only ran the test that exercises
    the error path through the `SurveyFrom clean` method, so the successful code path
    through that method comes up in red. In addition, the color coding of the `import`
    lines indicates that they were excluded. This is due to the default `COVERAGE_CODE_EXCLUDES`
    setting. Finally, the six empty lines in the file were ignored (lines with comments
    would also be ignored).
  prefs: []
  type: TYPE_NORMAL
- en: Using a tool like `coverage` is essential for ensuring that a test suite is
    doing its job. It is likely that in the future, Django will provide some integrated
    code coverage support. But in the meantime, as we have seen, it is not difficult
    to integrate `coverage` as an add-on to our projects. In the case of `django_coverage`,
    it provides options for using either of the ways of extending Django discussed
    earlier. The next integration task we will discuss requires neither, but rather
    needs only the standard Python hooks into unit test set up and tear down.
  prefs: []
  type: TYPE_NORMAL
- en: The twill web browsing and testing tool
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`twill` is a Python package that supports command-line interaction with web
    sites, primarily for testing purposes. Like the `coverage` and `django_coverage`
    packages, twill can be found on PyPI: [http://pypi.python.org/pypi/twill](http://pypi.python.org/pypi/twill).
    While `twill` offers a command-line tool for interactive use, the commands it
    provides are also available from a Python API, meaning it is possible to use `twill`
    from within a Django `TestCase`. When we do this, we essentially replace use of
    the Django test `Client` with an alternative `twill` implementation.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note that the latest official release of `twill` available on PyPI (0.9 at the
    time of this writing) is quite old. The latest development release is available
    at [http://darcs.idyll.org/~t/projects/twill-latest.tar.gz](http://darcs.idyll.org/~t/projects/twill-latest.tar.gz).
    Output from the latest development release as of January 2010 is what is shown
    in this section. The code included here was also tested with the official 0.9
    release. Everything works using the older `twill` code, but the error output from
    `twill` is slightly less helpful and there is some `twill` output that cannot
    be suppressed when running as part of a Django `TestCase`. Thus, I'd recommend
    the latest development release over the 0.9 release.
  prefs: []
  type: TYPE_NORMAL
- en: 'Why would we want to use `twill` instead of the Django test `Client`? To understand
    the motivation for using `twill` instead of the Django test `Client`, let''s revisit
    the admin customization test from the last chapter. Recall that we provided a
    custom form for adding and editing `Survey` objects. This form has a `clean` method
    that raises a `ValidationError` on any attempt to save a `Survey` with an `opens`
    date later than its `closes` date. The test to ensure that `ValidationError` is
    raised when it should be looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice that this test sends a POST to the server containing a dictionary of
    POST data without ever having issued a GET for the page. This caused a problem
    at first: recall that we did not initially include the `question_set-TOTAL_FORMS`
    and `question_set-INITIAL_FORMS` values in the POST dictionary. We were focused
    on testing the `Survey` part of the form on the page and did not realize the formset
    used by admin to display `Questions` in line with `Surveys` required these other
    values. When we found they were needed, we somewhat cavalierly set their values
    to `0` and hoped that would be acceptable for what we wanted to test.'
  prefs: []
  type: TYPE_NORMAL
- en: A better approach would have been to first `get` the survey add page. The response
    would include a form with a set of initial values that could be used as the basis
    for the dictionary to `post` back. Before issuing the `post` request, we would
    change only the values necessary for our test (`title`, `opens`, and `closes`).
    Thus, when we did issue the `post` call, any other form values that the server
    had provided initially in the form would be sent back unchanged. We would not
    have to make up additional values for parts of the form that our test did not
    intend to change.
  prefs: []
  type: TYPE_NORMAL
- en: Besides being a more realistic server interaction scenario, this approach also
    ensures that the server is responding correctly to the GET request. Testing the
    GET path isn't necessary in this particular case since the additional validation
    we added to admin doesn't affect how it responds to a GET of the page. However,
    for one of our own views that provides a form in the response, we would want to
    test the response to `get` as well as `post`.
  prefs: []
  type: TYPE_NORMAL
- en: So why didn't we write the test that way? The test `Client` supports `get` as
    well as `post`; we certainly could start off by retrieving the page containing
    the form. The problem is that the returned response is HTML, and the Django test
    `Client` doesn't provide any utility functions to parse the HTML form and turn
    it into something we can easily manipulate. There is no straightforward way for
    Django to just take the response, change a few values in the form, and `post`
    it back to the server. The `twill` package, on the other hand, makes this easy.
  prefs: []
  type: TYPE_NORMAL
- en: In the following sections, we will re-implement the `AdminSurveyTest` using
    `twill`. First, we'll see how to use its command line tool and then transfer what
    we learn into a Django `TestCase`.
  prefs: []
  type: TYPE_NORMAL
- en: Using the twill command line program
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `twill` package includes a shell script, named `twill-sh`, to allow command-line
    testing. This is a convenient way to do some initial testing and figure out what
    the test case code will need to do. From the shell program, we can use the `go`
    command to visit a page. Once we''ve visited a page, we can use the `showforms`
    command to see what forms are on the page, and what fields and initial values
    the forms contain. Since we are going to use `twill` to re-implement the `AdminSurveyTest`,
    let''s see what visiting the `Survey` add page for our test server produces:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Clearly, we didn''t actually get to the survey add page. Since we aren''t logged
    in, the server responded with a login page. We can fill the login form using the
    `formvalue` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The arguments to `formvalue` are first the form number, then the field name,
    and then the value we want to set for that field. Once we have filled the username
    and password in the form, we can `submit` the form:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Note the `submit` command optionally also accepts the name of the submit button
    to use. In the case where there is only one (as here), or if using the first submit
    button on the form is acceptable, we can simply use `submit` with no argument.
    Now that we have logged in, we can use `showforms` again to see if we have now
    really retrieved a `Survey` add page:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'That looks more like a `Survey` add page. And indeed, our setting of `question_set-TOTAL_FORMS`
    to `0` in our first test case is unrealistic, since the server actually serves
    up a form with that set to `4`. But it worked. This means that we did not have
    to manufacture values for the four inline questions, so it is not a fatal flaw.
    However, with `twill` we can take the more realistic path of leaving all those
    values as-is and just changing the fields we are interested in, again using the
    `formvalue` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'When we submit that form, we expect the server to respond with the same form
    re-displayed and the `ValidationError` message text from our custom `clean` method.
    We can verify that the text is on the returned page using the `find` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'That response to `find` may not make it immediately obvious whether it worked
    or not. Let''s see what it does with something that is most likely not on the
    page:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'OK, since `twill` clearly complains when the text is not found, the first `find`
    must have succeeded in locating the expected validation error text on the page.
    Now, we can use `showforms` again to see that indeed the server has sent back
    the form we submitted. Note that the initial values are what we submitted, not
    empty as they were when we first retrieved the page:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, we can simply adjust one of the dates in order to make the form
    valid and try submitting it again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Notice the **current page** has changed to be the survey changelist page (there
    is no longer an `add` at the end of the URL path). This is a clue that the `Survey`
    add worked this time, as the server redirects to the changelist page on a successful
    save. There is a `twill` command to display the HTML contents of a page, named
    `show`. It can be useful to see which page has been returned when you've got a
    display window you can scroll back through. However, HTML pages aren't very useful
    when reproduced on paper, so it's not shown here.
  prefs: []
  type: TYPE_NORMAL
- en: There are also many more useful commands that `twill` provides that are beyond
    the scope of what we are covering now. The discussion here is intended to simply
    give a taste of what `twill` provides and show how to use it in a Django test
    case. This second task will be covered next.
  prefs: []
  type: TYPE_NORMAL
- en: Using twill in a TestCase
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: What do we need to do to take what we've done in the `twill-sh` program and
    turn it into a `TestCase`? First, we will need to use `twill`'s Python API in
    the test code. The `twill` commands we used from within `twill-sh` are available
    in the `twill.commands` module. Additionally, `twill` provides a browser object
    (accessible via `twill.get_browser()`) that may be more appropriate to call from
    Python. The browser object version of a command may return a value, for example,
    instead of printing something on the screen. However, the browser object does
    not support all of the commands in `twill.commands` directly, thus it is common
    to use a mixture of `twill.commands` methods and browser methods. Mixing the usage
    is fine since the code in `twill.commands` internally operates on the same browser
    instance returned from `twill.get_browser()`.
  prefs: []
  type: TYPE_NORMAL
- en: Second, for test code purposes, we'd like to instruct `twill` to interact with
    our Django server application code directly, instead of sending requests to an
    actual server. It's fine when using the `twill-sh` code to test against our running
    development server, but we don't want to have a server running in order for our
    tests to pass. The Django test `Client` does this automatically since it was written
    specifically to be used from test code.
  prefs: []
  type: TYPE_NORMAL
- en: 'With `twill`, we must call its `add_wsgi_intercept` method to tell it to route
    requests for a particular host and port directly to a WSGI application instead
    of sending the requests out on the network. Django provides a class that supports
    the WSGI application interface (named `WSGIHandler`) in `django.core.handlers.wsgi`.
    Thus, in our setup code for using `twill` in tests, we can include code like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: This tells `twill` that a `WSGIHandler` instance should be used for the handling
    of any requests that are bound for the host named `twilltest` on the regular HTTP
    port, 80\. The actual hostname and port used here are not important; they must
    simply match the host name and port that our test code tries to access.
  prefs: []
  type: TYPE_NORMAL
- en: 'This brings us to the third thing we must consider in our test code. The URLs
    we use with the Django test `Client` have no hostname or port components as the
    test `Client` does not perform any routing based on that information, but it rather
    just sends the request directly to our application code. The `twill` interface,
    on the other hand, does expect host (and optionally port) components in the URLs
    passed to it. Thus, we need to build URLs that are correct for `twill` and will
    be routed appropriately by it. Since we are generally using Django''s `reverse`
    to create our URLs during testing, a utility function that takes a named URL and
    returns the result of reversing it into a form that will be handled properly by
    `twill` will come in handy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Note that since we used the default HTTP port in the `add_wsgi_intercept` call,
    we do not need to include the port number in the URLs.
  prefs: []
  type: TYPE_NORMAL
- en: One thing to note about using the `WSGIHandler` application interface for testing
    is that this interface, by default, suppresses any exceptions raised during processing
    of a request. This is the same interface that is used, for example, by the `mod_wsgi`
    module when running under Apache. It would be unacceptable in such an environment
    for `WSGIHandler` to expose exceptions to its caller, thus it catches all exceptions
    and turns them into server error (HTTP 500) responses.
  prefs: []
  type: TYPE_NORMAL
- en: Although suppressing exceptions is the correct behavior in a production environment,
    it is not very useful for testing. The server error response generated instead
    of the exception is completely unhelpful in determining where the problem originated.
    Thus, this behavior is likely to make it very hard to diagnose test failures in
    cases where the code under test raises an exception.
  prefs: []
  type: TYPE_NORMAL
- en: To fix this problem, Django has a setting, `DEBUG_PROPAGATE_EXCEPTIONS`, which
    can be set to `True` to tell the `WSGIHandler` interface to allow exceptions to
    propagate up. This setting is `False` by default and should never be set to `True`
    in a production environment. Our `twill` test setup code, however, should set
    it to `True` so that if an exception is raised during request processing, it will
    be seen when the test is run instead of being replaced by a generic server error
    response.
  prefs: []
  type: TYPE_NORMAL
- en: One final wrinkle involved with using Django's `WSGIHandler` interface for testing
    concerns maintaining a single database connection for multiple web page requests
    made by a single test. Ordinarily, each request (GET or POST of a page) uses its
    own newly-established database connection. At the end of the processing for a
    successful request, any open transaction on the database connection is committed
    and the database connection is closed.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, as noted at the end of [Chapter 4](ch04.html "Chapter 4. Getting Fancier:
    Django Unit Test Extensions"), *Getting Fancier: Django Unit Test Extensions*,
    the `TestCase` code prevents any database commits issued by the code under test
    from actually reaching the database. Thus, when testing the database will not
    see the commit normally present at the end of a request, but instead will just
    see the connection closed. Some databases, such as PostgreSQL and MySQL with the
    InnoDB storage engine, will automatically rollback the open transaction in this
    situation. This will cause problems for tests that need to issue multiple requests
    and have database updates made by earlier requests be accessible to later requests.
    For example, any test that requires a login will run into trouble since the login
    information is stored in the `django_session` database table.'
  prefs: []
  type: TYPE_NORMAL
- en: One way to fix this would be to use a `TransactionTestCase` instead of a `TestCase`
    as the base class for all of our tests that use `twill`. With a `TransactionTestCase`,
    the commit that normally happens at the end of request processing will be sent
    to the database as usual. However, the process of resetting the database to a
    clean state between each test is much slower for a `TransactionTestCase` than
    `TestCase`, so this approach could considerably slow down our tests.
  prefs: []
  type: TYPE_NORMAL
- en: An alternative solution is to prevent the closing of the database connection
    at the end of request processing. That way there is nothing to trigger the database
    to rollback any updates in the middle of a test. We can accomplish this by disconnecting
    the `close_connection` signal handler from the `request_finished` signal as part
    of the test `setUp` method. This is not a very clean solution, but it is worth
    the performance gain (and it is also what the test `Client` does to overcome the
    same problem).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start, then, by writing a `twill` version of the `setUp` method for
    the `AdminSurveyTest`. The test `Client` version from the previous chapter is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The `twill` version will need to do the same user creation steps, but something
    different for login. Instead of duplicating the user creation code, we will factor
    that out into a common base class (called `AdminTest`) for the `AdminSurveyTest`
    and the `twill` version `AdminSurveyTwillTest`. For logging into the `twill` version,
    we can fill in and submit the login form that will be returned if we attempt to
    go to any admin page before logging in. Thus, the `twill` version of `setUp` might
    look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: This `setUp` first calls the superclass `setUp` to create the admin user, and
    then saves the existing `DEBUG_PROPAGATE_EXCEPTIONS` setting before setting that
    to `True`. It then disconnects the `close_connection` signal handler from the
    `request_finished` signal. Next, it calls `twill.add_wsgi_intercept` to set up
    `twill` to route requests for the `twilltest` host to Django's `WSGIHandler`.
    For convenient access, it stashes the `twill` browser object in `self.browser`.
    It then uses the previously mentioned `reverse_for_twill` utility function to
    create the appropriate URL for the admin index page, and calls the browser `go`
    method to retrieve that page.
  prefs: []
  type: TYPE_NORMAL
- en: The returned page should have a single form containing `username` and `password`
    fields. These are set to the values for the user created by the superclass `setUp`
    using the `formvalue` command, and the form is submitted using the browser `submit`
    method. The result should be the admin index page, if the login works. That page
    will have the string `Welcome` on it, so the last thing this `setUp` routine does
    is verify that text is found on the page, so that if the login failed an error
    is raised at the point the problem was encountered rather than later.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we write `setUp`, we should also write the companion `tearDown` method
    to undo the effects of `setUp`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Here, we `go` to the admin logout page to log out from the admin site, call
    `remove_wsgi_intercept` to remove the special routing for the host named `twilltest`,
    reconnect the normal `close_connection` signal handler to the `request_finished`
    signal, and lastly restore the old value of `DEBUG_PROPAGATE_EXCEPTIONS`.
  prefs: []
  type: TYPE_NORMAL
- en: 'A `twill` version of the test case routine that checks for the error case of
    `closes` being earlier than `opens` would then be:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Unlike the test `Client` version, here we start by visiting the admin `Survey`
    add page. We expect the response to contain a single form, and set the values
    in it for `title`, `opens`, and `closes`. We don't care about anything else that
    may be in the form and leave it unchanged. We then `submit` the form.
  prefs: []
  type: TYPE_NORMAL
- en: We expect that in the error case (which this should be, given that we made `closes`
    one day before `opens`) the admin will redisplay the same page with an error message.
    We test for this by first using the `twill url` command to test that the current
    URL is still the `Survey` add page URL. We then also use the `twill find` command
    to verify that the expected error message is found on the page. (It's probably
    only necessary to perform one of those checks, but it doesn't hurt to do both.
    Hence, both are included here for illustration purposes.)
  prefs: []
  type: TYPE_NORMAL
- en: 'If we now run this test with `python manage.py test survey.AdminSurveyTwillTest`,
    we will see that it works, but `twill` is a bit chatty, even when using the Python
    API. At the end of the test output, we will see:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'We''d rather not have output from `twill` cluttering up our test output, so
    we''d like to redirect this output elsewhere. Luckily, `twill` provides a routine
    for this, `set_output`. So, we can add the following to our `setUp` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Place this prior to any `twill` commands that print output, and remember to
    include `from StringIO import StringIO` among the imports before referencing `StringIO`.
    We should also undo this in our `tearDown` routine by calling `twill.commands.reset_output()`
    there. That will restore the `twill` default behavior of sending output to the
    screen. After making those changes, if we run the test again, we will see that
    it passes, and the `twill` output is no longer present.
  prefs: []
  type: TYPE_NORMAL
- en: 'The last piece to write, then, is the test case for adding a `Survey` with
    dates that do not trigger the validation error. It might look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: This is much like the previous test except we attempt to verify that we are
    redirected to the admin changelist page on the expected successful submit. If
    we run this test, it will pass, but it is actually not correct. That is, it will
    not fail if in fact the admin re-displays the add page instead of redirecting
    to the changelist page. Thus, if we have broken something and caused submits that
    should be successful to fail, this test won't catch that.
  prefs: []
  type: TYPE_NORMAL
- en: To see this, change the `closes` date in this test case to be one day before
    `opens`. This will trigger an error as it does in the `testAddSurveyError` method.
    However, if we run the test with that change, it will still pass.
  prefs: []
  type: TYPE_NORMAL
- en: 'The reason for this is that the `twill url` command takes a regular expression
    as its argument. It isn''t checking for an exact match of the passed argument
    with the actual URL, but rather that the actual URL matches the regular expression
    passed to the `url` command. The changelist URL that we are passing into the `url`
    method is:'
  prefs: []
  type: TYPE_NORMAL
- en: '`http://twilltest/admin/survey/survey/`'
  prefs: []
  type: TYPE_NORMAL
- en: 'The URL for the add page that will be re-displayed in case of an error on submit
    will be:'
  prefs: []
  type: TYPE_NORMAL
- en: '`http://twilltest/admin/survey/survey/add/`'
  prefs: []
  type: TYPE_NORMAL
- en: 'An attempt to match the add page URL with the changelist page URL will be successful,
    since the changelist URL is contained within the add page URL. Thus, the `twill
    url` command will not raise an error as we want it to. To fix this, we must indicate
    in the regular expression we pass into `url` that we require the actual URL to
    end as the value we are passing in ends, by including an end of string marker
    on the value we pass:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'We could also include a string marker at the beginning, but that isn''t actually
    required to fix this particular problem. If we make that change and leave in the
    incorrect `closes` date setting, we will see that this test case now does fail
    as it should when the server re-displays the add page, instead of successfully
    processing the submit:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Once we verify the test does fail in the case where the server does not respond
    as we expect, we can restore the `closes` date setting to be acceptable for saving
    and again the tests will pass. One lesson here is to be careful when using the
    `url` command that `twill` provides. A second lesson is to always attempt to verify
    that a test will report failure when appropriate. When focusing on writing tests
    that pass, we can often forget to verify that tests will properly fail when they
    should.
  prefs: []
  type: TYPE_NORMAL
- en: We've now got working `twill`-based versions of our admin customization tests.
    Achieving that was not exactly easy—the need for some of the `setUp` code, for
    example, is not necessarily immediately obvious. However, once in place it can
    be easily reused by tests that require more sophisticated form manipulation than
    we needed here. Form manipulation is a weak point of Django's test framework,
    and it is unlikely that it will be addressed in Django by the addition of functions
    that would duplicate functions already available in external tools. It is more
    likely that in the future, Django will offer more easy integration with `twill`
    or another tool like it. Therefore, investing in learning how to use a tool like
    `twill` is likely a good use of time.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This brings us to the end of discussing the testing of Django applications.
    In this chapter, we focused on how to fill in any gaps of testing functions within
    Django by integrating with other test tools. It is impossible to cover the specifics
    of integrating with every tool out there, but we learned the general mechanisms
    available and discussed a couple of examples in detail. This provides a solid
    foundation for understanding how to accomplish the task in general.
  prefs: []
  type: TYPE_NORMAL
- en: 'As Django continues to develop, such gaps may become fewer, but it is unlikely
    that Django will ever be able to provide everything that everyone wants in terms
    of testing support. In some cases, Python''s class inheritance structure and unit
    test extension mechanisms allow for straightforward integration of other test
    tools into Django test cases. In other cases, this is not sufficient. Thus, it
    is helpful that Django also provides hooks for adding additional functionality.
    In this chapter, we:'
  prefs: []
  type: TYPE_NORMAL
- en: Learned what hooks Django provides for adding test functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Saw an example of how these hooks can be used, specifically in the case of adding
    code coverage reporting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Also explored an example where using these hooks was not necessary—when integrating
    the use of the `twill` test tool into our Django test cases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next chapter, we will move from testing to debugging, and begin to learn
    what facilities Django provides to aid in debugging our Django applications.
  prefs: []
  type: TYPE_NORMAL
