- en: Microservices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We are constantly looking for new ways to create software systems that cater
    for both happy customers who have applications that support their business needs
    and developers who are challenged by cutting-edge technologies. The balance of
    satisfying these two types of target user is important; it allows us to achieve
    our business goals and avoid losing skilled developers.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, as developers, we are also trying to create modules and specialized
    libraries that address specific technical or business needs. Later, we will reuse
    these modules and libraries across different projects to comply with the **don't
    repeat yourself** (**DRY**) principle.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using this introduction as a point of departure, we are going to review how
    microservices architectures can address these concerns and more. In this chapter,
    we are going to look at the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Principles of microservices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Modeling microservices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'How to implement microservices using Spring Cloud:'
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: Supporting dynamic configuration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enabling service discovery and registration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Edge services
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The circuit breaker pattern and Hystrix
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Principles of microservices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are a lot of definitions of microservices that are available on the web.
    One that comes up frequently is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '"Microservices are small and autonomous services that work well together."'
  prefs: []
  type: TYPE_NORMAL
- en: Let's start looking at this definition and what it means in a little more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Size
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The fact that the word microservices contains the word *micro* leads us to
    think that the service''s size must be really small. However, it''s almost impossible
    to define what the right size of the services should be using metrics such as
    how many lines of code or files there are, or the size of a particular deployable
    artifact. Instead, it''s much simpler to use the following idea:'
  prefs: []
  type: TYPE_NORMAL
- en: '"A service should be focused on doing one thing well."'
  prefs: []
  type: TYPE_NORMAL
- en: '- Sam Newman'
  prefs: []
  type: TYPE_NORMAL
- en: 'That *one thing* can be thought of as one business domain. If you''re building
    systems for an online store, for example, they might cover the following **business
    domains**:'
  prefs: []
  type: TYPE_NORMAL
- en: Customer management
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Product catalog
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shopping cart
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Orders
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The idea is to build one service that is able to address all the demands of
    a particular business domain. Eventually, you might also end up breaking a service
    into other microservices when the business domain becomes too big to be handled
    as just one microservice.
  prefs: []
  type: TYPE_NORMAL
- en: Autonomous
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Autonomy is really important when we are talking about microservices. A microservice
    should have the ability to change and evolve independently to the rest of the
    services around it.
  prefs: []
  type: TYPE_NORMAL
- en: The best way to verify whether a microservice is autonomous enough is by applying
    a change to it and deploying the new version of the service. The deployment process
    should not require you to modify anything other than the service itself. If you
    need to restart other services or anything else during the deployment process,
    you should consider ways of removing those additional steps. On the other hand,
    the autonomy of a service is also related to the organization of the team that
    is building it. We will discuss this in detail later on in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Working well together
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is not possible to build systems in isolation that don't interact with one
    another. Even though we are building separate services to address the requirements
    of different business domains, we eventually need to make them interact as a whole
    in order to meet the demands of the business. This interaction is carried out
    by using **application programming interfaces** (**API**).
  prefs: []
  type: TYPE_NORMAL
- en: '"An API is a set of commands, functions, protocols, and objects that programmers
    can use to create software or interact with an external system. It provides developers
    with standard commands for performing common operations so they do not have to
    write the code from scratch."'
  prefs: []
  type: TYPE_NORMAL
- en: '- API definition from https://techterms.com/definition/api'
  prefs: []
  type: TYPE_NORMAL
- en: Monolithic applications tend to carry out database integration. This is something
    that should be avoided at all costs; any required interaction between services
    should only be done using the provided service APIs.
  prefs: []
  type: TYPE_NORMAL
- en: Advantages
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Microservices offer many advantages that are worth knowing to understand how
    a company might benefit. The most common advantages are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Alignment to the single responsibility principle
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Continuous releases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Independent scalability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Increased adoption of new technology
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alignment to the single responsibility principle
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Using microservices involves creating separate components. Every component is
    designed to address a specific business domain model. Consequently, this domain
    model defines the service's single responsibility. The service should not violate
    its limits and it should request any information that falls outside of them using
    the provided APIs of other microservices. Each microservice should expose an API
    with all the required functionality to allow other microservices to obtain information
    from it.
  prefs: []
  type: TYPE_NORMAL
- en: Continuous releases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since large, monolithic applications handle many business domain models, they
    are comprised of a huge amount of source code and configuration files. This produces
    large artifacts that take a considerable amount of time to be deployed. Furthermore,
    large monolithic applications often involve large teams that are distributed around
    the world, which makes communication difficult. This becomes a problem when working
    on new features or fixing bugs in the application. Microservices are able to tackle
    this problem easily because one team will be in charge of one or more services,
    and a service is rarely written by more than one team. This means that new releases
    can be planned within the team, which allows them to roll out new versions faster
    and more frequently.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, even the smallest change in the code involves a new deployment
    of the large artifact, which makes the entire application unavailable during the
    deployment process. For microservices, however, only the service that has the
    patch for the bug or the new feature should be deployed. The deployment is fast
    and doesn't affect other services.
  prefs: []
  type: TYPE_NORMAL
- en: Independent scalability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If we need to scale a monolithic application, the entire system should be deployed
    on different servers. The servers should be really powerful to allow the application
    to perform well. Not all of the features have the same traffic, but since all
    the code is bundled as a single artifact, there is no way to scale only the desired
    features. With microservices, we have the freedom to scale only what we need.
    It's common to find cloud providers offering the chance to scale an application
    by provisioning more servers on demand or adding more resources automatically
    when they are needed.
  prefs: []
  type: TYPE_NORMAL
- en: Increased adoption of new technologies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Not all business domain models are equals, which is why different sets of technologies
    are needed. Since one microservice should only address the demands of one domain
    model, different services can adopt different technologies easily. It''s common
    to find companies using different programming languages, frameworks, cloud providers,
    and databases to code their microservices. Furthermore, we have the ability to
    experiment with new technologies for small applications, which can then be used
    elsewhere. As a consequence of embracing new technologies, companies end up with
    heterogeneous applications, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/442ed02e-81a5-4135-999c-05b281bb3aba.png)'
  prefs: []
  type: TYPE_IMG
- en: Heterogeneous applications allow us to create specialized systems to solve specific
    business demands using the right set of technologies. As a result of this, we
    end up having small artifacts that are easy to deploy and scale in isolation.
  prefs: []
  type: TYPE_NORMAL
- en: Drawbacks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Even though microservices have all the benefits that we listed earlier, it''s
    important to understand that they do have a few downsides as well. Let''s review
    these and consider how they can be dealt with:'
  prefs: []
  type: TYPE_NORMAL
- en: Too many options
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Slow at the beginning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transactions and eventual consistency
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Too many options
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since you have the opportunity to choose which technology you want to build
    a microservice with, you might feel overwhelmed because of the wide variety of
    options available. This can be solved by using just a few new technologies instead
    of trying to fit them all in at once.
  prefs: []
  type: TYPE_NORMAL
- en: Slow at the beginning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When you're in the process of adopting microservices, you have to build the
    entire ecosystem in order to make them work. You need to look for new ways to
    connect the distributed systems, secure them, and make them work as a whole. Writing
    just one application to do all this is easier. However, after a few months, the
    other microservices will reuse all the work that you put in at the beginning,
    meaning the process speeds up significantly. To take full advantage of this way
    of creating systems, it is important to try out new ways of deploying applications,
    making them scale on demand, monitoring them, and logging them. It's also important
    to review the functionality of microservices that handle the core of the business.
    These systems sometimes end up being semi-monoliths that should be split up in
    order to make them easier to manage.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Monitoring a single application is easier than monitoring many instances of
    different services. It's important to create dashboards and automated tools that
    provide metrics in order to make this task easier to accomplish. When a new error
    occurs, it can be hard to figure out where the problem is. A good log-tracing
    mechanism should be used to identify which service of the application is not working
    as expected. This means that you don't have to analyze all the services.
  prefs: []
  type: TYPE_NORMAL
- en: Transactions and eventual consistency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While large monoliths have their transaction boundaries well-defined, and because
    we often use techniques such as two-phase commits when we are writing microservices,
    we have to approach these requirements in another way.
  prefs: []
  type: TYPE_NORMAL
- en: We should remember that each microservice is the owner of its own data storage,
    and we should access their data using only their APIs. It is important to keep
    your data up to date and use compensating transactions when an operation doesn't
    work as expected. When we write monolithic applications, many operations are executed
    as a single transaction. For microservices, we need to rethink the operations
    and transactions to make them fit within each microservice boundary.
  prefs: []
  type: TYPE_NORMAL
- en: Modeling microservices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As developers, we always try to create reusable components to interact with
    systems or services in order to avoid writing code more than once. Most monolithic
    applications that we have built so far have followed a three-tier architectural
    pattern, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/01da1f47-1a59-4f7a-b61b-761ce7deb621.png)'
  prefs: []
  type: TYPE_IMG
- en: Three-tier architecture
  prefs: []
  type: TYPE_NORMAL
- en: When a change is required in an application that is built using this model,
    you often need to modify all three layers. Depending on how the application is
    created, you might need many deployments. Furthermore, since large monolithic
    applications share a lot of functionality, it's common to find more than one team
    working on them, which makes it even harder for them to evolve quickly. Sometimes,
    specialized teams work on particular layers because these layers are comprised
    of many components. In this way, changes are applied horizontally to make the
    application grow and evolve.
  prefs: []
  type: TYPE_NORMAL
- en: 'With microservices, applications evolve vertically because they are modeled
    around a specific business domain. The following diagram shows a few microservices
    for an online store application:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6e0cad3b-c090-41a7-bcdc-7a268937aeab.png)'
  prefs: []
  type: TYPE_IMG
- en: Microservices diagram
  prefs: []
  type: TYPE_NORMAL
- en: The names by themselves explain the intention and the collection of capabilities
    associated with the microservices. Just by reading the names, anyone can understand
    what they do; how the tasks are carried out and how they are implemented is irrelevant
    at this point. Since these services are built around a well-defined business domain,
    only one service should be modified when a new change is required. Since no more
    than one team should work on a microservice, making them evolve is easier in comparison
    to large monoliths. The team in charge of the service has a deep understanding
    of how that particular service works and how to make it evolve.
  prefs: []
  type: TYPE_NORMAL
- en: The team in charge of a microservice is composed of experts in that service's
    business domain, but not in the technology of the other services around it. After
    all, technology choices consist of details; the principle motivation of the service
    is the business domain.
  prefs: []
  type: TYPE_NORMAL
- en: Speeding up
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We mentioned earlier in this chapter that developing an application based on
    microservices is a time-consuming process at the beginning because you are literally
    starting from scratch. Whether you're starting a new project or splitting an existing
    legacy application into separate microservices, you have to work on all the necessary steps to
    bring an application from development to production.
  prefs: []
  type: TYPE_NORMAL
- en: Accelerating the development process
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s start at the development stage. When you''re working on old applications,
    you usually have to go through the following steps before writing the first line
    of code:'
  prefs: []
  type: TYPE_NORMAL
- en: Install the required tools in your local machine.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set up all the required dependencies.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create one or more configuration files.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Discover all the missing parts that were not listed as part of the documentation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load the test data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the application.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, let's say you're working as part of a team that owns many microservices
    that are written in different programming languages and that use different database
    technologies. Can you imagine the effort required before writing your first line
    of code?
  prefs: []
  type: TYPE_NORMAL
- en: Using microservices is supposed to be able to provide you with faster solutions,
    but all the setup required makes it slower initially. For a large monolithic application,
    you only have to set up one environment, but for heterogeneous applications, you'll
    have to set up many different environments. In order to approach this problem
    effectively, you need to embrace a culture of automation. Instead of executing
    all the aforementioned steps manually, you can run a script to do that for you.
    In this way, every time you want to work on a different project, you only need
    to execute the script instead of repeating all the steps listed.
  prefs: []
  type: TYPE_NORMAL
- en: There are some really cool tools available on the market, such as Nanobox ([https://nanobox.io](https://nanobox.io)),
    Docker Compose ([https://docs.docker.com/compose/](https://docs.docker.com/compose/)),
    and Vagrant ([https://www.vagrantup.com](https://www.vagrantup.com)) . These can
    help you by providing an environment similar to the production environment by
    running a single command.
  prefs: []
  type: TYPE_NORMAL
- en: Adopting tools such as the ones mentioned in the preceding tip will have a great
    impact on the productivity of the development team. You don't want developers
    wasting their time by providing their own environments; instead, you want them
    writing code to add new features to your product.
  prefs: []
  type: TYPE_NORMAL
- en: Embracing tests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's talk about the code-writing process. When we are working on large monoliths,
    many people need to be notified every time that a new feature or bug fix is released.
    In extreme cases, the QA team needs to check the entire environment themselves
    to ensure that the new changes didn't affect the application's existing functionality.
    Imagine how time-consuming it would be to repeat this task for every release with
    multiple microservices. For this reason, you need to adopt testing as an essential
    part of your development process.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many different levels of testing. Let''s take a look at the pyramid
    test introduced by Jason Huggins in 2005, which is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6c7dbaa5-4d0c-4e3d-a378-6efdaf5a0fcb.png)'
  prefs: []
  type: TYPE_IMG
- en: Pyramid test
  prefs: []
  type: TYPE_NORMAL
- en: The tests that are part of the pyramid's base are easy and quick to write and
    execute. Running unit tests only takes a few minutes, and is useful to validate
    that isolated pieces of code work as expected. Integration tests, on the other
    hand, are useful to validate that the code works when it's interacting with external
    services, such as databases, third-party applications, or other microservices.
    These tests will take a few tens of minutes to run. Finally, **end-to-end** (**e2e**)
    tests help you to validate that the code works as expected from an end user perspective.
    If you're writing a REST API, the e2e tests will validate the HTTP response codes
    from your API using different data. These tests are usually slow, and they change
    all the time.
  prefs: []
  type: TYPE_NORMAL
- en: Ideally, all of your new features should go through all of these tests to verify
    that your code is working as expected before going into production. The more tests
    you write, the more confidence you will gain. After all, if you've covered all
    the possible scenarios, what could go wrong? To add to this, Michael Bryzek introduced
    the idea of testing in production (see [https://www.infoq.com/podcasts/Michael-Bryzek-testing-in-production](https://www.infoq.com/podcasts/Michael-Bryzek-testing-in-production) for
    more information). This helps you to assess whether your services are working
    by executing automated tasks or bots regularly to exercise the key parts of your
    systems in production.
  prefs: []
  type: TYPE_NORMAL
- en: Going to production
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You have to automate the production environment in the same way that you automate
    the development environment. Today, it is common to find companies using cloud
    providers to deploy their systems and API-driven tools to provide servers.
  prefs: []
  type: TYPE_NORMAL
- en: Installing an OS and adding the dependencies needed to make an application work
    is something that must be automated. If you want to provide many servers, you
    just have to execute the same script several times. Technologies such as Docker,
    Puppet, and Chef can help you to do this. An indirect benefit of using code to
    provide environments is that you'll have the perfect documentation for all the
    required dependencies to make an application work. Over time, these scripts can
    be improved. They are stored in version control systems, which makes it easy to
    track every single change made to them. We will look at this further in [Chapter
    11](81e880b2-2345-4231-b7d5-d558f3c55955.xhtml), *DevOps and Release Management*.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing microservices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have a good understanding of what microservices are and what they
    are intended for, we are going to start looking at how to implement a microservice
    architecture using Spring Framework. Over the next few sections, we are going
    to look at some of the important concepts that we haven't covered so far. It's
    better to approach these from a practical viewpoint to make them easier to understand.
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic configuration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have all worked on applications that use different configuration files or
    associated metadata to allow you to specify configuration parameters that make
    an application work. When we are talking about microservices, we need to approach
    this configuration process in a different way. We should avoid configuration files
    and instead adopt the twelve-fact app configuration style (as outlined at [https://12factor.net](https://12factor.net)),
    proposed by Heroku. When we are using this configuration style, we want to externalize
    all the properties that are different in each environment and make them easy and convenient to
    create and change.
  prefs: []
  type: TYPE_NORMAL
- en: By default, Spring Boot applications can work using command-line arguments,
    JNDI names, or environment variables. Spring Boot also provides the ability to
    use a `.properties` or `.yaml` configuration file. In order to work with configuration
    variables in a safe way, Spring Boot has introduced the `@ConfigurationProperties`
    annotation, which allows you to map properties to **plain old Java objects** (**POJOs**).
    When the application is starting, it checks that all the configurations are provided,
    have the right format, and comply with the requirements demanded by the `@Valid`
    annotation. Let's take a look at how this mapping works.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s say that you have the following `application.yaml` file as part of your
    application:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s map these variables to two different POJOs using the `@ConfigurationProperties`
    annotation. Let''s start with the middleware configuration that is given:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code snippet represents the class needed for the `eventBus` configuration
    section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The `@Data` annotation from lombok has been used to avoid writing standard
    accessors methods. You can now print the `.toString()` result of these classes,
    and you will see the following output in your console:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'It can be useful to have all of these configuration variables hardcoded. This
    means that when you want to deploy the application in another environment, you
    can simply override them by providing additional parameters, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we are overriding one of the configuration variables before running the
    `.jar` file, so the output that you will get is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Even though this configuration is easy to achieve, it is not good enough for
    microservices, or any modern application in general. First of all, after applying
    any change, you need to restart the application, which is not desirable. The worst
    part is that you can't keep track of the changes that you have applied. This means
    that if an environment variable is provided, there is no way to know who provided
    it. In order to tackle this problem, Spring provides a way to centralize all of
    the configurations using the Spring Cloud Configuration server.
  prefs: []
  type: TYPE_NORMAL
- en: The server provides a centralized, journaled, and secure way to store the configuration
    values. Since it stores all the configuration values in a Git repository that
    can be local or remote, you'll have all the benefits associated with a version-control
    system for free.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a configuration server
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A Spring Cloud configuration server is built on the top of a regular Spring
    Boot application. All you need to do is add the following additional dependency:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the dependency has been added, you need to activate the configuration
    server using an additional annotation in the application, as shown in the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, you need to provide the Git repository URL, which stores the configuration
    for your microservices in the `application.yaml` file, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The preceding Git repository has separate configuration files to manage the
    configuration for each microservice. For example, the `configuration-demo.properties` file is
    used to manage the configuration for the configuration demo microservice.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a configuration client
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Configuration clients are regular Spring Boot applications. All you need to
    do is provide the server configuration URI to read the centralized configuration,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code snippet shows a REST endpoint reading a centralized configuration
    and serving the read value as its own response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot shows the configuration file stored in the Git repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f41e5b6e-ec7e-45fb-b14b-8461c1d5e3e4.png)'
  prefs: []
  type: TYPE_IMG
- en: Configuration file stored in a Git repository
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you execute a request against the preceding endpoint, it will produce
    the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Change the value of the configuration variable in the file stored in Git, as
    shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/92a82e19-3a7a-44fc-98ef-4ec55933662c.png)'
  prefs: []
  type: TYPE_IMG
- en: The configuration file with the change applied
  prefs: []
  type: TYPE_NORMAL
- en: 'If you hit the endpoint, you will retrieve the same output as before. In order
    to reload the configuration, you will need to reload the configuration variables
    by hitting the `/refresh` endpoint by using a `POST` request, as shown in the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'After reloading the configuration, the endpoint will serve a response using
    the new provided value, as you can see in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Service discovery and registration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the past, our applications lived on a single physical server where we had
    a 1:1 relation between the application and the backend implementing it. In this
    case, looking for a service is really simple: you only need to know the server
    IP address or the associated DNS name.'
  prefs: []
  type: TYPE_NORMAL
- en: Later on, applications were distributed, which means that they lived on many
    physical servers to provide high availability. In this case, we have a 1:*N* relationship
    between a service and the backend servers, where *N* can represent more than one.
    Incoming requests are managed using a load balancer to route the requests among
    the available servers.
  prefs: []
  type: TYPE_NORMAL
- en: The same approach is used when the physical servers are replaced by virtual
    machines. Load balancers need some configuration to register the new servers available
    and route the requests properly. This task used to be executed by the operations
    team.
  prefs: []
  type: TYPE_NORMAL
- en: Today, it's common to find applications deployed within containers, which we
    will discuss further in [Chapter 10](8762b4ca-6a2e-4b00-acf3-3f8f5e2f00b9.xhtml),
    *Containerizing your Applications*. Containers are constantly being provided and
    destroyed every millisecond, so registering new servers manually is an impossible
    task and must be automated. For this purpose, Netflix created a project named
    the Eureka project.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Eureka
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Eureka is a tool that allows you to discover and register servers automatically.
    You can think about it as a phone directory where all the services are registered.
    It helps to avoid establishing direct communications among servers. For example,
    let''s say you have three services and all of them are interacting with each other.
    The only way to make them work as a whole is by specifying the IP addresses and
    ports for the servers or their load balancers, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e1c9b79a-b9e3-4659-8b69-8ba590c91a23.png)'
  prefs: []
  type: TYPE_IMG
- en: Services interacting with each other
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see in the preceding image, the interaction occurs directly between
    the servers or their load balancers. When a new server is added, it should be
    registered in the load balancer either manually or with an existing automated
    mechanism. Additionally, using Eureka, you can establish a communication using
    the service names registered on it. The following diagram shows how the same interactions
    would work with Eureka:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ae639f43-2562-4a8d-ad5c-c629b24571a1.png)'
  prefs: []
  type: TYPE_IMG
- en: Services registered using Eureka
  prefs: []
  type: TYPE_NORMAL
- en: This means that when you need to establish a communication among services, you
    only need to provide the name instead of the IP address and port. Eureka will
    also work as a load balancer when more than one instance of a service is available.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a Netflix Eureka service registry
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Since Eureka was created to allow a smooth integration with Spring Boot, a
    service registry can be implemented simply by adding the following dependency:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The `application` class should be modified as well to indicate that the application
    will work as a Eureka server, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'After running the application, you can see the web console at `http://localhost:8901/`,
    as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5ea2fea4-1bdc-4882-bfc1-e16b8f311f2e.png)'
  prefs: []
  type: TYPE_IMG
- en: Eureka web console
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a service registry client
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Previously, we mentioned that a load balancer used to be used to offer high
    scalability by using more than one server as a backend. Eureka works in the same
    way, but the main benefit is that you won't need to add any configuration in the
    service registry when more instances of a server are provisioned. Instead, every
    instance should let Eureka know that it wants to be registered.
  prefs: []
  type: TYPE_NORMAL
- en: 'Registering a new service is quite simple. You just need to include the following
    dependency:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The service application class should include an additional annotation that
    will be discovered, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'To finish up, you will need to specify the Eureka server URI as part of the
    `application.properties` file, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'After running this Spring Boot application, it will be automatically registered
    in Eureka. You can verify this by refreshing the Eureka web console. You will
    see that the service is registered, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/784bdd55-5354-4bc4-9e78-1b3e24bfc6fc.png)'
  prefs: []
  type: TYPE_IMG
- en: Registered instances in Eureka
  prefs: []
  type: TYPE_NORMAL
- en: Once the services are registered, you will want to consume them. One of the
    easiest ways to consume services is by using Netflix Ribbon.
  prefs: []
  type: TYPE_NORMAL
- en: Netflix Ribbon
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ribbon is a client-side, load balancing solution that has a smooth integration
    with the Spring Cloud ecosystem. It can consume a service that is exposed using
    Eureka simply by specifying the service name. Since all the server instances are
    registered in Eureka, it will choose one of them to execute the request.
  prefs: []
  type: TYPE_NORMAL
- en: Let's say we have another service named `cinema-service`. Say that this service
    has an endpoint that can be used to query a cinema by its ID. As part of the cinema
    payload, we want to include all the movies that are available in the `movies-service`.
  prefs: []
  type: TYPE_NORMAL
- en: 'First of all, we need to add the following dependency:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, as part of the `application` class, we need to create a new `RestTemplate`
    bean that will be injected in order to consume the services available in Eureka:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The `RestTemplate` phrase is a client that is used to consume RESTful web services.
    It can execute a request against the `movies-service` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Note how the service name is specified, and we don't have to provide any other
    information, such as the IP address or the port. This is good because it would
    be impossible to determine this information when new servers are being created
    and destroyed on demand.
  prefs: []
  type: TYPE_NORMAL
- en: Edge services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'An edge service is an intermediary component that is exposed to both the outside
    world and the downstream services. It works as a gateway that allows for interaction
    between all the services around it. The following diagram shows how an edge service
    is used:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a6d4231e-c5b5-4c63-9916-65351fb64aab.png)'
  prefs: []
  type: TYPE_IMG
- en: An edge service
  prefs: []
  type: TYPE_NORMAL
- en: Note that all the incoming requests are pointing directly to the edge service,
    which will later look for the right service to redirect the request properly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Edge services are used in different ways to add additional behavior or functionalities
    according to the services around them. The most common example is a cross-origin
    resource sharing (CORS) ([https://developer.mozilla.org/en-US/docs/Web/HTTP/CORS](https://developer.mozilla.org/en-US/docs/Web/HTTP/CORS))
    filter. You can add a CORS filter to an edge service, and this would mean that
    the downstream services won''t need to implement anything. Say that we only want
    to allow incoming requests from the domain **abc.com**. We can implement this
    logic as part of the edge service, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/44c5a40f-277c-4153-9fc7-4f8b786940f9.png)'
  prefs: []
  type: TYPE_IMG
- en: A CORS filter using an edge service
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can see that all the logic is added in one place only, and that the
    downstream services don't have to implement anything to manage the required behavior.
  prefs: []
  type: TYPE_NORMAL
- en: Edge services are also used for many other requirements that we will discuss
    in the next section. Different implementations of edge services are available
    on the market. In the next section, we are going to talk about Netflix's Zuul,
    because it provides a smooth integration with Spring Cloud.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Zuul
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Zuul is an edge service created by Netflix that bases its functionality around
    filters. Zuul filters follow the interceptor filter pattern (as described at [http://www.oracle.com/technetwork/java/interceptingfilter-142169.html](http://www.oracle.com/technetwork/java/interceptingfilter-142169.html)).
    Using filters, you can perform a set of actions on HTTP requests and responses
    during their routing.
  prefs: []
  type: TYPE_NORMAL
- en: Zuul is the name of a gatekeeper that is taken from a movie (see [http://ghostbusters.wikia.com/wiki/Zuul](http://ghostbusters.wikia.com/wiki/Zuul) for
    more details), and represents exactly the functionality that this project has,
    namely that of a gatekeeper.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can apply the filter during four phases, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/17767e7f-7cd0-417a-933f-62bed7f88750.png)'
  prefs: []
  type: TYPE_IMG
- en: Zuul filters
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s review each one of these phases:'
  prefs: []
  type: TYPE_NORMAL
- en: '**pre**: Before the request is processed'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**route**: During the routing of the request to the service'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**post**: After the request has been processed'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**error**: When an error occurs during the request'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Using these phases, you can write your own filter to handle different requirements.
    Some common uses for filters during the `pre` phase are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Authentication
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Authorization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rate limits
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Translation and transformation operations in the request body
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Custom headers injection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adapters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Some common uses of filters in the `route` phase are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Canary releases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Proxying
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Once a request has been processed by the microservice, you have two scenarios:'
  prefs: []
  type: TYPE_NORMAL
- en: Succesful processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Error during the processing of the request
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If the request was successful, all the filters associated with the `post` phase
    will be executed. Some common uses of filters that are executed during this phase
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Translation and transformation operations in the response payload
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Storing of metrics associated with the business itself
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'On the other hand, when errors occur during the processing of the requests,
    then all the `error` filters will be executed. Some common uses of filters in
    this phase are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Saving the associated metadata of requests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Removing technical details from the response for security reasons
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The preceding points are just a few common uses of filters during each phase.
    Think about your own business when writing filters that target your needs.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to write a Zuul filter, the `ZuulFilter` class should be extended.
    This class has the following four abstract methods that needed to be implemented:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The two methods shown in bold are not directly declared in the `ZuulFilter`
    class, but are instead inherited from the `IZuulFilter` interface that is implemented
    by this class.
  prefs: []
  type: TYPE_NORMAL
- en: Let's review each one of these methods to understand how a Zuul filter works.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, you have the `filterType` method, where you need to specify the phase
    in which you want to execute the current filter. The valid values of this method
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`pre`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`post`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`route`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`error`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can write the preceding values by yourself, but it is better to use the
    `FilterConstant` class, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'All the phases are listed in the class that we mentioned previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The `filterOrder` method is used to define the order in which the filter will
    be executed. It's common to have more than one filter in each phase, so by using
    this method, you can configure the desired order for each filter. The highest
    value represents a low order of execution.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is easy to configure the execution order by using the `org.springframework.core.Ordered`
    interface, which has two values that can be used as references:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The `shouldFilter` method is used to determine whether the filter logic should
    be executed or not. In this method, you can access the request information using
    the `RequestContext` class, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: This method should return a boolean value that indicates whether the `run` method
    should be executed or not.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the `run` method contains the logic that's applied in the filter. In
    this method, you can also use the `RequestContext` class to perform the desired
    logic.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let''s use the endpoint implemented previously to query the movies
    screened by a cinema:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is a simple implementation to print the requested method and
    URL:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the request has been processed, you will have the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: CAP theorem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In 2000, during the **Symposium on Principles of Distributed Computing** (**SPDC**),
    Eric Brewer presented the following theory:'
  prefs: []
  type: TYPE_NORMAL
- en: '"It is impossible for a shared-data system to simultaneously provide more than
    two of the three properties (consistency, high-availability and partition tolerance)
    at the same time."'
  prefs: []
  type: TYPE_NORMAL
- en: '- Eric Brewer'
  prefs: []
  type: TYPE_NORMAL
- en: Let's review these three properties.
  prefs: []
  type: TYPE_NORMAL
- en: Consistency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A consistent system is able to report its current state in every subsequent
    operation until the state is explicitly changed by an external agent. In other
    words, every `read` operation should retrieve the data that was last written.
  prefs: []
  type: TYPE_NORMAL
- en: High availability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: High availability refers to the system's ability to always provide a valid response
    when it retrieves any request from an external agent. In a perfect world, the
    system should always be able to handle incoming requests and never produce errors.
    It should at least handle them in a way that is not perceptible to the user.
  prefs: []
  type: TYPE_NORMAL
- en: Partition tolerance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A partition tolerant distributed system should always keep operating, even when
    communication with one of its nodes cannot be established.
  prefs: []
  type: TYPE_NORMAL
- en: Brewer's theory can be applied to any distributed system in general. Since microservices
    architectures are based on the concepts of distributed computing, this means that
    this theory applies to them as well.
  prefs: []
  type: TYPE_NORMAL
- en: Even though the theory states that a system won't be able to accomplish all
    three properties at the same time, we should build systems that are able to handle
    failures gracefully. This is where the circuit breaker pattern can be applied.
  prefs: []
  type: TYPE_NORMAL
- en: Circuit breaker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The circuit breaker pattern is intended to handle failures that are created
    when a system interacts with other systems that are running in different processes
    using remote calls. The main idea behind this pattern is to wrap the call with
    an object that is able to monitor failures and produce successful responses, as
    shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/084f7ab6-ed84-41c0-871d-1d552f66326f.png)'
  prefs: []
  type: TYPE_IMG
- en: Circuit breaker pattern
  prefs: []
  type: TYPE_NORMAL
- en: Note that the circuit breaker pattern provides an alternate response once a
    connection can't be established with the targeted service. Let's look at how to
    implement this pattern and make it part of our application using Hystrix.
  prefs: []
  type: TYPE_NORMAL
- en: Hystrix
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Hystrix is a library that was created by Netflix in 2011\. It was created to
    deal with latency and connection problems when interactions with external services
    are executed. The main aim of Hystrix was to provide an alternate method to be
    executed when a communication problem occurs. It can be implemented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Note how the `getMovies` method tries to interact with another service to get
    a list of movies. The method is annotated with `@HystrixCommand(fallbackMethod
    = "emptyMoviesArray")`. The `fallbackMethod` value indicates the alternate method
    to be used as an alternative if an error occurs during communication with other
    services. In this case, the alternate method provides an array with a hard-coded
    movie. In this way, you can avoid cascade failures when interactions with external
    services are needed. This provides a better experience to end users by handling
    failures gracefully.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we looked at the principles of microservices and their advantages
    and drawbacks. After that, we learned how to model microservices and discussed
    some important concepts regarding distributed computing that are inherent to this
    architectural style. Finally, we reviewed the CAP theorem and how to handle failures gracefully during
    interaction with other services. In the next chapter, we are going to look at
    the serverless architectural style, which can also be integrated as part of your
    microservices environment.
  prefs: []
  type: TYPE_NORMAL
