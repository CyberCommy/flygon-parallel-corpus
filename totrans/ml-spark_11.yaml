- en: Real-Time Machine Learning with Spark Streaming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far in this book, we have focused on batch data processing. That is, all
    our analysis, feature extraction, and model training has been applied to a fixed
    set of data that does not change. This fits neatly into Spark's core abstraction
    of RDDs, which are immutable distributed datasets. Once created, the data underlying
    the RDD does not change, although we might create new RDDs from the original RDD
    through Spark's transformation and action operators.
  prefs: []
  type: TYPE_NORMAL
- en: Our attention has also been on batch machine learning models where we train
    a model on a fixed batch of training data that is usually represented as an RDD
    of feature vectors (and labels, in the case of supervised learning models).
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduce the concept of online learning, where models are trained and updated
    on new data as it becomes available
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explore stream processing using Spark Streaming
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See how Spark Streaming fits together with the online learning approach
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduce the concept of Structured Streaming
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The following sections use RDD as the distributed dataset. In a similar way,
    we can use DataFrame or SQL operations on streaming data.
  prefs: []
  type: TYPE_NORMAL
- en: See [https://spark.apache.org/docs/2.0.0-preview/sql-programming-guide.html](https://spark.apache.org/docs/2.0.0-preview/sql-programming-guide.html)
    for more details on DataFrame and SQL operations.
  prefs: []
  type: TYPE_NORMAL
- en: Online learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The batch machine learning methods that we have applied in this book focus on
    processing an existing fixed set of training data. Typically, these techniques
    are also iterative, and we have performed multiple passes over our training data
    in order to converge to an optimal model.
  prefs: []
  type: TYPE_NORMAL
- en: By contrast, online learning is based on performing only one sequential pass
    through the training data in a fully incremental fashion (that is, one training
    example at a time). After seeing each training example, the model makes a prediction
    for this example and then receives the true outcome (for example, the label for
    classification or real target for regression). The idea behind online learning
    is that the model continually updates as new information is received, instead
    of being retrained periodically in batch training.
  prefs: []
  type: TYPE_NORMAL
- en: In some settings, when the data volume is very large or the process that generates
    the data is changing rapidly, online learning methods can adapt more quickly and
    in near real time, without needing to be retrained in an expensive batch process.
  prefs: []
  type: TYPE_NORMAL
- en: However, online learning methods do not have to be used in a purely online manner.
    In fact, we have already seen an example of using an online learning model in
    the batch setting when we used **Stochastic gradient descent** (**SGD**) optimization
    to train our classification and regression models. SGD updates the model after
    each training example. However, we still made use of multiple passes over the
    training data in order to converge to a better result.
  prefs: []
  type: TYPE_NORMAL
- en: In the pure online setting, we do not (or perhaps cannot) make multiple passes
    over the training data; hence, we need to process each input as it arrives. Online
    methods also include mini-batch methods where, instead of processing one input
    at a time, we process a small batch of training data.
  prefs: []
  type: TYPE_NORMAL
- en: Online and batch methods can also be combined in real-world situations. For
    example, we can periodically retrain our models offline (say, every day) using
    batch methods. We can then deploy the trained model to production and update it
    using online methods in real time (that is, during the day, in between batch retraining)
    to adapt to any changes in the environment. This is very similar to lambda architecture
    which is a data processing architecture supporting both batch and streaming methods.
  prefs: []
  type: TYPE_NORMAL
- en: As we will see in this chapter, the online learning setting can fit neatly into
    stream processing and the Spark Streaming framework.
  prefs: []
  type: TYPE_NORMAL
- en: See [http://en.wikipedia.org/wiki/Online_machine_learning](http://en.wikipedia.org/wiki/Online_machine_learning)
    for more details on online machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: Stream processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before covering online learning with Spark, we will first explore the basics
    of stream processing and introduce the Spark Streaming library.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the core Spark API and functionality, the Spark project contains
    another major library (in the same way as MLlib is a major project library) called
    **Spark Streaming**, which focuses on processing data streams in real time.
  prefs: []
  type: TYPE_NORMAL
- en: A data stream is a continuous sequence of records. Common examples include activity
    stream data from a web or mobile application, time-stamped log data, transactional
    data, and event streams from sensor or device networks.
  prefs: []
  type: TYPE_NORMAL
- en: The batch processing approach typically involves saving the data stream to an
    intermediate storage system (for example, HDFS or a database) and running a batch
    process on the saved data. In order to generate up-to-date results, the batch
    process must be run periodically (for example, daily, hourly, or even every few
    minutes) on the latest data available.
  prefs: []
  type: TYPE_NORMAL
- en: By contrast, the stream-based approach applies processing to the data stream
    as it is generated. This allows near real-time processing (of the order of a subsecond
    to a few tenths of a second time-frames rather than minutes, hours, days, or even
    weeks with typical batch processing).
  prefs: []
  type: TYPE_NORMAL
- en: An introduction to Spark Streaming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are a few different general techniques to deal with stream processing.
    Two of the most common ones are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Treat each record individually and process it as soon as it is seen.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Combine multiple records into **mini-batches**. These mini-batches can be delineated
    either by time or by the number of records in a batch.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Spark Streaming takes the second approach. The core primitive in Spark Streaming
    is the **discretized stream**, or **DStream**. A DStream is a sequence of mini-batches,
    where each mini-batch is represented as a Spark RDD:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_11_001.png)'
  prefs: []
  type: TYPE_IMG
- en: The discretized stream abstraction
  prefs: []
  type: TYPE_NORMAL
- en: A DStream is defined by its input source and a time window called the **batch
    interval**. The stream is broken up into time periods equal to the batch interval
    (beginning from the starting time of the application). Each RDD in the stream
    will contain the records that are received by the Spark Streaming application
    during a given batch interval. If no data is present in a given interval, the
    RDD will simply be empty.
  prefs: []
  type: TYPE_NORMAL
- en: Input sources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spark Streaming **receivers** are responsible for receiving data from an **input
    source** and converting the raw data into a DStream made up of Spark RDDs.
  prefs: []
  type: TYPE_NORMAL
- en: Spark Streaming supports various input sources, including file-based sources
    (where the receiver watches for new files arriving at the input location and creates
    the DStream from the contents read from each new file) and network-based sources
    (such as receivers that communicate with socket-based sources, the Twitter API
    stream, Akka actors, or message queues and distributed stream and log transfer
    frameworks, such as Flume, Kafka, and Amazon Kinesis).
  prefs: []
  type: TYPE_NORMAL
- en: See the documentation on input sources at [http://spark.apache.org/docs/latest/streaming-programming-guide.html#input-dstreams](http://spark.apache.org/docs/latest/streaming-programming-guide.html#input-dstreams)
    for more details and for links to various advanced sources.
  prefs: []
  type: TYPE_NORMAL
- en: Transformations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we saw in [Chapter 1](c3a4e28e-a42f-430d-a556-444eb126b339.xhtml), *Getting
    Up and Running with Spark*, and throughout this book, Spark allows us to apply
    powerful transformations to RDDs. As DStreams are made up of RDDs, Spark Streaming
    provides a set of transformations available on DStreams; these transformations
    are similar to those available on RDDs. These include `map`, `flatMap`, `filter`,
    `join`, and `reduceByKey`.
  prefs: []
  type: TYPE_NORMAL
- en: Spark Streaming transformations, such as those applicable to RDDs, operate on
    each element of a DStream's underlying data. That is, the transformations are
    effectively applied to each RDD in the DStream, which, in turn, applies the transformation
    to the elements of the RDD.
  prefs: []
  type: TYPE_NORMAL
- en: Spark Streaming also provides operators such as reduce and count. These operators
    return a DStream made up of a single element (for example, the count value for
    each batch). Unlike the equivalent operators on RDDs, these do not trigger computation
    on DStreams directly. That is, they are not actions, but they are still transformations,
    as they return another DStream.
  prefs: []
  type: TYPE_NORMAL
- en: Keeping track of state
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we were dealing with batch processing of RDDs, keeping and updating a state
    variable was relatively straightforward. We could start with a certain state (for
    example, a count or sum of values) and then use broadcast variables or accumulators
    to update this state in parallel. Usually, we would then use an RDD action to
    collect the updated state to the driver and, in turn, update the global state.
  prefs: []
  type: TYPE_NORMAL
- en: With DStreams, this is a little more complex, as we need to keep track of states
    across batches in a fault-tolerant manner. Conveniently, Spark Streaming provides
    the `updateStateByKey` function on a DStream of key-value pairs, which takes care
    of this for us, allowing us to create a stream of arbitrary state information
    and update it with each batch of data seen. For example, the state could be a
    global count of the number of times each key has been seen. The state could, thus,
    represent the number of visits per web page, clicks per advert, tweets per user,
    or purchases per product, for example.
  prefs: []
  type: TYPE_NORMAL
- en: General transformations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Spark Streaming API also exposes a general `transform` function that gives
    us access to the underlying RDD for each batch in the stream. That is, where the
    higher-level functions such as `map` transform a DStream to another DStream, `transform`
    allows us to apply functions from an RDD to another RDD. For example, we can use
    the RDD `join` operator to join each batch of the stream to an existing RDD that
    we computed separately from our streaming application (perhaps, in Spark or some
    other system).
  prefs: []
  type: TYPE_NORMAL
- en: The full list of transformations and further information on each of them is
    provided in the Spark documentation at [http://spark.apache.org/docs/latest/streaming-programming-guide.html#transformations-on-dstreams](http://spark.apache.org/docs/latest/streaming-programming-guide.html#transformations-on-dstreams).
  prefs: []
  type: TYPE_NORMAL
- en: Actions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While some of the operators we have seen in Spark Streaming, such as count,
    are not actions as in the batch RDD case, Spark Streaming has the concept of actions
    on DStreams. Actions are output operators that, when invoked, trigger computation
    on the DStream. They are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`print`: This prints the first 10 elements of each batch to the console and
    is typically used for debugging and testing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`saveAsObjectFile`, `saveAsTextFiles`, and `saveAsHadoopFiles`: These functions
    output each batch to a Hadoop-compatible filesystem with a filename (if applicable)
    derived from the batch start timestamp.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`forEachRDD`: This operator is the most generic and allows us to apply any
    arbitrary processing to the RDDs within each batch of a DStream. It is used to
    apply side effects, such as saving data to an external system, printing it for
    testing, exporting it to a dashboard, and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that, like batch processing with Spark, DStream operators are **lazy**.
    In the same way in which we need to call an action, such as `count`, on an RDD
    to ensure that processing takes place, we need to call one of the preceding action
    operators in order to trigger computation on a DStream. Otherwise, our streaming
    application will not actually perform any computation.
  prefs: []
  type: TYPE_NORMAL
- en: Window operators
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As Spark Streaming operates on time-ordered batched streams of data, it introduces
    a new concept, which is that of **windowing**. A `window` function computes a
    transformation over a sliding window applied to the stream.
  prefs: []
  type: TYPE_NORMAL
- en: A window is defined by the length of the window and the sliding interval. For
    example, with a 10-second window and a 5-second sliding interval, we will compute
    results every 5 seconds, based on the latest 10 seconds of data in the DStream.
    For example, we might wish to calculate the top websites by page view numbers
    over the last 10 seconds and recompute this metric every 5 seconds using a sliding
    window.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure illustrates a windowed DStream:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_11_002.png)'
  prefs: []
  type: TYPE_IMG
- en: A windowed DStream
  prefs: []
  type: TYPE_NORMAL
- en: Caching and fault tolerance with Spark Streaming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Like Spark RDDs, DStreams can be cached in memory. The use cases for caching
    are similar to those for RDDs-if we expect to access the data in a DStream multiple
    times (perhaps performing multiple types of analysis or aggregation or outputting
    to multiple external systems), we will benefit from caching the data. Stateful
    operators, which include `window` functions and `updateStateByKey`, do this automatically
    for efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: Recall that RDDs are immutable datasets and are defined by their input data
    source and **lineage**-that is, the set of transformations and actions that are
    applied to the RDD. Fault tolerance in RDDs works by recreating the RDD (or partition
    of an RDD) that is lost due to the failure of a worker node.
  prefs: []
  type: TYPE_NORMAL
- en: As DStreams are themselves batches of RDDs, they can also be recomputed as required
    to deal with worker node failure. However, this depends on the input data still
    being available. If the data source itself is fault-tolerant and persistent (such
    as HDFS or some other fault-tolerant data store), then the DStream can be recomputed.
  prefs: []
  type: TYPE_NORMAL
- en: If data stream sources are delivered over a network (which is a common case
    with stream processing), Spark Streaming's default persistence behavior is to
    replicate data to two worker nodes. This allows network DStreams to be recomputed
    in the case of failure. Note, however, that any data received by a node but *not
    yet replicated* might be lost when a node fails.
  prefs: []
  type: TYPE_NORMAL
- en: Spark Streaming also supports recovery of the driver node in the event of failure.
    However, currently, for network-based sources, data in the memory of worker nodes
    will be lost in this case. Hence, Spark Streaming is not fully fault-tolerant
    in the face of failure of the driver node or application. Instead lambda architecture
    can be used here. For example, nightly batch can come through and correct things
    in the case of a failure.
  prefs: []
  type: TYPE_NORMAL
- en: See [http://spark.apache.org/docs/latest/streaming-programming-guide.html#caching-persistence](http://spark.apache.org/docs/latest/streaming-programming-guide.html#caching-persistence)
    and [http://spark.apache.org/docs/latest/streaming-programming-guide.html#fault-tolerance-properties](http://spark.apache.org/docs/latest/streaming-programming-guide.html#fault-tolerance-properties)
    for more details.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a basic streaming application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will now work through creating our first Spark Streaming application to illustrate
    some of the basic concepts around Spark Streaming that we introduced earlier.
  prefs: []
  type: TYPE_NORMAL
- en: We will expand on the example applications used in [Chapter 1](c3a4e28e-a42f-430d-a556-444eb126b339.xhtml),
    *Getting Up and Running with Spark*, where we used a small example dataset of
    product purchase events. For this example, instead of using a static set of data,
    we will create a simple producer application that will randomly generate events
    and send them over a network connection. We will then create a few Spark Streaming
    consumer applications that will process this event stream.
  prefs: []
  type: TYPE_NORMAL
- en: The sample project for this chapter contains the code you will need. It is called
    `scala-spark-streaming-app`. It consists of a Scala SBT project definition file,
    the example application source code, and a `srcmainresources` directory that contains
    a file called `names.csv`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `build.sbt` file for the project contains the following project definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Note that we added a dependency on Spark MLlib and Spark Streaming, which includes
    the dependency on the Spark core.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `names.csv` file contains a set of 20 randomly generated user names. We
    will use these names as part of our data generation function in our producer application:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The producer application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Our producer needs to create a network connection and generate some random
    purchase event data to send over this connection. First, we will define our object
    and main method definition. We will then read the random names from the `names.csv`
    resource and create a set of products with prices, from which we will generate
    our random product events:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the list of names and map of product name to price, we will create a
    function that will randomly pick a product and name from these sources, generating
    a specified number of product events:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we will create a network socket and set our producer to listen on
    this socket. As soon as a connection is made (which will come from our consumer
    streaming application), the producer will start generating random events at a
    random rate between 0 and 5 per second:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This producer example is based on the `PageViewGenerator` example in the Spark
    Streaming examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'The producer can be run by changing into the base directory of `scala-spark-streaming-app`
    and using SBT to run the application, as we did in [Chapter 1](c3a4e28e-a42f-430d-a556-444eb126b339.xhtml),
    *Getting Up and Running with Spark*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the `run` command to execute the application:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see output similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Select the `StreamingProducer` option. The application will start running,
    and you should see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: We can see that the producer is listening on port `9999`, waiting for our consumer
    application to connect.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a basic streaming application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Next, we will create our first streaming program. We will simply connect to
    the producer and print out the contents of each batch. Our streaming code looks
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: It looks fairly simple, and it is mostly due to the fact that Spark Streaming
    takes care of all the complexity for us. First, we initialized a `StreamingContext`
    (which is the streaming equivalent of the `SparkContext` we have used so far),
    specifying similar configuration options that are used to create a `SparkContext`.
    Notice, however, that here we are required to provide the batch interval, which
    we set to 10 seconds.
  prefs: []
  type: TYPE_NORMAL
- en: We then created our data stream using a predefined streaming source, `socketTextStream`,
    which reads text from a socket host and port and creates a `DStream[String]`.
    We then called the `print` function on the DStream; this function prints out the
    first few elements of each batch.
  prefs: []
  type: TYPE_NORMAL
- en: Calling `print` on a DStream is similar to calling `take` on an RDD. It displays
    only the first few elements.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can run this program using SBT. Open a second terminal window, leaving the
    producer program running, and run `sbt`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Again, you should see a few options to select:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the `SimpleStreamingApp` main class. You should see the streaming program
    start up, displaying output similar to the one shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'At the same time, you should see that the terminal window running the producer
    displays something like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'After about 10 seconds, which is the time of our streaming batch interval,
    Spark Streaming will trigger a computation on the stream due to our use of the
    `print` operator. This should display the first few events in the batch, which
    will look something like the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Note that you might see different results, as the producer generates a random
    number of random events each second.
  prefs: []
  type: TYPE_NORMAL
- en: You can terminate the streaming app by pressing *Ctrl* + *C*. If you want to,
    you can also terminate the producer (if you do, you will need to restart it again
    before starting the next streaming programs that we will create).
  prefs: []
  type: TYPE_NORMAL
- en: Streaming analytics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Next, we will create a slightly more complex streaming program. In [Chapter
    1](c3a4e28e-a42f-430d-a556-444eb126b339.xhtml), *Getting Up and Running with Spark*,
    we calculated a few metrics on our dataset of product purchases. These included
    the total number of purchases, the number of unique users, the total revenue,
    and the most popular product (together with its number of purchases and total
    revenue).
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we will compute the same metrics on our stream of purchase
    events. The key difference is that these metrics will be computed per batch and
    printed out.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will define our streaming application code here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: First, we created exactly the same `StreamingContext` and socket stream as we
    did earlier. Our next step is to apply a `map` transformation to the raw text,
    where each record is a comma-separated string representing the purchase event.
    The `map` function splits the text and creates a tuple of `(user, product, price)`.
    This illustrates the use of `map` on a DStream and how it is the same as if we
    had been operating on an RDD.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will use `foreachRDD` to apply arbitrary processing on each RDD in
    the stream to compute our desired metrics and print them to the console:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: If you compare the code operating on the RDDs inside the preceding `foreachRDD`
    block with that used in [Chapter 1](c3a4e28e-a42f-430d-a556-444eb126b339.xhtml),
    *Getting Up and Running with Spark*, you will notice that it is virtually the
    same code. This shows that we can apply any RDD-related processing we wish within
    the streaming setting by operating on the underlying RDDs, as well as using the
    built-in higher level streaming operations.
  prefs: []
  type: TYPE_NORMAL
- en: Let's run the streaming program again by calling `sbt run` and selecting `StreamingAnalyticsApp`.
  prefs: []
  type: TYPE_NORMAL
- en: Remember that you might also need to restart the producer if you previously
    terminated the program. This should be done before starting the streaming application.
  prefs: []
  type: TYPE_NORMAL
- en: 'After about 10 seconds, you should see output from the streaming program similar
    to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: You can again terminate the streaming program using *Ctrl* + *C*.
  prefs: []
  type: TYPE_NORMAL
- en: Stateful streaming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As a final example, we will apply the concept of **stateful** streaming using
    the `updateStateByKey` function to compute a global state of revenue and number
    of purchases per user, which will be updated with new data from each 10-second
    batch. Our `StreamingStateApp` app is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: We will first define an `updateState` function that will compute the new state
    from the running state value and the new data in the current batch. Our state,
    in this case, is a tuple of `(number of products, revenue)` pairs, which we will
    keep for each user. We will compute the new state given the set of `(product,
    revenue)` pairs for the current batch and the accumulated state at the current
    time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice that we will deal with an `Option` value for the current state, as it
    might be empty (which will be the case for the first batch), and we need to define
    a default value, which we will do using `getOrElse` as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: After applying the same string split transformation we used in our previous
    example, we called `updateStateByKey` on our DStream, passing in our defined `updateState`
    function. We then printed the results to the console.
  prefs: []
  type: TYPE_NORMAL
- en: Start the streaming example using `sbt run` and by selecting `[4] StreamingStateApp`
    (also restart the producer program if necessary).
  prefs: []
  type: TYPE_NORMAL
- en: 'After around 10 seconds, you will start to see the first set of state outputs.
    We will wait another 10 seconds to see the next set of outputs. You will see the
    overall global state being updated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: We can see that the number of purchases and revenue totals for each user are
    added to with each batch of data.
  prefs: []
  type: TYPE_NORMAL
- en: Now, see if you can adapt this example to use Spark Streaming's `window` functions.
    For example, you can compute similar statistics per user over the past minute,
    sliding every 30 seconds.
  prefs: []
  type: TYPE_NORMAL
- en: Online learning with Spark Streaming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we have seen, Spark Streaming makes it easy to work with data streams in
    a way that should be familiar to us from working with RDDs. Using Spark's stream
    processing primitives combined with the online learning capabilities of ML Library
    SGD-based methods, we can create real-time machine learning models that we can
    update on new data in the stream as it arrives.
  prefs: []
  type: TYPE_NORMAL
- en: Streaming regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spark provides a built-in streaming machine learning model in the `StreamingLinearAlgorithm`
    class. Currently, only a linear regression implementation is available-`StreamingLinearRegressionWithSGD`-but
    future versions will include classification.
  prefs: []
  type: TYPE_NORMAL
- en: 'The streaming regression model provides two methods for usage:'
  prefs: []
  type: TYPE_NORMAL
- en: '`trainOn`: This takes `DStream[LabeledPoint]` as its argument. This tells the
    model to train on every batch in the input DStream. It can be called multiple
    times to train on different streams.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`predictOn`: This also takes `DStream[LabeledPoint]`. This tells the model
    to make predictions on the input DStream, returning a new `DStream[Double]` that
    contains the model predictions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Under the hood, the streaming regression model uses `foreachRDD` and `map` to
    accomplish this. It also updates the model variable after each batch and exposes
    the latest trained model, which allows us to use this model in other applications
    or save it to an external location.
  prefs: []
  type: TYPE_NORMAL
- en: The streaming regression model can be configured with parameters for step size
    and number of iterations in the same way as standard batch regression-the model
    class used is the same. We can also set the initial model weight vector.
  prefs: []
  type: TYPE_NORMAL
- en: When we first start training a model, we can set the initial weights to a zero
    vector, or a random vector, or perhaps load the latest model from the result of
    an offline batch process. We can also decide to save the model periodically to
    an external system and use the latest model state as the starting point (for example,
    in the case of a restart after a node or application failure).
  prefs: []
  type: TYPE_NORMAL
- en: A simple streaming regression program
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To illustrate the use of streaming regression, we will create a simple example
    similar to the preceding one, which uses simulated data. We will write a producer
    program that generates random feature vectors and target variables, given a fixed,
    known weight vector, and writes each training example to a network stream.
  prefs: []
  type: TYPE_NORMAL
- en: Our consumer application will run a streaming regression model, training and
    then testing on our simulated data stream. Our first example consumer will simply
    print its predictions to the console.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a streaming data producer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The data producer operates in a manner similar to our product event producer
    example. Recall from [Chapter 5](d3bf76a8-26be-4db7-8310-b936d220407e.xhtml),
    *Building a Recommendation Engine with Spark*, that a linear model is a linear
    combination (or vector dot product) of a weight vector, *w*, and a feature vector,
    *x* (that is, *wTx*). Our producer will generate synthetic data using a fixed,
    known weight vector and randomly generated feature vectors. This data fits the
    linear model formulation exactly, so we will expect our regression model to learn
    the true weight vector fairly easily.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will set up a maximum number of events per second (say, 100) and
    the number of features in our feature vector (also 100 in this example):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The `generateRandomArray` function creates an array of the specified size where
    the entries are randomly generated from a normal distribution. We will use this
    function initially to generate our known weight vector, `w`, which will be fixed
    throughout the life of the producer. We will also create a random `intercept`
    value that will also be fixed. The weight vector and `intercept` will be used
    to generate each data point in our stream:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'We will also need a function to generate a specified number of random data
    points. Each event is made up of a random feature vector and the target that we
    get from computing the dot product of our known weight vector with the random
    feature vector and adding the `intercept` value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we will use code similar to our previous producer to instantiate a
    network connection and send a random number of data points (between 0 and 100)
    in text format over the network each second:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'You can start the producer using `sbt run`, followed by choosing to execute
    the `StreamingModelProducer` main method. This should result in the following
    output, thus indicating that the producer program is waiting for connections from
    our streaming regression application:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Creating a streaming regression model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the next step in our example, we will create a streaming regression program.
    The basic layout and setup is the same as our previous streaming analytics examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we will set up the number of features to match the records in our input
    data stream. We will then create a zero vector to use as the initial weight vector
    of our streaming regression model. Finally, we will select the number of iterations
    and step size:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will again use the `map` function to transform the input DStream,
    where each record is a string representation of our input data, into a `LabeledPoint`
    instance that contains the target value and feature vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The final step is to tell the model to train and test on our transformed DStream
    and also to print out the first few elements of each batch in the DStream of predicted
    values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Note that because we are using the same MLlib model classes for streaming as
    we did for batch processing, we can, if we choose, perform multiple iterations
    over the training data in each batch (which is just an RDD of `LabeledPoint` instances).
  prefs: []
  type: TYPE_NORMAL
- en: Here, we will set the number of iterations to `1` to simulate purely online
    learning. In practice, you can set the number of iterations higher, but note that
    the training time per batch will go up. If the training time per batch is much
    higher than the batch interval, the streaming model will start to lag behind the
    velocity of the data stream.
  prefs: []
  type: TYPE_NORMAL
- en: This can be handled by decreasing the number of iterations, increasing the batch
    interval, or increasing the parallelism of our streaming program by adding more
    Spark workers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we''re ready to run `SimpleStreamingModel` in our second terminal window
    using `sbt run` in the same way as we did for the producer (remember to select
    the correct main method for SBT to execute). Once the streaming program starts
    running, you should see the following output in the producer console:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'After about 10 seconds, you should start seeing the model predictions being
    printed to the streaming application console, similar to those shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Congratulations! You've created your first streaming online learning model!
  prefs: []
  type: TYPE_NORMAL
- en: You can shut down the streaming application (and, optionally, the producer)
    by pressing *Ctrl* + *C* in each terminal window.
  prefs: []
  type: TYPE_NORMAL
- en: Streaming K-means
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: MLlib also includes a streaming version of K-means clustering; this is called
    `StreamingKMeans`. This model is an extension of the mini-batch K-means algorithm
    where the model is updated with each batch based on a combination between the
    cluster centers computed from the previous batches and the cluster centers computed
    for the current batch.
  prefs: []
  type: TYPE_NORMAL
- en: '`StreamingKMeans` supports a *forgetfulness* parameter *alpha* (set using the
    `setDecayFactor` method); this controls how aggressive the model is in giving
    weight to newer data. An alpha value of 0 means the model will only use new data,
    while with an alpha value of `1`, all data since the beginning of the streaming
    application will be used.'
  prefs: []
  type: TYPE_NORMAL
- en: We will not cover streaming K-means further here (the Spark documentation at
    [http://spark.apache.org/docs/latest/mllib-clustering.html#streaming-clustering](http://spark.apache.org/docs/latest/mllib-clustering.html#streaming-clustering)
    contains further detail and an example). However, perhaps you could try to adapt
    the preceding streaming regression data producer to generate input data for a
    `StreamingKMeans` model. You could also adapt the streaming regression application
    to use `StreamingKMeans`.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can create the clustering data producer by first selecting a number of
    clusters, *K*, and then generating each data point by:'
  prefs: []
  type: TYPE_NORMAL
- en: Randomly selecting a cluster index.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating a random vector using specific normal distribution parameters for
    each cluster. That is, each of the *K* clusters will have a mean and variance
    parameter, from which the random vectors will be generated using an approach similar
    to our preceding `generateRandomArray` function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this way, each data point that belongs to the same cluster will be drawn
    from the same distribution, so our streaming clustering model should be able to
    learn the correct cluster centers over time.
  prefs: []
  type: TYPE_NORMAL
- en: Online model evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Combining machine learning with Spark Streaming has many potential applications
    and use cases, including keeping a model or set of models up to date on new training
    data as it arrives, thus enabling them to adapt quickly to changing situations
    or contexts.
  prefs: []
  type: TYPE_NORMAL
- en: Another useful application is to track and compare the performance of multiple
    models in an online manner and, possibly, also perform model selection in real
    time so that the best performing model is always used to generate predictions
    on live data.
  prefs: []
  type: TYPE_NORMAL
- en: This can be used to do real-time "A/B testing" of models, or combined with more
    advanced online selection and learning techniques, such as Bayesian update approaches
    and bandit algorithms. It can also be used simply to monitor model performance
    in real time, thus being able to respond or adapt if performance degrades for
    some reason.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will walk through a simple extension to our streaming regression
    example. In this example, we will compare the evolving error rate of two models
    with different parameters as they see more and more data in our input stream.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing model performance with Spark Streaming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we have used a known weight vector and intercept to generate the training
    data in our producer application, we would expect our model to eventually learn
    this underlying weight vector (in the absence of random noise, which we do not
    add for this example).
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, we should see the model's error rate decrease over time, as it sees
    more and more data. We can also use standard regression error metrics to compare
    the performance of multiple models.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we will create two models with different learning rates, training
    them both on the same data stream. We will then make predictions for each model
    and measure the **mean-squared error** (**MSE**) and **root mean-squared error**
    (**RMSE**) metrics for each batch.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our new monitored streaming model code is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that most of the preceding setup code is the same as our simple streaming
    model example. However, we created two instances of `StreamingLinearRegressionWithSGD`:
    one with a learning rate of `0.01` and one with the learning rate set to `1.0`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will train each model on our input stream, and using Spark Streaming''s
    `transform` function, we will create a new DStream that contains the error rates
    for each model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we will use `foreachRDD` to compute the MSE and RMSE metrics for each
    model and print them to the console:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: If you terminated the producer earlier, start it again by executing `sbt run`
    and selecting `StreamingModelProducer`. Once the producer is running again, in
    your second terminal window, execute `sbt run` and choose the main class for `MonitoringStreamingModel`.
  prefs: []
  type: TYPE_NORMAL
- en: 'You should see the streaming program startup, and after about 10 seconds, the
    first batch will be processed, printing output similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Since both models start with the same initial weight vector, we see that they
    both make the same predictions on this first batch and, therefore, have the same
    error.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we leave the streaming program running for a few minutes, we should eventually
    see that one of the models has started converging, leading to a lower and lower
    error, while the other model has tended to diverge to a poorer model due to the
    overly high learning rate:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'If you leave the program running for a number of minutes, you should eventually
    see the first model''s error rate becoming quite small:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Note again, that due to random data generation, you might see different results,
    but the overall result should be the same-in the first batch, the models will
    have the same error, and subsequently, the first model should start to generate
    to a smaller and smaller error.
  prefs: []
  type: TYPE_NORMAL
- en: Structured Streaming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With Spark version 2.0 we have structured streaming which states that the output
    of the application is equal to executing a batch job on a prefix of the data.
    Structured Streaming handles consistency and reliability within the engine and
    in interactions with external systems. Structured Stream is a simple data frame
    and dataset API.
  prefs: []
  type: TYPE_NORMAL
- en: Users provide the query they want to run along with the input and output locations.
    The system then executes the query incrementally, maintaining enough state to
    recover from failure, keeping the results consistent in external storage, and
    so on.
  prefs: []
  type: TYPE_NORMAL
- en: Structured Streaming promises a much simpler model for building real-time applications,
    built on the features that work best in Spark Streaming. However Structured Streaming
    is in alpha for Spark 2.0.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we connected some of the dots between online machine learning
    and streaming data analysis. We introduced the Spark Streaming library and API
    for continuous processing of data streams based on familiar RDD functionality
    and we worked through examples of streaming analytics applications that illustrate
    this functionality.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we used ML Library's streaming regression model in a streaming application
    that involves computing and comparing model performance on a stream of input feature
    vectors.
  prefs: []
  type: TYPE_NORMAL
