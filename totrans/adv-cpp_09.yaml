- en: 8\. Need for Speed – Performance and Optimization
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 8. 需要速度-性能和优化
- en: Learning Objectives
  id: totrans-1
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习目标
- en: 'By the end of this chapter, you will be able to:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 通过本章结束时，您将能够：
- en: Time your code performance manually
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 手动计时代码性能
- en: Use source code instrumentation to measure code execution time
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用源代码仪器来测量代码执行时间
- en: Use the perf tool to analyze program performance
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用perf工具分析程序性能
- en: Use the godbolt compiler explorer tool to analyze machine code generated by
    a compiler
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用godbolt编译器资源管理器工具分析编译器生成的机器代码
- en: Use compiler flags to generate better code
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用编译器标志生成更好的代码
- en: Apply code idioms that result in performance
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用导致性能的代码习惯
- en: Write cache-friendly code
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编写缓存友好的代码
- en: Apply algorithm-level optimizations to real-world problems
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将算法级优化应用于实际问题
- en: In this chapter, we will explore concepts that will allow us to write fast code
    in general and several practical techniques that apply to C++ in particular.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨允许我们在一般情况下编写快速代码以及适用于C++的几种实用技术的概念。
- en: Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍
- en: In today's world of extremely large and complicated software systems, `stability`
    and `maintainability` are usually considered the major goals for most software
    projects, whereas optimization has not been widely seen as a worthwhile goal since
    the 2000s. This is because of the rapid acceleration of hardware technology that
    overtook software demands on a regular schedule.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在当今极其庞大和复杂的软件系统中，`稳定性`和`可维护性`通常被认为是大多数软件项目的主要目标，而自2000年代以来，优化并未被广泛视为一个值得追求的目标。这是因为硬件技术的快速发展超过了软件对定期进步的需求。
- en: For many years, it seemed like the hardware improvements would continue to keep
    up with the performance demands of software, but applications continued to grow
    larger and more complex. Low-level native-compiled languages such as C and C++
    dropped in popularity compared to less performant but easier to use interpreted
    languages such as `Python` or `Ruby`.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 多年来，硬件的改进似乎会继续跟上软件的性能需求，但应用程序继续变得更大更复杂。与C和C++等低级本地编译语言相比，易于使用但性能较差的解释语言（如`Python`或`Ruby`）的流行度下降。
- en: By the late 2000s, though, the trend of CPU transistor count (and performance)
    doubling every 18 months (a consequence of `Moore's Law`) had stopped, and performance
    improvements had flattened out. The expectation of 5 to 10 GHz processors being
    widely available by the 2010s never materialized due to limitations of physics
    and manufacturing costs. However, the rapid adoption of mobile devices and the
    rise of high-performance computing applications for data science and machine learning,
    suddenly resurrected the demand for fast and efficient code. Performance per watt
    has become the new metric to aim for as large data centers consume enormous amounts
    of power. For example, Google servers in the US used more power than the entire
    nation of the UK in 2017.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 到了2000年代末，CPU晶体管数量（和性能）每18个月翻倍的趋势（`摩尔定律`的结果）停止了，性能改进趋于平稳。由于物理限制和制造成本的限制，人们对2010年代普遍可用的5到10
    GHz处理器的期望从未实现。然而，移动设备的快速采用和数据科学和机器学习的高性能计算应用的兴起，突然重新唤起了对快速和高效代码的需求。每瓦性能已成为新的衡量标准，因为大型数据中心消耗了大量电力。例如，2017年，谷歌在美国的服务器消耗的电力超过了整个英国国家的电力消耗。
- en: So far in this book, we've learned how the C++ language has evolved in terms
    of ease of use, without sacrificing any performance potential over a traditional
    language such as C. This means we can write fast code in C++ without necessarily
    sacrificing readability or stability. In the next section, we will learn about
    the concept of performance measurement.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在本书中，我们已经了解了C++语言在易用性方面的发展，而不会牺牲传统语言（如C）的性能潜力。这意味着我们可以在C++中编写快速的代码，而不一定要牺牲可读性或稳定性。在下一节中，我们将学习性能测量的概念。
- en: Performance Measurement
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 性能测量
- en: 'The most important aspect of optimization is the `measurement of code execution
    time`. Unless we measure our application''s performance with a wide range of input
    datasets, we will have no clue as to which part takes the most time and, our optimization
    efforts will be shot in the dark, with no guarantee of a result. There are several
    approaches for measurement, and some of them are listed here:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 优化最重要的方面是`代码执行时间的测量`。除非我们使用各种输入数据集来测量应用程序的性能，否则我们将不知道哪一部分花费了最多的时间，我们的优化工作将是一场盲目的射击，没有任何结果的保证。有几种测量方法，其中一些列在这里：
- en: Runtime instrumentation or profiling
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行时仪器或分析
- en: Source code instrumentation
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 源代码仪器
- en: Manual execution timing
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 手动执行计时
- en: Studying generated assembly code
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 研究生成的汇编代码
- en: Manual estimation by studying the code and algorithms used
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过研究使用的代码和算法进行手动估计
- en: The preceding list is ordered in terms of how accurate the measurement is (with
    the most accurate one first). However, each of these methods has different advantages.
    The choice of which approach to adopt depends on the goals and scope of the optimization
    effort. In an all-out effort to get the fastest possible implementation, all of
    these may be required. We will examine each of these approaches in the following
    sections.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 上述列表按测量准确性排序（最准确的排在最前面）。然而，每种方法都有不同的优势。选择采用哪种方法取决于优化工作的目标和范围。在全力以赴地实现最快的可能实现的努力中，可能需要所有这些方法。我们将在以下各节中研究每种方法。
- en: Manual Estimation
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 手动估计
- en: 'The biggest possible improvement in performance occurs when we replace an algorithm
    with a superior one. For example, consider the two versions of a trivial function
    that sums the integers from `1` to `n`:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们用更好的算法替换算法时，性能的最大可能改进发生。例如，考虑一个简单函数的两个版本，该函数对从`1`到`n`的整数求和：
- en: '[PRE0]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The first function, `sum1`, uses a simple loop to calculate the sum and has
    a runtime complexity that is proportional to `n`, whereas the second function,
    `sum2`, uses the algebraic summation formula and takes constant time independent
    of `n`. In this quite contrived example, we have optimized a function simply by
    using the basic knowledge of algebra.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个函数`sum1`使用简单的循环来计算总和，并且其运行时复杂度与`n`成正比，而第二个函数`sum2`使用代数求和公式，独立于`n`花费恒定的时间。在这个相当牵强的例子中，我们通过使用代数的基本知识来优化了一个函数。
- en: There are many well-known algorithms for every conceivable operation that have
    been proven to be the most optimal. The best way to make our code run as fast
    as possible is by using algorithms.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个可想象的操作，都有许多众所周知的算法被证明是最优的。使我们的代码尽可能快地运行的最佳方法是使用算法。
- en: It is essential to have a vocabulary of algorithms. We do not need to be an
    algorithm expert, but we need to at least be aware of the existence of efficient
    algorithms in various domains, even if we are not capable of implementing them
    from scratch. A slightly deeper knowledge of algorithms will help us find parts
    of our program that perform similar, if not exactly the same, computations as
    well-known algorithms. Certain code features such as nested loops or linear scanning
    of data are often obvious candidates for improvement, provided we can verify that
    these constructs are within hotspots in the code. A **hotspot** is a section of
    code that runs very often and affects performance significantly. The C++ standard
    library includes a lot of basic algorithms that can be used as building blocks
    to improve the efficiency of many common operations.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有算法词汇是至关重要的。我们不需要成为算法专家，但至少需要意识到各个领域存在高效算法的存在，即使我们无法从头开始实现它们。对算法的略微深入了解将有助于我们找到程序中执行类似的，即使不完全相同的计算的部分。某些代码特性，如嵌套循环或数据的线性扫描，通常是改进的明显候选，前提是我们可以验证这些结构是否在代码的热点内。**热点**是指运行非常频繁且显著影响性能的代码部分。C++标准库包含许多基本算法，可以用作改进许多常见操作的构建块。
- en: Studying Generated Assembly Code
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 研究生成的汇编代码
- en: '**Assembly language** is a human readable representiation of the binary machine
    code that actually executes on the processor. For any serious programmer of a
    compiled language such as C++, a basic understanding of assembly language is a
    great asset.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**汇编语言**是二进制机器代码的人类可读表示，实际上在处理器上执行。对于像C++这样的编译语言的严肃程序员来说，对汇编语言的基本理解是一项重要的资产。'
- en: Studying the generated assembly code for a program can give us some good insights
    into how the compiler works and estimates of code efficiency. There are many cases
    where this is the only approach possible to determine efficiency bottlenecks.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 研究程序生成的汇编代码可以让我们对编译器的工作方式和代码效率的估计有一些很好的见解。有许多情况下，这是确定效率瓶颈的唯一可能途径。
- en: Apart from this, a basic knowledge of assembly language is essential to be able
    to debug C++ code since some of the most difficult bugs to catch are those related
    to the low-level generated code.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 除此之外，对汇编语言的基本了解对于能够调试C++代码是至关重要的，因为一些最难以捕捉的错误与低级生成的代码有关。
- en: A very powerful and popular online tool that's used for analyzing compiler-generated
    code is the **Compiler Explorer** that we will be using in this chapter.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 用于分析编译器生成代码的一个非常强大和流行的在线工具是我们在本章中将要使用的**编译器探索者**。
- en: Note
  id: totrans-36
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The `Godbolt compiler explorer` can be found at [https://godbolt.org](https://godbolt.org).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '`Godbolt编译器探索者`可以在[https://godbolt.org](https://godbolt.org)找到。'
- en: 'The following is a screenshot of the Godbolt compiler explorer:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是Godbolt编译器探索者的屏幕截图：
- en: '![](img/C14583_08_01.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/C14583_08_01.jpg)'
- en: 'Figure 8.1: Godbolt compiler explorer'
  id: totrans-40
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8.1：Godbolt编译器探索者
- en: As you can see, the Godbolt compiler explorer consists of two panes. The one
    on the left is where we type the code in, while the one on the right displays
    the generated assembly code. The left-hand pane has a dropdown so that we can
    choose the desired language. For our purposes, we will use the C++ language with
    the gcc compiler.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，Godbolt编译器探索者由两个窗格组成。左侧是我们输入代码的地方，右侧显示生成的汇编代码。左侧窗格有一个下拉菜单，这样我们就可以选择所需的语言。为了我们的目的，我们将使用带有gcc编译器的C++语言。
- en: The right-hand pane has options that we can use to choose a compiler version.
    Almost all the versions of popular compilers such as `gcc`, `clang`, and `cl`
    (`Microsoft C++`) are present, including the ones for non-X86 architectures such
    as ARM.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 右侧窗格有选项，我们可以使用它来选择编译器版本。几乎所有流行编译器的版本，如`gcc`、`clang`和`cl`（`Microsoft C++`）都有，包括非X86架构的版本，如ARM。
- en: Note
  id: totrans-43
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: We will refer to the Intel processor architecture as `x86` for simplicity, even
    though the correct definition is `x86/64`. We will skip mentioning the "`64`"
    since almost all processors being manufactured today are `64-bit`. Even though
    `x86` was invented by Intel, now all PC processor manufacturers are licensed to
    use it.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简单起见，我们将把英特尔处理器架构称为`x86`，尽管正确的定义是`x86/64`。我们将跳过"`64`"，因为今天几乎所有的处理器都是`64位`的。尽管`x86`是由英特尔发明的，但现在所有的个人电脑处理器制造商都有使用许可。
- en: 'In order to get familiar with the basics of the `Compiler Explorer tool` and
    understand the `x86` assembly code at a basic level, let''s examine the assembly
    code generated by a compiler for a simple function that sums up the integers from
    `1` to `N`. Here is the sum function that needs to be written in the left-hand
    pane of the Compiler Explorer:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 为了熟悉`编译器探索者工具`的基础知识，并在基本水平上理解`x86`汇编代码，让我们来检查编译器为一个简单的从`1`加到`N`的整数求和函数生成的汇编代码。下面是需要在编译器探索者的左侧窗格中编写的求和函数：
- en: '[PRE1]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'In the right-hand pane, the compiler must be set to **x86-64 gcc 8.3**, like
    this:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在右侧窗格中，编译器必须设置为**x86-64 gcc 8.3**，就像这样：
- en: '![Figure 8.2: C++ compiler'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.2：C++编译器'
- en: '](img/C14583_08_02.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C14583_08_02.jpg)'
- en: 'Figure 8.2: C++ compiler'
  id: totrans-50
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8.2：C++编译器
- en: 'Once this is done, the left-hand pane''s code is automatically recompiled and
    the assembly code is generated and displayed on the right-hand pane. Here, the
    output is color-coded to show which lines of assembly code is generated from which
    lines of C++ code. The following screenshot shows the assembly code that was generated:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 完成后，左侧窗格的代码将自动重新编译，并在右侧窗格生成和显示汇编代码。这里，输出以颜色编码显示，以显示汇编代码的哪些行是从C++代码的哪些行生成的。以下屏幕截图显示了生成的汇编代码：
- en: '![Figure 8.3: Assembly result'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.3：汇编结果'
- en: '](img/C14583_08_03.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C14583_08_03.jpg)'
- en: 'Figure 8.3: Assembly result'
  id: totrans-54
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8.3：汇编结果
- en: 'Let''s analyze the preceding assembly code briefly. Each instruction in the
    assembly language consists of an **opcode** and one or more **operands**, which
    can be registers, constant values, or memory addresses. A **register** is a very
    fast storage location in the CPU. In the x86 architecture, there are eight main
    registers, namely **RAX**, **RBX**, **RCX**, **RDX**, **RSI**, **RDI**, **RSP**,
    and **RBP**. The Intel x86/x64 architecture uses a curious pattern of register
    naming:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们简要分析前面的汇编代码。汇编语言中的每条指令由一个**操作码**和一个或多个**操作数**组成，可以是寄存器、常量值或内存地址。**寄存器**是CPU中非常快速的存储位置。在x86架构中，有八个主要寄存器，即**RAX**，**RBX**，**RCX**，**RDX**，**RSI**，**RDI**，**RSP**和**RBP**。英特尔x86/x64架构使用一种奇特的寄存器命名模式：
- en: '**RAX** is a general-purpose 64-bit integer register.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**RAX**是一个通用的64位整数寄存器。'
- en: '`RAX`.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`RAX`。'
- en: '`EAX`.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`EAX`。'
- en: '`AX`, respectively.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`AX`。'
- en: 'The same convention applies to other general-purpose registers such as `RBX`,
    `RCX`, and `RDX`. The `RSI`, `RDI`, and `RBP` registers have 16-bit and 32-bit
    versions but not the 8-bit sub-registers. The opcode of an instruction can be
    of several types including arithmetic, logical, bitwise, comparison or jump operations.
    It is common to refer to an opcode as an instruction. For example, "`opcode` is
    `sum` function:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 相同的约定适用于其他通用寄存器，如`RBX`，`RCX`和`RDX`。`RSI`，`RDI`和`RBP`寄存器有16位和32位版本，但没有8位子寄存器。指令的操作码可以是多种类型，包括算术、逻辑、位运算、比较或跳转操作。通常将操作码称为指令。例如，“`opcode`是`sum`函数：
- en: '![Figure 8.4: Assembly code of the sum function'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.4：sum函数的汇编代码'
- en: '](img/C14583_08_04.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C14583_08_04.jpg)'
- en: 'Figure 8.4: Assembly code of the sum function'
  id: totrans-63
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8.4：sum函数的汇编代码
- en: In the preceding screenshot, the first few lines are called a `MOV RAX, RBX`
    assembly code means move the value in the `RBX` register to the `RAX` register.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的屏幕截图中，前几行称为`MOV RAX, RBX`汇编代码意味着将`RBX`寄存器中的值移动到`RAX`寄存器中。
- en: Note
  id: totrans-65
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: Assembly language is usually not case-sensitive, so `EAX` and `eax` mean the
    same thing.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 汇编语言通常不区分大小写，因此`EAX`和`eax`意思相同。
- en: The `(*(DWORD*)(rbp - 8))` C expression. In other words, the memory address
    `4` byte `DWORD` (a double word of memory – 32 bits). The square brackets in assembly
    code represent dereferencing, much like the * operator in C/C++. The `rbp` register
    is the base pointer that always contains the address of the base of the currently
    executing functions stack. It is not essential to know how exactly this stack
    frame works but remember that since the stack starts at a higher address and moves
    down, function arguments and local variables have addresses as negative offsets
    from `rbp`. If you see some negative offset from `rbp`, it refers to a local variable
    or argument.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '`(*(DWORD*)(rbp - 8))` C表达式。换句话说，内存地址`4`字节`DWORD`（内存的双字-32位）。汇编代码中的方括号表示解引用，就像C/C++中的*运算符一样。`rbp`寄存器是始终包含当前执行函数堆栈基址的地址的基址指针。不需要知道这个堆栈帧的工作原理，但请记住，由于堆栈从较高地址开始并向下移动，函数参数和局部变量的地址是从`rbp`的负偏移开始的。如果看到从`rbp`的负偏移，它指的是局部变量或参数。'
- en: In the preceding screenshot, the first `n` argument that was passed in. The
    last two `ret` variable and the `i` loop variable in our code to `0` and `1`,
    respectively.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的屏幕截图中，传递的第一个`n`参数。我们的代码中最后两个`ret`变量和`i`循环变量分别设置为`0`和`1`。
- en: 'Now, examine the snapshot of the assembly code that follows the prologue and
    initialization – this is our `for()` loop:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，检查跟随序言和初始化的汇编代码的快照-这是我们的`for()`循环：
- en: '![Figure 8.5: Assembly code of the for loop'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.5：for循环的汇编代码'
- en: '](img/C14583_08_05.jpg)'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C14583_08_05.jpg)'
- en: 'Figure 8.5: Assembly code of the for loop'
  id: totrans-72
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8.5：for循环的汇编代码
- en: In the preceding screenshot, the lines that have a string followed by a colon
    are called `BASIC`, `C/C++`, or `Pascal` and are used as targets of `goto` statements).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的屏幕截图中，具有字符串后跟冒号的行称为`BASIC`，`C/C++`或`Pascal`，并且用作`goto`语句的目标)。
- en: Instructions starting with J on x86 assembly are all jump instructions, such
    as `i` variable from memory to the `n` value in memory with the **cmp** instruction.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 以J开头的x86汇编指令都是跳转指令，例如使用**cmp**指令将内存中的`i`变量与内存中的`n`值进行比较。
- en: Note
  id: totrans-75
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The **JG** instruction here means **jump if greater**.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的**JG**指令意味着**如果大于则跳转**。
- en: 'If the comparison was greater, then the execution jumps to the **.L2** label
    (which is outside the loop). If not, the execution continues with the next instruction,
    like so:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 如果比较大，则执行跳转到**.L2**标签（在循环外）。如果不是，则执行继续下一条指令，如下所示：
- en: '![Figure 8.6: Assembly code of the next instruction'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.6：下一条指令的汇编代码'
- en: '](img/C14583_08_06.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C14583_08_06.jpg)'
- en: 'Figure 8.6: Assembly code of the next instruction'
  id: totrans-80
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8.6：下一条指令的汇编代码
- en: 'Here, the value of `i` is reloaded again into `ret`, after which `1` is added
    to `i`. Finally, the execution jumps back to the`for` loop and sums up the sequence
    of integers up to `n`, like so:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`i`的值再次重新加载到`ret`中，然后`1`被加到`i`上。最后，执行跳回到`for`循环并求和整数序列直到`n`，如下所示：
- en: '![Figure 8.7: Assembly code of the for loop'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.7：for循环的汇编代码'
- en: '](img/C14583_08_07.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C14583_08_07.jpg)'
- en: 'Figure 8.7: Assembly code of the for loop'
  id: totrans-84
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8.7：for循环的汇编代码
- en: This is called the `ret`, is moved into the `ret` returns from the `sum()` function.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这被称为`ret`，被移动到`ret`从`sum()`函数返回。
- en: Note
  id: totrans-86
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The "ret" in the above assembly listing is the mnemonic for the RETURN instruction
    and should not be confused with the "ret" variable in our C++ code example.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 上面汇编清单中的“ret”是RETURN指令的助记符，不应与我们C++代码示例中的“ret”变量混淆。
- en: 'It is not a simple job to figure out what a sequence of assembly instructions
    does, but a general idea of the mapping between the source code and instructions
    can be gained by observing the following points:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 弄清楚一系列汇编指令的作用并不是一件简单的工作，但是通过观察以下几点，可以对源代码和指令之间的映射有一个大致的了解：
- en: Constant values in code can be directly recognized in the assembly.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代码中的常量值可以直接在汇编中识别。
- en: Arithmetic operations such as `add`, `sub`, `imul`, `idiv`, and many others
    can be recognized.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 诸如`add`、`sub`、`imul`、`idiv`等算术运算可以被识别。
- en: Conditional jumps map to loops and conditionals.
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 条件跳转映射到循环和条件。
- en: Function calls can be directly read (the function name appears in the assembly
    code).
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 函数调用可以直接读取（函数名出现在汇编代码中）。
- en: 'Now, let''s observe the effect of the code if we add a compiler flag for optimization
    in the compiler options field at the top-right:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们观察一下，如果在顶部的编译器选项字段中为编译器添加优化标志，代码的效果会如何：
- en: '![Figure 8.8: Adding a compiler flag for optimization'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.8：为优化添加编译器标志'
- en: '](img/C14583_08_08.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C14583_08_08.jpg)'
- en: 'Figure 8.8: Adding a compiler flag for optimization'
  id: totrans-96
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8.8：为优化添加编译器标志
- en: In the preceding screenshot, `0` in `0` from memory into the register. Since
    memory takes several clock cycles to access (anywhere from `5` to `100` clock
    cycles), using registers alone will itself produce a massive speedup.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的截图中，`0`从内存中加载到寄存器中。由于内存访问需要几个时钟周期（从`5`到`100`个时钟周期不等），仅使用寄存器本身就会产生巨大的加速。
- en: 'When the compiler in the dropdown is changed to **x86-64 clang 8.0.0**, the
    assembly code is changed, which can be seen in the following screenshot:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 当下拉菜单中的编译器更改为**x86-64 clang 8.0.0**时，汇编代码会发生变化，可以在以下截图中看到：
- en: '![Figure 8.9: Assembly code with the new compiler'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.9：带有新编译器的汇编代码'
- en: '](img/C14583_08_09.jpg)'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C14583_08_09.jpg)'
- en: 'Figure 8.9: Assembly code with the new compiler'
  id: totrans-101
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8.9：带有新编译器的汇编代码
- en: 'In the preceding assembly listing, observe that there is no instruction starting
    with `J` (for jump). Thus, there is no looping construct at all! Let''s examine
    how the compiler is calculating the sum of `1` to `n`. If the value of `n` is
    `<= 0`, then it jumps to the`0`. Let''s analyze the following instructions:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的汇编清单中，注意到没有以`J`（跳转）开头的指令。因此，根本没有循环结构！让我们来看看编译器是如何计算`1`到`n`的和的。如果`n`的值`<=
    0`，那么它跳转到`0`。让我们分析以下指令：
- en: '![Figure 8.10: Assembly code with the new compiler'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.10：带有新编译器的汇编代码'
- en: '](img/C14583_08_10.jpg)'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C14583_08_10.jpg)'
- en: 'Figure 8.10: Assembly code with the new compiler'
  id: totrans-105
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8.10：带有新编译器的汇编代码
- en: 'The following code is the C equivalent of the previous instructions. Remember
    that `n` is in the `EDI` register (and hence also in the RDI register, since they
    overlap):'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码是前面指令的C等效代码。请记住，`n`在`EDI`寄存器中（因此也在RDI寄存器中，因为它们重叠）：
- en: '[PRE2]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Alternatively, if we were to write it in one line, it would look like this:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，如果我们将其写成一行，它会是这样的：
- en: '[PRE3]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'If we simplify this expression, we get the following:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们简化这个表达式，我们得到以下结果：
- en: '[PRE4]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Alternatively, we can write it in the following format:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们可以用以下格式来写：
- en: '[PRE5]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This can be simplified to the following:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以简化为以下形式：
- en: '[PRE6]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Alternatively, we can write the following:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们可以写成以下形式：
- en: '[PRE7]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This is the closed form equation for the summation of numbers `1` to `n` inclusive,
    and the fastest way to compute it. The compiler was extremely clever—rather than
    just looking at our code line by line, it reasoned that the effect of our loop
    was to calculate the sum, and it figured out the algebra on its own. It did not
    figure out the simplest possible expression, but an equivalent one that took a
    few extra operations. Nevertheless, taking out the loop makes this function very
    much optimal.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这是求和公式的封闭形式，用于计算`1`到`n`的数，也是计算它的最快方式。编译器非常聪明——它不仅仅是逐行查看我们的代码，而是推理出我们的循环的效果是计算总和，并且自己找出了代数。它没有找出最简单的表达式，而是找出了一个等价的表达式，需要一些额外的操作。尽管如此，去掉循环使得这个函数非常优化。
- en: If we modify the initial or final values of the `i` variable in the `for` loop
    to create a different summation, the compiler is still able to perform the necessary
    algebraic manipulation to derive a closed form solution needing no loops.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们修改`for`循环中`i`变量的初始或最终值以创建不同的求和，编译器仍然能够执行必要的代数操作，得出不需要循环的封闭形式解决方案。
- en: This is just one example of how compilers have become extremely efficient and
    appear almost intelligent. However, we must understand that this particular optimization
    of summations has been specifically programmed into the `clang` compiler. It does
    not mean that the compiler can do this kind of trick for any possible loop computation
    — that would actually require the compiler to have general artificial intelligence,
    as well as all the mathematical knowledge in the world.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是编译器变得非常高效并且几乎智能化的一个例子。然而，我们必须明白，这种求和的特定优化已经被编程到了`clang`编译器中。这并不意味着编译器可以为任何可能的循环计算做出这种技巧——这实际上需要编译器具有通用人工智能，以及世界上所有的数学知识。
- en: 'Let''s explore another example of compiler optimization via generated assembly
    code. Look at the following code:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过生成的汇编代码来探索编译器优化的另一个例子。看看以下代码：
- en: '[PRE8]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'In the compiler options, if we select the **x86-64 clang 8.0.0** compiler and
    add **-O3 -stdlib=libc++**, the following assembly code is generated:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在编译器选项中，如果我们选择**x86-64 clang 8.0.0**编译器并添加**-O3 -stdlib=libc++**，将生成以下汇编代码：
- en: '![Figure 8.11: Assembly code generated with the new compiler'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.11：使用新编译器生成的汇编代码'
- en: '](img/C14583_08_11.jpg)'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C14583_08_11.jpg)'
- en: 'Figure 8.11: Assembly code generated with the new compiler'
  id: totrans-126
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8.11：使用新编译器生成的汇编代码
- en: 'As you can see in the preceding screenshot, the compiler decided correctly
    that the vector was not relevant to the function and removed all the baggage.
    It also did the addition at compile time and directly used the result, `3`, as
    a constant. The main things to take forward from this section are as follows:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在前面的屏幕截图中所看到的，编译器正确地决定向量与函数无关，并移除了所有的负担。它还在编译时进行了加法运算，并直接使用结果`3`作为常数。从本节中可以得出的主要观点如下：
- en: Compilers can be extremely clever when optimizing code, given the right options.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在给予正确选项的情况下，编译器在优化代码时可以非常聪明。
- en: Studying generated assembly code is very useful to get a high-level estimate
    of execution complexity.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 研究生成的汇编代码对于获得执行复杂性的高级估计非常有用。
- en: A basic understanding of how machine code works is valuable for any C++ programmer.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '对机器码工作原理的基本理解对于任何C++程序员都是有价值的。 '
- en: In the next section, we'll learn about manual execution timing.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将学习关于手动执行计时的内容。
- en: Manual Execution Timing
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 手动执行计时
- en: 'This is the easiest way to quickly time small programs. We can use a command-line
    tool to measure the time taken for a program to execute. On Windows 7 and above,
    the following PowerShell command can be used:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这是快速计时小程序的最简单方法。我们可以使用命令行工具来测量程序执行所需的时间。在Windows 7及以上版本中，可以使用以下PowerShell命令：
- en: '[PRE9]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'On `Linux`, `MacOS`, and other `UNIX-like` systems, the `time` command can
    be used:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在`Linux`、`MacOS`和其他类`UNIX`系统上，可以使用`time`命令：
- en: '[PRE10]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: In the next section, we'll implement a small program and examine some caveats
    about timing a program's execution in general.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将实现一个小程序，并检查一般情况下计时程序执行的一些注意事项。
- en: 'Exercise 1: Timing a Program''s Execution'
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习1：计时程序的执行
- en: In this exercise, we will write a program that performs a summation of an array.
    The idea here is to time the summation function. This method is useful when we
    wish to test a function written in isolation. Thus, the test program's only purpose
    is to execute one single function. Since the calculation is very simple, we will
    need to run the function thousands of times in order to get a measurable execution
    time. In this case, we'll just call the `sumVector()` function from the `main()`
    function, passing an `std::vector` of random integers.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将编写一个程序来对数组进行求和。这里的想法是计时求和函数。当我们希望测试一个独立编写的函数时，这种方法是有用的。因此，测试程序的唯一目的是执行一个单一的函数。由于计算非常简单，我们需要运行函数数千次才能获得可测量的执行时间。在这种情况下，我们将从`main()`函数中调用`sumVector()`函数，传递一个随机整数的`std::vector`。
- en: Note
  id: totrans-140
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: A program that's meant to test a single function is sometimes referred to as
    a **driver program** (not to be confused with a device driver).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 一个旨在测试单个函数的程序有时被称为**驱动程序**（不要与设备驱动程序混淆）。
- en: 'Perform the following steps to complete this exercise:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤完成此练习：
- en: Create a file named **Snippet1.cpp**.
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为**Snippet1.cpp**的文件。
- en: 'Define a function named `sumVector` that sums up each element in a loop:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个名为`sumVector`的函数，它在循环中对每个元素求和：
- en: '[PRE11]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Define the `main` function. Use the C++11 random number generation facilities
    to initialize a vector of `10,000` elements and then call the `sumVector` function
    `1,000` times. Write the following code to implement this:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义`main`函数。使用C++11的随机数生成工具初始化一个包含`10,000`个元素的向量，然后调用`sumVector`函数`1,000`次。编写以下代码来实现这一点：
- en: '[PRE12]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Compile, run, and time this program on a Linux Terminal using the following
    commands:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令在Linux终端上编译、运行和计时此程序：
- en: '[PRE13]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The output of the previous command is as follows:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 上一个命令的输出如下：
- en: '![Figure 8.12: Output of timing the Snippet1.cpp code'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.12：对Snippet1.cpp代码进行计时的输出'
- en: '](img/C14583_08_12.jpg)'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C14583_08_12.jpg)'
- en: 'Figure 8.12: Output of timing the Snippet1.cpp code'
  id: totrans-153
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8.12：对Snippet1.cpp代码进行计时的输出
- en: As you can see from the preceding output, for this system, the program executed
    in `0.122` seconds (note that the results will vary, depending on your system's
    configuration). If we run this timing command repeatedly, we may get slight variations
    in the results as the program will be loaded in the memory after the first run
    and will be marginally faster. It is best to run and time the program about `5`
    times and get an average value. We are usually not interested in the absolute
    value of the time taken, but rather how the value improves as we optimize our
    code.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的输出中可以看出，对于这个系统，程序在`0.122`秒内执行（请注意，结果会根据您系统的配置而有所不同）。如果我们反复运行此计时命令，可能会得到结果略有不同，因为程序在第一次运行后将加载到内存中，并且速度会略有提高。最好运行并计时程序约`5`次，并获得平均值。我们通常对所花费的时间的绝对值不感兴趣，而是对我们优化代码后数值的改善感兴趣。
- en: 'Use the following commands to explore the effect of using compiler optimization
    flags:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令来探索使用编译器优化标志的效果：
- en: '[PRE14]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The output is as follows:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 8.13: Output of timing the Snippet1.cpp code compiled with -O3'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.13：使用-O3编译的Snippet1.cpp代码的计时输出'
- en: '](img/C14583_08_13.jpg)'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C14583_08_13.jpg)'
- en: 'Figure 8.13: Output of timing the Snippet1.cpp code compiled with -O3'
  id: totrans-160
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8.13：使用-O3编译的Snippet1.cpp代码的计时输出
- en: From the preceding output, it seems that the program has become about `60` times
    faster, which seems quite unbelievable.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的输出中，似乎程序变快了约`60`倍，这似乎令人难以置信。
- en: 'Change the code to execute the loop `100,000` times rather than `1,000` times:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将代码更改为执行循环`100,000`次而不是`1,000`次：
- en: '[PRE15]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Recompile and time again using the following commands:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新编译并使用以下命令再次计时：
- en: '[PRE16]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The output after executing previous command is as follows:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 执行上一个命令后的输出如下：
- en: '![Figure 8.14: Output of timing the Snippet1.cpp code with 10,000 iterations'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.14：对Snippet1.cpp代码进行计时，迭代次数为10,000'
- en: '](img/C14583_08_14.jpg)'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C14583_08_14.jpg)'
- en: 'Figure 8.14: Output of timing the Snippet1.cpp code with 10,000 iterations'
  id: totrans-169
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8.14：对Snippet1.cpp代码进行计时，迭代次数为10,000
- en: From the preceding output, it still seems to take the exact same time. This
    seems impossibe, but actually what happens is that since we never caused any side
    effect in our program, such as printing the sum, the compiler is free to replace
    our code with an empty program. Functionally, according to the C++ standard, this
    program and an empty program are identical because there are no side effects of
    running it.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: 'Open the Compiler Explorer and paste in the entire code. Set the compiler options
    to `-O3` and observe the generated code:![Figure 8.15: Snippet1.cpp code in Compiler
    Explorer'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/C14583_08_15.jpg)'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.15: Snippet1.cpp code in Compiler Explorer'
  id: totrans-173
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As you can see from the preceding screenshot, the lines within the `for` loop
    are not color-coded and no assembly code was generated for them.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: 'Change the code to make sure that the sum must be performed by printing a value
    that depends on the computation with the following line:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Here, we are just summing the result of `sumVector()` to a dummy double value
    many time and printing it. After you make the changes in the code, open the Terminal
    and write the following commands:'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The output of the previous commands is as follows:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.16: Output of timing the Snippet1.cpp code with a side effect of
    printing the value'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C14583_08_16.jpg)'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.16: Output of timing the Snippet1.cpp code with a side effect of printing
    the value'
  id: totrans-182
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In the preceding output, we can see that the program actually performed the
    computation instead of just running as an empty program. Printing the total to
    `cout` is a side effect that causes the compiler not to elide the code. Causing
    a side effect (such as printing a result) that depends on the code's execution
    is one way to prevent the compiler optimizer from removing code. In the next section,
    we'll learn how to time programs without side effects.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: Timing Programs without Side Effects
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As seen in the previous exercise, we needed to create a side effect (using
    `cout`) in our program so that the compiler did not ignore all the code we wrote.
    Another technique to make the compiler believe that a piece of code has a side
    effect is to assign its result to a **volatile** variable. The volatile qualifier
    tells the compiler: "This variable must always be read from memory and written
    to memory, and not from a register." The main purpose of a volatile variable is
    to access device memory, and such device memory access must follow the rule mentioned
    above. Effectively, volatile variables are considered by the compiler as if they
    could change from effects outside of the current program, and thus will never
    be optimized. We will use this technique in the upcoming sections.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: There are more advanced ways to bypass this problem, that is, by specifying
    special assembly code directives to the compiler rather than using side effects.
    But they are outside the scope of this introductory material. For the examples
    that follow, we'll always add code that ensures that a function's result is used
    in a side effect or is assigned to a volatile variable. In future sections, we'll
    learn how to examine the compiler generated assembly code and detect instances
    when the compiler elides code for optimization purposes.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: Source Code Instrumentation
  id: totrans-187
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Instrumentation** is a term that refers to the process of adding extra code
    to a program, without changing its behavior, and capturing the information as
    it executes. This may include performance timing (and possibly other measurements
    such as memory allocation, or disk usage patterns). In the case of source code
    instrumentation, we manually add code to time the execution of our program and
    log that data when the program ends, for analysis. The advantage of this approach
    is its portability and avoidance of any external tools. It also allows us to selectively
    add the timing to any part of the code we choose.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 2: Writing a Code Timer Class'
  id: totrans-189
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we''ll create an `RAII` class that allows us to measure the
    execution time of individual blocks of code. We will use this as the primary timing
    mechanism for the code in the exercises that follow. It is not as sophisticated
    as other methods of performance measurement but is much easier to use and serves
    most purposes. The basic requirement of our class is as follows:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将创建一个`RAII`类，允许我们测量单个代码块的执行时间。我们将把这个作为后续练习中代码的主要计时机制。它不像其他性能测量方法那样复杂，但使用起来更加简单，并且可以满足大多数需求。我们类的基本要求如下：
- en: We need to be able to record the cumulative time taken by a block of code.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们需要能够记录代码块所花费的累积时间。
- en: We need to be able to record the number of times it is invoked.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们需要能够记录调用的次数。
- en: 'Perform the following steps to complete this exercise:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤完成这个练习：
- en: Create a file named **Snippet2.cpp**.
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为**Snippet2.cpp**的文件。
- en: 'Include the following headers:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 包括以下头文件：
- en: '[PRE19]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Define the `Timer` class and the class member functions by writing the following
    code:'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过编写以下代码来定义`Timer`类和类成员函数：
- en: '[PRE20]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: As you can see from the preceding code, the class members consist of a name,
    a starting timestamp, and two `static maps`. Every instance of this class is meant
    to time a certain block of code. The block can be a function scope or any other
    block delimited by curly braces. The usage pattern is to define an instance of
    the `Timer` class at the top of the block while passing in a name (can be a function
    name or some other convenient label). When instantiated, the current timestamp
    is recorded, and when the block exits, the destructor of this class records the
    cumulative time elapsed for this block, as well as the count of the number of
    times this block executed. The times and counts are stored in the static maps
    `ms_Times` and `ms_Counts`, respectively.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 从上述代码中可以看出，类成员包括名称、起始时间戳和两个`static map`。这个类的每个实例都用于计时某个代码块。该代码块可以是函数作用域或由花括号分隔的任何其他块。使用模式是在块的顶部定义一个`Timer`类的实例，同时传入一个名称（可以是函数名或其他方便的标签）。实例化时，记录当前时间戳，当块退出时，该类的析构函数记录了该块的累积经过时间，以及该块执行的次数。时间和次数分别存储在`ms_Times`和`ms_Counts`这两个`static
    map`中。
- en: 'Define the constructor of the `Timer` class by writing the following code:'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过编写以下代码来定义`Timer`类的构造函数：
- en: '[PRE21]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Define the destructor of the `Timer` class by writing the following code:'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义`Timer`类的析构函数，编写以下代码：
- en: '[PRE22]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: In the preceding code, the elapsed time is calculated in milliseconds. Then,
    we add that to the cumulative elapsed time for this block name and increment the
    count of how many times this block was executed.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，经过时间以毫秒计算。然后，我们将其加到此块名称的累积经过时间中，并增加此块执行的次数。
- en: 'Define a `static` function named `dump()` that prints out the summary of the
    timed results:'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个名为`dump()`的`static`函数，打印出定时结果的摘要：
- en: '[PRE23]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: In the preceding code, the name, execution count, total time, and average time
    is printed in a tabular form. We use multiple tabs between the field names and
    field values to make them line up vertically on a console. This function can be
    modified as we wish. For example, we can modify this code to dump the output as
    a CSV file, so that it can be imported into a spreadsheet for further analysis.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，以表格形式打印名称、执行次数、总时间和平均时间。我们在字段名称和字段值之间使用多个制表符，使它们在控制台上垂直对齐。这个函数可以根据我们的需要进行修改。例如，我们可以修改这段代码，将输出转储为CSV文件，以便可以将其导入电子表格进行进一步分析。
- en: 'Finally, define the `static` members to complete the class:'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，定义`static`成员以完成这个类：
- en: '[PRE24]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Now that we have defined the `Timer` class, define two simple functions that
    we will time as an example. One will add and the other will multiply. Since these
    operations are trivial, we will loop `1 billion times` so that we can have some
    measurable result.
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经定义了`Timer`类，定义两个简单的函数作为示例进行计时。一个函数将进行加法，另一个函数将进行乘法。由于这些操作很简单，我们将循环`10亿次`，以便可以得到一些可测量的结果。
- en: Note
  id: totrans-211
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: '[PRE25]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: In the preceding code, we used `unsigned int` for the variable that we repeatedly
    `add`/`multiply`. We used an unsigned type so that overflow during arithmetic
    does not result in undefined behavior. Had we used a signed type, the program
    would have undefined behavior and not be guaranteed to work in any way. Secondly,
    we return the calculated value from the `testAdd()` and `testMul()` functions
    so that we can ensure that the compiler does not remove the code (because of the
    lack of side effects). In order to time each of these functions, we need to simply
    declare an instance of a `Timer` class with a suitable label at the start of the
    function. The timing is started as soon as the `Timer` object is instantiated
    and stopped when that object goes out of scope.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，我们使用`unsigned int`作为我们重复进行`add`/`multiply`的变量。我们使用无符号类型，以便在算术运算期间不会发生溢出导致未定义行为。如果我们使用了有符号类型，程序将具有未定义行为，并且不能保证以任何方式工作。其次，我们从`testAdd()`和`testMul()`函数返回计算的值，以便确保编译器不会删除代码（因为缺乏副作用）。为了计时这两个函数中的每一个，我们只需要在函数开始时声明一个带有合适标签的`Timer`类的实例。当`Timer`对象实例化时，计时开始，当该对象超出范围时，计时停止。
- en: 'Write the `main` function, where we will simply call both test functions `10`
    times each:'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写`main`函数，在其中我们将分别调用两个测试函数`10`次：
- en: '[PRE26]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: As you can see in the preceding code, we're calling each function `10` times
    so that we can demonstrate the `Timer` class timing multiple runs of a function.
    Assigning the result of the functions to a volatile variable forces the compiler
    to assume that there is a global side effect. Hence, it will not elide the code
    in our test functions. Before exiting, we call the `Timer::dump` static function
    to display the results.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 如上述代码所示，我们分别调用每个函数`10`次，以便演示`Timer`类计时函数的多次运行。将函数的结果赋给一个`volatile`变量会迫使编译器假定存在全局副作用。因此，它不会删除我们测试函数中的代码。在退出之前，调用`Timer::dump`静态函数显示结果。
- en: 'Save the program and open a terminal. Compile and run the program with different
    optimization levels – on the `gcc` and `clang` compilers, this is specified by
    the `-ON` compiler flag, where `N` is a number from `1` to `3`. Add the `-O1`
    compiler flag first:'
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'This code generates the following output:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.17: Snippet2.cpp code performance when compiled with the -O1 option'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C14583_08_17.jpg)'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.17: Snippet2.cpp code performance when compiled with the -O1 option'
  id: totrans-222
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now, add the `-O2` compiler flag in the terminal and execute the program:'
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'This generates the following output:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.18: Snippet2.cpp code performance when compiled with the -O2 option'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C14583_08_18.jpg)'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.18: Snippet2.cpp code performance when compiled with the -O2 option'
  id: totrans-228
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Add the `-O3` compiler flag in the terminal and execute the program:'
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'This generates the following output:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.19: Snippet2.cpp code performance when compiled with the -O3 option'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C14583_08_19.jpg)'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.19: Snippet2.cpp code performance when compiled with the -O3 option'
  id: totrans-234
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Notice that the `testMul` function became faster only at `O3`, but the `testAdd`
    function got faster at `O2` and much faster at `O3`. We can verify this by running
    the program multiple times and averaging the times. There are no obvious reasons
    why some functions speed up while others do not. We would have to exhaustively
    check the generated code to understand why. It is not guaranteed that this will
    happen on all the systems with different compilers or even compiler versions.
    The main point to take home is that we can never assume performance but have to
    always measure it, and always re-measure if we believe any change we made affects
    the performance.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: 'To make it easier to use our `Timer` class for timing individual functions,
    we can write a macro. C++ 11 and above support a special compiler built-in macro
    called `__func__` that always contains the currently executing function''s name
    as a `const char*`. Use this to define a macro so that we don''t need to specify
    a label for our `Timer` instances, as follows:'
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Add the `TIME_IT` macro to the start of the two functions, changing the existing
    line that creates a Timer object:'
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Save the program and open the terminal. Compile and run it again by using the
    following command:'
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The output of the previous command is as follows:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.20: Snippet2.cpp code output when using a macro for timing'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C14583_08_20.jpg)'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.20: Snippet2.cpp code output when using a macro for timing'
  id: totrans-245
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In the preceding output, notice that the actual function name is printed now.
    Another advantage of using this macro is that we can add this to all potentially
    time-consuming functions by default, and disable it for production builds by simply
    changing the definition to a no-op, which will cause the timing code to never
    run - avoiding the need to edit the code extensively. We will use this same Timer
    class for timing code in forthcoming exercises.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: Runtime Profiling
  id: totrans-247
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Profiling** is a non-intrusive method of measuring the performance of the
    functions in a program. Profilers work by sampling a program''s current execution
    address at frequent intervals (hundreds of times in a second) and making a log
    of which functions happened to be executing at the time. This is a statistical
    sampling approach that has reasonable accuracy. Sometimes, though, the results
    can be confusing as a program may spend a lot of time on functions that are a
    part of the operating system kernel. The most popular runtime profiling tool on
    Linux is **perf**. In the next section, we''ll make use of perf to profile our
    program.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 3: Using perf to Profile Programs'
  id: totrans-249
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`perf` can be installed on `Ubuntu` as follows:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'To get familiar with the basics of using `perf`, we''ll profile and analyze
    the program from the previous exercise with the help of the `perf` tool. Perform
    the following steps to complete this exercise:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: Open the `TIME_IT` macros from the two functions.
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Open the terminal, recompile the code again with the `-O3` flag, and then create
    a profile data sample with `perf` as follows:'
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开终端，使用`-O3`标志重新编译代码，然后使用`perf`创建一个配置文件数据样本，如下所示：
- en: '[PRE34]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The output of the previous command is as follows:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 前一个命令的输出如下：
- en: '![](img/C14583_08_21.jpg)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![](img/C14583_08_21.jpg)'
- en: 'Figure 8.21: Using the perf command to analyze the code in Snippet2.cpp'
  id: totrans-258
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8.21：使用perf命令分析Snippet2.cpp中的代码
- en: This creates a file called `perf.data` which can be analyzed or visualized.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 这将创建一个名为`perf.data`的文件，可以进行分析或可视化。
- en: 'Now, use the following command to visualize the recorded data:'
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，使用以下命令可视化记录的数据：
- en: '[PRE35]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'A console-based GUI will show the following data after executing the previous
    command:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 执行前一个命令后，控制台基于GUI将显示以下数据：
- en: '![Figure 8.22: Using the perf command to analyze the code in Snippet2.cpp'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.22：使用perf命令分析Snippet2.cpp中的代码'
- en: '](img/C14583_08_22.jpg)'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C14583_08_22.jpg)'
- en: 'Figure 8.22: Using the perf command to analyze the code in Snippet2.cpp'
  id: totrans-265
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8.22：使用perf命令分析Snippet2.cpp中的代码
- en: You can move the cursor up and down to select a function and press *Enter* to
    get a list of options.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以上下移动光标选择一个函数，然后按*Enter*获取选项列表。
- en: 'Highlight `testMul`, press *Enter*, and choose `Annotate testMul` in the resulting
    list. A list of assembly code is shown, with annotations describing the percentage
    of execution time for each line of code, as follows:'
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 突出显示`testMul`，按*Enter*，并在结果列表中选择`Annotate testMul`。显示一系列汇编代码，其中包含描述每行代码执行时间百分比的注释，如下所示：
- en: '![Figure 8.23: Viewing the timing statistics using the perf command for the
    Snippet2.cpp code'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.23：使用perf命令查看Snippet2.cpp代码的时间统计信息'
- en: '](img/C14583_08_23.jpg)'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C14583_08_23.jpg)'
- en: 'Figure 8.23: Viewing the timing statistics using the perf command for the Snippet2.cpp
    code'
  id: totrans-270
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8.23：使用perf命令查看Snippet2.cpp代码的时间统计信息
- en: Notice that the `99%` of the time to execute. Traditionally, integer multiplications
    are always expensive on the `x86` architecture and this continues to be true,
    even in the latest generation of CPUs. This annotation view displays arrows next
    to each jump or branching instruction which, when highlighted, shows what comparison
    instruction it is associated with and what address it jumps to with line drawings.
    You can navigate to the previous view by pressing the left arrow key and exit
    the program using the *q* key.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 注意`99%`的执行时间。传统上，在`x86`架构上，整数乘法始终很昂贵，即使在最新一代CPU中也是如此。此注释视图在每个跳转或分支指令旁显示箭头，突出显示时显示其关联的比较指令和跳转到的地址以线条绘制。您可以按左箭头键导航到上一个视图，并使用*q*键退出程序。
- en: Up until now, we've looked at several methods that are used to assess the performance
    of our programs. This is the most critical stage of optimization since it tells
    us where we need to direct our efforts. In the upcoming sections, we will explore
    various techniques that will help us optimize our code.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看了几种用于评估程序性能的方法。这是优化的最关键阶段，因为它告诉我们需要将精力放在哪里。在接下来的章节中，我们将探索各种技术，帮助我们优化我们的代码。
- en: Optimization Strategies
  id: totrans-273
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优化策略
- en: 'Optimization of code can be done in several ways, such as the following:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 代码优化可以通过多种方式进行，例如：
- en: Compiler-based optimization
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于编译器的优化
- en: Source code micro-optimization
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 源代码微优化
- en: Cache-friendly code
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缓存友好的代码
- en: Algorithmic optimization
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 算法优化
- en: Here, each technique has its pros and cons. We will examine each of these methods
    in detail in the upcoming sections. Roughly speaking, these are ordered in terms
    of effort required and also potential gains in performance. We'll look at compiler-based
    optimization in the next section.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，每种技术都有其优缺点。我们将在接下来的章节中详细研究这些方法。粗略地说，这些方法按照所需的工作量和性能潜力排序。我们将在下一节中研究基于编译器的优化。
- en: Compiler-Based Optimization
  id: totrans-280
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于编译器的优化
- en: Passing the correct options to the compiler can net many performance benefits.
    A real-world example of this is the Clear Linux `gcc` and `clang` family of compilers,
    the most basic option for optimization is `-O<N>`, where `N` is one of the numbers
    `1`, `2`, or `3`. `-O3` enables almost every optimization in the compiler, but
    there are several others not enabled by that flag that can make a difference.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 向编译器传递正确的选项可以获得许多性能优势。这方面的一个现实例子是Clear Linux的`gcc`和`clang`系列编译器，优化的最基本选项是`-O<N>`，其中`N`是`1`、`2`或`3`中的一个数字。`-O3`几乎启用了编译器中的每个优化，但还有一些未通过该标志启用的其他优化可以产生差异。
- en: Loop Unrolling
  id: totrans-282
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 循环展开
- en: '**Loop unrolling** is a technique that can be used by compilers to reduce the
    number of branches that are executed. Every time a branch is executed, there is
    a certain performance overhead. This can be reduced by repeating the loop body
    multiple times, and reducing the number of times the loop is executed. Loop unrolling
    can be done at the source level by the programmer, but modern compilers do a very
    good job automatically.'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '**循环展开**是编译器可以使用的一种技术，用于减少执行的分支数。每次执行分支时，都会有一定的性能开销。这可以通过多次重复循环体并减少循环执行次数来减少。循环展开可以由程序员在源级别上完成，但现代编译器会自动完成得很好。'
- en: Even though modern processors mitigate the overhead of branching by means of
    `gcc` and `clang` family of compilers with the `-funroll-loops` command-line flag.
    In the next section, we'll test the performance of a program with and without
    loop unrolling enabled.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管现代处理器通过`gcc`和`clang`系列编译器的`-funroll-loops`命令行标志来减少分支开销。在下一节中，我们将测试启用和未启用循环展开的程序性能。
- en: 'Exercise 4: Using Loop Unrolling Optimizations'
  id: totrans-285
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习4：使用循环展开优化
- en: In this exercise, we'll write a simple program that uses nested loops and test
    its performance with and without loop unrolling enabled. We'll understand the
    way compilers implement the automatic unrolling of loops.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将编写一个简单的程序，使用嵌套循环并测试其性能，启用和未启用循环展开。我们将了解编译器如何实现循环的自动展开。
- en: 'Perform these steps to complete this exercise:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤完成此练习：
- en: Create a file named **Snippet3.cpp**.
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建名为**Snippet3.cpp**的文件。
- en: 'Write a program that takes the first `10,000` numbers and prints out how many
    of these are factors of each other (the full code can be found in **Snippet3.cpp**):'
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写一个程序，取前`10,000`个数字，并打印出这些数字中有多少个是彼此的因子（完整代码可以在**Snippet3.cpp**中找到）：
- en: '[PRE36]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Save the program and open the terminal. Compile the program with the `-O3`
    flag first and time it using the following command:'
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保存程序并打开终端。首先使用`-O3`标志编译程序，并使用以下命令计时：
- en: '[PRE37]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The output of the previous command is as follows:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 前一个命令的输出如下：
- en: '![Figure 8.24: Output of the code in Snippet3.cpp'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.24：Snippet3.cpp代码的输出'
- en: '](img/C14583_08_24.jpg)'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C14583_08_24.jpg)'
- en: 'Figure 8.24: Output of the code in Snippet3.cpp'
  id: totrans-296
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8.24：Snippet3.cpp代码的输出
- en: 'Now, compile the same code with the loop unrolling enabled and time it again:'
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，启用循环展开编译相同的代码并再次计时：
- en: '[PRE38]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The output of the previous command is as follows:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 前一个命令的输出如下：
- en: '![Figure 8.25: Output of the code in Snippet3.cpp compiled with the loop unrolling
    option'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.25：使用循环展开选项编译的Snippet3.cpp代码的输出'
- en: '](img/C14583_08_25.jpg)'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C14583_08_25.jpg)'
- en: 'Figure 8.25: Output of the code in Snippet3.cpp compiled with the loop unrolling
    option'
  id: totrans-302
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8.25：使用循环展开选项编译的Snippet3.cpp代码的输出
- en: Open the `Godbolt compiler explorer` and paste the preceding complete code into
    the left-side.
  id: totrans-303
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开`Godbolt编译器资源管理器`，并将前面的完整代码粘贴到左侧。
- en: 'On the right-hand side, select `x86-64 gcc 8.3` from the compiler options and
    write the `-O3` flag in the options. Assembly code will be generated. For the
    for loop, you''ll see the following output:![Figure 8.26: Assembly code of the
    for loop'
  id: totrans-304
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在右侧，从编译器选项中选择`x86-64 gcc 8.3`，并在选项中写入`-O3`标志。将生成汇编代码。对于for循环，你会看到以下输出：![图8.26：for循环的汇编代码
- en: '](img/C14583_08_26.jpg)'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C14583_08_26.jpg)'
- en: 'Figure 8.26: Assembly code of the for loop'
  id: totrans-306
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8.26：for循环的汇编代码
- en: From the preceding screenshot, you can clearly see `RCX` being compared to `10,000`
    with the `CMP` instruction, followed by a conditional jump, `JNE` (Jump if Not
    Equal). Just after this code, the outer loop comparison is seen, with `RSI` being
    compared to `10,000`, followed by another conditional jump to the `L4` label.
    Overall, the inner conditional branch and jump executes `100,000,000` times.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的截图中，你可以清楚地看到`RCX`与`10,000`进行比较，使用`CMP`指令，然后是一个条件跳转，`JNE`（如果不相等则跳转）。就在这段代码之后，可以看到外部循环比较，`RSI`与`10,000`进行比较，然后是另一个条件跳转到`L4`标签。总的来说，内部条件分支和跳转执行了`100,000,000`次。
- en: 'Now, add the following options: `-O3 –funroll-loops`. Assembly code will be
    generated. In this code, you''ll notice this code pattern repeating eight times
    (except for the `LEA` instruction, whose offset value changes):'
  id: totrans-308
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，添加以下选项：`-O3 –funroll-loops`。将生成汇编代码。在这段代码中，你会注意到这段代码模式重复了八次（除了`LEA`指令，其偏移值会改变）：
- en: '![Figure 8.27: Assembly code of the for loop'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.27：for循环的汇编代码'
- en: '](img/C14583_08_27.jpg)'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C14583_08_27.jpg)'
- en: 'Figure 8.27: Assembly code of the for loop'
  id: totrans-311
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8.27：for循环的汇编代码
- en: The compiler decided to unroll the body of the loop eight times, reducing the
    number of conditional jump instructions executed by a factor of `87.5%` (about
    `8,300,000` times). This alone caused the execution time to improve by `10%`,
    which is a very significant speedup. In this exercise we have seen the benefits
    of loop unrolling - next, we'll learn about profile guided optimization.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 编译器决定展开循环体八次，将条件跳转指令的执行次数减少了`87.5%`（约`8,300,000`次）。这单独就导致执行时间提高了`10%`，这是一个非常显著的加速。在这个练习中，我们已经看到了循环展开的好处
    - 接下来，我们将学习profile guided optimization。
- en: Profile Guided Optimization
  id: totrans-313
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Profile Guided Optimization
- en: '**Profile Guided Optimization** (PGO) is a feature that most compilers support.
    When a program is compiled with PGO enabled, the compiler adds instrumentation
    code to the program. Running this PGO-enabled executable creates a log file that
    contains information about the execution statistics of the program. The term **profiling**
    refers to the process of running a program to gather performance metrics. Typically,
    this profiling stage should be run with a real-world dataset so that an accurate
    log is produced. After this profiling run, the program is recompiled with a special
    compiler flag. This flag enables the compiler to perform special optimizations
    based on the statistical execution data that was recorded. Significant performance
    gains can be achieved with this approach. Let''s solve an exercise based on profile
    guided optimization to get a better understanding of this.'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '**Profile Guided Optimization**（PGO）是大多数编译器支持的一个特性。当使用PGO编译程序时，编译器会向程序添加插装代码。运行这个启用了PGO的可执行文件会创建一个包含程序执行统计信息的日志文件。术语**profiling**指的是运行程序以收集性能指标的过程。通常，这个profiling阶段应该使用真实的数据集运行，以便产生准确的日志。在这个profiling运行之后，程序会使用特殊的编译器标志重新编译。这个标志使编译器能够根据记录的统计执行数据执行特殊的优化。采用这种方法可以实现显著的性能提升。让我们解决一个基于profile
    guided optimization的练习，以更好地理解这个过程。'
- en: 'Exercise 5: Using Profile Guided Optimization'
  id: totrans-315
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习5：使用Profile Guided Optimization
- en: In this exercise, we will use profile guided optimization on the code from the
    previous exercise. We'll understand how to use profile guided optimization with
    the `gcc` compiler.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将在前一个练习的代码上使用profile guided optimization。我们将了解如何在`gcc`编译器中使用profile
    guided optimization。
- en: 'Perform these steps to complete this exercise:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤完成这个练习：
- en: 'Open the terminal and compile the code from the previous exercise with profiling
    enabled. Include any other optimization flags that we need (in this case, `-O3`).
    Write the following code to implement this:'
  id: totrans-318
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开终端，并使用启用了profiling的前一个练习的代码进行编译。包括我们需要的任何其他优化标志（在本例中为`-O3`）。编写以下代码来实现这一点：
- en: '[PRE39]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Now, run the profiled version of the code by writing the following command:'
  id: totrans-320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，通过编写以下命令运行代码的profiled版本：
- en: '[PRE40]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: The program runs normally and prints the result, and no other output is seen
    - but it generates a file containing data that will help the compiler in the following
    step. Note that with profiling enabled, the program executes several times slower
    than it would normally. This is something to keep in mind with large programs.
    After executing the previous command, a file called `Snippet3.gcda` will be generated,
    which contains profile data. When doing this with large, complex applications,
    it is important to run the program with the datasets and workflows that it will
    most commonly encounter in the production environment. By choosing the data correctly
    here, the eventual performance gain will be higher.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 程序正常运行并打印结果，没有看到其他输出 - 但它生成了一个包含数据的文件，这将帮助编译器进行下一步。请注意，启用了性能分析后，程序的执行速度会比正常情况下慢几倍。这是在处理大型程序时需要牢记的事情。执行前一个命令后，将生成一个名为`Snippet3.gcda`的文件，其中包含性能分析数据。在处理大型、复杂的应用程序时，重要的是使用它在生产环境中最常遇到的数据集和工作流来运行程序。通过在这里正确选择数据，最终的性能提升将更高。
- en: 'Recompile with the PGO optimization flags, that is, `-fprofile-use` and `-fprofile-correction`,
    as illustrated in the following code:'
  id: totrans-323
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新编译使用PGO优化标志，即`-fprofile-use`和`-fprofile-correction`，如下所示：
- en: '[PRE41]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Note that other than the profile-related compiler options, the other options
    must be exactly the same as the ones in the previous compilation step.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，除了与之前编译步骤中的与性能相关的编译器选项外，其他选项必须完全相同。
- en: 'Now, if we time the executable, we will see a large performance improvement:'
  id: totrans-326
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，如果我们计时可执行文件，我们将看到性能大幅提升：
- en: '[PRE42]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The output of the previous command is as follows:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 前一个命令的输出如下：
- en: '![Figure 8.28: Timing results of the code in Snippet3.cpp with PGO optimization'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.28：使用PGO优化编译的Snippet3.cpp代码的时间结果'
- en: '](img/C14583_08_28.jpg)'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C14583_08_28.jpg)'
- en: 'Figure 8.28: Timing results of the code in Snippet3.cpp with PGO optimization'
  id: totrans-331
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8.28：使用PGO优化编译的Snippet3.cpp代码的时间结果
- en: In this exercise, we have seen the performance benefits gained by using profile
    guided optimizations provided by the compiler. For this code, the improvement
    in performance was about `2.7x` - on larger programs, this can be even higher.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们已经看到了使用编译器提供的基于性能指导的优化所获得的性能优势。对于这段代码，性能提升约为`2.7倍` - 在更大的程序中，这个提升甚至可能更高。
- en: Parallelization
  id: totrans-333
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 并行化
- en: Most CPUs today have multiple cores, and even mobile phones have quad core processors.
    We can exploit this parallel processing power very simply with compiler flags
    that instruct it to generate parallelized code. One mechanism of parallelizing
    code is to use the `OpenMP` extensions of the C/C++ language. However, this means
    changing the source code and having detailed knowledge of how to use those extensions.
    The other simpler option is a feature specific to the `gcc` compiler – it provides
    an extended standard library that implements most algorithms to run as parallel
    ones.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 如今大多数CPU都有多个核心，甚至手机也有四核处理器。我们可以通过简单地使用编译器标志来利用这种并行处理能力，让它生成并行化的代码。一种并行化代码的机制是使用C/C++语言的`OpenMP`扩展。然而，这意味着改变源代码并且需要详细了解如何使用这些扩展。另一个更简单的选择是`gcc`编译器特有的一个特性
    - 它提供了一个扩展标准库，实现了大多数算法作为并行算法运行。
- en: Note
  id: totrans-335
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: This automatic parallelization is only available for STL algorithms on gcc and
    is not part of the C++ standard. The C++ 17 standard proposes extensions to the
    standard library for parallel versions of most algorithms but is not supported
    by all compilers yet. Also, in order to take advantage of this feature, the code
    would have to be rewritten extensively.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 这种自动并行化只适用于gcc上的STL算法，并不是C++标准的一部分。C++ 17标准提出了标准库的扩展，用于大多数算法的并行版本，但并不是所有编译器都支持。此外，为了利用这个特性，代码需要进行大量重写。
- en: 'Exercise 6: Using Compiler Parallelization'
  id: totrans-337
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习6：使用编译器并行化
- en: In this exercise, we will use the `gcc` parallel extensions feature to accelerate
    standard library functions. Our aim is to understand how to use `gcc` parallel
    extensions.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将使用`gcc`的并行扩展特性来加速标准库函数。我们的目标是了解如何使用`gcc`的并行扩展。
- en: 'Perform these steps to complete this exercise:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 执行这些步骤来完成这个练习：
- en: Create a file named **Snippet4.cpp**.
  id: totrans-340
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为**Snippet4.cpp**的文件。
- en: 'Write a simple program to sum up an initialized array with `std::accumulate.`
    Add the following code to implement this:'
  id: totrans-341
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写一个简单的程序，使用`std::accumulate`来对初始化的数组进行求和。添加以下代码来实现这一点：
- en: '[PRE43]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Save the program and open the terminal. Compile the program normally and time
    the execution using the following commands:'
  id: totrans-343
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保存程序并打开终端。正常编译程序并使用以下命令计时执行：
- en: '[PRE44]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The output of the previous command is as follows:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 前一个命令的输出如下：
- en: '![Figure 8.29: Output of the code in Snippet4.cpp'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.29：Snippet4.cpp代码的输出'
- en: '](img/C14583_08_29.jpg)'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C14583_08_29.jpg)'
- en: 'Figure 8.29: Output of the code in Snippet4.cpp'
  id: totrans-348
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8.29：Snippet4.cpp代码的输出
- en: 'Now, compile the code with the parallelization options, that is, `-O3 -fopenmp`
    and `-D_GLIBCXX_PARALLEL`:'
  id: totrans-349
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，使用并行化选项编译代码，即`-O3 -fopenmp`和`-D_GLIBCXX_PARALLEL`：
- en: '[PRE45]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'The output is as follows:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 8.30: Output of the code in Snippet4.cpp compiled with parallelization
    options'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.30：使用并行化选项编译的Snippet4.cpp代码的输出'
- en: '](img/C14583_08_30.jpg)'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C14583_08_30.jpg)'
- en: 'Figure 8.30: Output of the code in Snippet4.cpp compiled with parallelization
    options'
  id: totrans-354
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8.30：使用并行化选项编译的Snippet4.cpp代码的输出
- en: In the previous output, the `user` field shows the cumulative CPU time and the
    `real` field shows the wall time. The ratio seen between the two is about `7x`.
    This ratio will vary, depending on how many CPU cores the system has (in this
    particular case, there were eight cores). For this system, the ratio could reach
    8x if the compiler was able to perform `100%` parallelization. Note that even
    though eight cores were used, the actual improvement in execution time was only
    about `1.3x`. This is probably because the allocation and initialization of the
    vector takes up most of the time. This is a case of `1.3x` speedup in our code,
    which is a very good optimization result.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have covered some of the more impactful compiler optimization features
    available in modern compilers. Apart from these, there are several other optimization
    flags, but they may not produce very large improvements in performance. Two particular
    optimization flags that apply to large projects with many different source files
    is **Link time optimization** or **Link time code generation**. These are worth
    enabling for large projects. In the next section, we'll look into source code
    micro optimizations.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: Source Code Micro Optimizations
  id: totrans-357
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'These are techniques that involve using certain idioms and patterns in the
    source code that are usually faster than their equivalents. In earlier times,
    these kinds of micro-optimizations were very fruitful, because compilers were
    not very clever. But today, compiler technology is very much advanced, and the
    effect of these micro-optimizations are not so marked. In spite of this, it is
    a very good habit to use these because they will make the code faster even if
    compiled without optimization. Even in development builds, code that is faster
    saves time when testing and debugging. We''ll look at the std::vector container
    in the next section:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: Using the std::vector Container Efficiently
  id: totrans-359
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`std::vector` is one of the most simple and useful containers in the standard
    library. It has no overhead over normal C style arrays, but has the ability to
    grow, as well as optional bounds checking. You should almost always use `std::vector`
    when the number of elements is not known at compile time.'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: A common idiom that's used with `std::vector` is to call `push_back` on it in
    a loop – as it grows, the vector reallocates a new buffer, which is larger than
    the existing one by a certain factor (the exact value of this growth factor depends
    on the standard library implementation). In theory, this reallocation has minimal
    costs because it occurs infrequently, but in practice, the operation of resizing
    in a vector involves copying the elements of its buffer to a newly allocated larger
    buffer, which can be very expensive.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: We can avoid these multiple allocations and copies by using the `reserve()`
    method. When we know how many elements a vector will contain, calling the `reserve()`
    method to pre-allocate the storage makes quite a difference. Let's implement an
    exercise in the next section to optimize vector growth.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 7: Optimizing Vector Growth'
  id: totrans-363
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will time the effect of the `push_back` method in a loop,
    with and without calling the reserve method. First, we will extract the `Timer`
    class we used in the previous sections into a separate header and implementation
    file – this will allow us to use it as common code for all the succeeding code
    snippets. Perform these steps to complete this exercise:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: Create a header file named **Timer.h**.
  id: totrans-365
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Include the necessary header files:'
  id: totrans-366
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-367
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Create a class named `Timer`. Within the `Timer` class, declare four variables,
    namely `ms_Counts`, `ms_Times`, `m_tmStart`, and `m_sName`. Declare a constructor,
    destructor, and the `dump()` method. Add the following code to implement this:'
  id: totrans-368
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-369
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Define a helper macro named `TIME_IT` to time functions by writing the following
    code:'
  id: totrans-370
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-371
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Once the header file has been created, create a new file named `dump()` method
    inside the **Timer.cpp** file. Write the following code to implement this:'
  id: totrans-372
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-373
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Now, create a new file named `1,000,000` integers using the `push_back()` method.
    The second function calls the `reserve()` method beforehand, but the first one
    does not. Write the following code to implement this:'
  id: totrans-374
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-375
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Now, write the `main` function. Note the use of redundant braces to ensure
    that the `v1` and `v2` vectors are destroyed after every iteration of the loop:'
  id: totrans-376
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-377
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: The reason we pass the vector by reference is to prevent the compiler from optimizing
    out the entire code in the two functions. If we passed the vectors by value, the
    functions would have no visible side effects and the compiler may just elide the
    functions totally.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
- en: 'Save the program and open the terminal. Compile the **Timer.cpp** and **Snippet5.cpp**
    files and run them as follows:'
  id: totrans-379
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-380
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'The output is as follows:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.31: Output of the code in Snippet5.cpp showing the effect of vector::reserve()'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C14583_08_31.jpg)'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.31: Output of the code in Snippet5.cpp showing the effect of vector::reserve()'
  id: totrans-384
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As we can see, the effect of calling `reserve()` has resulted in an improvement
    of about 4% in execution time. In a larger program that has run for a long time,
    the system memory often becomes very fragmented. In such cases, the improvement
    by pre-allocating memory with `reserve()` could be much better. In general, it
    is usually faster to reserve memory beforehand, rather than doing it incrementally
    on the fly. Even the Java Virtual Machine, for performance reasons, uses this
    technique of allocating a huge chunk of memory upfront when starting up.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
- en: Short-Circuit Logical Operators
  id: totrans-386
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `&&` and `||` logical operators are **short-circuited**, which means that
    the following:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: If the left-hand side of the `||` operator is `true`, the right-hand side is
    not evaluated.
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the left-hand side of the `&&` operator is `false`, the right-hand side is
    not evaluated.
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By keeping the more unlikely (or less expensive) expression on the left-hand
    side, we can reduce the amount of work that needs to be done. In the next section,
    we'll solve an exercise and learn how to write logical expressions optimally.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 8: Optimizing Logical Operators'
  id: totrans-391
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will examine the impact of ordering conditional expressions
    when used with logical operators. Perform these steps to complete this exercise:'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
- en: Create a new file named **Snippet6.cpp**.
  id: totrans-393
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Include the necessary libraries and the Timer.h file that we created in the
    previous exercise by writing the following code:'
  id: totrans-394
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-395
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Define a function named `sum1()` that computes the sum of the integers between
    `0` and `N`. Each number is summed only if it meets either or of two specific
    criteria. The first condition is that the number must be less than `N/2`. The
    second condition is that the number, when divided by 3, must return 2 as a remainder.
    Here, we set `N` to `100,000,000` so we have some measurable time taken by the
    code. Write the following code to implement this:'
  id: totrans-396
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-397
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Now, define another function named `sum2()`. It must contain the same logic
    that we wrote for the previous function, `sum1()`. The only change here is that
    we reverse the order of the conditional expression of the `if` statement. Write
    the following code to implement this:'
  id: totrans-398
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-399
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: Note that in the `sum2` function, the `b < N/2` condition will evaluate to true
    half of the time. Thus, the second condition, that is, `b % 3 == 2`, is only evaluated
    for half of the iterations. If we assume for simplicity that both conditionals
    take 1 unit of time, the total time taken for `sum2()` would be `N/2 + (2 * N/2)
    = N * 3/2`. In the case of the `sum1()` function, the condition on the left-hand
    side will evaluate to `true` only 33% of the time, and the remaining 66% of the
    time, both conditions will be evaluated. Thus, the estimated time taken would
    be `N/3 + (2 * N * 2/3) = N * 5/3`. We expect that the ratio between the times
    for the `sum1` and `sum2` function would be `5/3` to `3/2` – that is, `sum1` is
    `11%` slower.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
- en: 'Add the following code in the main function:'
  id: totrans-401
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-402
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Save the file and open the terminal. Compile and time the preceding program,
    as well as the **Timer.cpp** file, by writing the following commands:'
  id: totrans-403
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-404
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'The output is as follows:'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.32: Output of the code in Snippet6.cpp showing the effect of optimizing
    boolean conditions'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C14583_08_32.jpg)'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.32: Output of the code in Snippet6.cpp showing the effect of optimizing
    boolean conditions'
  id: totrans-408
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As you can see from the preceding output, we ended up with about `38%` improvement
    in speed, which is much more than expected. Why would this happen? The answer
    is that the `%` operator performs an integer division, which is much more expensive
    than a comparison, but the compiler will not generate a division instruction for
    the `N/2` expression because it is a constant value.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
- en: The `sum1()` function code executes the modulus operation for every iteration
    of the loop and the overall execution time is dominated by the division. To summarize
    this, we must always consider short-circuit logical operators and calculate how
    each side of the expression is, and how many times it exectures in order to choose
    the optimal order in which they should appear in the expression. This is equivalent
    of doing an expected value calculation of probability theory. In the next section,
    we'll learn about branch prediction.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
- en: Branch Prediction
  id: totrans-411
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Modern processors use a pipelined architecture, which is similar to a factory
    assembly line, where an instruction flows along a pipeline and is processed by
    various workers simultaneously. After each clock cycle, the instruction moves
    along the pipeline to the next stage. This means that although each instruction
    may take many cycles to go from start to finish, the overall throughput is one
    instruction completed per cycle.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
- en: The drawback here is that, if there is a conditional branch instruction, the
    CPU has no idea which set of instructions are to be loaded after that (since there
    are two possible alternatives). This condition is called a **pipeline stall**,
    and the processor must wait until the condition of the branch has been evaluated
    completely, wasting precious cycles.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
- en: To mitigate this, modern processors use something called **branch prediction**
    – they try to predict which way the branch goes. As the branch is encountered
    a greater number of times, it gets more confident as to which way the branch is
    likely to take.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
- en: Despite this, the CPU is not omniscient, so if it starts loading the instructions
    of one predicted branch, and later the conditional branch turned out to go the
    other way, the entire pipeline after the branch has to be cleared and the actual
    branch needs to be loaded from scratch. All the work done on the "`assembly line`"
    downstream of the branch instruction has to be discarded and any changes need
    to be reversed.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
- en: This is a major bottleneck for performance, and it can be avoided – the simplest
    way is to make sure a branch always goes one way as much as possible – like a
    loop.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 9: Optimization for Branch Prediction'
  id: totrans-417
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this exercise, we will explore and demonstrate the effect of CPU branch prediction
    on performance. To explore this, we'll write two functions in a program – both
    perform the same computation using two nested loops which iterate `100` and `100,000,000`
    times, respectively. The difference between the two functions is that, in the
    first function, the outer loop is the bigger one, whereas in the second function,
    the outer loop is the smaller one.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
- en: 'For the first function, the outer loop fails branch prediction only once when
    it exits, but the inner loop fails branch prediction `100,000,000` times – each
    time it exits. For the second one, once again, the outer loop fails branch prediction
    only once when it exits, but the inner loop fails branch prediction only 100 times
    – each time it exits. The factor of `1,000,000` between these branch prediction
    failure counts will result in the first function being slower than the second.
    Perform these steps to complete this exercise:'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a file named **Snippet7.cpp** and include the necessary libraries:'
  id: totrans-420
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-421
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Define a function named `sum1()` with a nested loop. The outer `for` loop should
    cycle `N` times, whereas the inner for loop should iterate `100` times. Set the
    value of `N` to `100000000`. Write the following code to implement this:'
  id: totrans-422
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  id: totrans-423
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: If we assume that the processor predicts branches in loops (statistically, the
    branch instruction at the end of the loop is more likely to jump to the start
    of the loop than not), then it will end up mispredicting every time j reaches
    `100` – in other words, `N` times.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
- en: 'Define a new function, `sum2()`, with a nested loop. The only change here is
    that we must set the inner loop count to `N` and the outer loop count to `100`.
    Add the following code to implement this:'
  id: totrans-425
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-426
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: Now, our reasoning is that the branch misprediction happens only `100` times.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
- en: 'Add the following code in the main function:'
  id: totrans-428
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  id: totrans-429
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Save the file and open the terminal. Compile the preceding program, along with
    the **Timer.cpp** file, and time them using the following commands. Remember that
    you need to have the Timer.cpp and Timer.h files you created earlier in the same
    directory:'
  id: totrans-430
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  id: totrans-431
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'The output of executing the previous command is as follows:'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.33: Output of the code in Snippet7.cpp showing the effect'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
- en: of branch prediction optimization
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C14583_08_33.jpg)'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.33: Output of the code in Snippet7.cpp showing the effect of branch
    prediction optimization'
  id: totrans-436
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As you can see from the preceding output, there is a small but certainly significant
    speedup of about `2%` that can be attributed to the processor being able to predict
    branches better for the `sum2` function. In the next section, we'll explore more
    optimization techniques.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
- en: Further Optimizations
  id: totrans-438
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Several other techniques exist that can be implemented as you code; some of
    them are not guaranteed to produce better code, but it takes very little effort
    to change your coding habits to do these reflexively. They cost nothing but may
    result in gains. A few of these techniques are as follows:'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
- en: Pass parameters that are not primitive types by `const` reference when possible.
    Even though `const` reference.
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use pre-increment (`++i`) or pre-decrement (`--i`) operators rather than the
    postfix versions. This usually has no utility for simple types such as integers
    but may do so for complex types with a custom increment operator. Getting into
    a habit of writing `++i` rather than `i++` is good practice unless post-increment
    is actually the desired behavior. Apart from performance benefits, such code declares
    the intent more clearly by using the right operator.
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Declare variables as late as possible – it is common in C to declare every variable
    at the top of a function, but in C++, since variables can have non-trivial constructors,
    it makes sense to only declare them in the actual block where they are used.
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In terms of **loop hoisting**, if there is any code or calculation in a loop
    that does not change with the loop iteration, it makes sense to move it outside
    the loop. This includes creating objects in a loop body. Often, it is more efficient
    to declare them once, outside the loop. Modern compilers do this automatically,
    but it doesn't take extra effort to do this yourself.
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use `const` wherever possible. It does not change the meaning of the code, but
    it lets the compiler make stronger assumptions about your code that may lead to
    better optimization. Apart from this, using `const` makes code more readable and
    reasonable.
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integer division, modulus, and multiplication (especially by numbers that are
    not powers of 2) are some of the slowest operations possible on X86 hardware.
    If you need to perform such operations in a loop, perhaps you can do some algebraic
    manipulation to get rid of them.
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As we mentioned, several such optimizations may be done by the compiler itself,
    but doing them as a habit makes the code fast even in debug mode, which is a big
    advantage when debugging. We have examined a few techniques for micro-optimizing
    code already – the level of code change required to do these is relatively minor,
    and some of these can produce major improvements in efficiency. If you want to
    write faster code in general, you should aim to integrate these techniques as
    a default coding style over time. In the next section, we'll learn about cache-friendly
    code.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
- en: Cache Friendly Code
  id: totrans-447
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Computer science was developed in the mid-20th century, when computers hardly
    existed, but nevertheless, by the 1980s, most of the useful data structures and
    algorithms had been discovered and refined. Algorithmic complexity analysis is
    a topic that anyone who learns computer science encounters – and there are well-accepted
    textbook definitions of the complexity of data structure operations. However,
    after 50 years since these things were analyzed, computers have evolved in a way
    that is quite different from what could have been envisaged. For example, a common
    "fact" is that the list data structures are faster for insertion operations than
    arrays. This seems like common sense because inserting an element into an array
    involves moving all the items after that point to new locations, whereas inserting
    into a list is merely a few pointer manipulations. We will test this hypothesis
    in the following exercise.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 10: Exploring the Effect of Caches on Data Structures'
  id: totrans-449
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will examine the impact of the cache on arrays and lists
    in the C++ standard library. Perform these steps to complete this exercise:'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
- en: Create a file named **Snippet8.cpp**.
  id: totrans-451
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Include the necessary libraries, along with the **Timer.h** header file. Write
    the following code to implement this:'
  id: totrans-452
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  id: totrans-453
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Create a constant integer variable, `N`, and set its value to `100000`:'
  id: totrans-454
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  id: totrans-455
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Initialize a random number generator and create a distribution range from `0`
    to `1000`. Add the following code to achieve this:'
  id: totrans-456
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  id: totrans-457
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Create a method named `insertRandom()` and insert elements from `0` to `N`
    into a container at random positions. Add the following code to implement this:'
  id: totrans-458
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  id: totrans-459
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Create a method named `insertStart()` and insert elements from `0` to `N` into
    a container at the start. Add the following code to implement this:'
  id: totrans-460
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  id: totrans-461
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Create a method named `insertEnd()` and insert elements from `0` to `N` into
    a container at the end. Add the following code to implement this:'
  id: totrans-462
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  id: totrans-463
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Write the following code in the `main` method:'
  id: totrans-464
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  id: totrans-465
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Save the file and open the terminal. Compile the preceding program, along with
    the **Timer.cpp** file, by writing the following commands:'
  id: totrans-466
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  id: totrans-467
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'The preceding command generates the following output:'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.34: Output of the code in Snippet8.cpp contrasting the timing'
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
- en: of std::list and std::vector insertion
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C14583_08_34.jpg)'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.34: Output of the code in Snippet8.cpp contrasting the timing of std::list
    and std::vector insertion'
  id: totrans-472
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As you can see from the preceding output, the code measures the time taken to
    insert `100000` integers at the start, end, and random locations for `std::vector`
    and `std::list`. The vector clearly wins by a factor of 100 or more for the random
    case, and even the worst case for the vector is 10x faster than the random case
    for the list.
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
- en: Why does this happen? The answer lies in the way modern computer architecture
    has evolved. CPU clock speeds have increased from about `1 Mhz` in the early 80s
    to `5 GHz` as of mid-2019 – a speedup of `5,000x` in clock frequency – and while
    the earliest CPUs used multiple cycles per instruction, modern ones execute several
    instructions per cycle on a single core (due to advanced techniques such as pipelining,
    which we described earlier).
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
- en: For example, the `IDIV` instruction on the original `Intel 8088` took over 100
    clock cycles to complete, whereas on modern processors, it can be completed in
    less than 5 cycles. On the other hand, RAM bandwidth (the time taken to read or
    write a byte of memory) has increased very slowly.
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
- en: Historically, processors have increased in speed by a factor of about `16,000x`
    between 1980 and 2010\. At the same time, the speed increase in RAM has been an
    order of magnitude smaller – less than 100x. Thus, it is possible that single
    access to RAM by an instruction causes the CPU to wait for a huge number of clock
    cycles. This would be an unacceptable degradation of performance, and there have
    been a lot of technologies to mitigate this issue. Before we explore this, let's
    measure the impact of memory access in the next exercise.
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 11: Measuring the Impact of Memory Access'
  id: totrans-477
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will examine the performance impact of randomly accessing
    memory. Perform these steps to complete this exercise:'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
- en: Create a new file named **Snippet9.cpp**.
  id: totrans-479
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Include the necessary libraries, along with the `SIZE` and `N`, and set their
    values to `100000000`. Also, create a random number generator and a distribution
    range from `0` to `N-1`. Write the following code to implement this:'
  id: totrans-480
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  id: totrans-481
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Create the `getPRIndex()` function, which returns a pseudo random index between
    `0` and `SIZE-1`, where `SIZE` is the number of elements in the array. Write the
    following code to implement this:'
  id: totrans-482
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  id: totrans-483
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE72]'
  id: totrans-484
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'Write a function named `sum1()` that accesses a large array of data randomly
    and sums those elements:'
  id: totrans-485
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  id: totrans-486
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Write a function named `sum2()` that sums random numbers without any memory
    access:'
  id: totrans-487
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  id: totrans-488
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'In the main function, initialize the vector so that `v[i] == i`, thus, the
    only difference between `sum1()` and `sum2()` is that `sum1()` accesses memory
    and `sum2()` only performs computations. As usual, we use volatile to prevent
    the compiler from removing all the code, since it has no side effects. Write the
    following code in the `main()` function:'
  id: totrans-489
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  id: totrans-490
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Save the program and open the terminal. Compile and run the program by writing
    the following commands:'
  id: totrans-491
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  id: totrans-492
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'The preceding code generates the following output:'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.35: Output of the code in Snippet9.cpp contrasting the timing'
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
- en: of computation versus random memory access
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C14583_08_35.jpg)'
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.35: Output of the code in Snippet9.cpp contrasting the timing of computation
    versus random memory access'
  id: totrans-497
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: From the preceding output, we can clearly see a factor of about `14x` difference
    in performance.
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new file named `sum3()` that accesses memory linearly instead of randomly.
    Also, edit the main function. The updated code is as follows:'
  id: totrans-499
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  id: totrans-500
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Save the file and open the Terminal. Compile and run the program:'
  id: totrans-501
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  id: totrans-502
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'The preceding commands generate the following output:'
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.36: Output of the code in Snippet10.cpp contrasting the timing'
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
- en: of computation versus random and linear memory access
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C14583_08_36.jpg)'
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.36: Output of the code in Snippet10.cpp contrasting the timing of
    computation versus random and linear memory access'
  id: totrans-507
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In the preceding output, notice that the memory access is now more than `35`
    times faster than before, and `2.5` times faster than the calculation in `sum2()`.
    We used the random access pattern in `sum1()` to demonstrate the contrast between
    linear and random memory access. What makes linear memory access so much faster
    than random access? The answer lies in two mechanisms in modern processors that
    are used to mitigate the effects of slow memory – **caching** and **prefetch**
    – both of which we will discuss in the following sections.
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
- en: Caching
  id: totrans-509
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Modern processors have multiple layers of cache memory between the processor
    registers and the RAM. These caches are labeled L1, L2, L3, L4, and so on, where
    L1 is closest to the processor and L4 is the furthest. Every cache layer is faster
    (and usually smaller) than the level below it. Here is an example of the cache/memory
    sizes and latencies for a `Haswell` family processor:'
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
- en: 'L1: 32 KB, 4 cycles'
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'L2: 256 KB, 12 cycles'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'L3: 6 MB, 20 cycles'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'L4: 128 MB, 58 cycles'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'RAM: many GB, 115 cycles'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A simple model of how caches work to help performance is as follows: when a
    memory address is accessed, it is looked up in the L1 cache – if found, it is
    retrieved from there. If not, it is looked up in the L2 cache, if not found, then
    the L3 cache and so on – if it wasn''t found in any of the caches, it is fetched
    from memory. When fetched from memory, it is stored in each of the caches for
    faster access later. This method in itself would be fairly useless because it
    would only improve performance if we accessed the same memory address again and
    again.The second aspect, called **prefetching**, is the mechanism that can make
    caches really pay off.'
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
- en: Prefetching
  id: totrans-517
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Prefetching is a process where, when memory access is performed, nearby data
    is also fetched into caches, even though it was not accessed directly. The first
    aspect of prefetching is related to memory bus granularity – it can be thought
    of as "What is the minimum amount of data that the RAM subsystem can send to the
    processor?". In most modern processors, this is 64 bits – in other words, whether
    you ask for a single byte or a 64-bit value from memory, the entire `machine word`
    of 64 bits that includes that address is read from RAM. This data is stored in
    each layer of cache for faster access later. Obviously, this would immediately
    improve memory performance – say we read a byte of memory at address `0x1000`;
    we also get the 7 other bytes after that address into the caches. If we then access
    the byte at address `0x1001`, it comes from the cache, avoiding expensive RAM
    access.
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
- en: The second aspect of prefetch takes this one step further – when the contents
    of the RAM at an address is read, the processor reads not only that memory word,
    but much more. On the x86 family of processors, this is between 32 and 128 bytes.
    This is called the **cache line** size – the processor always writes and reads
    memory in chunks of that size. When the CPU hardware detects that memory is being
    accessed in a linear fashion, it prefetches memory into one cache line, based
    on its prediction of what addresses are likely to be accessed subsequently.
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
- en: CPUs are very clever in detecting regular access patterns both forwards and
    backwards, and will prefetch efficiently. You can also provide hints to the processor
    using special instructions to make it prefetch data according to the programmer's
    direction. These instructions are provided as intrinsic functions on most compilers
    in order to avoid the use of inline assembly language. When a memory address is
    read or written that is not in a cache, it is termed a **cache miss**, and is
    a very expensive event and to be avoided at all costs. The CPU hardware tries
    its best to mitigate cache misses, but the programmer can analyze and modify the
    data access patterns to reduce cache misses maximally. The description of caching
    here is a simplified model for instructional purposes – in reality, CPUs have
    L1 caches for instructions as well as data, multiple cache lines, and very complex
    mechanisms to make sure that multiple processors can keep their separate caches
    in synchronization.
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-521
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A comprehensive description of cache implementations (and lots of other information
    about memory subsystems) can be found in this famous online article: [https://lwn.net/Articles/250967/](https://lwn.net/Articles/250967/).'
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
- en: Effects of Caching on Algorithms
  id: totrans-523
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Having learned about caches, we can now reason why our first example of vector
    versus list showed surprising results – from a computer science perspective, the
    following is true:'
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
- en: '**For a list**:'
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
- en: Iterating to the Nth position is order N complexity.
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inserting or deleting an element is an order 1 complexity.
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**For an array (or vector)**:'
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
- en: Iterating to the Nth position is order 1 complexity.
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inserting or deleting an element at location N has complexity proportional to
    (S - N), where S is the size of the array.
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, for modern architectures, the cost of a memory access is extremely
    high, but the cost of accessing an adjacent address subsequently is almost 0 because
    it would already be in the cache. This means that the iteration upon elements
    in a `std::list` that are located non-sequentially in memory are likely to always
    cause a cache miss, causing slow performance. On the other hand, since the elements
    of an array or `std::vector` are always adjacent, caching and prefetching would
    reduce the overall cost of copying (S-N) elements to a new location by a very
    large margin. Hence, the traditional analysis of the two data structures that
    declares that lists work better for random insertions, while technically correct,
    is not practically true, especially given the clearly sophisticated caching behavior
    of modern CPU hardware. When our programs are *data bound*, the analysis of algorithm
    complexity has to be augmented by understanding of what is known as **data locality**.
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
- en: Data locality can be defined simply as the average distance from the memory
    address that was just accessed to the one that was accessed previously. In other
    words, making memory access across addresses that are far from each other is a
    severe slowdown, since data from closer addresses are likely to have been prefetched
    into the caches. When data is already present in the cache(s), it is termed "hot";
    otherwise, it is termed "cold". Code that takes advantage of the cache is termed
    **cache friendly**. Cache-unfriendly code, on the other hand would cause the cache
    lines to be wastefully reloaded (termed **cache invalidation**). In the remainder
    of this section, we will look at strategies regarding how to write cache-friendly
    code.
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing for Cache-Friendliness
  id: totrans-533
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the old days, optimization of code involved trying to minimize the number
    of machine instructions in code, using more efficient instructions, and even reordering
    instructions to allow pipelines to remain full. As of this day and age, compilers
    perform all the aforementioned optimization to a level that most programmers would
    be unable to – especially considering that compilers can do it across entire programs
    of hundreds of millions of instructions. What remains firmly the responsibility
    of the programmer even now is the ability to optimize data access patterns to
    take advantage of caching.
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
- en: The task is very simple – make sure that memory is accessed close to the memory
    that was accessed before – but the methodology to achieve this can require lots
    of effort.
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-536
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The famous game programmer and code optimization guru Terje Mathisen, in the
    90s, is claimed to have said: "All programming is an exercise in caching." Today,
    in 2019, this statement applies more than ever in this sub-domain of trying to
    write fast code.'
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
- en: 'There are some basic rules of thumb for increasing cache-friendliness:'
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
- en: The stack is always "hot", and so we should use local variables as much as possible.
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dynamically allocated objects rarely have data locality with each other – avoid
    them or use a preallocated pool of objects so they are in sequential in memory.
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pointer-based data structures such as trees – and especially lists – consist
    of multiple nodes allocated on the heap, and are very cache unfriendly.
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Runtime dispatch of virtual functions in OO code invalidates the instruction
    cache – avoid a dynamic dispatch in performance-critical code.
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next section, we'll explore the cost of heap allocations.
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 12: Exploring the Cost of Heap Allocations'
  id: totrans-544
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will examine the performance impact of dynamically allocated
    memory and examine how heap memory affects the code''s performance. Perform these
    steps to complete this exercise:'
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
- en: Create a file named **Snippet11.cpp**.
  id: totrans-546
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Add the following code to include the necessary libraries:'
  id: totrans-547
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  id: totrans-548
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'Declare a constant variable, N, and a character array called fruits. Assign
    values to them:'
  id: totrans-549
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  id: totrans-550
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'Create a function named `fun1()` that just loops over each string in fruits,
    copies it to a string, and sums the characters of that string:'
  id: totrans-551
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  id: totrans-552
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'Create another function named `sum2()` that uses a locally declared character
    array instead of a string and a loop to copy:'
  id: totrans-553
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  id: totrans-554
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'Write the following code inside the `main()` function:'
  id: totrans-555
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  id: totrans-556
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'Save the file and open the terminal. Compile and run the program:'
  id: totrans-557
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  id: totrans-558
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'The preceding commands generate the following output:'
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.37: Output of the code in Snippet11.cpp showing the effect of heap
    allocation on the timing'
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C14583_08_37.jpg)'
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.37: Output of the code in Snippet11.cpp showing the effect of heap
    allocation on the timing'
  id: totrans-562
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: From the preceding output, notice that `fun2()` is almost twice as fast as `fun1()`.
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, use the `perf` command to profile:'
  id: totrans-564
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE85]'
  id: totrans-565
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'The preceding command generates the following output:'
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.38: Output of the perf command profiling the code in Snippet11.cpp'
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C14583_08_38.jpg)'
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.38: Output of the perf command profiling the code in Snippet11.cpp'
  id: totrans-569
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now, we can check the performance report with the following code:'
  id: totrans-570
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE86]'
  id: totrans-571
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'We receive the following output:'
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/C14583_08_39.jpg)'
  id: totrans-573
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.39: Output of the perf command''s timing report for the code in Snippet11.cpp'
  id: totrans-574
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In the preceding output, notice that about `33%` of the execution time was taken
    by the `std::string` constructor, `strlen()`, and `memmove()`. All of these are
    associated with the `std::string` that was used in `fun1()`. The heap allocation
    in particular is the slowest operation.
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
- en: Struct of Arrays Pattern
  id: totrans-576
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In many programs, we often use an array of objects of the same type – these
    could represent records from a database, entities in a game, and so on. A common
    pattern is to iterate through a large array of structures and perform an operation
    on some fields. Even though the structs are sequential in memory, if we access
    only a few of fields, a larger size structure will make caching less effective.
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
- en: The processor may prefetch several structs into cache, but the program only
    accesses a fraction of that cached data. Since it is not using every field of
    each struct, most of the cached data is discarded. To avoid this, another kind
    of data layout can be used – instead of using an **array of structs** (**AoS**)
    pattern, we use a **struct of arrays** (**SoA**) pattern. In the next section,
    we'll solve an exercise wherein we'll examine the performance benefit of using
    the SoA pattern versus the AoS pattern.
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 13: Using the Struct of Arrays Pattern'
  id: totrans-579
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will examine the performance benefits of using the SoA
    versus AoS pattern. Perform these steps to complete this exercise:'
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
- en: Create a file named **Snippet12.cpp**.
  id: totrans-581
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Include the necessary libraries, along with the `Timer.h` header file. Initialize
    a random number generator and also create a distribution range from 1 to N-1\.
    Create a constant integer variable, N, and initialize it with a value of 100,000,000\.
    Add the following code to implement this:'
  id: totrans-582
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE87]'
  id: totrans-583
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'Write two different ways to represent data -– anarray of structs and a struct
    of arrays. Use six fields of `uint64_t` so that we can emulate a large-sized structure
    that would be more representative of a real- world program:'
  id: totrans-584
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE88]'
  id: totrans-585
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'Define two functions, namely `sumAOS` and `sumSOA`, that sum the values in
    `field1`, `field2`, and `field3` for the two preceding data structures. Write
    the following code to implement this:'
  id: totrans-586
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE89]'
  id: totrans-587
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'Write the following code inside the `main` function:'
  id: totrans-588
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE90]'
  id: totrans-589
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'Save the program and open the Terminal. Run the program to time it by adding
    the following commands:'
  id: totrans-590
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE91]'
  id: totrans-591
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'The preceding code generates the following output:'
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.40: Output of the code in Snippet12.cpp contrasting the timing'
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
- en: of the AOS and SOA patterns
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C14583_08_40.jpg)'
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.40: Output of the code in Snippet12.cpp contrasting the timing of
    the AOS and SOA patterns'
  id: totrans-596
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The struct of arrays approach is twice as fast as the array of structs approach.
    Considering that the addresses of the vectors in the struct would be quite far
    apart, we may wonder why the caching behavior is better in the SoA case. The reason
    is because of how caches are designed – rather than treating a cache as a single
    monolithic block, it is divided into multiple lines, as we discussed earlier.
    When a memory address is accessed, the 32- or 64-bit address is converted into
    a "tag" of a few bits and the cache line associated with that tag is used. Memory
    addresses that are very close together will get the same tag and reach the same
    cache line. If a highly differing address is accessed, it reaches a different
    cache line. The effect of this line-based cache design on our test program is
    that it is as if we have separate independent caches for each vector.
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
- en: The preceding explanation for cache lines is a very much simplified one, but
    the basic concept of cache lines applies. Code readability may seem slightly worse
    for this structure of array pattern, but considering the increase in performance,
    it is well worth it. This particular optimization becomes more effective as the
    size of the structure grows larger. Also, remember that padding structures can
    inflate their size by a big factor if the fields are of various sizes. We have
    explored the performance effects of memory latency and learned a few ways to help
    the processor's caches be effective. When writing a program that is performance-critical,
    we should keep caching effects in mind. Sometimes, it makes sense to start out
    with a more cache-friendly architecture in the first place. As always, we should
    always measure the performance of code before we attempt to make radical changes
    in data structures. Optimization should be focused on the most time-consuming
    areas of a program and not every part of it.
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
- en: Algorithmic Optimizations
  id: totrans-599
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The simplest form of algorithmic optimization is to look for libraries that
    perform your task – the most popular libraries are highly optimized and well-written.
    For example, the `Boost` library provides many useful libraries that can come
    in handy in many projects, such as `Boost.Geometry`, `Boost.Graph`, `Boost.Interval`,
    and `Boost.Multiprecision`, to mention a few. It is far easier and wiser to use
    a professionally written library than to attempt to create them yourself. For
    example, `Boost.Graph` implements a dozen algorithms to process topological graphs,
    and each of them is highly optimized.
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
- en: 'Many computations can be reduced to a series of standard algorithms composed
    together – if done correctly, these can result in extremely efficient code – and
    often even be parallelized to take advantage of multiple cores or SIMD by the
    compiler. For the rest of this section, we will take one single program and attempt
    to optimize it in various ways – this will be a word count program with the following
    specifications:'
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
- en: To isolate the time taken by disk I/O, we will read the entire file to memory
    before processing.
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unicode support will be ignored, and we will assume English text in ASCII.
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will use a large public domain literary text available online as test data.
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Exercise 14: Optimizing a Word Count Program'
  id: totrans-605
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this lengthy exercise, we'll optimize a program using various optimization
    techniques. We'll perform the incremental optimization of the practical program.
    The test data that we will be using consists of the book named "A Tale of Two
    Cities", which has been appended together 512 times.
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-607
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The dataset that''s used in this exercise is available here: [https://github.com/TrainingByPackt/Advanced-CPlusPlus/blob/master/Lesson8/Exercise14/data.7z](https://github.com/TrainingByPackt/Advanced-CPlusPlus/blob/master/Lesson8/Exercise14/data.7z).
    You will need to extract this 7zip archive and copy the resulting file, called
    data.txt, into the folder where you work with this exercise.'
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform these steps to complete this exercise:'
  id: totrans-609
  prefs: []
  type: TYPE_NORMAL
- en: Write the basic boilerplate code that reads the file (the full code can be found
    in `main()` itself to get the overall execution time.
  id: totrans-610
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note that `push_back` adds a space at the end – this makes sure that the data
    ends with a whitespace, simplifying the algorithms we use.
  id: totrans-611
  prefs: []
  type: TYPE_NORMAL
- en: 'Write a basic word count function. The logic is very simple – for every character
    in the string, if the character is not whitespace and the following one is, then
    it is the end of a word and should be counted. Since our boilerplate code has
    added a space at the end, any final word will be counted. This function is defined
    in **Snippet13.cpp**:'
  id: totrans-612
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE92]'
  id: totrans-613
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: 'Let''s compile, run, and get an idea of the performance. We will verify that
    it is working right by comparing the result of our code with the results provided
    by the standard `wc` program:'
  id: totrans-614
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE93]'
  id: totrans-615
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: 'We receive the following output:'
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.41: Output of the code in Snippet13.cpp with a baseline wordcount
    implementation'
  id: totrans-617
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C14583_08_41.jpg)'
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.41: Output of the code in Snippet13.cpp with a baseline wordcount
    implementation'
  id: totrans-619
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Let''s time the wc program:'
  id: totrans-620
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]'
  id: totrans-621
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: 'We receive the following output:'
  id: totrans-622
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.42: Output of timing the wc program'
  id: totrans-623
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C14583_08_42.jpg)'
  id: totrans-624
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.42: Output of timing the wc program'
  id: totrans-625
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The *wc* program displays the same word count, that is, `71108096`, so we know
    our code is correct. Our code took about `3.6 seconds`, including reading the
    file, which is much slower than wc.
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
- en: 'Our first strategy to optimize is to see if there is a better way to implement
    `isspace()`. Instead of a function, we can use a lookup table that can tell if
    a character is a space or not (the code for this can be found in **Snippet14.cpp**):'
  id: totrans-627
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE95]'
  id: totrans-628
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: 'Remember that boolean variables in C/C++ take on integer values 0 or 1, and
    so we can directly write the following:'
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE96]'
  id: totrans-630
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: 'This means we don''t have to write this:'
  id: totrans-631
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE97]'
  id: totrans-632
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: Using booleans directly as numbers can sometimes result in faster code because
    we avoid the conditional logic operators && and ||, which may result in a branch
    instruction.
  id: totrans-633
  prefs: []
  type: TYPE_NORMAL
- en: 'Compile and test the performance now:'
  id: totrans-634
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE98]'
  id: totrans-635
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: 'We receive the following output:'
  id: totrans-636
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.43: Output of the code in Snippet14.cpp'
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C14583_08_43.jpg)'
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.43: Output of the code in Snippet14.cpp'
  id: totrans-639
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We have achieved a speedup of 8x for the word counting code, with the simple
    principle of using a lookup table. Can we do even better than this? Yes – we can
    take the lookup table concept further – for every pair of characters, there are
    four possibilities, which should result in a corresponding action:'
  id: totrans-640
  prefs: []
  type: TYPE_NORMAL
- en: '[Space Space]: No action, [Non-Space Space]: Add 1 to count, [Space Non-Space]:
    No action, [Non-Space, Non-Space]: No action'
  id: totrans-641
  prefs: []
  type: TYPE_NORMAL
- en: So, we can manufacture a table of `65536` entries (`256 * 256`) to cover all
    possible pairs of characters.
  id: totrans-642
  prefs: []
  type: TYPE_NORMAL
- en: 'Write the following code to create the table:'
  id: totrans-643
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE99]'
  id: totrans-644
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: The loop to count the words becomes the following (the full code can be found
    in `memcpy()`. The compiler is smart enough to use the CPU memory access instructions
    rather than actually call `memcpy()` for 2 bytes. We have ended up with the loop
    containing no conditional statement, which should make it much faster. Remember
    that X86 architecture is *little-endian* – so a 16-bit value read from a character
    array will have the first character as its LSB and the second as the MSB.
  id: totrans-645
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, time the code we wrote:'
  id: totrans-646
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE100]'
  id: totrans-647
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: '![Figure 8.44: Output of the code in Snippet15.cpp'
  id: totrans-648
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C14583_08_44.jpg)'
  id: totrans-649
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.44: Output of the code in Snippet15.cpp'
  id: totrans-650
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This larger lookup table resulted in a 1.8x speed improvement for `wordCount()`.
    Let's step back and look at this from another angle so that we can use existing
    the standard library effectively. The advantages of this are two-fold – firstly,
    the code is less prone to errors, and secondly, we could take advantage of the
    parallelization available with some compilers.
  id: totrans-651
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s rewrite the version of the program that uses the lookup table for `isspace`
    using the standard algorithms. If we look at the main loop that counts the words,
    we are taking 2 characters, and depending on some logic, we are accumulating 1
    or 0 into the `count` variable. This is a common pattern seen in a lot of code:'
  id: totrans-652
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE101]'
  id: totrans-653
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
- en: Here, `a` and `b` are arrays of size `N`, `X` is an initial value, and `OP`
    and `OP2` are operators. There is a standard algorithm that encapsulates this
    pattern called `std::inner_product` – it takes two sequences, applies an operator
    (OP2) between each pair of elements, and applies another operator (OP) across
    these, starting with an initial value X.
  id: totrans-654
  prefs: []
  type: TYPE_NORMAL
- en: We can write the function as follows (the full code can be found in `inner_product()`
    call applies the `isWordEnd()` lambda on every `s[n]` and `s[n+1]` and applies
    the standard addition function between the results of these. In effect, we are
    adding 1 to the total when `s[n]` and `s[n+1]` are on a word ending.
  id: totrans-655
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  id: totrans-656
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Even though this looks like a number of nested function calls, the compiler
    inlines everything and there is no overhead.
  id: totrans-657
  prefs: []
  type: TYPE_NORMAL
- en: 'Compile and time the execution of this version:'
  id: totrans-658
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE102]'
  id: totrans-659
  prefs: []
  type: TYPE_PRE
  zh: '[PRE102]'
- en: 'We receive the following output:'
  id: totrans-660
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.45: Output of the code in Snippet16.cpp'
  id: totrans-661
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C14583_08_45.jpg)'
  id: totrans-662
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.45: Output of the code in Snippet16.cpp'
  id: totrans-663
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Surprisingly, the code is slightly faster than our initial looped version in
    **Snippet14.cpp**.
  id: totrans-664
  prefs: []
  type: TYPE_NORMAL
- en: Can we adapt the same code to use the large lookup table? Indeed, we can – the
    new function looks like this (the full code can be found in `memcpy()` to convert
    two consecutive bytes into a word, we use a bitwise `OR` operator to combine them.
  id: totrans-665
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Compile and time the code:'
  id: totrans-666
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE103]'
  id: totrans-667
  prefs: []
  type: TYPE_PRE
  zh: '[PRE103]'
- en: 'We receive the following output:'
  id: totrans-668
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.46: Output of the code in Snippet17.cpp'
  id: totrans-669
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C14583_08_46.jpg)'
  id: totrans-670
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.46: Output of the code in Snippet17.cpp'
  id: totrans-671
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This code is not quite as fast as the loop0based version we had in `short` to
    get the index, which requires no computation, but here, we read 2 bytes into a
    `short` with a bitwise operation.
  id: totrans-672
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have the code where the bulk of the work is done by a standard
    library function, we can now get automatic parallelization for free – compile
    and test as follows:'
  id: totrans-673
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE104]'
  id: totrans-674
  prefs: []
  type: TYPE_PRE
  zh: '[PRE104]'
- en: 'We receive the following output:'
  id: totrans-675
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.47: Output of the code in Snippet17.cpp with the parallelized standard
    library'
  id: totrans-676
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C14583_08_47.jpg)'
  id: totrans-677
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.47: Output of the code in Snippet17.cpp with the parallelized standard
    library'
  id: totrans-678
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Clearly, it cannot be completely parallelized, so we only get about 2.5x improvement
    in terms of speed, but we got it without having to do anything to the code. Could
    we have made the loop-based code parallelizable in the same way? In theory, yes
    – we could manually use **OpenMP** directives to achieve this; however, it would
    require changes to the code and a knowledge of how to use OpenMP. What about the
    version in **Snippet16.cpp**?
  id: totrans-679
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE105]'
  id: totrans-680
  prefs: []
  type: TYPE_PRE
  zh: '[PRE105]'
- en: 'We receive the following output:'
  id: totrans-681
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.48: Output of the code in Snippet16.cpp with the parallelized standard
    library'
  id: totrans-682
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C14583_08_48.jpg)'
  id: totrans-683
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.48: Output of the code in Snippet16.cpp with the parallelized standard
    library'
  id: totrans-684
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Similar improvements can be seen for this version too. Are we finished or can
    this be even faster? **Michael Abrash**, a famous game programmer, coined the
    acronym **TANSTATFC** – it stands for "There ain't no such thing as the fastest
    code". What he meant is that, given enough effort, it was always possible to make
    code faster. This seems impossible, but time and again, people have found faster
    and faster ways of performing a computation – our code is no exception and we
    can still go a bit further. One of the tradeoffs we can make for optimization
    is to make the code less general – we already put some constraints on our code
    – for example, that we only handle **ASCII** English text. By adding some more
    constraints on the input data, we can do even better. Let's assume that there
    are no non-printable characters in the file. This is a reasonable assumption for
    our input data. If we assume this, then we can simplify the condition for detecting
    spaces – since all the whitespace characters are greater than or equal to ASCII
    32, we can avoid the lookup table itself.
  id: totrans-685
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s implement the code based on our previous idea (the full code can be
    found in **Snippet18.cpp**):'
  id: totrans-686
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE106]'
  id: totrans-687
  prefs: []
  type: TYPE_PRE
  zh: '[PRE106]'
- en: 'Compile and run the program:'
  id: totrans-688
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE107]'
  id: totrans-689
  prefs: []
  type: TYPE_PRE
  zh: '[PRE107]'
- en: 'We receive the following output:'
  id: totrans-690
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.49: Output of the code in Snippet18.cpp with simplified logic for
    detecting spaces'
  id: totrans-691
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C14583_08_49.jpg)'
  id: totrans-692
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.49: Output of the code in Snippet18.cpp with simplified logic for
    detecting spaces'
  id: totrans-693
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This version is twice as fast as the one that was parallelized, and it is just
    a few lines of code. Will using parallelization improve it even more?
  id: totrans-694
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE108]'
  id: totrans-695
  prefs: []
  type: TYPE_PRE
  zh: '[PRE108]'
- en: 'We receive the following output:'
  id: totrans-696
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.50: Output of the code in Snippet18.cpp with the parallelized standard
    library'
  id: totrans-697
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C14583_08_50.jpg)'
  id: totrans-698
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.50: Output of the code in Snippet18.cpp with the parallelized standard
    library'
  id: totrans-699
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Unfortunately, this is not the case – it is actually slower. The overhead of
    managing multiple threads and thread contention is sometimes more expensive than
    the benefits of multithreaded code. At this point, we can see that the file-read
    code is taking up most of the time – can we do anything about this?
  id: totrans-700
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s change the `main()` function to time the individual parts of it (the
    full code can be found in **SnippetWC2.cpp**):'
  id: totrans-701
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE109]'
  id: totrans-702
  prefs: []
  type: TYPE_PRE
  zh: '[PRE109]'
- en: 'Compile and run the preceding code:'
  id: totrans-703
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE110]'
  id: totrans-704
  prefs: []
  type: TYPE_PRE
  zh: '[PRE110]'
- en: 'We receive the following output:'
  id: totrans-705
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.51: Output of the code in Snippet18.cpp with all operations timed'
  id: totrans-706
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C14583_08_51.jpg)'
  id: totrans-707
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.51: Output of the code in Snippet18.cpp with all operations timed'
  id: totrans-708
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The majority of the time is taken by `push_back()` and copying the string.
    Since the string is exactly the size of the file, `push_back()` ends up allocating
    a new buffer for the string and copying the contents. How can we eliminate this
    `push_back()` call? We appended a space to the end to be able to consistently
    count the last word, if any, since our algorithm counts the ends of words. There
    are three ways to avoid this: count the start of a word, rather than the end;
    count the last word, if any, separately; and use the `c_str()` function so that
    we have a `NUL` character at the end. Let''s try each of these in turn now.'
  id: totrans-709
  prefs: []
  type: TYPE_NORMAL
- en: 'First, write the main function without `push_back` (the full code can be found
    in **SnippetWC3.cpp**):'
  id: totrans-710
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE111]'
  id: totrans-711
  prefs: []
  type: TYPE_PRE
  zh: '[PRE111]'
- en: 'Change the code in wordCount() by renaming `isWordEnd()` to `isWordStart()`
    and invert the logic. Consider a word as starting, if the current character is
    a space and the succeeding one is not. Also, count one extra word if the string
    starts with a non-space (the full code can be found in **Snippet19.cpp**):'
  id: totrans-712
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE112]'
  id: totrans-713
  prefs: []
  type: TYPE_PRE
  zh: '[PRE112]'
- en: 'Now, write the second alternative – to count the last word, if any. The code
    is almost same as the **Snippet18.cpp** version, except we check for the last
    word (the full code can be found in **Snippet20.cpp**):'
  id: totrans-714
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE113]'
  id: totrans-715
  prefs: []
  type: TYPE_PRE
  zh: '[PRE113]'
- en: Write the third version that uses `c_str()` – all we need to do is change the
    parameters for `inner_product()` (the full code can be found in `c_str()` has
    a `NUL` at the end, it works as before.
  id: totrans-716
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Compile and time all three versions:'
  id: totrans-717
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE114]'
  id: totrans-718
  prefs: []
  type: TYPE_PRE
  zh: '[PRE114]'
- en: 'We receive the following output:'
  id: totrans-719
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.52: Output of the code in Snippet19.cpp, which counts the beginnings'
  id: totrans-720
  prefs: []
  type: TYPE_NORMAL
- en: of words rather than the ends
  id: totrans-721
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C14583_08_52.jpg)'
  id: totrans-722
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.52: Output of the code in Snippet19.cpp, which counts the beginnings
    of words rather than the ends'
  id: totrans-723
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now enter the following command:'
  id: totrans-724
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE115]'
  id: totrans-725
  prefs: []
  type: TYPE_PRE
  zh: '[PRE115]'
- en: 'We receive the following output:'
  id: totrans-726
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.53: Output of the code in Snippet20.cpp'
  id: totrans-727
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C14583_08_53.jpg)'
  id: totrans-728
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.53: Output of the code in Snippet20.cpp'
  id: totrans-729
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now enter the following command:'
  id: totrans-730
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE116]'
  id: totrans-731
  prefs: []
  type: TYPE_PRE
  zh: '[PRE116]'
- en: 'We receive the following output:'
  id: totrans-732
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.54: Output of the code in Snippet21.cpp'
  id: totrans-733
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C14583_08_54.jpg)'
  id: totrans-734
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.54: Output of the code in Snippet21.cpp'
  id: totrans-735
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: All three run in approximately the same time – the minor difference of a few
    milliseconds can be ignored.
  id: totrans-736
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can tackle the time taken for string copying – instead of using `std::stringstream`,
    we will directly read the file into a string buffer (the full code can be found
    in **SnippetWC4.cpp**):'
  id: totrans-737
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE117]'
  id: totrans-738
  prefs: []
  type: TYPE_PRE
  zh: '[PRE117]'
- en: 'Compile and run this version:'
  id: totrans-739
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE118]'
  id: totrans-740
  prefs: []
  type: TYPE_PRE
  zh: '[PRE118]'
- en: 'We receive the following output:'
  id: totrans-741
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.55: Output of the code with changed file load code in SnippetWC4.cpp'
  id: totrans-742
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C14583_08_55.jpg)'
  id: totrans-743
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.55: Output of the code with changed file load code in SnippetWC4.cpp'
  id: totrans-744
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We have now reduced the time taken by our file read code from about 1,000 ms
    to 250 ms – a 4x improvement. The word count code started at about `2,500ms` and
    reduced to about 60 ms – a 40x improvement. The total performance improvement
    for the entire program is 3.6x. We can still ask if this is the limit – indeed,
    TANSTATFC still applies and there are a few more things that can be done: instead
    of reading data into a `std::string`, use `memory-mapped I/O` to get a buffer
    that directly points to the file. This could possibly be faster than allocation
    and reading – it will require changing the word count code to accept a `const
    char*` and a length, or an `std::string_view`. Use a different, faster allocator
    to allocate memory. Compile for the native CPU using the `-march=native` flag.
    However, it seems unlikely that we will be able to get very large performance
    gains from this, since these optimizations have nothing to do with the word counting
    algorithm itself. Another final attempt could be to forego the C++ constructs
    and write inline SIMD code using `compiler intrinsics` (these are the functions
    that the compiler translates directly into single assembly instructions). The
    knowledge that''s required to do this is beyond the scope of this introductory
    material.'
  id: totrans-745
  prefs: []
  type: TYPE_NORMAL
- en: 'Nevertheless, for the curious student, an `AVX2` (256-bit SIMD) version of
    `wordCount()` is provided (Snippet23.cpp). This version needs the input string
    to have a length that is a multiple of 32 and a space at the end. This means that
    the main function has to be rewritten (SnippetWC5.cpp):'
  id: totrans-746
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE119]'
  id: totrans-747
  prefs: []
  type: TYPE_PRE
  zh: '[PRE119]'
- en: 'We receive the following output:'
  id: totrans-748
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.56: Output of the code in Snippet22.cpp that uses SIMD intrinsics'
  id: totrans-749
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C14583_08_56.jpg)'
  id: totrans-750
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.56: Output of the code in Snippet22.cpp that uses SIMD intrinsics'
  id: totrans-751
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note that we need to use the `-march=native` flag so that the compiler uses
    the AVX SIMD instruction set. If the processor does not support it, a compile
    error will result. If this executable is compiled for an AVX target, and run on
    a system where the processor does not support those instructions, the program
    crashes with an "Illegal instruction" exception. There seems to be a very small
    improvement, but not significant – the effort and learning curve required to optimize
    with the assembler or SIMD is usually too high to be justified unless your application
    or industry has those demands. The SIMD version processes 32 bytes at a time –
    yet there is practically no performance improvement. In fact, if you check the
    generated assembly code for the regular C++ implementation in the other snippets
    with the compiler explorer, you will see that the compiler itself has used SIMD
    – this just goes to show how far compilers go in terms of making your code fast.
  id: totrans-752
  prefs: []
  type: TYPE_NORMAL
- en: Another point to note is that our file read and memory allocation is taking
    up most of the time now – leaving aside memory allocation, we can conclude that
    our code has become **I/O bound** as opposed to **CPU bound**. This means that
    no matter how fast we write the code, it will be limited by how fast the data
    can be fetched. We started with a very simple implementation of a word count algorithm,
    increased its complexity and speed, and finally were able to go back to a very
    simple implementation that ended up being the fastest. The overall speed improvement
    for the algorithm was a factor of 40x. We used a number of approaches that ranged
    from just rearranging code a bit, to reimagining the problem in different ways,
    to performing micro-optimizations. No single approach can work all the time, and
    optimization remains a creative endeavor that needs imagination and skill and
    often, lateral thinking. As compilers get smarter and smarter, it gets harder
    and harder to outdo them – yet, the programmer is the only one who actually understands
    the code's intent, and there is always scope to make the code faster.
  id: totrans-753
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 1: Optimizing a Spell Check Algorithm'
  id: totrans-754
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this activity, we will attempt to optimize a program step by step. This activity
    is about a simple spell checker that takes a dictionary and a text file and prints
    out a list of the words in the text that are not present in the dictionary. A
    basic skeleton program is provided in `7zip archive`, that is, `activity1.7z`.
  id: totrans-755
  prefs: []
  type: TYPE_NORMAL
- en: The dictionary is taken from the Linux word list that is provided with many
    Linux distributions. The text file is like the one we used in the previous exercise
    – it is the same large file we used in the word count exercise, with all punctuation
    removed and converted into lower case.
  id: totrans-756
  prefs: []
  type: TYPE_NORMAL
- en: Note that the dictionary is only an example, so do not assume that all valid
    words exist in it – many of the words in the output could well be correctly spelled
    words. The skeleton code reads the dictionary and text files and calls the spell
    check code (which you will write) on it. After that, it compares the resultant
    output with the contents of **out.txt** and prints whether the program worked
    as expected. The function that does the spell check returns a vector of indices
    of the words that were not in the dictionary. Since we are focusing on only the
    spellcheck algorithm, only that code is timed. The time taken for reading the
    files and comparing the output is not taken into consideration. You will develop
    successively faster versions of this program – reference implementations are provided
    in the reference folder as **Speller1.cpp**, **Speller2.cpp**, and so on.
  id: totrans-757
  prefs: []
  type: TYPE_NORMAL
- en: At each step, you will be only given hints as to what to change to make it faster
    – only the code in the `getMisspelt()` function is to be modified, and not any
    other code. The student is free to implement the code however they wish, as long
    as it produces the correct results and the code within `main()` is not changed.
  id: totrans-758
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-759
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Optimization is a creative and non-deterministic process – it is not guaranteed
    nor always possible for the student to come up with the same code as the reference
    implementations. It should not be a surprise if the code that you write does not
    perform as well as the reference implementations. In fact, it may even be possible
    that your code is faster than the reference.
  id: totrans-760
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps to implement this activity:'
  id: totrans-761
  prefs: []
  type: TYPE_NORMAL
- en: Make a copy of Speller.cpp called Speller1.cpp and implement the code for the
    `getMisspelt()` function.Use `std::set` and its `count()` method to implement
    this.
  id: totrans-762
  prefs: []
  type: TYPE_NORMAL
- en: Write the next version of the program as Speller2.cpp, and then compile it and
    time it as before. Try using `std::unordered_set` rather than `std::set`. You
    should get about a 2x speedup with this implementation.
  id: totrans-763
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the final version, **Speller3.cpp**, use a **Bloom filter** data structure
    to implement the spell check algorithm. Experiment with different numbers of hash
    functions and sizes for the bloom filter to see what works best.
  id: totrans-764
  prefs: []
  type: TYPE_NORMAL
- en: 'For each of the preceding steps, compile the program and run it as follows
    (change the input file name as required):'
  id: totrans-765
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE120]'
  id: totrans-766
  prefs: []
  type: TYPE_PRE
  zh: '[PRE120]'
- en: Note
  id: totrans-767
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: You should not expect the timings to be exactly as shown here, but if you implement
    the code correctly, the relative improvement in speed should be close to what
    we see here.
  id: totrans-768
  prefs: []
  type: TYPE_NORMAL
- en: 'After executing the preceding commands for each step, the following outputs
    will be generated. The outputs will show the timing for your code and an initial
    message if your output is correct. The following is the output for Step 1:'
  id: totrans-769
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.57: Example output of the code for Step 1'
  id: totrans-770
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C14583_08_57.jpg)'
  id: totrans-771
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.57: Example output of the code for Step 1'
  id: totrans-772
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The following is the output for Step 2:'
  id: totrans-773
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.58: Example output of the code for Step 2'
  id: totrans-774
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C14583_08_58.jpg)'
  id: totrans-775
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.58: Example output of the code for Step 2'
  id: totrans-776
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The following is the output for Step 3:'
  id: totrans-777
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.59: Example output of the code for Step 3'
  id: totrans-778
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C14583_08_59.jpg)'
  id: totrans-779
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.59: Example output of the code for Step 3'
  id: totrans-780
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  id: totrans-781
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution for this activity can be found on page 725.
  id: totrans-782
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-783
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have covered a lot of complex material in this chapter. Optimizing code is
    a difficult but necessary skill for any modern C++ developer. The demands of machine
    learning, hyper-realistic games, big data analysis, and energy-efficient computing
    make this a very vital area to learn about for any C++ professional. We have learned
    that the process of performance optimization is divided into two stages.
  id: totrans-784
  prefs: []
  type: TYPE_NORMAL
- en: Firstly, optimization starts with a proper performance measurement strategy,
    with test conditions mirroring real-world data and usage patterns. We have learned
    how to measure performance by various methods – studying assembler code, manual
    timing, source code instrumentation, and using runtime profilers. Once we have
    accurate measurements, we can actually understand which portions of our programs
    are actually slow and focus our efforts there to get the maximum improvements.The
    second stage involves actually modifying the program – we learned about several
    strategies, starting with using the best compiler options for our code, using
    parallelization features, and also using profile data to help the compiler, followed
    by some simple code transformations that produce small but useful performance
    gains without major code changes. We then learned about how to improve performance
    by structuring our loops and conditionals in a way that makes the code more friendly
    to branch prediction.
  id: totrans-785
  prefs: []
  type: TYPE_NORMAL
- en: Then, we learned about the dramatic and significant effects of caching on performance
    and looked at some techniques, such as the SOA pattern, to make our code take
    advantage of the caches in modern CPUs. Finally, we put all these things together
    for a real-world example of a word count program and simple spell checker to practice
    what we learned hands-on. There are a lot of other advanced techniques and theory
    that need to be studied over and above the material in this chapter, but what
    we have covered here should give any student a solid foundation for future learning.
  id: totrans-786
  prefs: []
  type: TYPE_NORMAL
- en: By the end of these chapters, you have explored a number of topics related to
    using advanced C++. In the first few chapters, you have learned how to write portable
    software, use the type system to your advantage with templates, and learned to
    use pointers and inheritance effectively. Then you have explored the C++ standard
    library, including streams and concurrency, which are essential tools for building
    large real world applications. In the final sections, you learned how to test
    and debug your programs, and optimize your code to run efficiently. Among the
    widely used programming languages C++ is perhaps the most complex, as well as
    being the most expressive. This book is only a beginning, and would have given
    you a solid platform to continue your further learning.
  id: totrans-787
  prefs: []
  type: TYPE_NORMAL
