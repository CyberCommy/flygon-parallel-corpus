- en: 8\. Need for Speed â€“ Performance and Optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Learning Objectives
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you will be able to:'
  prefs: []
  type: TYPE_NORMAL
- en: Time your code performance manually
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use source code instrumentation to measure code execution time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the perf tool to analyze program performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the godbolt compiler explorer tool to analyze machine code generated by
    a compiler
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use compiler flags to generate better code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply code idioms that result in performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Write cache-friendly code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply algorithm-level optimizations to real-world problems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, we will explore concepts that will allow us to write fast code
    in general and several practical techniques that apply to C++ in particular.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In today's world of extremely large and complicated software systems, `stability`
    and `maintainability` are usually considered the major goals for most software
    projects, whereas optimization has not been widely seen as a worthwhile goal since
    the 2000s. This is because of the rapid acceleration of hardware technology that
    overtook software demands on a regular schedule.
  prefs: []
  type: TYPE_NORMAL
- en: For many years, it seemed like the hardware improvements would continue to keep
    up with the performance demands of software, but applications continued to grow
    larger and more complex. Low-level native-compiled languages such as C and C++
    dropped in popularity compared to less performant but easier to use interpreted
    languages such as `Python` or `Ruby`.
  prefs: []
  type: TYPE_NORMAL
- en: By the late 2000s, though, the trend of CPU transistor count (and performance)
    doubling every 18 months (a consequence of `Moore's Law`) had stopped, and performance
    improvements had flattened out. The expectation of 5 to 10 GHz processors being
    widely available by the 2010s never materialized due to limitations of physics
    and manufacturing costs. However, the rapid adoption of mobile devices and the
    rise of high-performance computing applications for data science and machine learning,
    suddenly resurrected the demand for fast and efficient code. Performance per watt
    has become the new metric to aim for as large data centers consume enormous amounts
    of power. For example, Google servers in the US used more power than the entire
    nation of the UK in 2017.
  prefs: []
  type: TYPE_NORMAL
- en: So far in this book, we've learned how the C++ language has evolved in terms
    of ease of use, without sacrificing any performance potential over a traditional
    language such as C. This means we can write fast code in C++ without necessarily
    sacrificing readability or stability. In the next section, we will learn about
    the concept of performance measurement.
  prefs: []
  type: TYPE_NORMAL
- en: Performance Measurement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The most important aspect of optimization is the `measurement of code execution
    time`. Unless we measure our application''s performance with a wide range of input
    datasets, we will have no clue as to which part takes the most time and, our optimization
    efforts will be shot in the dark, with no guarantee of a result. There are several
    approaches for measurement, and some of them are listed here:'
  prefs: []
  type: TYPE_NORMAL
- en: Runtime instrumentation or profiling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Source code instrumentation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Manual execution timing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Studying generated assembly code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Manual estimation by studying the code and algorithms used
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The preceding list is ordered in terms of how accurate the measurement is (with
    the most accurate one first). However, each of these methods has different advantages.
    The choice of which approach to adopt depends on the goals and scope of the optimization
    effort. In an all-out effort to get the fastest possible implementation, all of
    these may be required. We will examine each of these approaches in the following
    sections.
  prefs: []
  type: TYPE_NORMAL
- en: Manual Estimation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The biggest possible improvement in performance occurs when we replace an algorithm
    with a superior one. For example, consider the two versions of a trivial function
    that sums the integers from `1` to `n`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The first function, `sum1`, uses a simple loop to calculate the sum and has
    a runtime complexity that is proportional to `n`, whereas the second function,
    `sum2`, uses the algebraic summation formula and takes constant time independent
    of `n`. In this quite contrived example, we have optimized a function simply by
    using the basic knowledge of algebra.
  prefs: []
  type: TYPE_NORMAL
- en: There are many well-known algorithms for every conceivable operation that have
    been proven to be the most optimal. The best way to make our code run as fast
    as possible is by using algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: It is essential to have a vocabulary of algorithms. We do not need to be an
    algorithm expert, but we need to at least be aware of the existence of efficient
    algorithms in various domains, even if we are not capable of implementing them
    from scratch. A slightly deeper knowledge of algorithms will help us find parts
    of our program that perform similar, if not exactly the same, computations as
    well-known algorithms. Certain code features such as nested loops or linear scanning
    of data are often obvious candidates for improvement, provided we can verify that
    these constructs are within hotspots in the code. A **hotspot** is a section of
    code that runs very often and affects performance significantly. The C++ standard
    library includes a lot of basic algorithms that can be used as building blocks
    to improve the efficiency of many common operations.
  prefs: []
  type: TYPE_NORMAL
- en: Studying Generated Assembly Code
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Assembly language** is a human readable representiation of the binary machine
    code that actually executes on the processor. For any serious programmer of a
    compiled language such as C++, a basic understanding of assembly language is a
    great asset.'
  prefs: []
  type: TYPE_NORMAL
- en: Studying the generated assembly code for a program can give us some good insights
    into how the compiler works and estimates of code efficiency. There are many cases
    where this is the only approach possible to determine efficiency bottlenecks.
  prefs: []
  type: TYPE_NORMAL
- en: Apart from this, a basic knowledge of assembly language is essential to be able
    to debug C++ code since some of the most difficult bugs to catch are those related
    to the low-level generated code.
  prefs: []
  type: TYPE_NORMAL
- en: A very powerful and popular online tool that's used for analyzing compiler-generated
    code is the **Compiler Explorer** that we will be using in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The `Godbolt compiler explorer` can be found at [https://godbolt.org](https://godbolt.org).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a screenshot of the Godbolt compiler explorer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/C14583_08_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.1: Godbolt compiler explorer'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As you can see, the Godbolt compiler explorer consists of two panes. The one
    on the left is where we type the code in, while the one on the right displays
    the generated assembly code. The left-hand pane has a dropdown so that we can
    choose the desired language. For our purposes, we will use the C++ language with
    the gcc compiler.
  prefs: []
  type: TYPE_NORMAL
- en: The right-hand pane has options that we can use to choose a compiler version.
    Almost all the versions of popular compilers such as `gcc`, `clang`, and `cl`
    (`Microsoft C++`) are present, including the ones for non-X86 architectures such
    as ARM.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We will refer to the Intel processor architecture as `x86` for simplicity, even
    though the correct definition is `x86/64`. We will skip mentioning the "`64`"
    since almost all processors being manufactured today are `64-bit`. Even though
    `x86` was invented by Intel, now all PC processor manufacturers are licensed to
    use it.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to get familiar with the basics of the `Compiler Explorer tool` and
    understand the `x86` assembly code at a basic level, let''s examine the assembly
    code generated by a compiler for a simple function that sums up the integers from
    `1` to `N`. Here is the sum function that needs to be written in the left-hand
    pane of the Compiler Explorer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'In the right-hand pane, the compiler must be set to **x86-64 gcc 8.3**, like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.2: C++ compiler'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C14583_08_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.2: C++ compiler'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Once this is done, the left-hand pane''s code is automatically recompiled and
    the assembly code is generated and displayed on the right-hand pane. Here, the
    output is color-coded to show which lines of assembly code is generated from which
    lines of C++ code. The following screenshot shows the assembly code that was generated:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.3: Assembly result'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C14583_08_03.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.3: Assembly result'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Let''s analyze the preceding assembly code briefly. Each instruction in the
    assembly language consists of an **opcode** and one or more **operands**, which
    can be registers, constant values, or memory addresses. A **register** is a very
    fast storage location in the CPU. In the x86 architecture, there are eight main
    registers, namely **RAX**, **RBX**, **RCX**, **RDX**, **RSI**, **RDI**, **RSP**,
    and **RBP**. The Intel x86/x64 architecture uses a curious pattern of register
    naming:'
  prefs: []
  type: TYPE_NORMAL
- en: '**RAX** is a general-purpose 64-bit integer register.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RAX`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`EAX`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`AX`, respectively.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The same convention applies to other general-purpose registers such as `RBX`,
    `RCX`, and `RDX`. The `RSI`, `RDI`, and `RBP` registers have 16-bit and 32-bit
    versions but not the 8-bit sub-registers. The opcode of an instruction can be
    of several types including arithmetic, logical, bitwise, comparison or jump operations.
    It is common to refer to an opcode as an instruction. For example, "`opcode` is
    `sum` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.4: Assembly code of the sum function'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C14583_08_04.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.4: Assembly code of the sum function'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In the preceding screenshot, the first few lines are called a `MOV RAX, RBX`
    assembly code means move the value in the `RBX` register to the `RAX` register.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Assembly language is usually not case-sensitive, so `EAX` and `eax` mean the
    same thing.
  prefs: []
  type: TYPE_NORMAL
- en: The `(*(DWORD*)(rbp - 8))` C expression. In other words, the memory address
    `4` byte `DWORD` (a double word of memory â€“ 32 bits). The square brackets in assembly
    code represent dereferencing, much like the * operator in C/C++. The `rbp` register
    is the base pointer that always contains the address of the base of the currently
    executing functions stack. It is not essential to know how exactly this stack
    frame works but remember that since the stack starts at a higher address and moves
    down, function arguments and local variables have addresses as negative offsets
    from `rbp`. If you see some negative offset from `rbp`, it refers to a local variable
    or argument.
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding screenshot, the first `n` argument that was passed in. The
    last two `ret` variable and the `i` loop variable in our code to `0` and `1`,
    respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, examine the snapshot of the assembly code that follows the prologue and
    initialization â€“ this is our `for()` loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.5: Assembly code of the for loop'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C14583_08_05.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.5: Assembly code of the for loop'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In the preceding screenshot, the lines that have a string followed by a colon
    are called `BASIC`, `C/C++`, or `Pascal` and are used as targets of `goto` statements).
  prefs: []
  type: TYPE_NORMAL
- en: Instructions starting with J on x86 assembly are all jump instructions, such
    as `i` variable from memory to the `n` value in memory with the **cmp** instruction.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The **JG** instruction here means **jump if greater**.
  prefs: []
  type: TYPE_NORMAL
- en: 'If the comparison was greater, then the execution jumps to the **.L2** label
    (which is outside the loop). If not, the execution continues with the next instruction,
    like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.6: Assembly code of the next instruction'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C14583_08_06.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.6: Assembly code of the next instruction'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Here, the value of `i` is reloaded again into `ret`, after which `1` is added
    to `i`. Finally, the execution jumps back to the`for` loop and sums up the sequence
    of integers up to `n`, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.7: Assembly code of the for loop'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C14583_08_07.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.7: Assembly code of the for loop'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This is called the `ret`, is moved into the `ret` returns from the `sum()` function.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The "ret" in the above assembly listing is the mnemonic for the RETURN instruction
    and should not be confused with the "ret" variable in our C++ code example.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is not a simple job to figure out what a sequence of assembly instructions
    does, but a general idea of the mapping between the source code and instructions
    can be gained by observing the following points:'
  prefs: []
  type: TYPE_NORMAL
- en: Constant values in code can be directly recognized in the assembly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Arithmetic operations such as `add`, `sub`, `imul`, `idiv`, and many others
    can be recognized.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conditional jumps map to loops and conditionals.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Function calls can be directly read (the function name appears in the assembly
    code).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, let''s observe the effect of the code if we add a compiler flag for optimization
    in the compiler options field at the top-right:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.8: Adding a compiler flag for optimization'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C14583_08_08.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.8: Adding a compiler flag for optimization'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In the preceding screenshot, `0` in `0` from memory into the register. Since
    memory takes several clock cycles to access (anywhere from `5` to `100` clock
    cycles), using registers alone will itself produce a massive speedup.
  prefs: []
  type: TYPE_NORMAL
- en: 'When the compiler in the dropdown is changed to **x86-64 clang 8.0.0**, the
    assembly code is changed, which can be seen in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.9: Assembly code with the new compiler'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C14583_08_09.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.9: Assembly code with the new compiler'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In the preceding assembly listing, observe that there is no instruction starting
    with `J` (for jump). Thus, there is no looping construct at all! Let''s examine
    how the compiler is calculating the sum of `1` to `n`. If the value of `n` is
    `<= 0`, then it jumps to the`0`. Let''s analyze the following instructions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.10: Assembly code with the new compiler'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C14583_08_10.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.10: Assembly code with the new compiler'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The following code is the C equivalent of the previous instructions. Remember
    that `n` is in the `EDI` register (and hence also in the RDI register, since they
    overlap):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, if we were to write it in one line, it would look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'If we simplify this expression, we get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, we can write it in the following format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'This can be simplified to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, we can write the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: This is the closed form equation for the summation of numbers `1` to `n` inclusive,
    and the fastest way to compute it. The compiler was extremely cleverâ€”rather than
    just looking at our code line by line, it reasoned that the effect of our loop
    was to calculate the sum, and it figured out the algebra on its own. It did not
    figure out the simplest possible expression, but an equivalent one that took a
    few extra operations. Nevertheless, taking out the loop makes this function very
    much optimal.
  prefs: []
  type: TYPE_NORMAL
- en: If we modify the initial or final values of the `i` variable in the `for` loop
    to create a different summation, the compiler is still able to perform the necessary
    algebraic manipulation to derive a closed form solution needing no loops.
  prefs: []
  type: TYPE_NORMAL
- en: This is just one example of how compilers have become extremely efficient and
    appear almost intelligent. However, we must understand that this particular optimization
    of summations has been specifically programmed into the `clang` compiler. It does
    not mean that the compiler can do this kind of trick for any possible loop computation
    â€” that would actually require the compiler to have general artificial intelligence,
    as well as all the mathematical knowledge in the world.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s explore another example of compiler optimization via generated assembly
    code. Look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'In the compiler options, if we select the **x86-64 clang 8.0.0** compiler and
    add **-O3 -stdlib=libc++**, the following assembly code is generated:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.11: Assembly code generated with the new compiler'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C14583_08_11.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.11: Assembly code generated with the new compiler'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'As you can see in the preceding screenshot, the compiler decided correctly
    that the vector was not relevant to the function and removed all the baggage.
    It also did the addition at compile time and directly used the result, `3`, as
    a constant. The main things to take forward from this section are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Compilers can be extremely clever when optimizing code, given the right options.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Studying generated assembly code is very useful to get a high-level estimate
    of execution complexity.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A basic understanding of how machine code works is valuable for any C++ programmer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next section, we'll learn about manual execution timing.
  prefs: []
  type: TYPE_NORMAL
- en: Manual Execution Timing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This is the easiest way to quickly time small programs. We can use a command-line
    tool to measure the time taken for a program to execute. On Windows 7 and above,
    the following PowerShell command can be used:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'On `Linux`, `MacOS`, and other `UNIX-like` systems, the `time` command can
    be used:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: In the next section, we'll implement a small program and examine some caveats
    about timing a program's execution in general.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 1: Timing a Program''s Execution'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this exercise, we will write a program that performs a summation of an array.
    The idea here is to time the summation function. This method is useful when we
    wish to test a function written in isolation. Thus, the test program's only purpose
    is to execute one single function. Since the calculation is very simple, we will
    need to run the function thousands of times in order to get a measurable execution
    time. In this case, we'll just call the `sumVector()` function from the `main()`
    function, passing an `std::vector` of random integers.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A program that's meant to test a single function is sometimes referred to as
    a **driver program** (not to be confused with a device driver).
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps to complete this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a file named **Snippet1.cpp**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Define a function named `sumVector` that sums up each element in a loop:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the `main` function. Use the C++11 random number generation facilities
    to initialize a vector of `10,000` elements and then call the `sumVector` function
    `1,000` times. Write the following code to implement this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Compile, run, and time this program on a Linux Terminal using the following
    commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the previous command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.12: Output of timing the Snippet1.cpp code'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C14583_08_12.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.12: Output of timing the Snippet1.cpp code'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As you can see from the preceding output, for this system, the program executed
    in `0.122` seconds (note that the results will vary, depending on your system's
    configuration). If we run this timing command repeatedly, we may get slight variations
    in the results as the program will be loaded in the memory after the first run
    and will be marginally faster. It is best to run and time the program about `5`
    times and get an average value. We are usually not interested in the absolute
    value of the time taken, but rather how the value improves as we optimize our
    code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the following commands to explore the effect of using compiler optimization
    flags:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.13: Output of timing the Snippet1.cpp code compiled with -O3'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C14583_08_13.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.13: Output of timing the Snippet1.cpp code compiled with -O3'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: From the preceding output, it seems that the program has become about `60` times
    faster, which seems quite unbelievable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Change the code to execute the loop `100,000` times rather than `1,000` times:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Recompile and time again using the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The output after executing previous command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.14: Output of timing the Snippet1.cpp code with 10,000 iterations'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C14583_08_14.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.14: Output of timing the Snippet1.cpp code with 10,000 iterations'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: From the preceding output, it still seems to take the exact same time. This
    seems impossibe, but actually what happens is that since we never caused any side
    effect in our program, such as printing the sum, the compiler is free to replace
    our code with an empty program. Functionally, according to the C++ standard, this
    program and an empty program are identical because there are no side effects of
    running it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Open the Compiler Explorer and paste in the entire code. Set the compiler options
    to `-O3` and observe the generated code:![Figure 8.15: Snippet1.cpp code in Compiler
    Explorer'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/C14583_08_15.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.15: Snippet1.cpp code in Compiler Explorer'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As you can see from the preceding screenshot, the lines within the `for` loop
    are not color-coded and no assembly code was generated for them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Change the code to make sure that the sum must be performed by printing a value
    that depends on the computation with the following line:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we are just summing the result of `sumVector()` to a dummy double value
    many time and printing it. After you make the changes in the code, open the Terminal
    and write the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the previous commands is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.16: Output of timing the Snippet1.cpp code with a side effect of
    printing the value'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C14583_08_16.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.16: Output of timing the Snippet1.cpp code with a side effect of printing
    the value'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In the preceding output, we can see that the program actually performed the
    computation instead of just running as an empty program. Printing the total to
    `cout` is a side effect that causes the compiler not to elide the code. Causing
    a side effect (such as printing a result) that depends on the code's execution
    is one way to prevent the compiler optimizer from removing code. In the next section,
    we'll learn how to time programs without side effects.
  prefs: []
  type: TYPE_NORMAL
- en: Timing Programs without Side Effects
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As seen in the previous exercise, we needed to create a side effect (using
    `cout`) in our program so that the compiler did not ignore all the code we wrote.
    Another technique to make the compiler believe that a piece of code has a side
    effect is to assign its result to a **volatile** variable. The volatile qualifier
    tells the compiler: "This variable must always be read from memory and written
    to memory, and not from a register." The main purpose of a volatile variable is
    to access device memory, and such device memory access must follow the rule mentioned
    above. Effectively, volatile variables are considered by the compiler as if they
    could change from effects outside of the current program, and thus will never
    be optimized. We will use this technique in the upcoming sections.'
  prefs: []
  type: TYPE_NORMAL
- en: There are more advanced ways to bypass this problem, that is, by specifying
    special assembly code directives to the compiler rather than using side effects.
    But they are outside the scope of this introductory material. For the examples
    that follow, we'll always add code that ensures that a function's result is used
    in a side effect or is assigned to a volatile variable. In future sections, we'll
    learn how to examine the compiler generated assembly code and detect instances
    when the compiler elides code for optimization purposes.
  prefs: []
  type: TYPE_NORMAL
- en: Source Code Instrumentation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Instrumentation** is a term that refers to the process of adding extra code
    to a program, without changing its behavior, and capturing the information as
    it executes. This may include performance timing (and possibly other measurements
    such as memory allocation, or disk usage patterns). In the case of source code
    instrumentation, we manually add code to time the execution of our program and
    log that data when the program ends, for analysis. The advantage of this approach
    is its portability and avoidance of any external tools. It also allows us to selectively
    add the timing to any part of the code we choose.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 2: Writing a Code Timer Class'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we''ll create an `RAII` class that allows us to measure the
    execution time of individual blocks of code. We will use this as the primary timing
    mechanism for the code in the exercises that follow. It is not as sophisticated
    as other methods of performance measurement but is much easier to use and serves
    most purposes. The basic requirement of our class is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: We need to be able to record the cumulative time taken by a block of code.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We need to be able to record the number of times it is invoked.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Perform the following steps to complete this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a file named **Snippet2.cpp**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Include the following headers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the `Timer` class and the class member functions by writing the following
    code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: As you can see from the preceding code, the class members consist of a name,
    a starting timestamp, and two `static maps`. Every instance of this class is meant
    to time a certain block of code. The block can be a function scope or any other
    block delimited by curly braces. The usage pattern is to define an instance of
    the `Timer` class at the top of the block while passing in a name (can be a function
    name or some other convenient label). When instantiated, the current timestamp
    is recorded, and when the block exits, the destructor of this class records the
    cumulative time elapsed for this block, as well as the count of the number of
    times this block executed. The times and counts are stored in the static maps
    `ms_Times` and `ms_Counts`, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the constructor of the `Timer` class by writing the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the destructor of the `Timer` class by writing the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, the elapsed time is calculated in milliseconds. Then,
    we add that to the cumulative elapsed time for this block name and increment the
    count of how many times this block was executed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Define a `static` function named `dump()` that prints out the summary of the
    timed results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, the name, execution count, total time, and average time
    is printed in a tabular form. We use multiple tabs between the field names and
    field values to make them line up vertically on a console. This function can be
    modified as we wish. For example, we can modify this code to dump the output as
    a CSV file, so that it can be imported into a spreadsheet for further analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, define the `static` members to complete the class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have defined the `Timer` class, define two simple functions that
    we will time as an example. One will add and the other will multiply. Since these
    operations are trivial, we will loop `1 billion times` so that we can have some
    measurable result.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we used `unsigned int` for the variable that we repeatedly
    `add`/`multiply`. We used an unsigned type so that overflow during arithmetic
    does not result in undefined behavior. Had we used a signed type, the program
    would have undefined behavior and not be guaranteed to work in any way. Secondly,
    we return the calculated value from the `testAdd()` and `testMul()` functions
    so that we can ensure that the compiler does not remove the code (because of the
    lack of side effects). In order to time each of these functions, we need to simply
    declare an instance of a `Timer` class with a suitable label at the start of the
    function. The timing is started as soon as the `Timer` object is instantiated
    and stopped when that object goes out of scope.
  prefs: []
  type: TYPE_NORMAL
- en: 'Write the `main` function, where we will simply call both test functions `10`
    times each:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: As you can see in the preceding code, we're calling each function `10` times
    so that we can demonstrate the `Timer` class timing multiple runs of a function.
    Assigning the result of the functions to a volatile variable forces the compiler
    to assume that there is a global side effect. Hence, it will not elide the code
    in our test functions. Before exiting, we call the `Timer::dump` static function
    to display the results.
  prefs: []
  type: TYPE_NORMAL
- en: 'Save the program and open a terminal. Compile and run the program with different
    optimization levels â€“ on the `gcc` and `clang` compilers, this is specified by
    the `-ON` compiler flag, where `N` is a number from `1` to `3`. Add the `-O1`
    compiler flag first:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'This code generates the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.17: Snippet2.cpp code performance when compiled with the -O1 option'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C14583_08_17.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.17: Snippet2.cpp code performance when compiled with the -O1 option'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now, add the `-O2` compiler flag in the terminal and execute the program:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'This generates the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.18: Snippet2.cpp code performance when compiled with the -O2 option'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C14583_08_18.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.18: Snippet2.cpp code performance when compiled with the -O2 option'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Add the `-O3` compiler flag in the terminal and execute the program:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'This generates the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.19: Snippet2.cpp code performance when compiled with the -O3 option'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C14583_08_19.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.19: Snippet2.cpp code performance when compiled with the -O3 option'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Notice that the `testMul` function became faster only at `O3`, but the `testAdd`
    function got faster at `O2` and much faster at `O3`. We can verify this by running
    the program multiple times and averaging the times. There are no obvious reasons
    why some functions speed up while others do not. We would have to exhaustively
    check the generated code to understand why. It is not guaranteed that this will
    happen on all the systems with different compilers or even compiler versions.
    The main point to take home is that we can never assume performance but have to
    always measure it, and always re-measure if we believe any change we made affects
    the performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'To make it easier to use our `Timer` class for timing individual functions,
    we can write a macro. C++ 11 and above support a special compiler built-in macro
    called `__func__` that always contains the currently executing function''s name
    as a `const char*`. Use this to define a macro so that we don''t need to specify
    a label for our `Timer` instances, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Add the `TIME_IT` macro to the start of the two functions, changing the existing
    line that creates a Timer object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Save the program and open the terminal. Compile and run it again by using the
    following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the previous command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.20: Snippet2.cpp code output when using a macro for timing'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C14583_08_20.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.20: Snippet2.cpp code output when using a macro for timing'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In the preceding output, notice that the actual function name is printed now.
    Another advantage of using this macro is that we can add this to all potentially
    time-consuming functions by default, and disable it for production builds by simply
    changing the definition to a no-op, which will cause the timing code to never
    run - avoiding the need to edit the code extensively. We will use this same Timer
    class for timing code in forthcoming exercises.
  prefs: []
  type: TYPE_NORMAL
- en: Runtime Profiling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Profiling** is a non-intrusive method of measuring the performance of the
    functions in a program. Profilers work by sampling a program''s current execution
    address at frequent intervals (hundreds of times in a second) and making a log
    of which functions happened to be executing at the time. This is a statistical
    sampling approach that has reasonable accuracy. Sometimes, though, the results
    can be confusing as a program may spend a lot of time on functions that are a
    part of the operating system kernel. The most popular runtime profiling tool on
    Linux is **perf**. In the next section, we''ll make use of perf to profile our
    program.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 3: Using perf to Profile Programs'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`perf` can be installed on `Ubuntu` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'To get familiar with the basics of using `perf`, we''ll profile and analyze
    the program from the previous exercise with the help of the `perf` tool. Perform
    the following steps to complete this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: Open the `TIME_IT` macros from the two functions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Open the terminal, recompile the code again with the `-O3` flag, and then create
    a profile data sample with `perf` as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the previous command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/C14583_08_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.21: Using the perf command to analyze the code in Snippet2.cpp'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This creates a file called `perf.data` which can be analyzed or visualized.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, use the following command to visualize the recorded data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'A console-based GUI will show the following data after executing the previous
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.22: Using the perf command to analyze the code in Snippet2.cpp'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C14583_08_22.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.22: Using the perf command to analyze the code in Snippet2.cpp'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You can move the cursor up and down to select a function and press *Enter* to
    get a list of options.
  prefs: []
  type: TYPE_NORMAL
- en: 'Highlight `testMul`, press *Enter*, and choose `Annotate testMul` in the resulting
    list. A list of assembly code is shown, with annotations describing the percentage
    of execution time for each line of code, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.23: Viewing the timing statistics using the perf command for the
    Snippet2.cpp code'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C14583_08_23.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.23: Viewing the timing statistics using the perf command for the Snippet2.cpp
    code'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Notice that the `99%` of the time to execute. Traditionally, integer multiplications
    are always expensive on the `x86` architecture and this continues to be true,
    even in the latest generation of CPUs. This annotation view displays arrows next
    to each jump or branching instruction which, when highlighted, shows what comparison
    instruction it is associated with and what address it jumps to with line drawings.
    You can navigate to the previous view by pressing the left arrow key and exit
    the program using the *q* key.
  prefs: []
  type: TYPE_NORMAL
- en: Up until now, we've looked at several methods that are used to assess the performance
    of our programs. This is the most critical stage of optimization since it tells
    us where we need to direct our efforts. In the upcoming sections, we will explore
    various techniques that will help us optimize our code.
  prefs: []
  type: TYPE_NORMAL
- en: Optimization Strategies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Optimization of code can be done in several ways, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Compiler-based optimization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Source code micro-optimization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cache-friendly code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Algorithmic optimization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here, each technique has its pros and cons. We will examine each of these methods
    in detail in the upcoming sections. Roughly speaking, these are ordered in terms
    of effort required and also potential gains in performance. We'll look at compiler-based
    optimization in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Compiler-Based Optimization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Passing the correct options to the compiler can net many performance benefits.
    A real-world example of this is the Clear Linux `gcc` and `clang` family of compilers,
    the most basic option for optimization is `-O<N>`, where `N` is one of the numbers
    `1`, `2`, or `3`. `-O3` enables almost every optimization in the compiler, but
    there are several others not enabled by that flag that can make a difference.
  prefs: []
  type: TYPE_NORMAL
- en: Loop Unrolling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Loop unrolling** is a technique that can be used by compilers to reduce the
    number of branches that are executed. Every time a branch is executed, there is
    a certain performance overhead. This can be reduced by repeating the loop body
    multiple times, and reducing the number of times the loop is executed. Loop unrolling
    can be done at the source level by the programmer, but modern compilers do a very
    good job automatically.'
  prefs: []
  type: TYPE_NORMAL
- en: Even though modern processors mitigate the overhead of branching by means of
    `gcc` and `clang` family of compilers with the `-funroll-loops` command-line flag.
    In the next section, we'll test the performance of a program with and without
    loop unrolling enabled.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 4: Using Loop Unrolling Optimizations'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this exercise, we'll write a simple program that uses nested loops and test
    its performance with and without loop unrolling enabled. We'll understand the
    way compilers implement the automatic unrolling of loops.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform these steps to complete this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a file named **Snippet3.cpp**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Write a program that takes the first `10,000` numbers and prints out how many
    of these are factors of each other (the full code can be found in **Snippet3.cpp**):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Save the program and open the terminal. Compile the program with the `-O3`
    flag first and time it using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the previous command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.24: Output of the code in Snippet3.cpp'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C14583_08_24.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.24: Output of the code in Snippet3.cpp'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now, compile the same code with the loop unrolling enabled and time it again:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the previous command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.25: Output of the code in Snippet3.cpp compiled with the loop unrolling
    option'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C14583_08_25.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.25: Output of the code in Snippet3.cpp compiled with the loop unrolling
    option'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Open the `Godbolt compiler explorer` and paste the preceding complete code into
    the left-side.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'On the right-hand side, select `x86-64 gcc 8.3` from the compiler options and
    write the `-O3` flag in the options. Assembly code will be generated. For the
    for loop, you''ll see the following output:![Figure 8.26: Assembly code of the
    for loop'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/C14583_08_26.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.26: Assembly code of the for loop'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: From the preceding screenshot, you can clearly see `RCX` being compared to `10,000`
    with the `CMP` instruction, followed by a conditional jump, `JNE` (Jump if Not
    Equal). Just after this code, the outer loop comparison is seen, with `RSI` being
    compared to `10,000`, followed by another conditional jump to the `L4` label.
    Overall, the inner conditional branch and jump executes `100,000,000` times.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, add the following options: `-O3 â€“funroll-loops`. Assembly code will be
    generated. In this code, you''ll notice this code pattern repeating eight times
    (except for the `LEA` instruction, whose offset value changes):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.27: Assembly code of the for loop'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C14583_08_27.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.27: Assembly code of the for loop'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The compiler decided to unroll the body of the loop eight times, reducing the
    number of conditional jump instructions executed by a factor of `87.5%` (about
    `8,300,000` times). This alone caused the execution time to improve by `10%`,
    which is a very significant speedup. In this exercise we have seen the benefits
    of loop unrolling - next, we'll learn about profile guided optimization.
  prefs: []
  type: TYPE_NORMAL
- en: Profile Guided Optimization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Profile Guided Optimization** (PGO) is a feature that most compilers support.
    When a program is compiled with PGO enabled, the compiler adds instrumentation
    code to the program. Running this PGO-enabled executable creates a log file that
    contains information about the execution statistics of the program. The term **profiling**
    refers to the process of running a program to gather performance metrics. Typically,
    this profiling stage should be run with a real-world dataset so that an accurate
    log is produced. After this profiling run, the program is recompiled with a special
    compiler flag. This flag enables the compiler to perform special optimizations
    based on the statistical execution data that was recorded. Significant performance
    gains can be achieved with this approach. Let''s solve an exercise based on profile
    guided optimization to get a better understanding of this.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 5: Using Profile Guided Optimization'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this exercise, we will use profile guided optimization on the code from the
    previous exercise. We'll understand how to use profile guided optimization with
    the `gcc` compiler.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform these steps to complete this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open the terminal and compile the code from the previous exercise with profiling
    enabled. Include any other optimization flags that we need (in this case, `-O3`).
    Write the following code to implement this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, run the profiled version of the code by writing the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: The program runs normally and prints the result, and no other output is seen
    - but it generates a file containing data that will help the compiler in the following
    step. Note that with profiling enabled, the program executes several times slower
    than it would normally. This is something to keep in mind with large programs.
    After executing the previous command, a file called `Snippet3.gcda` will be generated,
    which contains profile data. When doing this with large, complex applications,
    it is important to run the program with the datasets and workflows that it will
    most commonly encounter in the production environment. By choosing the data correctly
    here, the eventual performance gain will be higher.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recompile with the PGO optimization flags, that is, `-fprofile-use` and `-fprofile-correction`,
    as illustrated in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Note that other than the profile-related compiler options, the other options
    must be exactly the same as the ones in the previous compilation step.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, if we time the executable, we will see a large performance improvement:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the previous command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.28: Timing results of the code in Snippet3.cpp with PGO optimization'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C14583_08_28.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.28: Timing results of the code in Snippet3.cpp with PGO optimization'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In this exercise, we have seen the performance benefits gained by using profile
    guided optimizations provided by the compiler. For this code, the improvement
    in performance was about `2.7x` - on larger programs, this can be even higher.
  prefs: []
  type: TYPE_NORMAL
- en: Parallelization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Most CPUs today have multiple cores, and even mobile phones have quad core processors.
    We can exploit this parallel processing power very simply with compiler flags
    that instruct it to generate parallelized code. One mechanism of parallelizing
    code is to use the `OpenMP` extensions of the C/C++ language. However, this means
    changing the source code and having detailed knowledge of how to use those extensions.
    The other simpler option is a feature specific to the `gcc` compiler â€“ it provides
    an extended standard library that implements most algorithms to run as parallel
    ones.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This automatic parallelization is only available for STL algorithms on gcc and
    is not part of the C++ standard. The C++ 17 standard proposes extensions to the
    standard library for parallel versions of most algorithms but is not supported
    by all compilers yet. Also, in order to take advantage of this feature, the code
    would have to be rewritten extensively.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 6: Using Compiler Parallelization'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this exercise, we will use the `gcc` parallel extensions feature to accelerate
    standard library functions. Our aim is to understand how to use `gcc` parallel
    extensions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform these steps to complete this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a file named **Snippet4.cpp**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Write a simple program to sum up an initialized array with `std::accumulate.`
    Add the following code to implement this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Save the program and open the terminal. Compile the program normally and time
    the execution using the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the previous command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.29: Output of the code in Snippet4.cpp'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C14583_08_29.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.29: Output of the code in Snippet4.cpp'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now, compile the code with the parallelization options, that is, `-O3 -fopenmp`
    and `-D_GLIBCXX_PARALLEL`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.30: Output of the code in Snippet4.cpp compiled with parallelization
    options'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C14583_08_30.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.30: Output of the code in Snippet4.cpp compiled with parallelization
    options'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In the previous output, the `user` field shows the cumulative CPU time and the
    `real` field shows the wall time. The ratio seen between the two is about `7x`.
    This ratio will vary, depending on how many CPU cores the system has (in this
    particular case, there were eight cores). For this system, the ratio could reach
    8x if the compiler was able to perform `100%` parallelization. Note that even
    though eight cores were used, the actual improvement in execution time was only
    about `1.3x`. This is probably because the allocation and initialization of the
    vector takes up most of the time. This is a case of `1.3x` speedup in our code,
    which is a very good optimization result.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have covered some of the more impactful compiler optimization features
    available in modern compilers. Apart from these, there are several other optimization
    flags, but they may not produce very large improvements in performance. Two particular
    optimization flags that apply to large projects with many different source files
    is **Link time optimization** or **Link time code generation**. These are worth
    enabling for large projects. In the next section, we'll look into source code
    micro optimizations.
  prefs: []
  type: TYPE_NORMAL
- en: Source Code Micro Optimizations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'These are techniques that involve using certain idioms and patterns in the
    source code that are usually faster than their equivalents. In earlier times,
    these kinds of micro-optimizations were very fruitful, because compilers were
    not very clever. But today, compiler technology is very much advanced, and the
    effect of these micro-optimizations are not so marked. In spite of this, it is
    a very good habit to use these because they will make the code faster even if
    compiled without optimization. Even in development builds, code that is faster
    saves time when testing and debugging. We''ll look at the std::vector container
    in the next section:'
  prefs: []
  type: TYPE_NORMAL
- en: Using the std::vector Container Efficiently
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`std::vector` is one of the most simple and useful containers in the standard
    library. It has no overhead over normal C style arrays, but has the ability to
    grow, as well as optional bounds checking. You should almost always use `std::vector`
    when the number of elements is not known at compile time.'
  prefs: []
  type: TYPE_NORMAL
- en: A common idiom that's used with `std::vector` is to call `push_back` on it in
    a loop â€“ as it grows, the vector reallocates a new buffer, which is larger than
    the existing one by a certain factor (the exact value of this growth factor depends
    on the standard library implementation). In theory, this reallocation has minimal
    costs because it occurs infrequently, but in practice, the operation of resizing
    in a vector involves copying the elements of its buffer to a newly allocated larger
    buffer, which can be very expensive.
  prefs: []
  type: TYPE_NORMAL
- en: We can avoid these multiple allocations and copies by using the `reserve()`
    method. When we know how many elements a vector will contain, calling the `reserve()`
    method to pre-allocate the storage makes quite a difference. Let's implement an
    exercise in the next section to optimize vector growth.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 7: Optimizing Vector Growth'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will time the effect of the `push_back` method in a loop,
    with and without calling the reserve method. First, we will extract the `Timer`
    class we used in the previous sections into a separate header and implementation
    file â€“ this will allow us to use it as common code for all the succeeding code
    snippets. Perform these steps to complete this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a header file named **Timer.h**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Include the necessary header files:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a class named `Timer`. Within the `Timer` class, declare four variables,
    namely `ms_Counts`, `ms_Times`, `m_tmStart`, and `m_sName`. Declare a constructor,
    destructor, and the `dump()` method. Add the following code to implement this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a helper macro named `TIME_IT` to time functions by writing the following
    code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the header file has been created, create a new file named `dump()` method
    inside the **Timer.cpp** file. Write the following code to implement this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, create a new file named `1,000,000` integers using the `push_back()` method.
    The second function calls the `reserve()` method beforehand, but the first one
    does not. Write the following code to implement this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, write the `main` function. Note the use of redundant braces to ensure
    that the `v1` and `v2` vectors are destroyed after every iteration of the loop:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: The reason we pass the vector by reference is to prevent the compiler from optimizing
    out the entire code in the two functions. If we passed the vectors by value, the
    functions would have no visible side effects and the compiler may just elide the
    functions totally.
  prefs: []
  type: TYPE_NORMAL
- en: 'Save the program and open the terminal. Compile the **Timer.cpp** and **Snippet5.cpp**
    files and run them as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.31: Output of the code in Snippet5.cpp showing the effect of vector::reserve()'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C14583_08_31.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.31: Output of the code in Snippet5.cpp showing the effect of vector::reserve()'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As we can see, the effect of calling `reserve()` has resulted in an improvement
    of about 4% in execution time. In a larger program that has run for a long time,
    the system memory often becomes very fragmented. In such cases, the improvement
    by pre-allocating memory with `reserve()` could be much better. In general, it
    is usually faster to reserve memory beforehand, rather than doing it incrementally
    on the fly. Even the Java Virtual Machine, for performance reasons, uses this
    technique of allocating a huge chunk of memory upfront when starting up.
  prefs: []
  type: TYPE_NORMAL
- en: Short-Circuit Logical Operators
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `&&` and `||` logical operators are **short-circuited**, which means that
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: If the left-hand side of the `||` operator is `true`, the right-hand side is
    not evaluated.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the left-hand side of the `&&` operator is `false`, the right-hand side is
    not evaluated.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By keeping the more unlikely (or less expensive) expression on the left-hand
    side, we can reduce the amount of work that needs to be done. In the next section,
    we'll solve an exercise and learn how to write logical expressions optimally.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 8: Optimizing Logical Operators'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will examine the impact of ordering conditional expressions
    when used with logical operators. Perform these steps to complete this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a new file named **Snippet6.cpp**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Include the necessary libraries and the Timer.h file that we created in the
    previous exercise by writing the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a function named `sum1()` that computes the sum of the integers between
    `0` and `N`. Each number is summed only if it meets either or of two specific
    criteria. The first condition is that the number must be less than `N/2`. The
    second condition is that the number, when divided by 3, must return 2 as a remainder.
    Here, we set `N` to `100,000,000` so we have some measurable time taken by the
    code. Write the following code to implement this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, define another function named `sum2()`. It must contain the same logic
    that we wrote for the previous function, `sum1()`. The only change here is that
    we reverse the order of the conditional expression of the `if` statement. Write
    the following code to implement this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: Note that in the `sum2` function, the `b < N/2` condition will evaluate to true
    half of the time. Thus, the second condition, that is, `b % 3 == 2`, is only evaluated
    for half of the iterations. If we assume for simplicity that both conditionals
    take 1 unit of time, the total time taken for `sum2()` would be `N/2 + (2 * N/2)
    = N * 3/2`. In the case of the `sum1()` function, the condition on the left-hand
    side will evaluate to `true` only 33% of the time, and the remaining 66% of the
    time, both conditions will be evaluated. Thus, the estimated time taken would
    be `N/3 + (2 * N * 2/3) = N * 5/3`. We expect that the ratio between the times
    for the `sum1` and `sum2` function would be `5/3` to `3/2` â€“ that is, `sum1` is
    `11%` slower.
  prefs: []
  type: TYPE_NORMAL
- en: 'Add the following code in the main function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Save the file and open the terminal. Compile and time the preceding program,
    as well as the **Timer.cpp** file, by writing the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.32: Output of the code in Snippet6.cpp showing the effect of optimizing
    boolean conditions'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C14583_08_32.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.32: Output of the code in Snippet6.cpp showing the effect of optimizing
    boolean conditions'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As you can see from the preceding output, we ended up with about `38%` improvement
    in speed, which is much more than expected. Why would this happen? The answer
    is that the `%` operator performs an integer division, which is much more expensive
    than a comparison, but the compiler will not generate a division instruction for
    the `N/2` expression because it is a constant value.
  prefs: []
  type: TYPE_NORMAL
- en: The `sum1()` function code executes the modulus operation for every iteration
    of the loop and the overall execution time is dominated by the division. To summarize
    this, we must always consider short-circuit logical operators and calculate how
    each side of the expression is, and how many times it exectures in order to choose
    the optimal order in which they should appear in the expression. This is equivalent
    of doing an expected value calculation of probability theory. In the next section,
    we'll learn about branch prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Branch Prediction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Modern processors use a pipelined architecture, which is similar to a factory
    assembly line, where an instruction flows along a pipeline and is processed by
    various workers simultaneously. After each clock cycle, the instruction moves
    along the pipeline to the next stage. This means that although each instruction
    may take many cycles to go from start to finish, the overall throughput is one
    instruction completed per cycle.
  prefs: []
  type: TYPE_NORMAL
- en: The drawback here is that, if there is a conditional branch instruction, the
    CPU has no idea which set of instructions are to be loaded after that (since there
    are two possible alternatives). This condition is called a **pipeline stall**,
    and the processor must wait until the condition of the branch has been evaluated
    completely, wasting precious cycles.
  prefs: []
  type: TYPE_NORMAL
- en: To mitigate this, modern processors use something called **branch prediction**
    â€“ they try to predict which way the branch goes. As the branch is encountered
    a greater number of times, it gets more confident as to which way the branch is
    likely to take.
  prefs: []
  type: TYPE_NORMAL
- en: Despite this, the CPU is not omniscient, so if it starts loading the instructions
    of one predicted branch, and later the conditional branch turned out to go the
    other way, the entire pipeline after the branch has to be cleared and the actual
    branch needs to be loaded from scratch. All the work done on the "`assembly line`"
    downstream of the branch instruction has to be discarded and any changes need
    to be reversed.
  prefs: []
  type: TYPE_NORMAL
- en: This is a major bottleneck for performance, and it can be avoided â€“ the simplest
    way is to make sure a branch always goes one way as much as possible â€“ like a
    loop.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 9: Optimization for Branch Prediction'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this exercise, we will explore and demonstrate the effect of CPU branch prediction
    on performance. To explore this, we'll write two functions in a program â€“ both
    perform the same computation using two nested loops which iterate `100` and `100,000,000`
    times, respectively. The difference between the two functions is that, in the
    first function, the outer loop is the bigger one, whereas in the second function,
    the outer loop is the smaller one.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the first function, the outer loop fails branch prediction only once when
    it exits, but the inner loop fails branch prediction `100,000,000` times â€“ each
    time it exits. For the second one, once again, the outer loop fails branch prediction
    only once when it exits, but the inner loop fails branch prediction only 100 times
    â€“ each time it exits. The factor of `1,000,000` between these branch prediction
    failure counts will result in the first function being slower than the second.
    Perform these steps to complete this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a file named **Snippet7.cpp** and include the necessary libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a function named `sum1()` with a nested loop. The outer `for` loop should
    cycle `N` times, whereas the inner for loop should iterate `100` times. Set the
    value of `N` to `100000000`. Write the following code to implement this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: If we assume that the processor predicts branches in loops (statistically, the
    branch instruction at the end of the loop is more likely to jump to the start
    of the loop than not), then it will end up mispredicting every time j reaches
    `100` â€“ in other words, `N` times.
  prefs: []
  type: TYPE_NORMAL
- en: 'Define a new function, `sum2()`, with a nested loop. The only change here is
    that we must set the inner loop count to `N` and the outer loop count to `100`.
    Add the following code to implement this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: Now, our reasoning is that the branch misprediction happens only `100` times.
  prefs: []
  type: TYPE_NORMAL
- en: 'Add the following code in the main function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'Save the file and open the terminal. Compile the preceding program, along with
    the **Timer.cpp** file, and time them using the following commands. Remember that
    you need to have the Timer.cpp and Timer.h files you created earlier in the same
    directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of executing the previous command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.33: Output of the code in Snippet7.cpp showing the effect'
  prefs: []
  type: TYPE_NORMAL
- en: of branch prediction optimization
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C14583_08_33.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.33: Output of the code in Snippet7.cpp showing the effect of branch
    prediction optimization'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As you can see from the preceding output, there is a small but certainly significant
    speedup of about `2%` that can be attributed to the processor being able to predict
    branches better for the `sum2` function. In the next section, we'll explore more
    optimization techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Further Optimizations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Several other techniques exist that can be implemented as you code; some of
    them are not guaranteed to produce better code, but it takes very little effort
    to change your coding habits to do these reflexively. They cost nothing but may
    result in gains. A few of these techniques are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Pass parameters that are not primitive types by `const` reference when possible.
    Even though `const` reference.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use pre-increment (`++i`) or pre-decrement (`--i`) operators rather than the
    postfix versions. This usually has no utility for simple types such as integers
    but may do so for complex types with a custom increment operator. Getting into
    a habit of writing `++i` rather than `i++` is good practice unless post-increment
    is actually the desired behavior. Apart from performance benefits, such code declares
    the intent more clearly by using the right operator.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Declare variables as late as possible â€“ it is common in C to declare every variable
    at the top of a function, but in C++, since variables can have non-trivial constructors,
    it makes sense to only declare them in the actual block where they are used.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In terms of **loop hoisting**, if there is any code or calculation in a loop
    that does not change with the loop iteration, it makes sense to move it outside
    the loop. This includes creating objects in a loop body. Often, it is more efficient
    to declare them once, outside the loop. Modern compilers do this automatically,
    but it doesn't take extra effort to do this yourself.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use `const` wherever possible. It does not change the meaning of the code, but
    it lets the compiler make stronger assumptions about your code that may lead to
    better optimization. Apart from this, using `const` makes code more readable and
    reasonable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integer division, modulus, and multiplication (especially by numbers that are
    not powers of 2) are some of the slowest operations possible on X86 hardware.
    If you need to perform such operations in a loop, perhaps you can do some algebraic
    manipulation to get rid of them.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As we mentioned, several such optimizations may be done by the compiler itself,
    but doing them as a habit makes the code fast even in debug mode, which is a big
    advantage when debugging. We have examined a few techniques for micro-optimizing
    code already â€“ the level of code change required to do these is relatively minor,
    and some of these can produce major improvements in efficiency. If you want to
    write faster code in general, you should aim to integrate these techniques as
    a default coding style over time. In the next section, we'll learn about cache-friendly
    code.
  prefs: []
  type: TYPE_NORMAL
- en: Cache Friendly Code
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Computer science was developed in the mid-20th century, when computers hardly
    existed, but nevertheless, by the 1980s, most of the useful data structures and
    algorithms had been discovered and refined. Algorithmic complexity analysis is
    a topic that anyone who learns computer science encounters â€“ and there are well-accepted
    textbook definitions of the complexity of data structure operations. However,
    after 50 years since these things were analyzed, computers have evolved in a way
    that is quite different from what could have been envisaged. For example, a common
    "fact" is that the list data structures are faster for insertion operations than
    arrays. This seems like common sense because inserting an element into an array
    involves moving all the items after that point to new locations, whereas inserting
    into a list is merely a few pointer manipulations. We will test this hypothesis
    in the following exercise.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 10: Exploring the Effect of Caches on Data Structures'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will examine the impact of the cache on arrays and lists
    in the C++ standard library. Perform these steps to complete this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a file named **Snippet8.cpp**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Include the necessary libraries, along with the **Timer.h** header file. Write
    the following code to implement this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a constant integer variable, `N`, and set its value to `100000`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize a random number generator and create a distribution range from `0`
    to `1000`. Add the following code to achieve this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a method named `insertRandom()` and insert elements from `0` to `N`
    into a container at random positions. Add the following code to implement this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a method named `insertStart()` and insert elements from `0` to `N` into
    a container at the start. Add the following code to implement this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a method named `insertEnd()` and insert elements from `0` to `N` into
    a container at the end. Add the following code to implement this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'Write the following code in the `main` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'Save the file and open the terminal. Compile the preceding program, along with
    the **Timer.cpp** file, by writing the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding command generates the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.34: Output of the code in Snippet8.cpp contrasting the timing'
  prefs: []
  type: TYPE_NORMAL
- en: of std::list and std::vector insertion
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C14583_08_34.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.34: Output of the code in Snippet8.cpp contrasting the timing of std::list
    and std::vector insertion'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As you can see from the preceding output, the code measures the time taken to
    insert `100000` integers at the start, end, and random locations for `std::vector`
    and `std::list`. The vector clearly wins by a factor of 100 or more for the random
    case, and even the worst case for the vector is 10x faster than the random case
    for the list.
  prefs: []
  type: TYPE_NORMAL
- en: Why does this happen? The answer lies in the way modern computer architecture
    has evolved. CPU clock speeds have increased from about `1 Mhz` in the early 80s
    to `5 GHz` as of mid-2019 â€“ a speedup of `5,000x` in clock frequency â€“ and while
    the earliest CPUs used multiple cycles per instruction, modern ones execute several
    instructions per cycle on a single core (due to advanced techniques such as pipelining,
    which we described earlier).
  prefs: []
  type: TYPE_NORMAL
- en: For example, the `IDIV` instruction on the original `Intel 8088` took over 100
    clock cycles to complete, whereas on modern processors, it can be completed in
    less than 5 cycles. On the other hand, RAM bandwidth (the time taken to read or
    write a byte of memory) has increased very slowly.
  prefs: []
  type: TYPE_NORMAL
- en: Historically, processors have increased in speed by a factor of about `16,000x`
    between 1980 and 2010\. At the same time, the speed increase in RAM has been an
    order of magnitude smaller â€“ less than 100x. Thus, it is possible that single
    access to RAM by an instruction causes the CPU to wait for a huge number of clock
    cycles. This would be an unacceptable degradation of performance, and there have
    been a lot of technologies to mitigate this issue. Before we explore this, let's
    measure the impact of memory access in the next exercise.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 11: Measuring the Impact of Memory Access'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will examine the performance impact of randomly accessing
    memory. Perform these steps to complete this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a new file named **Snippet9.cpp**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Include the necessary libraries, along with the `SIZE` and `N`, and set their
    values to `100000000`. Also, create a random number generator and a distribution
    range from `0` to `N-1`. Write the following code to implement this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the `getPRIndex()` function, which returns a pseudo random index between
    `0` and `SIZE-1`, where `SIZE` is the number of elements in the array. Write the
    following code to implement this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'Write a function named `sum1()` that accesses a large array of data randomly
    and sums those elements:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'Write a function named `sum2()` that sums random numbers without any memory
    access:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'In the main function, initialize the vector so that `v[i] == i`, thus, the
    only difference between `sum1()` and `sum2()` is that `sum1()` accesses memory
    and `sum2()` only performs computations. As usual, we use volatile to prevent
    the compiler from removing all the code, since it has no side effects. Write the
    following code in the `main()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'Save the program and open the terminal. Compile and run the program by writing
    the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code generates the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.35: Output of the code in Snippet9.cpp contrasting the timing'
  prefs: []
  type: TYPE_NORMAL
- en: of computation versus random memory access
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C14583_08_35.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.35: Output of the code in Snippet9.cpp contrasting the timing of computation
    versus random memory access'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: From the preceding output, we can clearly see a factor of about `14x` difference
    in performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new file named `sum3()` that accesses memory linearly instead of randomly.
    Also, edit the main function. The updated code is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'Save the file and open the Terminal. Compile and run the program:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding commands generate the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.36: Output of the code in Snippet10.cpp contrasting the timing'
  prefs: []
  type: TYPE_NORMAL
- en: of computation versus random and linear memory access
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C14583_08_36.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.36: Output of the code in Snippet10.cpp contrasting the timing of
    computation versus random and linear memory access'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In the preceding output, notice that the memory access is now more than `35`
    times faster than before, and `2.5` times faster than the calculation in `sum2()`.
    We used the random access pattern in `sum1()` to demonstrate the contrast between
    linear and random memory access. What makes linear memory access so much faster
    than random access? The answer lies in two mechanisms in modern processors that
    are used to mitigate the effects of slow memory â€“ **caching** and **prefetch**
    â€“ both of which we will discuss in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Caching
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Modern processors have multiple layers of cache memory between the processor
    registers and the RAM. These caches are labeled L1, L2, L3, L4, and so on, where
    L1 is closest to the processor and L4 is the furthest. Every cache layer is faster
    (and usually smaller) than the level below it. Here is an example of the cache/memory
    sizes and latencies for a `Haswell` family processor:'
  prefs: []
  type: TYPE_NORMAL
- en: 'L1: 32 KB, 4 cycles'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'L2: 256 KB, 12 cycles'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'L3: 6 MB, 20 cycles'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'L4: 128 MB, 58 cycles'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'RAM: many GB, 115 cycles'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A simple model of how caches work to help performance is as follows: when a
    memory address is accessed, it is looked up in the L1 cache â€“ if found, it is
    retrieved from there. If not, it is looked up in the L2 cache, if not found, then
    the L3 cache and so on â€“ if it wasn''t found in any of the caches, it is fetched
    from memory. When fetched from memory, it is stored in each of the caches for
    faster access later. This method in itself would be fairly useless because it
    would only improve performance if we accessed the same memory address again and
    again.The second aspect, called **prefetching**, is the mechanism that can make
    caches really pay off.'
  prefs: []
  type: TYPE_NORMAL
- en: Prefetching
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Prefetching is a process where, when memory access is performed, nearby data
    is also fetched into caches, even though it was not accessed directly. The first
    aspect of prefetching is related to memory bus granularity â€“ it can be thought
    of as "What is the minimum amount of data that the RAM subsystem can send to the
    processor?". In most modern processors, this is 64 bits â€“ in other words, whether
    you ask for a single byte or a 64-bit value from memory, the entire `machine word`
    of 64 bits that includes that address is read from RAM. This data is stored in
    each layer of cache for faster access later. Obviously, this would immediately
    improve memory performance â€“ say we read a byte of memory at address `0x1000`;
    we also get the 7 other bytes after that address into the caches. If we then access
    the byte at address `0x1001`, it comes from the cache, avoiding expensive RAM
    access.
  prefs: []
  type: TYPE_NORMAL
- en: The second aspect of prefetch takes this one step further â€“ when the contents
    of the RAM at an address is read, the processor reads not only that memory word,
    but much more. On the x86 family of processors, this is between 32 and 128 bytes.
    This is called the **cache line** size â€“ the processor always writes and reads
    memory in chunks of that size. When the CPU hardware detects that memory is being
    accessed in a linear fashion, it prefetches memory into one cache line, based
    on its prediction of what addresses are likely to be accessed subsequently.
  prefs: []
  type: TYPE_NORMAL
- en: CPUs are very clever in detecting regular access patterns both forwards and
    backwards, and will prefetch efficiently. You can also provide hints to the processor
    using special instructions to make it prefetch data according to the programmer's
    direction. These instructions are provided as intrinsic functions on most compilers
    in order to avoid the use of inline assembly language. When a memory address is
    read or written that is not in a cache, it is termed a **cache miss**, and is
    a very expensive event and to be avoided at all costs. The CPU hardware tries
    its best to mitigate cache misses, but the programmer can analyze and modify the
    data access patterns to reduce cache misses maximally. The description of caching
    here is a simplified model for instructional purposes â€“ in reality, CPUs have
    L1 caches for instructions as well as data, multiple cache lines, and very complex
    mechanisms to make sure that multiple processors can keep their separate caches
    in synchronization.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A comprehensive description of cache implementations (and lots of other information
    about memory subsystems) can be found in this famous online article: [https://lwn.net/Articles/250967/](https://lwn.net/Articles/250967/).'
  prefs: []
  type: TYPE_NORMAL
- en: Effects of Caching on Algorithms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Having learned about caches, we can now reason why our first example of vector
    versus list showed surprising results â€“ from a computer science perspective, the
    following is true:'
  prefs: []
  type: TYPE_NORMAL
- en: '**For a list**:'
  prefs: []
  type: TYPE_NORMAL
- en: Iterating to the Nth position is order N complexity.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inserting or deleting an element is an order 1 complexity.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**For an array (or vector)**:'
  prefs: []
  type: TYPE_NORMAL
- en: Iterating to the Nth position is order 1 complexity.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inserting or deleting an element at location N has complexity proportional to
    (S - N), where S is the size of the array.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, for modern architectures, the cost of a memory access is extremely
    high, but the cost of accessing an adjacent address subsequently is almost 0 because
    it would already be in the cache. This means that the iteration upon elements
    in a `std::list` that are located non-sequentially in memory are likely to always
    cause a cache miss, causing slow performance. On the other hand, since the elements
    of an array or `std::vector` are always adjacent, caching and prefetching would
    reduce the overall cost of copying (S-N) elements to a new location by a very
    large margin. Hence, the traditional analysis of the two data structures that
    declares that lists work better for random insertions, while technically correct,
    is not practically true, especially given the clearly sophisticated caching behavior
    of modern CPU hardware. When our programs are *data bound*, the analysis of algorithm
    complexity has to be augmented by understanding of what is known as **data locality**.
  prefs: []
  type: TYPE_NORMAL
- en: Data locality can be defined simply as the average distance from the memory
    address that was just accessed to the one that was accessed previously. In other
    words, making memory access across addresses that are far from each other is a
    severe slowdown, since data from closer addresses are likely to have been prefetched
    into the caches. When data is already present in the cache(s), it is termed "hot";
    otherwise, it is termed "cold". Code that takes advantage of the cache is termed
    **cache friendly**. Cache-unfriendly code, on the other hand would cause the cache
    lines to be wastefully reloaded (termed **cache invalidation**). In the remainder
    of this section, we will look at strategies regarding how to write cache-friendly
    code.
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing for Cache-Friendliness
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the old days, optimization of code involved trying to minimize the number
    of machine instructions in code, using more efficient instructions, and even reordering
    instructions to allow pipelines to remain full. As of this day and age, compilers
    perform all the aforementioned optimization to a level that most programmers would
    be unable to â€“ especially considering that compilers can do it across entire programs
    of hundreds of millions of instructions. What remains firmly the responsibility
    of the programmer even now is the ability to optimize data access patterns to
    take advantage of caching.
  prefs: []
  type: TYPE_NORMAL
- en: The task is very simple â€“ make sure that memory is accessed close to the memory
    that was accessed before â€“ but the methodology to achieve this can require lots
    of effort.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The famous game programmer and code optimization guru Terje Mathisen, in the
    90s, is claimed to have said: "All programming is an exercise in caching." Today,
    in 2019, this statement applies more than ever in this sub-domain of trying to
    write fast code.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are some basic rules of thumb for increasing cache-friendliness:'
  prefs: []
  type: TYPE_NORMAL
- en: The stack is always "hot", and so we should use local variables as much as possible.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dynamically allocated objects rarely have data locality with each other â€“ avoid
    them or use a preallocated pool of objects so they are in sequential in memory.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pointer-based data structures such as trees â€“ and especially lists â€“ consist
    of multiple nodes allocated on the heap, and are very cache unfriendly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Runtime dispatch of virtual functions in OO code invalidates the instruction
    cache â€“ avoid a dynamic dispatch in performance-critical code.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next section, we'll explore the cost of heap allocations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 12: Exploring the Cost of Heap Allocations'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will examine the performance impact of dynamically allocated
    memory and examine how heap memory affects the code''s performance. Perform these
    steps to complete this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a file named **Snippet11.cpp**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Add the following code to include the necessary libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: 'Declare a constant variable, N, and a character array called fruits. Assign
    values to them:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a function named `fun1()` that just loops over each string in fruits,
    copies it to a string, and sums the characters of that string:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: 'Create another function named `sum2()` that uses a locally declared character
    array instead of a string and a loop to copy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: 'Write the following code inside the `main()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: 'Save the file and open the terminal. Compile and run the program:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding commands generate the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.37: Output of the code in Snippet11.cpp showing the effect of heap
    allocation on the timing'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C14583_08_37.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.37: Output of the code in Snippet11.cpp showing the effect of heap
    allocation on the timing'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: From the preceding output, notice that `fun2()` is almost twice as fast as `fun1()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, use the `perf` command to profile:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding command generates the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.38: Output of the perf command profiling the code in Snippet11.cpp'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C14583_08_38.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.38: Output of the perf command profiling the code in Snippet11.cpp'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now, we can check the performance report with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: 'We receive the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/C14583_08_39.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.39: Output of the perf command''s timing report for the code in Snippet11.cpp'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In the preceding output, notice that about `33%` of the execution time was taken
    by the `std::string` constructor, `strlen()`, and `memmove()`. All of these are
    associated with the `std::string` that was used in `fun1()`. The heap allocation
    in particular is the slowest operation.
  prefs: []
  type: TYPE_NORMAL
- en: Struct of Arrays Pattern
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In many programs, we often use an array of objects of the same type â€“ these
    could represent records from a database, entities in a game, and so on. A common
    pattern is to iterate through a large array of structures and perform an operation
    on some fields. Even though the structs are sequential in memory, if we access
    only a few of fields, a larger size structure will make caching less effective.
  prefs: []
  type: TYPE_NORMAL
- en: The processor may prefetch several structs into cache, but the program only
    accesses a fraction of that cached data. Since it is not using every field of
    each struct, most of the cached data is discarded. To avoid this, another kind
    of data layout can be used â€“ instead of using an **array of structs** (**AoS**)
    pattern, we use a **struct of arrays** (**SoA**) pattern. In the next section,
    we'll solve an exercise wherein we'll examine the performance benefit of using
    the SoA pattern versus the AoS pattern.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 13: Using the Struct of Arrays Pattern'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will examine the performance benefits of using the SoA
    versus AoS pattern. Perform these steps to complete this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a file named **Snippet12.cpp**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Include the necessary libraries, along with the `Timer.h` header file. Initialize
    a random number generator and also create a distribution range from 1 to N-1\.
    Create a constant integer variable, N, and initialize it with a value of 100,000,000\.
    Add the following code to implement this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: 'Write two different ways to represent data -â€“ anarray of structs and a struct
    of arrays. Use six fields of `uint64_t` so that we can emulate a large-sized structure
    that would be more representative of a real- world program:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: 'Define two functions, namely `sumAOS` and `sumSOA`, that sum the values in
    `field1`, `field2`, and `field3` for the two preceding data structures. Write
    the following code to implement this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: 'Write the following code inside the `main` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: 'Save the program and open the Terminal. Run the program to time it by adding
    the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code generates the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.40: Output of the code in Snippet12.cpp contrasting the timing'
  prefs: []
  type: TYPE_NORMAL
- en: of the AOS and SOA patterns
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C14583_08_40.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.40: Output of the code in Snippet12.cpp contrasting the timing of
    the AOS and SOA patterns'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The struct of arrays approach is twice as fast as the array of structs approach.
    Considering that the addresses of the vectors in the struct would be quite far
    apart, we may wonder why the caching behavior is better in the SoA case. The reason
    is because of how caches are designed â€“ rather than treating a cache as a single
    monolithic block, it is divided into multiple lines, as we discussed earlier.
    When a memory address is accessed, the 32- or 64-bit address is converted into
    a "tag" of a few bits and the cache line associated with that tag is used. Memory
    addresses that are very close together will get the same tag and reach the same
    cache line. If a highly differing address is accessed, it reaches a different
    cache line. The effect of this line-based cache design on our test program is
    that it is as if we have separate independent caches for each vector.
  prefs: []
  type: TYPE_NORMAL
- en: The preceding explanation for cache lines is a very much simplified one, but
    the basic concept of cache lines applies. Code readability may seem slightly worse
    for this structure of array pattern, but considering the increase in performance,
    it is well worth it. This particular optimization becomes more effective as the
    size of the structure grows larger. Also, remember that padding structures can
    inflate their size by a big factor if the fields are of various sizes. We have
    explored the performance effects of memory latency and learned a few ways to help
    the processor's caches be effective. When writing a program that is performance-critical,
    we should keep caching effects in mind. Sometimes, it makes sense to start out
    with a more cache-friendly architecture in the first place. As always, we should
    always measure the performance of code before we attempt to make radical changes
    in data structures. Optimization should be focused on the most time-consuming
    areas of a program and not every part of it.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithmic Optimizations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The simplest form of algorithmic optimization is to look for libraries that
    perform your task â€“ the most popular libraries are highly optimized and well-written.
    For example, the `Boost` library provides many useful libraries that can come
    in handy in many projects, such as `Boost.Geometry`, `Boost.Graph`, `Boost.Interval`,
    and `Boost.Multiprecision`, to mention a few. It is far easier and wiser to use
    a professionally written library than to attempt to create them yourself. For
    example, `Boost.Graph` implements a dozen algorithms to process topological graphs,
    and each of them is highly optimized.
  prefs: []
  type: TYPE_NORMAL
- en: 'Many computations can be reduced to a series of standard algorithms composed
    together â€“ if done correctly, these can result in extremely efficient code â€“ and
    often even be parallelized to take advantage of multiple cores or SIMD by the
    compiler. For the rest of this section, we will take one single program and attempt
    to optimize it in various ways â€“ this will be a word count program with the following
    specifications:'
  prefs: []
  type: TYPE_NORMAL
- en: To isolate the time taken by disk I/O, we will read the entire file to memory
    before processing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unicode support will be ignored, and we will assume English text in ASCII.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will use a large public domain literary text available online as test data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Exercise 14: Optimizing a Word Count Program'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this lengthy exercise, we'll optimize a program using various optimization
    techniques. We'll perform the incremental optimization of the practical program.
    The test data that we will be using consists of the book named "A Tale of Two
    Cities", which has been appended together 512 times.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The dataset that''s used in this exercise is available here: [https://github.com/TrainingByPackt/Advanced-CPlusPlus/blob/master/Lesson8/Exercise14/data.7z](https://github.com/TrainingByPackt/Advanced-CPlusPlus/blob/master/Lesson8/Exercise14/data.7z).
    You will need to extract this 7zip archive and copy the resulting file, called
    data.txt, into the folder where you work with this exercise.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform these steps to complete this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: Write the basic boilerplate code that reads the file (the full code can be found
    in `main()` itself to get the overall execution time.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note that `push_back` adds a space at the end â€“ this makes sure that the data
    ends with a whitespace, simplifying the algorithms we use.
  prefs: []
  type: TYPE_NORMAL
- en: 'Write a basic word count function. The logic is very simple â€“ for every character
    in the string, if the character is not whitespace and the following one is, then
    it is the end of a word and should be counted. Since our boilerplate code has
    added a space at the end, any final word will be counted. This function is defined
    in **Snippet13.cpp**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s compile, run, and get an idea of the performance. We will verify that
    it is working right by comparing the result of our code with the results provided
    by the standard `wc` program:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: 'We receive the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.41: Output of the code in Snippet13.cpp with a baseline wordcount
    implementation'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C14583_08_41.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.41: Output of the code in Snippet13.cpp with a baseline wordcount
    implementation'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Let''s time the wc program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: 'We receive the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.42: Output of timing the wc program'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C14583_08_42.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.42: Output of timing the wc program'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The *wc* program displays the same word count, that is, `71108096`, so we know
    our code is correct. Our code took about `3.6 seconds`, including reading the
    file, which is much slower than wc.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our first strategy to optimize is to see if there is a better way to implement
    `isspace()`. Instead of a function, we can use a lookup table that can tell if
    a character is a space or not (the code for this can be found in **Snippet14.cpp**):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: 'Remember that boolean variables in C/C++ take on integer values 0 or 1, and
    so we can directly write the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: 'This means we don''t have to write this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: Using booleans directly as numbers can sometimes result in faster code because
    we avoid the conditional logic operators && and ||, which may result in a branch
    instruction.
  prefs: []
  type: TYPE_NORMAL
- en: 'Compile and test the performance now:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: 'We receive the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.43: Output of the code in Snippet14.cpp'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C14583_08_43.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.43: Output of the code in Snippet14.cpp'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We have achieved a speedup of 8x for the word counting code, with the simple
    principle of using a lookup table. Can we do even better than this? Yes â€“ we can
    take the lookup table concept further â€“ for every pair of characters, there are
    four possibilities, which should result in a corresponding action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Space Space]: No action, [Non-Space Space]: Add 1 to count, [Space Non-Space]:
    No action, [Non-Space, Non-Space]: No action'
  prefs: []
  type: TYPE_NORMAL
- en: So, we can manufacture a table of `65536` entries (`256 * 256`) to cover all
    possible pairs of characters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Write the following code to create the table:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: The loop to count the words becomes the following (the full code can be found
    in `memcpy()`. The compiler is smart enough to use the CPU memory access instructions
    rather than actually call `memcpy()` for 2 bytes. We have ended up with the loop
    containing no conditional statement, which should make it much faster. Remember
    that X86 architecture is *little-endian* â€“ so a 16-bit value read from a character
    array will have the first character as its LSB and the second as the MSB.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, time the code we wrote:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 8.44: Output of the code in Snippet15.cpp'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C14583_08_44.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.44: Output of the code in Snippet15.cpp'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This larger lookup table resulted in a 1.8x speed improvement for `wordCount()`.
    Let's step back and look at this from another angle so that we can use existing
    the standard library effectively. The advantages of this are two-fold â€“ firstly,
    the code is less prone to errors, and secondly, we could take advantage of the
    parallelization available with some compilers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s rewrite the version of the program that uses the lookup table for `isspace`
    using the standard algorithms. If we look at the main loop that counts the words,
    we are taking 2 characters, and depending on some logic, we are accumulating 1
    or 0 into the `count` variable. This is a common pattern seen in a lot of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: Here, `a` and `b` are arrays of size `N`, `X` is an initial value, and `OP`
    and `OP2` are operators. There is a standard algorithm that encapsulates this
    pattern called `std::inner_product` â€“ it takes two sequences, applies an operator
    (OP2) between each pair of elements, and applies another operator (OP) across
    these, starting with an initial value X.
  prefs: []
  type: TYPE_NORMAL
- en: We can write the function as follows (the full code can be found in `inner_product()`
    call applies the `isWordEnd()` lambda on every `s[n]` and `s[n+1]` and applies
    the standard addition function between the results of these. In effect, we are
    adding 1 to the total when `s[n]` and `s[n+1]` are on a word ending.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Even though this looks like a number of nested function calls, the compiler
    inlines everything and there is no overhead.
  prefs: []
  type: TYPE_NORMAL
- en: 'Compile and time the execution of this version:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: 'We receive the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.45: Output of the code in Snippet16.cpp'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C14583_08_45.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.45: Output of the code in Snippet16.cpp'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Surprisingly, the code is slightly faster than our initial looped version in
    **Snippet14.cpp**.
  prefs: []
  type: TYPE_NORMAL
- en: Can we adapt the same code to use the large lookup table? Indeed, we can â€“ the
    new function looks like this (the full code can be found in `memcpy()` to convert
    two consecutive bytes into a word, we use a bitwise `OR` operator to combine them.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Compile and time the code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE
- en: 'We receive the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.46: Output of the code in Snippet17.cpp'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C14583_08_46.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.46: Output of the code in Snippet17.cpp'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This code is not quite as fast as the loop0based version we had in `short` to
    get the index, which requires no computation, but here, we read 2 bytes into a
    `short` with a bitwise operation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have the code where the bulk of the work is done by a standard
    library function, we can now get automatic parallelization for free â€“ compile
    and test as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs: []
  type: TYPE_PRE
- en: 'We receive the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.47: Output of the code in Snippet17.cpp with the parallelized standard
    library'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C14583_08_47.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.47: Output of the code in Snippet17.cpp with the parallelized standard
    library'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Clearly, it cannot be completely parallelized, so we only get about 2.5x improvement
    in terms of speed, but we got it without having to do anything to the code. Could
    we have made the loop-based code parallelizable in the same way? In theory, yes
    â€“ we could manually use **OpenMP** directives to achieve this; however, it would
    require changes to the code and a knowledge of how to use OpenMP. What about the
    version in **Snippet16.cpp**?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs: []
  type: TYPE_PRE
- en: 'We receive the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.48: Output of the code in Snippet16.cpp with the parallelized standard
    library'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C14583_08_48.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.48: Output of the code in Snippet16.cpp with the parallelized standard
    library'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Similar improvements can be seen for this version too. Are we finished or can
    this be even faster? **Michael Abrash**, a famous game programmer, coined the
    acronym **TANSTATFC** â€“ it stands for "There ain't no such thing as the fastest
    code". What he meant is that, given enough effort, it was always possible to make
    code faster. This seems impossible, but time and again, people have found faster
    and faster ways of performing a computation â€“ our code is no exception and we
    can still go a bit further. One of the tradeoffs we can make for optimization
    is to make the code less general â€“ we already put some constraints on our code
    â€“ for example, that we only handle **ASCII** English text. By adding some more
    constraints on the input data, we can do even better. Let's assume that there
    are no non-printable characters in the file. This is a reasonable assumption for
    our input data. If we assume this, then we can simplify the condition for detecting
    spaces â€“ since all the whitespace characters are greater than or equal to ASCII
    32, we can avoid the lookup table itself.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s implement the code based on our previous idea (the full code can be
    found in **Snippet18.cpp**):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs: []
  type: TYPE_PRE
- en: 'Compile and run the program:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs: []
  type: TYPE_PRE
- en: 'We receive the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.49: Output of the code in Snippet18.cpp with simplified logic for
    detecting spaces'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C14583_08_49.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.49: Output of the code in Snippet18.cpp with simplified logic for
    detecting spaces'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This version is twice as fast as the one that was parallelized, and it is just
    a few lines of code. Will using parallelization improve it even more?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs: []
  type: TYPE_PRE
- en: 'We receive the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.50: Output of the code in Snippet18.cpp with the parallelized standard
    library'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C14583_08_50.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.50: Output of the code in Snippet18.cpp with the parallelized standard
    library'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Unfortunately, this is not the case â€“ it is actually slower. The overhead of
    managing multiple threads and thread contention is sometimes more expensive than
    the benefits of multithreaded code. At this point, we can see that the file-read
    code is taking up most of the time â€“ can we do anything about this?
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s change the `main()` function to time the individual parts of it (the
    full code can be found in **SnippetWC2.cpp**):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE109]'
  prefs: []
  type: TYPE_PRE
- en: 'Compile and run the preceding code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE110]'
  prefs: []
  type: TYPE_PRE
- en: 'We receive the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.51: Output of the code in Snippet18.cpp with all operations timed'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C14583_08_51.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.51: Output of the code in Snippet18.cpp with all operations timed'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The majority of the time is taken by `push_back()` and copying the string.
    Since the string is exactly the size of the file, `push_back()` ends up allocating
    a new buffer for the string and copying the contents. How can we eliminate this
    `push_back()` call? We appended a space to the end to be able to consistently
    count the last word, if any, since our algorithm counts the ends of words. There
    are three ways to avoid this: count the start of a word, rather than the end;
    count the last word, if any, separately; and use the `c_str()` function so that
    we have a `NUL` character at the end. Let''s try each of these in turn now.'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, write the main function without `push_back` (the full code can be found
    in **SnippetWC3.cpp**):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE111]'
  prefs: []
  type: TYPE_PRE
- en: 'Change the code in wordCount() by renaming `isWordEnd()` to `isWordStart()`
    and invert the logic. Consider a word as starting, if the current character is
    a space and the succeeding one is not. Also, count one extra word if the string
    starts with a non-space (the full code can be found in **Snippet19.cpp**):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE112]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, write the second alternative â€“ to count the last word, if any. The code
    is almost same as the **Snippet18.cpp** version, except we check for the last
    word (the full code can be found in **Snippet20.cpp**):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE113]'
  prefs: []
  type: TYPE_PRE
- en: Write the third version that uses `c_str()` â€“ all we need to do is change the
    parameters for `inner_product()` (the full code can be found in `c_str()` has
    a `NUL` at the end, it works as before.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Compile and time all three versions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE114]'
  prefs: []
  type: TYPE_PRE
- en: 'We receive the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.52: Output of the code in Snippet19.cpp, which counts the beginnings'
  prefs: []
  type: TYPE_NORMAL
- en: of words rather than the ends
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C14583_08_52.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.52: Output of the code in Snippet19.cpp, which counts the beginnings
    of words rather than the ends'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now enter the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE115]'
  prefs: []
  type: TYPE_PRE
- en: 'We receive the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.53: Output of the code in Snippet20.cpp'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C14583_08_53.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.53: Output of the code in Snippet20.cpp'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now enter the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE116]'
  prefs: []
  type: TYPE_PRE
- en: 'We receive the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.54: Output of the code in Snippet21.cpp'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C14583_08_54.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.54: Output of the code in Snippet21.cpp'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: All three run in approximately the same time â€“ the minor difference of a few
    milliseconds can be ignored.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can tackle the time taken for string copying â€“ instead of using `std::stringstream`,
    we will directly read the file into a string buffer (the full code can be found
    in **SnippetWC4.cpp**):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE117]'
  prefs: []
  type: TYPE_PRE
- en: 'Compile and run this version:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE118]'
  prefs: []
  type: TYPE_PRE
- en: 'We receive the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.55: Output of the code with changed file load code in SnippetWC4.cpp'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C14583_08_55.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.55: Output of the code with changed file load code in SnippetWC4.cpp'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We have now reduced the time taken by our file read code from about 1,000 ms
    to 250 ms â€“ a 4x improvement. The word count code started at about `2,500ms` and
    reduced to about 60 ms â€“ a 40x improvement. The total performance improvement
    for the entire program is 3.6x. We can still ask if this is the limit â€“ indeed,
    TANSTATFC still applies and there are a few more things that can be done: instead
    of reading data into a `std::string`, use `memory-mapped I/O` to get a buffer
    that directly points to the file. This could possibly be faster than allocation
    and reading â€“ it will require changing the word count code to accept a `const
    char*` and a length, or an `std::string_view`. Use a different, faster allocator
    to allocate memory. Compile for the native CPU using the `-march=native` flag.
    However, it seems unlikely that we will be able to get very large performance
    gains from this, since these optimizations have nothing to do with the word counting
    algorithm itself. Another final attempt could be to forego the C++ constructs
    and write inline SIMD code using `compiler intrinsics` (these are the functions
    that the compiler translates directly into single assembly instructions). The
    knowledge that''s required to do this is beyond the scope of this introductory
    material.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Nevertheless, for the curious student, an `AVX2` (256-bit SIMD) version of
    `wordCount()` is provided (Snippet23.cpp). This version needs the input string
    to have a length that is a multiple of 32 and a space at the end. This means that
    the main function has to be rewritten (SnippetWC5.cpp):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE119]'
  prefs: []
  type: TYPE_PRE
- en: 'We receive the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.56: Output of the code in Snippet22.cpp that uses SIMD intrinsics'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C14583_08_56.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.56: Output of the code in Snippet22.cpp that uses SIMD intrinsics'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note that we need to use the `-march=native` flag so that the compiler uses
    the AVX SIMD instruction set. If the processor does not support it, a compile
    error will result. If this executable is compiled for an AVX target, and run on
    a system where the processor does not support those instructions, the program
    crashes with an "Illegal instruction" exception. There seems to be a very small
    improvement, but not significant â€“ the effort and learning curve required to optimize
    with the assembler or SIMD is usually too high to be justified unless your application
    or industry has those demands. The SIMD version processes 32 bytes at a time â€“
    yet there is practically no performance improvement. In fact, if you check the
    generated assembly code for the regular C++ implementation in the other snippets
    with the compiler explorer, you will see that the compiler itself has used SIMD
    â€“ this just goes to show how far compilers go in terms of making your code fast.
  prefs: []
  type: TYPE_NORMAL
- en: Another point to note is that our file read and memory allocation is taking
    up most of the time now â€“ leaving aside memory allocation, we can conclude that
    our code has become **I/O bound** as opposed to **CPU bound**. This means that
    no matter how fast we write the code, it will be limited by how fast the data
    can be fetched. We started with a very simple implementation of a word count algorithm,
    increased its complexity and speed, and finally were able to go back to a very
    simple implementation that ended up being the fastest. The overall speed improvement
    for the algorithm was a factor of 40x. We used a number of approaches that ranged
    from just rearranging code a bit, to reimagining the problem in different ways,
    to performing micro-optimizations. No single approach can work all the time, and
    optimization remains a creative endeavor that needs imagination and skill and
    often, lateral thinking. As compilers get smarter and smarter, it gets harder
    and harder to outdo them â€“ yet, the programmer is the only one who actually understands
    the code's intent, and there is always scope to make the code faster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 1: Optimizing a Spell Check Algorithm'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this activity, we will attempt to optimize a program step by step. This activity
    is about a simple spell checker that takes a dictionary and a text file and prints
    out a list of the words in the text that are not present in the dictionary. A
    basic skeleton program is provided in `7zip archive`, that is, `activity1.7z`.
  prefs: []
  type: TYPE_NORMAL
- en: The dictionary is taken from the Linux word list that is provided with many
    Linux distributions. The text file is like the one we used in the previous exercise
    â€“ it is the same large file we used in the word count exercise, with all punctuation
    removed and converted into lower case.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the dictionary is only an example, so do not assume that all valid
    words exist in it â€“ many of the words in the output could well be correctly spelled
    words. The skeleton code reads the dictionary and text files and calls the spell
    check code (which you will write) on it. After that, it compares the resultant
    output with the contents of **out.txt** and prints whether the program worked
    as expected. The function that does the spell check returns a vector of indices
    of the words that were not in the dictionary. Since we are focusing on only the
    spellcheck algorithm, only that code is timed. The time taken for reading the
    files and comparing the output is not taken into consideration. You will develop
    successively faster versions of this program â€“ reference implementations are provided
    in the reference folder as **Speller1.cpp**, **Speller2.cpp**, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: At each step, you will be only given hints as to what to change to make it faster
    â€“ only the code in the `getMisspelt()` function is to be modified, and not any
    other code. The student is free to implement the code however they wish, as long
    as it produces the correct results and the code within `main()` is not changed.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Optimization is a creative and non-deterministic process â€“ it is not guaranteed
    nor always possible for the student to come up with the same code as the reference
    implementations. It should not be a surprise if the code that you write does not
    perform as well as the reference implementations. In fact, it may even be possible
    that your code is faster than the reference.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps to implement this activity:'
  prefs: []
  type: TYPE_NORMAL
- en: Make a copy of Speller.cpp called Speller1.cpp and implement the code for the
    `getMisspelt()` function.Use `std::set` and its `count()` method to implement
    this.
  prefs: []
  type: TYPE_NORMAL
- en: Write the next version of the program as Speller2.cpp, and then compile it and
    time it as before. Try using `std::unordered_set` rather than `std::set`. You
    should get about a 2x speedup with this implementation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the final version, **Speller3.cpp**, use a **Bloom filter** data structure
    to implement the spell check algorithm. Experiment with different numbers of hash
    functions and sizes for the bloom filter to see what works best.
  prefs: []
  type: TYPE_NORMAL
- en: 'For each of the preceding steps, compile the program and run it as follows
    (change the input file name as required):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE120]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: You should not expect the timings to be exactly as shown here, but if you implement
    the code correctly, the relative improvement in speed should be close to what
    we see here.
  prefs: []
  type: TYPE_NORMAL
- en: 'After executing the preceding commands for each step, the following outputs
    will be generated. The outputs will show the timing for your code and an initial
    message if your output is correct. The following is the output for Step 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.57: Example output of the code for Step 1'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C14583_08_57.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.57: Example output of the code for Step 1'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The following is the output for Step 2:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.58: Example output of the code for Step 2'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C14583_08_58.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.58: Example output of the code for Step 2'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The following is the output for Step 3:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.59: Example output of the code for Step 3'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C14583_08_59.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.59: Example output of the code for Step 3'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution for this activity can be found on page 725.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have covered a lot of complex material in this chapter. Optimizing code is
    a difficult but necessary skill for any modern C++ developer. The demands of machine
    learning, hyper-realistic games, big data analysis, and energy-efficient computing
    make this a very vital area to learn about for any C++ professional. We have learned
    that the process of performance optimization is divided into two stages.
  prefs: []
  type: TYPE_NORMAL
- en: Firstly, optimization starts with a proper performance measurement strategy,
    with test conditions mirroring real-world data and usage patterns. We have learned
    how to measure performance by various methods â€“ studying assembler code, manual
    timing, source code instrumentation, and using runtime profilers. Once we have
    accurate measurements, we can actually understand which portions of our programs
    are actually slow and focus our efforts there to get the maximum improvements.The
    second stage involves actually modifying the program â€“ we learned about several
    strategies, starting with using the best compiler options for our code, using
    parallelization features, and also using profile data to help the compiler, followed
    by some simple code transformations that produce small but useful performance
    gains without major code changes. We then learned about how to improve performance
    by structuring our loops and conditionals in a way that makes the code more friendly
    to branch prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we learned about the dramatic and significant effects of caching on performance
    and looked at some techniques, such as the SOA pattern, to make our code take
    advantage of the caches in modern CPUs. Finally, we put all these things together
    for a real-world example of a word count program and simple spell checker to practice
    what we learned hands-on. There are a lot of other advanced techniques and theory
    that need to be studied over and above the material in this chapter, but what
    we have covered here should give any student a solid foundation for future learning.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of these chapters, you have explored a number of topics related to
    using advanced C++. In the first few chapters, you have learned how to write portable
    software, use the type system to your advantage with templates, and learned to
    use pointers and inheritance effectively. Then you have explored the C++ standard
    library, including streams and concurrency, which are essential tools for building
    large real world applications. In the final sections, you learned how to test
    and debug your programs, and optimize your code to run efficiently. Among the
    widely used programming languages C++ is perhaps the most complex, as well as
    being the most expressive. This book is only a beginning, and would have given
    you a solid platform to continue your further learning.
  prefs: []
  type: TYPE_NORMAL
