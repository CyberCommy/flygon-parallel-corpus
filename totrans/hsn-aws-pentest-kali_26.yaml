- en: Putting it All Together - Real - World AWS Pentesting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will be looking at a real-world AWS pentest from start to
    finish. This should help tie together many of the chapters in this book and demonstrate
    the flow of penetration testing an AWS environment. We will skip over many of
    the technical details of how certain attacks work, because they have already been
    outlined in their respective chapter in this book.
  prefs: []
  type: TYPE_NORMAL
- en: When pentesting an AWS environment, it is important to be thorough and to investigate
    every attack possible with the access that you are granted. This ensures that
    the results you provide the client at the end of the engagement are thorough,
    complete, and useful, and assure them that they can feel confident that their
    infrastructure was investigated on a wide scale.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout this chapter, we will be referencing two IAM users at different points.
    One IAM user will be referred to as `PersonalUser`. `PersonalUser` is an IAM user
    that we have created in our own attacker-controlled AWS account to use for such
    activities as cross-account enumeration. This user is required to have the `iam:UpdateAssumeRolePolicy`
    and `s3:ListBucket` permissions for the cross-account recon to work correctly.
    The other IAM user will be referred to as `CompromisedUser`, and that user is
    who we compromised in this attack scenario and who we will use throughout the
    normal process. Our scenario will mock a scenario where a company, `Acme Co.`,
    that uses AWS, comes to our pentesting company, looking for an AWS pentest.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Pentest kickoff
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unauthenticated reconnaissance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Authenticated reconnaissance plus permissions enumeration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Privilege escalation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Persistence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Post-exploitation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Auditing for compliance and best practices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pentest kickoff
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before jumping into a pentest and hacking away, it is important to go through
    the kickoff process with your client to ensure everyone has an understanding of
    the scope of the pentest, the type of access to be granted to the environment,
    the goal of the pentest, and more. This process is necessary because no one likes
    surprises in the pentesting business, and communication makes everyone happy.
    In this section, we will be covering some of the important aspects of what needs
    to be done prior to when the pentest begins.
  prefs: []
  type: TYPE_NORMAL
- en: Scoping
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the most important aspects of an AWS pentest (or any type of pentest,
    really) is determining the scope of the engagement. AWS engagements are difficult
    to scope in the sense of traditional scoping methods, such as the number of IP
    addresses, number of users, size of the web application, and so on. It requires
    a little bit of a more personal touch, because, sure, almost regardless of the
    size, we could just run some scanners and call it a day, but that's not what pentesting
    is all about and it will reflect poorly on your own company if this is how you
    take care of things. Lots of manual effort needs to go into an AWS pentest to
    really dig deep and find the vulnerabilities that are there, so it is important
    to scope appropriately so that you have enough time to perform an in-depth assessment,
    but not too much time where you are wasting your own time and your client's money.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is difficult to provide an exact methodology behind scoping an AWS engagement,
    but the following list of questions can help provide context around the client''s
    environment to help determine the size of it:'
  prefs: []
  type: TYPE_NORMAL
- en: Are you using multiple AWS accounts for this environment?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How many?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Are you interested in having them all tested, or just a portion?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What kind of access will be provided to the environment?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What/how many AWS services are you using?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How many regions do your resources span across?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How many EC2 instances/Lambda functions are in use?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How many IAM users, roles, and groups do you have?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do your users access your environment? (regular IAM users, SSO | AssumeRole,
    and so on)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Beyond those questions, more specific questions can be asked about the other
    AWS services they are using. How many RDS databases do you have? It is not a useful
    question if they don't even use the RDS service, but something like—how many Lightsail
    instances do you have? might be. This might not normally come up, unless the client
    tells you that they use Lightsail.
  prefs: []
  type: TYPE_NORMAL
- en: These questions are meant to provide you with a basic idea of how large the
    AWS environment you are planning to attack is. This can then help you determine
    an estimated timeline that it would take to fully test.
  prefs: []
  type: TYPE_NORMAL
- en: These questions are very contextual, though, and they will likely vary on a
    per-client basis. This is because, for example, you might be testing an environment
    with 5,000 EC2 instances, 300 Lambda functions, and 100 RDS databases, but the
    client only wants to provide you access to a single user who has IAM permissions
    and some Lightsail permissions. The numbers behind EC2, Lambda, and RDS are nearly
    irrelevant at this point, because unless you can escalate your privileges in the
    environment, you won't be touching those services, based on the client's expectations.
  prefs: []
  type: TYPE_NORMAL
- en: AWS pentesting rules and guidelines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before beginning an AWS pentest, it is important to confirm that you won''t
    be breaking any rules that AWS has put forth regarding pentesting. As of March,
    2019, AWS no longer requires approval for pentests on multiple different services,
    but there is still a list of prohibited activity outlined on their pentesting
    page. Useful information on pentesting an AWS infrastructure, such as the restrictions
    you must follow, can be found here: [https://aws.amazon.com/security/penetration-testing/](https://aws.amazon.com/security/penetration-testing/).
    We don''t want to start pentesting without an understanding of the rules, because
    then we risk breaking the Acceptable Use Policy ([https://aws.amazon.com/aup/](https://aws.amazon.com/aup/))
    of AWS, which could potentially end up with the target account being suspended
    or terminated completely. This information must be conveyed to our client prior
    to the engagement, or we risk a delay of when we can start.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Something important to note is that AWS states that our policy only permits
    testing of the following resources on their penetration testing page: EC2, RDS,
    Aurora, CloudFront, API Gateway, Lambda, Lightsail, and Elastic Beanstalk. This
    section makes it sound like we can''t pentest a full AWS environment, but is in
    reference to traditional penetration techniques, such as port scanning, CVEs/exploits,
    bruteforcing, and so on. It is not referring to everything that we are referring
    to as pentesting within this book, because a majority of that is just using the
    AWS APIs to perform specific actions in the account, which is not against the
    AWS Acceptable Use Policy. For example, we can try to exploit misconfigurations
    in AWS systems manager to try and gain remote access to EC2 instances by using
    the AWS APIs, but we cannot port scan and try to abuse a buffer overflow in an
    AWS ElastiCache instance due to these rules.'
  prefs: []
  type: TYPE_NORMAL
- en: Credentials and client expectations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After the AWS pentesting authorization form has been taken care of (or during
    the process), the next step would be to determine what exactly the client is expecting
    from the AWS pentest. Is this a red team style engagement where our activity will
    be actively monitored and defended against by a blue team? Is this just an audit
    of configuration? Is this a go as far as possible type of engagement without an
    activate defense against us?
  prefs: []
  type: TYPE_NORMAL
- en: Beyond that, is the client supplying us credentials? If so, credentials for
    how many users and what information do we get about them? If not, should we be
    social engineering to gain access?
  prefs: []
  type: TYPE_NORMAL
- en: 'Other important questions may include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Is this a test/development/production environment?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is there anything we should not touch in the environment?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Are there other users who are actively using this environment?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are many other questions to ask around scoping, and that is ultimately
    determined by what you do as a pentesting company and what your client wants as
    your customer. Throughout this chapter, we will assume a scenario where we are
    provided a set of keys for a single IAM user and nothing else. This means we don't
    know what kind of access to expect or how their infrastructure works from the
    inside. Also, in our scenario, we will be acting as if there is not an active
    blue team that is trying to stop and shut down our access, but we will be monitored
    by existing tooling in the account. For all of those reasons, this means that
    we should view this engagement as if we just compromised access to the keys that
    they provided us and to simulate the attack as if we are a real attacker, even
    though we know the blue team won't stop us.
  prefs: []
  type: TYPE_NORMAL
- en: These types of engagements can be quite useful for clients because it offers
    them a variety of information to work off. It provides us pentesters with the
    full ability to show *what's possible* when their keys are compromised, and it
    provides them with a (Cloud)trail of logs and activity to see what kind of attacks
    they are detecting, what they are missing, and it even allows them to analyse
    this data as if this was an incident-response/forensics type situation. If the
    blue team was actively shutting us down during an engagement, we might not uncover
    all the actual vulnerabilities within the AWS environment, because our access
    was blocked. Without the blue team interfering, we can go as in-depth as possible,
    and it also allows us to perform configuration and best practice audits on services
    and resources in the account. In a real **red-team** type scenario, it would not
    make sense to check for certain configuration issues and best practices, because
    it would not directly benefit our attack and would only create more of a trail
    of our activity.
  prefs: []
  type: TYPE_NORMAL
- en: Providing auditing and configuration checks in addition to just an attack narrative
    can be extremely helpful to clients for compliance and security within the account,
    so it is best to be able to provide this information. On the other hand, what
    the client wants is most important, so it is essential to modify this attack narrative
    as they see fit.
  prefs: []
  type: TYPE_NORMAL
- en: Once client expectations have been determined, the AWS pentest authorization
    form has been approved, and you have received credentials, you are almost ready
    to start.
  prefs: []
  type: TYPE_NORMAL
- en: Setup
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before beginning any actual work, we need to make sure we are set up correctly.
    This setup might look different, but for this scenario, we need to ensure that
    the AWS CLI and Pacu are both installed on our system. Notes on how to do this
    were reviewed in previous chapters, but as a reminder, you can get Pacu from its
    GitHub page and the AWS CLI through Python `pip`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/RhinoSecurityLabs/pacu](https://github.com/RhinoSecurityLabs/pacu)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Once those tools are installed, we will want to integrate the AWS keys that
    we have available into them. The easiest way to do this would be to use the AWS
    CLI to create a credential profile, and then import that profile into Pacu. For
    both the `PersonalUser` and `CompromisedUser` set of keys that we noted earlier,
    we will run the `aws configure` command with the `--profile` argument, specifying
    each of those names, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we''ll enter our keys. After that, we can start up Pacu by using Python3
    and create a new session. We''ll name the session `Acme` because this engagement
    is for Acme Co. Then ,we can use the Pacu command `import_keys` to import our
    two key pairs from the AWS CLI into Pacu:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The reason we are adding our own personal user into the AWS CLI and Pacu is
    for when we are performing unauthenticated reconnaissance against our target,
    as those modules tend to require keys outside of the target account.
  prefs: []
  type: TYPE_NORMAL
- en: If the client told us that they only use a specific set of regions, then we
    could also use the `set_regions` command to set that up in Pacu, but for our scenario,
    we will say that we don't have this information (yet).
  prefs: []
  type: TYPE_NORMAL
- en: At this point, we are ready to move on to unauthenticated (cross-account) recon.
  prefs: []
  type: TYPE_NORMAL
- en: Unauthenticated reconnaissance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most unauthenticated recon within AWS isn't technically unauthenticated, because
    there are credentials that are required. The difference is that for unauthenticated
    recon, we use our own attacker AWS keys, so we are unauthenticated to our target
    environment, and any logs of our enumeration/attempts will show up in our own
    account only. This is almost as unauthenticated as you can get when enumerating
    AWS resources, besides something like open S3 buckets, but even then, some kind
    of credential can help the process due to how permissions are set up in some buckets.
  prefs: []
  type: TYPE_NORMAL
- en: One integral part to most unauthenticated/cross-account attacks is the knowledge
    of the target AWS account ID. The account ID allows us to associate resources
    with that specific account from our own. This means that our first API call to
    AWS will actually be from the `CompromisedUser` and not our `PersonalUser`. The
    reason for this is because we don't have the account ID yet, and we need it. Luckily,
    there has been research done to gain information about a set of keys without logging
    anything to CloudTrail, like we covered in [Chapter 15](88017977-3b7b-4e4f-b4fe-60fa603200f3.xhtml), *Pentesting
    CloudTrail* .
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll be using the `iam__detect_honeytokens` module to gather the information
    that we require:'
  prefs: []
  type: TYPE_NORMAL
- en: 'As the `CompromisedUser,` we will run the Pacu command, `run iam__detect_honeytokens`.
    The reason for this is because the module uses an AWS API call that is not logged
    to CloudTrail to enumerate the current user''s ARN, which contains the account
    ID, we will have gathered the account ID without them being aware. The following
    screenshot shows the output when running that module in our test environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/69fc7502-0c1d-42e8-ad94-77a9ad9d1c56.png)'
  prefs: []
  type: TYPE_IMG
- en: The iam__detect_honeytokens module fetching our ARN without logging to CloudTrail
  prefs: []
  type: TYPE_NORMAL
- en: We can see that our `CompromisedUser` has the username `CompromisedUser` and
    it resides in account ID `216825089941`. We could run the `whoami` command now
    to see that this information was added to the Pacu database if we wanted to do
    so. Now that we have the account ID, we can get started with out unauthenticated
    recon. This unauthenticated portion will involve enumerating IAM users and roles
    in the account and potentially S3 buckets associated with the company or account.
  prefs: []
  type: TYPE_NORMAL
- en: We'll kick that off by first noting the account ID we just enumerated, then
    swapping keys to the `PersonalUser` in Pacu by running the `swap_keys` command.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'As the `PersonalUser,` we will then run the `iam__enum_users` module to try
    and detect any users in the target account. We''ll pass the account ID we just
    got to this module so that it knows where to look for users. We will also pass
    `Test` as the value for the `--role-name` argument, because we have a role in
    our personal account named `Test` and it is required for the `UpdateAssumeRolePolicy`
    API call. The final command will end up being `run iam__enum_users --role-name
    Test --account-id 216825089941`. Many logs will be created in your own account''s
    CloudTrail, but not the target''s account. The following screenshot shows the
    execution of that comment, where we can see that three separate IAM users were
    discovered:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/e159c177-da1b-46c5-9261-4f3d2f2051dc.png)'
  prefs: []
  type: TYPE_IMG
- en: Some of the output from the iam__enum_users module, indicating that we discovered
    three users in our target account
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we are going to want to do the same thing with the `iam__enum_roles`
    module by running the following command: `run iam__enum_roles --role-name Test
    --account-id 216825089941`. The following screenshot shows the execution of that
    module, where we can see that four IAM roles were enumerated:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/fc75780e-4314-4fc5-a152-04aa88362a5d.png)'
  prefs: []
  type: TYPE_IMG
- en: Part of the output from the `iam__enum_roles` module, indicating four roles
    were found, but none could be assumed for credentials
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s look at the user and role names that we enumerated. We found three
    users:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Test`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ExampleUser`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`LambdaReadOnlyTest`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Test` and `ExampleUser` aren''t all that helpful in our recon, but `LambdaReadOnlyTest`
    indicates that our target is probably using the Lambda service in their account.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We also found four roles:'
  prefs: []
  type: TYPE_NORMAL
- en: '`MyOwnRole`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`LambdaEC2FullAccess`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CloudFormationAdmin`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`SSM`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These role names are much more helpful that the users we enumerated. `MyOwnRole`
    is kind of useless, but `LambdaEC2FullAccess` indicates that Lambda is in use
    in their environment, just like we deduced from that one user, but this role name
    also indicates two more potential possibilities:'
  prefs: []
  type: TYPE_NORMAL
- en: There may be Lambda functions that are launched into VPCs, giving them internal
    access to that network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There may be Lambdas that directly interact with the EC2 service, meaning that
    our target also probably uses the EC2 service within their environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `CloudFormationAdmin` role indicates that CloudFormation is likely utilized
    within the environment, so we will want to keep that in mind as we begin our attack.
    It may be able to help us gather more information about the target environment
    with a small amount of API calls.
  prefs: []
  type: TYPE_NORMAL
- en: The `SSM` role indicates that this role was created for the systems manager.
    We can assume that this means they are using the systems manager in their environment
    to remotely control/manage EC2 instances or on-premise servers.
  prefs: []
  type: TYPE_NORMAL
- en: Now, without creating any logs in the target account, we have enumerated multiple
    users and roles that exist, as well as gathered a reasonable amount of information
    on how their infrastructure might be set up across different AWS services.
  prefs: []
  type: TYPE_NORMAL
- en: 'The last part of our unauthenticated reconnaissance will be to look at S3 buckets
    with the Pacu `s3__bucket_finder` module. Hypothetically, we will assume our target
    Acme Co. owns the domain `acme.com`, so we will pass that to this module to look
    for existing buckets. We can do this with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The output should show us if there are any buckets that were discovered and
    then if any of those buckets were listable. Unfortunately, our scan did not provide
    any actionable results, as can be seen in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d855b1c0-0243-44db-bc6a-1a461166d5c3.png)'
  prefs: []
  type: TYPE_IMG
- en: The module did not find any buckets for us to look at
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from the preceding screenshot, the module has external dependencies.
    Currently, this is the only module that utilizes the `install_dependencies` function
    and it does so to Git clone `Sublist3r` for sub-domain mutations and `Buckets.txt`
    for bucket bruteforcing. Because we only used the `-d` argument, neither of those
    external dependencies were utilized.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we have done what we can from outside of our target account. It is time
    to grab the `CompromisedUser` credentials and start the authenticated phase of
    our two-part reconnaissance.
  prefs: []
  type: TYPE_NORMAL
- en: Authenticated reconnaissance plus permissions enumeration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To begin the authenticated recon portion of our assessment, we will need to
    use the `swap_keys` Pacu command to switch from our `PersonalUser` to the `CompromisedUser`:'
  prefs: []
  type: TYPE_NORMAL
- en: Run `swap_keys` in Pacu to switch to the `CompromisedUser`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The first thing to do for authenticated recon is to find out our own privileges
    so that we know what kind of access we have to the AWS account. This can be done
    by using the `iam__enum_permissions` Pacu module. It doesn''t need any arguments
    for our current purpose, so we can run the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we can check out what permissions were enumerated with the `whoami` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/32d75759-9464-4d53-bf88-90d147311bbc.png)'
  prefs: []
  type: TYPE_IMG
- en: Running iam__enum_permissions and checking out what data was enumerated with
    the whoami command
  prefs: []
  type: TYPE_NORMAL
- en: We can see that there are three IAM policies attached to our user, two of which
    are AWS-managed policies (`AmazonEC2FullAccess`, `DatabaseAdministrator`), and
    one of which is an inline policy (`IAM-Read-List-PassRole`). We can determine
    that these are AWS-managed policies because of the included ARN under the `Policies`
    section of the results of the `whoami` command. The `IAM-Read-List-PassRole` policy
    does not have an ARN listed, which means it is an inline policy, rather than a
    managed policy.
  prefs: []
  type: TYPE_NORMAL
- en: If we were to scroll down, we would see the list of permissions that our user
    is allowed/denied and the resources/conditions those permissions apply to.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have enumerated our own permissions, and saved them to the database,
    we can see that we have full access to AWS EC2, whatever access the `DatabaseAdministrator`
    policy grants us (we can view this policy directly from our own personal account
    if we wished to do so, or we can look at the list of permissions Pacu provides),
    and whatever the `IAM-Read-List-PassRole` policy grants us (we can assume it grants
    us permission to read and list to the IAM service, as well as pass IAM roles to
    other AWS services/resources). All of this can be confirmed by reviewing the list
    of permissions that Pacu provides in the `whoami` command.
  prefs: []
  type: TYPE_NORMAL
- en: It is very important to enumerate our own user's permissions but be wary that
    enumerating such permissions might trigger a GuardDuty alert based on IAM enumeration
    within the account. We don't only want just our own permissions, though; we also
    would like to look at the permissions for every other user and role in the account
    so that we can provide our client with a full list of possible misconfigurations
    within the environment. We could use the `iam__enum_users_roles_policies_groups`
    module to do this, but that will only enumerate basic information about each of
    those IAM resources. We would rather use the `iam__enum_permissions` module again
    to gather the full set of permissions for each user/role in the environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can begin enumerating all user and roles permissions by using the `--all-users`
    and `--all-roles` arguments, which can be see in the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, Pacu will cycle through each user and role in the account and dump their
    permissions to a JSON file in our Pacu folder. This information can then be manually
    reviewed and/or passed to the Pacu privilege escalation module to check for privilege
    escalation vectors across all of them:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a51c0c9f-6814-4b90-978b-3bc1950a382b.png)'
  prefs: []
  type: TYPE_IMG
- en: The output of the `iam__enum_permissions` module when targeting all users and
    roles
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can see in the preceding screenshot, Pacu hadn''t enumerated users and
    roles in the target account, so it asked us if we wanted to do that before executing.
    Then, we can see that it is saving the permissions of each user and role to `sessions/Acme/downloads/confirmed_permissions/`
    within the Pacu folder. When the module is complete, we can inspect those files
    for the permissions of those users/roles, which will be in a similar format to
    the output of the `whoami` command for our own user:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/05c01791-3e2c-4199-99e4-51e20b60544d.png)'
  prefs: []
  type: TYPE_IMG
- en: Part of the contents stored within the JSON file that contains the permissions
    of the SSM role
  prefs: []
  type: TYPE_NORMAL
- en: The next step(s) of enumeration can theoretically wait until we are ready to
    attack a specific service, but this could also be done all at once, prior to that.
    A good couple of modules to run at this point could be the `aws__enum_account`
    and `aws__enum_spend` modules to provide insights into the organization that the
    user is a part of and the type of money that is being spent on various AWS services.
    This data can provide you with information that allows you to determine what AWS
    services are being used (and to what extent), without querying the specific services
    themselves. For example, if we can see that the total account spend is $1,000.00,
    and that the spend on the EC2 service is $750.00, then we can assume that most
    of their resources reside in EC2\. Your assumptions may not always be 100% accurate,
    but it can often give a high-level overview of what to expect.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, run the `run aws__enum_account` command in Pacu, followed by the `run
    aws__enum_spend` command to receive output similar to what''s shown in the following
    screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/ee18a396-804d-4f9f-b1aa-9273ac45445a.png)'
  prefs: []
  type: TYPE_IMG
- en: The output of the aws__enum_account module and part of the output of the aws__enum_spend
    module
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see that the `aws__enum_account` module provided us with the total account
    spend in USD ($), which came out to $0.98, but we were not authorized to gather
    any information on the account''s organization. We can also see the beginning
    of the output of the `aws__enum_spend` module, which is checking the metrics for
    each AWS service to determine the money spent on it. The results are shown in
    the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/73df535b-f7a0-4862-821a-c50f43762a4a.png)'
  prefs: []
  type: TYPE_IMG
- en: The AWS account spend for our target account
  prefs: []
  type: TYPE_NORMAL
- en: We can see that most of the account spend shows up under the AWS Glue service
    and the Amazon Document DB service, with some in GuardDuty and AWS Amplify. Although
    this information is helpful, it should not be relied on as 100% factual, because
    any spend that qualifies for the AWS free tier will not be logged here; this is
    not an up-to-date by-the-minute list of account spend, and not all AWS resources
    cost money to have. For those reasons, it is still worth checking out the specific
    services directly, but it can be helpful to start off with this list.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can usually form our attack around the data that''s returned from the `aws__enum_spend`
    module, but in this case, our example Acme Co. discussed EC2 at one point prior
    to the engagement. Working off that information, and the fact that EC2 is often
    one of the most fruitful services to target, we are going to run the `ec2__enum`
    module to discover any EC2 resources in the account. We can do that with the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Because we haven''t set any session regions in Pacu, we will be prompted and
    asked if we want to target every AWS region, which we will reply to with yes.
    This is because we don''t know what regions are being used yet, so it is worth
    checking out each one until we can find that information out:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cc088745-2b32-4326-8bdc-5d7432f3cf86.png)'
  prefs: []
  type: TYPE_IMG
- en: The summary results of the ec2__enum module
  prefs: []
  type: TYPE_NORMAL
- en: We can see that seven total EC2 instances were discovered in the scan across
    every region. If we scroll up in the results, we can determine that there is one
    EC2 instance in `us-east-1` and six EC2 instances in `us-west-2`.
  prefs: []
  type: TYPE_NORMAL
- en: If we wanted to assume that only `us-east-1` and `us-west-2` are used across
    the whole AWS account, we could set the Pacu session regions to those two regions,
    but it is difficult to make that assumption just based off a single service, so
    we aren't going to do that.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have enumerated what EC2 resources exist, we'll look at the `EC2
    userdata` for each of the instances, as that is one of the simplest, yet most
    fruitful, security checks that can be run against EC2 instances. Often, we can
    find private information (that shouldn't be in there) or other general information
    that can help us gather a better overview of what is going on in the environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do this, run the `run ec2__download_userdata` command in Pacu. The following
    screenshot shows that we found `userdata` in two of the instances we enumerated
    in the environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/e648c369-ecae-4f24-9bb2-db244be3b043.png)'
  prefs: []
  type: TYPE_IMG
- en: The results of using the ec2__download_userdata module
  prefs: []
  type: TYPE_NORMAL
- en: As we can see from the preceding screenshot, the module first asks if we want
    to enumerate `EC2 LaunchTemplates` (which can hold `userdata` as well), because
    there are none in the database, which we respond to with no, because we know that
    we have already enumerated those (with `ec2__enum`) and none were found. Then,
    we can see that two out of the seven EC2 instances have `userdata` attached to
    them, which was then stored in our Pacu folder at :`./sessions/Acme/downloads/ec2_user_data`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s check out that `userdata` by reviewing those files to see if there is
    anything interesting in them. We''ll do this with the `cat` command, which will
    output the contents of the text file we specify to the screen:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/2b576881-ad5c-4f0b-b2e0-b97ec0cd4e26.png)'
  prefs: []
  type: TYPE_IMG
- en: Outputting the contents of the two files with EC2 user data in them
  prefs: []
  type: TYPE_NORMAL
- en: Based on the output of the first instance (`i-07fdb3fbb2a9a2444`), we can see
    that when it was launched, it used `apt-get` to install the AWS CLI and then used
    it to copy a file from a private S3 bucket to the root folder. This tells us that
    there is likely an IAM role attached to that EC2 instance, because no credentials
    are set up within the `userdata`, but we could confirm that with the `data EC2`
    command in Pacu, where we could find the details of that instance.
  prefs: []
  type: TYPE_NORMAL
- en: The second instance that we looked at for the `userdata` looks juicy. It is
    using the `curl` program to get an authorization token from Acme.com's API. It
    is using basic authentication, so we can see the administrator username (`admin`)
    and password (`P@ssW0rd`) right there in the command. We can now perform some
    simple recon on the Acme.com website to find out what access the administrator
    account will provide us. Once that's done, we can just request our own authorization
    token, using the same credentials and API, where we then could pivot access into
    the main `Acme.com` website.
  prefs: []
  type: TYPE_NORMAL
- en: Attacking a random web application is beyond the scope of this book, but this
    would be an extremely valid attack path to take during an AWS pentest, if a few
    conditions are met. First, the web application should be hosted within the AWS
    environment we are attacking for it to be considered in-scope and, second, we
    need to determine if this is within the client's expectations. If either of these
    are questionable, it would be worth it to contact our client and ask them directly.
    If this attack is allowed, we may be able to escalate this attack to take control
    of the web application, or we may be able to expand our AWS access even further,
    depending on what we find within it.
  prefs: []
  type: TYPE_NORMAL
- en: There are other services we could enumerate and other enumeration modules we
    could run within Pacu, but we are going to move on from that for now and look
    at privilege escalation. After we attempt to abuse our users' privileges for privilege
    escalation through regular means, it will then be time to review the other services
    in the account and try to use those for privilege escalation (and/or other attacks).
  prefs: []
  type: TYPE_NORMAL
- en: Privilege escalation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have already enumerated our own users'' privileges, as well as every other
    user''s and the roles within the account we are targeting. We can now pass the
    information that the `iam__enum_permissions` module generated to the `iam__privesc_scan`
    module to check for any instances of privilege escalation within the account.
    We''ll first use the `--offline` argument so that the module knows we are checking
    everyone''s privilege escalation paths. Without that argument, it will only check
    our own user''s privilege escalation paths and then try to exploit them to gain
    escalated access to the environment. The following screenshot shows the output
    of the `iam__privesc_scan` module, where it has identified multiple users who
    already have administrator privileges to the environment and multiple users who
    are vulnerable to a few different kinds of privilege escalation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d8ebcfaf-d238-4c27-8b9d-6dff0c983229.png)'
  prefs: []
  type: TYPE_IMG
- en: Running the iam__privesc_scan module with the --offline argument
  prefs: []
  type: TYPE_NORMAL
- en: There are a few things that we can take away from this output. We can see that
    the users `Spencer`, `DaveY`, `ExampleUser`, and `Alex` and the roles `EC2Admin`
    and `CloudFormationAdmin` all already have administrator access to the environment.
    After that, we can see that the roles `AWSBatchServiceRole`, `AWSServiceRoleForAutoScaling`,
    and `aws-elasticbeanstalk-service-role` and the user `CompromisedUser` are potentially
    vulnerable to various privilege escalation methods.
  prefs: []
  type: TYPE_NORMAL
- en: The good news is that our own user, `CompromisedUser`, is potentially vulnerable
    to four different escalation methods, which means we will likely be able to gain
    further access to the environment. If we wanted to look at this data again later,
    we could navigate to the Pacu `./sessions/Acme/downloads/` folder to review the
    JSON file that was generated, where the privilege escalation data is stored, as
    indicated at the bottom of the module output. When we are finished with our pentest
    (after we have verified the results of the privilege escalation scan), we will
    want to make sure that we report this information to the client, even if it isn't
    directly our own user that is vulnerable.
  prefs: []
  type: TYPE_NORMAL
- en: 'The results of the privilege escalation scan aim to be self-explanatory by
    their names, but if you are interested in the specifics of each privilege escalation
    method, it is suggested that you check out this link: [https://rhinosecuritylabs.com/aws/aws-privilege-escalation-methods-mitigation/](https://rhinosecuritylabs.com/aws/aws-privilege-escalation-methods-mitigation/).
    The module is built around the content of that blog post, so you can match up
    privilege escalation methods with the manual guides explained in the blog post.'
  prefs: []
  type: TYPE_NORMAL
- en: If we look at the `privesc` methods that our `CompromisedUser` is vulnerable
    to, it tells us that it is potentially vulnerable to four different methods. The
    `CreateEC2WithExistingIP` method means that we potentially have the privileges
    to launch a new EC2 instance and pass an existing instance profile to it, where
    we would then be able to gain access to the IAM role credentials associated with
    the instance profile. The `"PassExistingRoleToNewLambdaThenTriggerWithNewDynamo"`
    and `"PassExistingRoleToNewLambdaThenTriggerWithExistingDynamo"` `privesc` methods
    mean that we potentially have access to create a new Lambda function, pass it
    an IAM role, and then invoke the function through either a new or existing DynamoDB
    event source mapping.
  prefs: []
  type: TYPE_NORMAL
- en: The `PassExistingRoleToNewDataPipeline` method tells us that we potentially
    have the privileges to launch a new data pipeline to execute the AWS CLI as the
    role that we pass. We could manually go through each one of these methods to try
    and gain further access, but it would be much more efficient to use the exploitation
    feature of the `iam__privesc_scan` module, which will automatically try to escalate
    our users' privileges using the available methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'To auto-exploit the privilege escalation methods, we can simply run the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, it will find our users vulnerable `privesc` methods automatically, and
    it will cycle through each one until it successfully gains additional privileges.
    Due to the complexity of some of the privilege escalation methods, user input
    may be required at various points. When we first run it, it will find those privilege
    escalation methods again and then dive into the `CreateEC2WithExistingIP` privilege
    escalation method, which can be seen in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7c3fa2a1-417c-4e45-b674-be63bd6c6b96.png)'
  prefs: []
  type: TYPE_IMG
- en: The privesc scan module attempting to gain privileges through the first method
  prefs: []
  type: TYPE_NORMAL
- en: 'It is asking for a region because we haven''t set any session regions for the
    Pacu session, so we will supply `15` to target the `us-west-2` region:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4a484c89-9fec-4752-8d8e-ee56d6d0c96d.png)'
  prefs: []
  type: TYPE_IMG
- en: The EC2 privilege escalation method wants us to choose an instance profile to
    attach to the instance
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can see in the preceding screenshot, there are six EC2 instance profiles
    that are eligible to be attached to our instance. We want to choose the one with
    the highest privileges, because it is the role we will gain access to through
    this method. We could determine this information by viewing the output of the
    full account `iam__enum_permissions` module from earlier, but if we look back
    to a minute ago at the full account privilege escalation scan, we will see that
    it told us that the `EC2Admin` role already has administrator permissions. That
    makes it an obvious choice for this question:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3ce2c35e-3aa9-48fa-b0c3-911896cb3d39.png)'
  prefs: []
  type: TYPE_IMG
- en: The next question we are asked after choosing an instance profile
  prefs: []
  type: TYPE_NORMAL
- en: Next up, we will be presented with a question and five options to pick from.
    The question is asking us how we would like to use this EC2 instance to escalate
    our privileges. Option one is to open a reverse shell to our own server on startup,
    allowing us to do what we want from within the instance. Option two is to run
    an AWS CLI command from within the target instance, using the role credentials
    that we attached to the instance. Option three is to make an HTTP request outbound
    from the EC2 instance to our own server that contains the current credentials
    of the IAM role. Option four is to create a new SSH key in AWS, provide you with
    the private key, and then launch the instance with that key to allow you to SSH
    into it. Finally, option five is to skip this `privesc` method and move to the
    next one. Depending on your personal setup and the setup of the environment, you
    will have to choose what will work best for you.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this pentest, I am going to choose option one, a reverse shell, because
    it won''t trigger GuardDuty and it only requires the default EC2 security group
    to allow outbound internet access to the port we specify (rather than something
    like port `22` inbound for option four). From that reverse shell, we can then
    use the AWS CLI from within the instance, curl the role credentials from the EC2
    metadata API, or whatever else we want:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f13b4a0d-1263-4662-b7fb-e04ac5bcda56.png)'
  prefs: []
  type: TYPE_IMG
- en: Using the reverse shell option for this privilege escalation method
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous screenshot, we can see that we provided the IP address (censored)
    and port of our attacker-owned server. Then, the module outputs the details about
    the EC2 instance that it created. Now, all we need to do is wait for our reverse
    shell to show up:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4489248c-f671-4774-acbc-9d5da94e14b8.png)'
  prefs: []
  type: TYPE_IMG
- en: Setting up our netcat listener, where we receive our reverse shell as the root
    user
  prefs: []
  type: TYPE_NORMAL
- en: As we can see in the previous screenshot, we listened on port `5050` with netcat,
    ran the `whoami` command to see that we are the root user, and then used the AWS
    CLI to run the `STS GetCallerIdentity` command. The output of that command shows
    us that we are authenticating with AWS as the assumed-role `EC2Admin`, which we
    know has full administrator privileges to the environment.
  prefs: []
  type: TYPE_NORMAL
- en: Although we have access to an administrator in the AWS environment, it is only
    temporary. We might lose this EC2 instance at any minute or the credentials will
    expire before we can do anything useful with them, so we need to take quick action
    to escalate our original `CompromisedUser` permissions and save the EC2 instance
    as a backup. Essentially, once we escalate our own user's permissions, the EC2
    instance will act as pseudo-persistence in the account, potentially allowing us
    to gain administrator-level permissions again in the future, if need be.
  prefs: []
  type: TYPE_NORMAL
- en: 'To escalate our own user to an administrator, we will run the following AWS
    CLI command, which attaches the `AdministratorAccess` AWS-managed IAM policy to
    our `CompromisedUser`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'This command does not return any output if it was successful, so we can go
    back to the `iam__enum_permissions` Pacu module again to confirm that we are an
    administrator:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0327c5a5-f797-4cb3-8567-a49eeb52da60.png)'
  prefs: []
  type: TYPE_IMG
- en: Re-running iam__enum_permissions, then running whoami, and seeing that the AdministratorAccess
    IAM policy is attached to us
  prefs: []
  type: TYPE_NORMAL
- en: If we wanted to confirm even further, we could try running an AWS CLI command
    or Pacu module that we know we didn't previously have access to, but the fact
    that the policy is attached to our user shows that we are, in fact, an administrator.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have enumerated IAM and EC2 data, launched a backdoor EC2 instance
    to allow for privilege escalation, and then used an EC2 instance to make our `CompromisedUser`
    an administrator in the environment. At this point, we should establish some persistence
    before moving on to other AWS services.
  prefs: []
  type: TYPE_NORMAL
- en: Persistence
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although we already have an EC2 instance that we have access to and that provides
    us access to an administrator level role in the environment, we shouldn't rely
    on it as our sole method of persistence for a few reasons. The role could change
    at any moment, such as if it was deleted or had its privileges modified, which
    would remove or weaken our persistent access.
  prefs: []
  type: TYPE_NORMAL
- en: The EC2 instance could be noted as suspicious and shut down at any point, removing
    our persistent access. Also, EC2 security groups rules could be modified, blocking
    outbound access from the instance, meaning we will no longer receive our reverse
    shell. Finally, we might lose the reverse shell connection, which means we would
    need to wait until the instance is restarted to get the reverse shell connection
    sent back again. There are a lot of ways things could go wrong, even without a
    defender trying to stop us, so an EC2 instance with an attached role is not a
    reliable method of persistence, although it does work for at least a short time
    period.
  prefs: []
  type: TYPE_NORMAL
- en: 'Just to be thorough/safe, we will launch a few different methods of persistence
    into our target account:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first method of persistence we will use will be to create new access key
    pairs for another user or two in the account with the `iam__backdoor_users_keys`
    Pacu module by running the `run iam__backdoor_users_keys` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/06b1140c-7a0e-4032-b2f0-e8173562ad4b.png)'
  prefs: []
  type: TYPE_IMG
- en: Backdooring the DaveY and Spencer users with the `iam__backdoor_users_keys`
    module
  prefs: []
  type: TYPE_NORMAL
- en: As we can see in the preceding screenshot, the module will prompt us, asking
    which users we want to create backdoor AWS keys for.
  prefs: []
  type: TYPE_NORMAL
- en: We chose `DaveY` and `Spencer` for this example, because they showed up as administrative
    users when we ran the privilege escalation scanner earlier, which means we'll
    have elevated persistence for as long as these keys are alive.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, we are going to create a new Lambda backdoor within the account to backdoor
    any newly created IAM roles so that we can assume their credentials cross-account.
    We can do this with the `lambda__backdoor_new_roles` Pacu module. We need a role
    that has the IAM `UpdateAssumeRolePolicy` and `GetRole` permissions for our backdoor,
    so we are going to add that permission to an existing role that allows Lambda
    to be assumed. We can do this with the AWS CLI by running the following command,
    which targets the `LambdaEC2FullAccess` role:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'There is one thing left to do. The module tells us that CloudTrail must be
    enabled in the `us-east-1` region for our backdoor function to ever trigger, so
    we should double-check this, just in case. The following command can do just what
    we want:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'In our case, there is one residing in `us-east-1`, so we are good to go with
    the backdoor module, which can be seen in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8d10fd48-2301-4683-b3ff-4518b5285398.png)'
  prefs: []
  type: TYPE_IMG
- en: Creating a backdoor Lambda function and CloudWatch Events rule
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can see in the previous screenshot, we ran the following Pacu command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: This command assumes that we are hosting an HTTP listener at the IP `x.x.x.x`
    (censored) on port `5050` and that our `PersonalUser` AWS user resides in AWS
    account ID `000000000000`. When it is run, Pacu will generate the code for the
    Lambda function, zip it, and then upload it to Lambda. After that, it will create
    a CloudWatch Events rule that triggers on any IAM `CreateRole` API calls. Now,
    every time a new IAM role is created, our CloudWatch Events rule will be triggered,
    which causes our Lambda function to be invoked, which then will use the IAM `UpdateAssumeRolePolicy`
    API to add our external user (`PersonalUser`) as a trusted entity that can assume
    it. When that is done, it will exfiltrate the ARN of the new role to the URL we
    provided in the command so that we can use it to gain access to the account whenever
    we want.
  prefs: []
  type: TYPE_NORMAL
- en: 'After a short while of waiting, we finally receive a request to our **command
    and control** (**C2**) server with an IAM role ARN, which means that one was created
    and that we backdoored it automatically with our Lambda function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e6df982c-0623-4e60-9838-ec3d44e784a6.png)'
  prefs: []
  type: TYPE_IMG
- en: Our own server listening on port 5050 for IAM role ARNs from our backdoor Lambda
    function
  prefs: []
  type: TYPE_NORMAL
- en: As we can see in the preceding screenshot, an `HTTP` `POST` request was made
    to our server with a URL-encoded IAM role ARN (named `A-New-Role`) in the body.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we want to request credentials for this backdoored role, we would use the
    STS `AssumeRole` API. We can do this by running the following AWS CLI command,
    using the credentials of our `PersonalUser`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: We could use this same command for any other role that ends up getting created
    and exfiltrated to our server; we would just need to modify the ARN in it.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we are an administrator in the account, we have several forms of elevated
    persistence, and we have also performed some basic reconnaissance in the account.
    Now, we are ready to move on to the service exploitation phase.
  prefs: []
  type: TYPE_NORMAL
- en: Post-exploitation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The post-exploitation (or service exploitation) phase is essentially where we
    target as many AWS services as possible to try and uncover weaknesses, misconfigurations,
    and bad practices. We'll cover some of the primary AWS services in this section,
    but any AWS service is a potential for exploitation and misconfigurations, so
    it is almost always fruitful to look at any service or resources that are being
    used, even if you may be unfamiliar with the service itself.
  prefs: []
  type: TYPE_NORMAL
- en: EC2 exploitation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have already begun working on some EC2-related stuff, so that's where we
    are going to start. EC2 is also one of the most common services you will encounter
    during your pentests, so it is a good idea to become intimately familiar with
    it and with testing it. EC2 can yield some high impact findings when misconfigured
    as well, so you can't go wrong by starting with it as your primary service.
  prefs: []
  type: TYPE_NORMAL
- en: The first thing we could check out is what, if any, EC2 instances have public
    IP addresses. This is simple in the AWS web console, as you can simply sort the
    results by instances with public IPs. If we wanted to gain console access from
    our `CompromisedUser`, we could use the IAM `CreateLoginProfile` API to create
    a password for us to login with, but if we didn't want to do so, we could use
    the `data EC2` command in Pacu to review the results of the enumeration we performed
    earlier.
  prefs: []
  type: TYPE_NORMAL
- en: Then, for each of the instances that have public IP addresses, we could check
    out the EC2 security groups attached to them. Ideally, we look through the security
    group rules to try and find any services that may be running on the instance.
    If we see port 80 open to some IP address, we know there is likely a web server
    running on the instance. If we see port 22 open to some IP address, we know there
    is likely an SSH server running (and so on). If any of these ports are open to
    the public, we could attempt to access these and look for any low-hanging-fruit,
    such as weak/lack-of authentication, known exploits, or anything else you might
    look for in a network-style pentest.
  prefs: []
  type: TYPE_NORMAL
- en: We could even perform those same tasks on instances without public IP addresses,
    if the right conditions are met, but with administrator access, we can likely
    make anything work. We already launched an EC2 instance into the account for our
    privilege escalation, so we are potentially within the VPC of other EC2 instances.
    If not, we could just launch another instance and gain access that way. From that
    instance, we can access the internal IPs of other EC2 instances, so we could likely
    gain further access like that.
  prefs: []
  type: TYPE_NORMAL
- en: 'If none of this worked out, we could just modify the security group rules on
    these instances to allow ourselves access. You could do this manually with the
    EC2 `AuthorizeSecurityGroupIngress` API, or we could use the `ec2__backdoor_ec2_sec_groups`
    module to create backdoor rules that allow us access to any port. The Pacu command
    to make this happen would look as follows, where we are opening every port to
    the `1.1.1.1` IP address (simulating that it is our own IP) for all security groups:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Now, we should be able to access any port on any instance if we are originating
    from the `1.1.1.1` IP address. At this point, we could attack these services like
    you would in a regular internal network pentest.
  prefs: []
  type: TYPE_NORMAL
- en: If we wanted to directly gain RCE on any EC2 instances, there are a couple methods
    we could attempt. If you don't care about restarting any of the EC2 instances
    (which you should care about, as we don't typically want to do this to client
    servers), then you could use the `ec2__startup_shell_script` Pacu module to stop
    all (or specified) EC2 instances, modify their `userdata` to input a reverse shell
    as `root/SYSTEM` on startup, and then start all those instances back up. They
    would only be offline for a few minutes, but this could cause major problems if
    you are unfamiliar with the setup of the environment, so it is typically not recommended.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we wanted to gain RCE on EC2 instances and the right conditions have been
    met, we could use the `systemsmanager__rce_ec2` module in Pacu. It tries to identify
    what EC2 instances have the systems manager Agent installed on them (by default
    or not), and then if it identifies any, it will try to attach the systems manager
    role to them. Once that is done, instances with the correct conditions met will
    then show up as available targets for the systems manager `run` command, which
    allows you to execute code as the `root/SYSTEM` user on the target instance. An
    example Pacu command, which runs a reverse bash shell on Linux targets, might
    look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The value that's supplied to the `--command` argument is a bash reverse shell
    that will call out to the `1.1.1.1` IP address on port `5050`. On my server (assuming
    I control `1.1.1.1`), I would run a netcat listener, such as `nc -nlvp 5050`,
    to wait for my shell to come in. Keep in mind that this will only work for a single
    instance and that you will need to modify your payload if you want to drop some
    sort of malware or reverse shell on multiple instances. You also would likely
    need another payload for Windows hosts.
  prefs: []
  type: TYPE_NORMAL
- en: If `PacuProxy` is enabled and listening when running this module, you can omit
    the `--command` argument. If you do so, then Pacu will automatically use its custom
    Linux/Windows one-liner stagers to take control of the target servers. This way,
    you don't need to worry about the target operating system or come up with your
    own command.
  prefs: []
  type: TYPE_NORMAL
- en: If we wanted to test other protections/monitoring capabilities, or we wanted
    to be just plain malicious, we could attempt to spin up multiple EC2 instances
    for something like cryptocurrency mining, but this should almost never be performed
    during a pentest because of the cost implications of such an attack. Only perform
    an attack like this if your client fully understands and wants the tests that
    you will be performing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another attack we might want to try out would be inspecting EBS volumes and
    snapshots in the account. We could do this in a couple ways, but essentially these
    are the steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a snapshot of the EBS volume you want to look at.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Share that snapshot with your attacker account, or create an EC2 instance in
    the compromised account.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a new EBS volume from the snapshot you created.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Mount that EBS volume on your EC2 instance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Dig through the filesystem of the mounted volume, looking for secrets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The benefit of sharing the EBS snapshot cross-account is that you can then use
    EC2 in your own account to check everything out, but typically a shared/public
    EBS snapshot is audited for by many configuration checkers, which means you might
    get flagged and caught. The benefit of using an EC2 instance in the compromised
    account is that you can avoid sharing snapshots cross-account, but you risk getting
    caught and removed at any moment.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `ebs__explore_snapshots` Pacu module was built to automate this process.
    You can just run it and pass in an instance ID of an EC2 instance within the account
    and its availability zone, then it will cycle through all the EBS volumes in the
    account (a few at a time), mount them to your EC2 instance, and then wait until
    you are done searching through the filesystems. When you are done, it will then
    detach all the volumes it attached to your instance, delete them, and then it
    will delete any of the snapshots that it created as well. An example command to
    run this module might look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: This will then incrementally attach EBS volumes to that instance in availability
    zone `us-east-1a`, allowing you to check them out in small groups at a single
    time, and then it will clean everything up for you after.
  prefs: []
  type: TYPE_NORMAL
- en: Code review and analysis in Lambda
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Lambda is another extremely common and extremely fruitful service to look at,
    just as we saw in the Lambda pentesting chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing we will want to do is enumerate Lambda functions in our target
    account with the `lambda__enum` Pacu module. We can run it without any arguments,
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: When this is complete, we can then run `data Lambda` to review the function
    data that was enumerated. To start the review process, we should cycle through
    each function and look at the environment variables associated with it to try
    and find some sensitive data/values that might be useful in our attack.
  prefs: []
  type: TYPE_NORMAL
- en: After checking out environment variables for interesting data, if we found anything,
    such as if we found API keys or passwords, then we'll want to screenshot and make
    notes about it so that we can report it to the client. If what we found is open
    for abuse in some way, then now would likely be the time to do so, but only do
    so if it is still within the scope of your engagement. Sometimes, the secrets
    you find will belong to third-party services and you likely shouldn't be attacking
    them, but other times, where you could capitalize with privilege escalation or
    gain cross-AWS-account access to somewhere, it will likely be worth it after confirming
    with your client point-of-contact.
  prefs: []
  type: TYPE_NORMAL
- en: When we are done with that, you could go through the Pacu Lambda data and download
    the code for each Lambda function for local analysis. Once downloaded, you can
    then run static source code security tools on them, such as Bandit for Python,
    to try and discover any inherent weaknesses in the code.
  prefs: []
  type: TYPE_NORMAL
- en: After an automated and manual review of the code, if you discovered any potential
    vulnerabilities, now would be the time to exploit them to confirm the findings.
    If you see that a Lambda function gets triggered by S3 and then places user-controllable
    data into an unsafe operating system command, you could use this to gain remote-code
    execution on the Lambda function to steal the IAM credentials of the attached
    IAM role.
  prefs: []
  type: TYPE_NORMAL
- en: Getting past authentication in RDS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With the correct RDS permissions, we can potentially gain full access to any
    RDS database instance in our target account as the administrator user, which would
    grant us full access to the data stored within.
  prefs: []
  type: TYPE_NORMAL
- en: 'This attack process can be done manually, or with the `rds__explore_snapshots`
    Pacu module. The goal is to abuse RDS database instance backups to create a new
    copy of the existing databases with our own private access. If we gained access
    to RDS and there was a single instance and no backups, the process would entail
    the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a snapshot of the running database instance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Restore that snapshot to a new database instance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Change the master password of our new database instance to something we know.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Change the database to be publicly accessible and modify any security group
    rules to allow us inbound access to the correct ports.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Connect to the database with the credentials we set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use something like `mysqldump` to exfiltrate the entire database.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once connected, it will be a complete copy of the single production database
    in the account, meaning we can do anything we want with it. A good move, depending
    on the amount of data in the database, would be to use a tool like `mysqldump`
    to exfiltrate the SQL database to comb manually or import it into another external
    database that isn't at risk of having access revoked at any point. Make sure to
    delete the snapshot you created of the original database and the database instance
    that you created when you're done; otherwise, you may run up some charges in the
    target account. That could be bad for a few reasons, including making your client
    angry and/or getting your activity caught by billing alerts.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is a simple process to do manually, but often it will be a better decision
    to automate, so that you don''t make any manual mistakes and screw up a production
    database in the process. You can simply run the following Pacu command to automate
    most of the process for all database instances (use the `--region`s flag for specific
    regions):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/067a9c2b-06f5-40a3-8683-768393a778c7.png)'
  prefs: []
  type: TYPE_IMG
- en: Part of the output from the `rds__explore_snapshots` module
  prefs: []
  type: TYPE_NORMAL
- en: The preceding screenshot shows part of the output from the `rds__explore_snapshots`
    module. It will scan the regions you specify for RDS instances, give you their
    names, and then prompt you to copy it or not. If you select yes, it will create
    a snapshot of that database, restore that snapshot to a new database, modify the
    master password, and then provide you with the connection credentials. Then, you
    can go about dumping the database with something like `mysqldump` or grabbing
    specific data you require from within the DB. After that, you would press *Enter*
    in Pacu to move on to the next database that's available, to which the module
    would then delete the database snapshot and database instance that it just created.
    If the module fails at all during any of its processes, it will try to clean up
    any outstanding resources from previous runs when you run it again. That way,
    you don't need to worry about deleting any resources that you created for your
    attack.
  prefs: []
  type: TYPE_NORMAL
- en: Another interesting point about this attack on RDS is that modifying the master
    password is lumped in with a whole bunch of other configuration changes, so it
    isn't necessarily a highly monitored API call. It uses the RDS `ModifyDbInstance`
    API to change the master password, but that same API is also used to modify networking
    settings, monitoring settings, authentication settings, logging settings, and
    a lot more.
  prefs: []
  type: TYPE_NORMAL
- en: The authenticated side of S3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There is already plenty of research out there regarding AWS S3, but from the
    authenticated side of things, it is a little bit different. When moving into S3
    during the exploitation phase, most of the process is built around identifying
    public resources (buckets/objects) that shouldn't be, but it is also more than
    that. It is time to review automation built around S3 and to see how it is exploitable,
    and it also is time to review the contents of the various buckets to see if you
    can gain further access from what you find.
  prefs: []
  type: TYPE_NORMAL
- en: It can be helpful for a client to know that their developers have access to
    the X, Y, and Z S3 buckets, and that you found a private SSH key stored in bucket
    Y, which then led to the compromise of an EC2 instance, which provided further
    AWS credentials, and so on. Clients not following the principal of least privilege
    will often be open to a wide range of attacks, especially within S3.
  prefs: []
  type: TYPE_NORMAL
- en: When reviewing files stored in S3, it will often take far too long to look at
    every file in every bucket, so it's best to prioritize what you are looking for.
    Often, bucket, file, and folder names will be the best indicator of whether a
    file is worth viewing or not. Something like `names.txt` would likely not be worth
    your time, but something like `backup.sql` would be worth your time. Typically,
    it is best to scour these files for credentials, API keys, customer data, or anything
    sensitive, really. You could use this data to show privilege escalation paths,
    cross-account compromise attacks, and anything else, depending on what kind of
    data it is that you find. Maybe it grants you access to their corporate website,
    or maybe their internal VPN. There are endless possibilities and it all just depends
    on what you find.
  prefs: []
  type: TYPE_NORMAL
- en: When looking for public resources, it is best to alert the client of all findings,
    even if the content is not sensitive. If an entire bucket is set to public, someone
    may inadvertently upload a file that isn't supposed to be public, or if the bucket
    is publicly listable, a user who finds the bucket name would be able to enumerate
    every file within the bucket. It is important to note that even if the files in
    the bucket need to be public, the bucket does not need to be publicly listable.
  prefs: []
  type: TYPE_NORMAL
- en: When reviewing automation that was built around S3, it is best to check for
    S3 events and logging on each bucket. This way, you can see how they are acting
    (or not) on activity within their private buckets.
  prefs: []
  type: TYPE_NORMAL
- en: S3 bucket and filenames can also be helpful as a type of recon within the environment.
    Often, you can discover that certain AWS services are being used within the account
    just based on S3 bucket names. Many services and functions will auto-create S3
    buckets with templated names, so it is simple to make the correlation in that
    situation.
  prefs: []
  type: TYPE_NORMAL
- en: Auditing for compliance and best practices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In addition to the flat-out exploitation of AWS services and resources, it
    is also important to provide your client with a general security audit in as many
    locations as you can. These types of checks typically fall into a small set of
    categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Public access**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can X be accessed publicly? Should that be possible?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Encryption**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is Y encrypted at-rest? Is Z encrypted in-transit?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Logging**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Are logs enabled for C? Is anything being done with those logs?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Backups**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is D being backed up? How often?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Other security controls**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is MFA being used?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Password policy strength?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deletion protection on the right resources?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Of course, there is more to it than just those few, but generally these are
    the most common types of findings.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are already many tools out there to provide this kind of insight into
    an environment, including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Prowler
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Security Monkey
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scout2/ScoutSuite
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are many others, as well, and they all do something a little different
    than the next one, so it can often be a personal choice as to which one you end
    up using.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: AWS pentesting is an extensive process that requires a wide variety of knowledge
    and dedication, and it really is a never-ending process. There are always new
    services and functionality being released by AWS, so there will always be new
    security checks and attacks for those services.
  prefs: []
  type: TYPE_NORMAL
- en: As a pentester, it is difficult to be able to say you are done pentesting an
    AWS environment because of how massive and complicated they can be, so it is important
    to hit as many different services and attacks as possible, all while staying within
    the timeline that you agreed upon with your client.
  prefs: []
  type: TYPE_NORMAL
- en: Every real-world pentest that you do will likely vary greatly from the previous
    one. With the size and complexity of AWS and its offerings, people will be doing
    things differently wherever you go, so it is important to never get comfortable
    and instead always expect to be learning, teaching, and succeeding.
  prefs: []
  type: TYPE_NORMAL
- en: We hope that what you have learned in this chapter about real-world AWS penetration
    testing can help you in your own work and move the entire AWS security community
    forward. We covered the initial pentest kickoff and unauthenticated plus authenticated
    reconnaissance, including enumeration of our permissions. Then, we moved on to
    escalating those permissions through IAM misconfigurations, where we then used
    our elevated access to establish a means of persistence in the environment. After
    our access was secured, we moved on to the general post-exploitation of AWS services,
    where all the real magic happens. Beyond that, we took a short look at how to
    go about identifying and aggregating compliance and best practice checks to provide
    a thorough, useful report to our clients.
  prefs: []
  type: TYPE_NORMAL
- en: AWS pentesting is a fun, complicated process that can only be expanded on, so
    now we need you to get out there and contribute your knowledge and experience
    to create a safe AWS experience for all of the users out there.
  prefs: []
  type: TYPE_NORMAL
