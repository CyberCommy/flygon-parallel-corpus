- en: Learning Event Sourcing and CQRS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the last chapter, we looked into the drawbacks of our current business model,
    and now, in this chapter, we'll look at how **Event Sourcing** (**ES**) and **CQRS**
    (**Command Query Responsibility Segregation**) would be helpful to overcome those.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will talk about some architectural designs that deal with
    massive scalability. We will also look at two patterns, Event Sourcing and CQRS,
    which are all about solving the problem response behavior for such an enormous
    number of requests.
  prefs: []
  type: TYPE_NORMAL
- en: Many of us think that compliance with twelve-factor apps will make our application
    a cloud native application with higher scalability, but there are other strategies,
    such as ES and CQRS, which can make our application more reliable.
  prefs: []
  type: TYPE_NORMAL
- en: Since cloud native applications are internet facing, we expect thousands or
    millions of requests from different sources. Implementing infrastructure architecture
    to handle the requests by scaling up or down aren't enough. You need to make your
    application support such enormous scaling. That's when these patterns come into
    the picture.
  prefs: []
  type: TYPE_NORMAL
- en: 'The topics that we will cover in this chapter are listed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to Event Sourcing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to Command Query Responsibility Segregation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Example code to implement ES and CQRS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Event Sourcing with Apache Kafka
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s start with reviewing the *n*-tier architecture, where we have some clients,
    a network, a business model, some business logic, some data storage, and so on.
    This is a basic model, which you will find as part of any architectural design.
    It looks something like the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00078.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see in this architecture, we have these different models that come
    into action:'
  prefs: []
  type: TYPE_NORMAL
- en: '**View Model**: This is basically for client-side interaction'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**DTO Model**: This is for communication between the client and the REST Endpoints'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Business Model**: This is a combination of **DAO** (**Data Access Object**)
    and business service, which interprets the user requests, and communicates with
    the storage service'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**E-R Model**: This defines the relationship between entities (that is, **DTO**
    and **RDMS**/**NDMS**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now that you have some idea about the architecture, let''s understand its characteristics,
    which are listed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Identical stack for application**: In this model, we use the same stack of
    elements for all read and write operations, starting from REST API to business
    service, and then we access the storage services, and so on, as all the different
    component codes are deployed together as a single entity.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following diagram shows the **Read**/**Write** operation flow through different
    models:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00079.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: '**Identical Data Model**: In this scenario, you will find that most of the
    times, we use the same or a similar data model for business logic processing,
    or for reading and writing data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deployment Units**: We use coarse-grained deployment units, which consist
    of the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A build (an executable collection of components)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Documents (end-user support material and release notes)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installation artifacts, which combine both the read and write code together
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Accessing data directly:** If we want to change data, we usually go ahead.
    Especially, in the case of RDBMS, we change the data directly, as in the following
    case--if we want to update the row with **User ID** **1** with another dataset,
    we usually do it directly. Also, once we have updated this value, the old value
    will be void from the application as well as the storage side, and cannot be retrieved:![](img/00080.jpeg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So far, we have been making use of the preceding approach, and I would say that
    it is pretty much proven and successful in terms of the response from user requests.
    However, there are other alternate approaches which can perform much better than
    this when compared.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s discuss the drawbacks of the aforementioned business architecture approach,
    which are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Inability to scale independently**: Since our code for the read and write
    operations reside at the same location, we cannot scale our read or write for
    the application independently. Say you have 90% read and 10% write from the application
    side at a particular point in time, we can''t scale our read independently. In
    order to scale reads, we need to scale out the complete architecture, which is
    of no use, and increases the waste of resources.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**No data history**: Since we are dealing with the scenario where we update
    the data directly, once the data is updated, the application will start showing
    the latest dataset after some period of time. Also, once the dataset is updated,
    old data values are not tracked, and hence, are lost. Even if we want to implement
    such kinds of features, we need to write lots of code to enable it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Monolithic approach**: This approach tends to be a monolithic approach, as
    we try to merge things together. Moreover, we have coarse-grained deployment units,
    and we try to keep the code of the different components together. So, this kind
    of approach will ultimately result in a mess, which will be difficult to resolve.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One kind of approach which addresses these challenges is Event Sourcing.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Event Sourcing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By simple definition, Event Sourcing is an architectural pattern which determines
    the state of an application by a sequence of events.
  prefs: []
  type: TYPE_NORMAL
- en: The best way to understand Event Sourcing is by using an analogy. One of the
    best examples would be **online** **shopping**, which is an event processing system.
    Somebody places an order, which gets registered in an order queue for a vendor
    ordering system. Then, this status is notified to the customer at different stages
    of the order being delivered.
  prefs: []
  type: TYPE_NORMAL
- en: 'All these events, which occur one after the other, form a sequence of events
    called an event stream, which should look something like the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00081.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: So, Event Sourcing takes consideration of events which happened in the past,
    and are recorded for processing based on certain transactions.
  prefs: []
  type: TYPE_NORMAL
- en: 'An ideal Event Sourcing system is based on the building blocks shown in the
    following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00082.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The preceding diagram depicts an ideal event processing system, starting from
    the application to the creation of **Events** related to a certain incident, and
    then putting them in an **Event Queue** for further processing, which is performed
    by an **Event Handler**. Based on the description of the **Events**, the **Event
    Handler** processes them accordingly, and registers them in the **Store**.
  prefs: []
  type: TYPE_NORMAL
- en: Event Sourcing follows certain laws/tenets, which make application development
    a structured and disciplined process. Most people usually feel that Event Sourcing
    is hard or they think it is outright because of these tenets, which must not be
    broken, as doing so will create a huge chaos in the application.
  prefs: []
  type: TYPE_NORMAL
- en: Laws of Event Sourcing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Listed next are some of the Event Sourcing laws which need to be maintained
    while implementing ES on any system (that is, application design):'
  prefs: []
  type: TYPE_NORMAL
- en: '**Idempotency**: An ideal event-sourced business logic must be idempotent.
    This means that when you execute a business logic against an input stream of data,
    the resultant state of the application will always remain the same. Yes, that''s
    true, it will remain the same irrespective of the number of times you execute
    the business logic.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Isolation**: Event Sourcing must not depend on the external event streams.
    This is one of the most important tenets of Event Sourcing. Generally, business
    logic is rarely ever executed in a vacuum. Applications usually interact with
    external entities for reference. Moreover, applications make use of cached information
    from external sources, even if developers don''t consider that point. Now, the
    question that arises is what happens if your business logic uses the external
    input to compute results? Let''s take the example of a stock exchange, where stock
    prices keep on changing, which means that the stock price at the time of state
    computation won''t be the same on multiple evaluations, which violates the idempotent
    rule.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As per the developer's understanding, this is a very difficult condition to
    satisfy. However, the solution to deal with this is to inject notifications into
    the main event stream from external events. Since these notifications are now
    part of the main events stream, you will get the expected result every time.
  prefs: []
  type: TYPE_NORMAL
- en: '**Quality assurance**: An event-sourced application, after being developed
    completely, should be a well-tested application. Writing test cases for the event-sourced
    application is easy--it usually takes a list of inputs and returns some state,
    considering that you are writing test cases following the previously defined principles.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Recoverable**: Event-sourced applications should support recovery and replay.
    If you have a cloud native application which adheres to all the guidelines of
    the twelve-factor apps to create an application suitable for the cloud platform,
    Event Sourcing plays a vital role in disaster recovery.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assuming that the event stream is durable, an event-sourced application's initial
    advantage is to compute the state of the application. Usually, in a cloud environment,
    it is possible that the application crashes because of numerous reasons; Event
    Sourcing can help us identify the last state of the application, and recover it
    quickly to reduce the downtime. Moreover, Event Sourcing's replay functionality
    gives you the ability to look at the past state at the time of auditing, as well
    as troubleshooting.
  prefs: []
  type: TYPE_NORMAL
- en: '**Big Data**: Event Sourcing applications often generate huge amounts of data.
    Since an event-sourced application keeps track of every event, it is possible
    that it will generate huge amounts of data. It depends on how many events you
    have, how often they arrive, and how huge the data payload is for the events.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Consistency**: Event-sourced applications often maintain consistency for
    the registering of events. Think of banking transactions--every event happening
    during a bank transaction is very crucial. It should be noted that consistency
    should be maintained while recording it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It is very important to understand that these events are something that happened
    in the past, because when we name these events, they should be understandable.
    Examples of a few valid names for events could be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`PackageDeliveredEvent`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`UserVerifiedEvent`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PaymentVerifiedEvent`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Invalid events would be named as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`CreateUserEvent`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`AddtoCartEvent`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following is some example code for an event:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'There are a few points that you should know:'
  prefs: []
  type: TYPE_NORMAL
- en: Every event is immutable, which means that an event, once fired, cannot be reverted.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You never delete an event. Even if we try to delete an event, we consider deletion
    also as an event.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Event streams are driven by message-broker architecture. Some of the message
    brokers are RabbitMQ, ActiveMQ, and so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, let''s discuss some of the pros of Event Sourcing, which are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Event Sourcing gives the capability to rebuild the system very quickly
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Event Sourcing gives you command over the data, which means that the data we
    require for our processing is easy to acquire by looking at the event stream for
    your processing purpose, say by audit, analysis, and so it should be audit, analysis,
    and so on
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By looking at the events, it is easy to understand what went wrong during a
    period of time, considering a set of data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Event replay would be advantageous during troubleshooting or bug fixing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, the question arises that since we are generating such a huge amount of
    events, does this affect the performance of the application? I would say, YES!
  prefs: []
  type: TYPE_NORMAL
- en: As our application is generating events for every transaction which needs to
    be processed by the event handler, the response time of the application is reduced.
    The solution to this problem is CQRS.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to CQRS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Command Query Responsibility Segregation is a fancy pattern name, which means
    decoupling the input and the output of your system. In CQRS, we mainly talk about
    the read and write characteristics of our application; so, the commands in the
    context of CQRS are mainly write operations, while the queries are read operations,
    and responsibility means that we separate our read and write operations.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we look at the architecture described in the first section, Introduction,
    and apply CQRS, the architecture will be divided into half, and would look something
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00083.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Now we will look at some code examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'A traditional interface module would look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Split-up, or as I prefer to call them, CQRS-ified interfaces, would look something
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'So, the overall architecture, after the implementation of CQRS and Event Sourcing,
    would be something like the one shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00084.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: This is the complete architecture after the implementation of Event Sourcing
    and CQRS.
  prefs: []
  type: TYPE_NORMAL
- en: In a classic monolithic application, you have endpoints that write to a database,
    and endpoints that read from it. The same database is used for both read and write
    operations, and you don't reply to the endpoints until an acknowledgement or commit
    is received from the database.
  prefs: []
  type: TYPE_NORMAL
- en: On a massive scale, with a high inbound event throughput and complex event processing
    requirements, you can't afford to run slow queries for reads, nor can you afford
    to sit and wait for processing to take place every time you get a new inbound
    event.
  prefs: []
  type: TYPE_NORMAL
- en: 'The flow for both read and write operations works as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Write model**: In this case, when a command is fired from the endpoint and
    received at the **Command Business Service**, it first issues the events for every
    incident to the **Event Store**. In the **Event Store**, you also have a **Command
    processor**, or, in other words, event handler, and this **Command processor**
    is able to derive the application state into a separate **Storage**, which could
    be a relational storage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Read model**: In the case of the **Read** model, we simply use the **Query
    Endpoints** to query the data which we want to **Read** or retrieve for the application
    usage by the client.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The biggest advantage is that we don't need to go through the **Write** model
    (which is on the right-hand side of the preceding image). When it comes to querying
    the database, this process makes our query execution faster, and reduces the response
    time which, in turn, increases the application's performance.
  prefs: []
  type: TYPE_NORMAL
- en: Advantages of the CQRS-ified architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This architecture has the following advantages:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Independent scalability and deployment**: We can now scale and deploy an
    individual component based on its usage. As in the case of microservices, we can
    now have separate microservices for each of the tasks, say a read microservice
    and a write microservice, in this architecture stack.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Choice of technologies**: Freedom with regards to the choice of technologies
    for the different sections of the business model. For instance, for the command
    functionality, we could choose Scala or similar (assuming that we have a complex
    business model, and we have a lot of data to write). In the case of a query, we
    can choose, for example, ROR (Ruby on Rails) or Python (which we are already using).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This type of architecture is best suited for bounded context from **DDD** (**Domain-Driven
    design**), because we can define the business context for the microservices.
  prefs: []
  type: TYPE_NORMAL
- en: Challenges related to ES and CQRS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Every architecture design model has its own challenges for implementation.
    Let''s discuss the challenges of ES and CQRS:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Inconsistency**: Systems developed using ES and CQRS are mostly consistent.
    However, as we store the events issued by the **Command Business Service** at
    the **Event Store**, and store the state of the application in the main **Storage**
    as well, I would say this kind of system is not fully consistent. If we really
    want to make our system fully consistent using ES and CQRS, we need to keep our
    **Event Store** and main **Storage** on a single **Relational Database**, and
    our **Command processor** should handle all our incoming events, and store them
    in both storages at the same time, as depicted in the following diagram:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/00085.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: I would say that the consistency level should be defined by understanding the
    business domain. How much consistency you would need in events, and how much these
    consistencies would cost, needs to be understood. After inspecting your business
    domain, you will be able to make these decisions considering the aforementioned
    factors.
  prefs: []
  type: TYPE_NORMAL
- en: '**Validation**: It is very easy when we talk in terms of validating the customer
    registration form, where we need to validate the individual field, and so on.
    But actual validation comes when we have to do validation based on uniqueness--say
    we have a customer with certain user credentials (username/password). So, to make
    sure that the username is unique is a crucial validation when we have more than
    2 million customers who need to be registered. A few questions that need to be
    asked in terms of validation are as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the data requirement for validation?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Where to retrieve the data for validation from?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the probability of validation?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the impact of validation failure on the business?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Parallel data updates**: This is very crucial in terms of data consistency.
    Say, you have a user who wants to update certain records at the same time, or
    within a difference of nanoseconds. In this case, the possibility of consistency
    as well as validation checks is challenging, as there is a possibility that one
    user might end up overwriting the other user information which could create chaos.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overcoming challenges
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One way to solve such a problem in Event Sourcing is to add versions in events,
    which will act as a handle for making changes to the data and make sure it is
    validated fully.
  prefs: []
  type: TYPE_NORMAL
- en: Problem solving
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s take the use case shown in the following diagram for Event Sourcing
    and CQRS to understand it in terms of writing code for it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00086.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Explanation of the problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this case, we are provided with **User Details** such as **User ID** (which
    should be unique), **username**, **password**, **email ID**, and so on, and we
    have to create two write **Commands** to be fired--**UserRegistrationCommand**
    and **UpdatePasswordCommand**, which trigger two **Events:** **UserRegisterEvents**
    and **UpdatePasswordEvents**. The idea is that a user, once registered, should
    be able to reset the password as per their needs.
  prefs: []
  type: TYPE_NORMAL
- en: The solution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to solve this problem, we will need to write functions related to write
    commands to receive the inputs and update the event store.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s add the following code to the `commands.py` file, which will have
    code related to the write commands that need to be performed as described:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: So, we have added the functions related to the commands, but it should be called
    from somewhere with the user details.
  prefs: []
  type: TYPE_NORMAL
- en: Let's add a new file called `main.py` from where the preceding command's function
    will be called.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code, we call the preceding code by triggering events:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s understand the preceding code, function by function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The last code initializes the `self` object with some default values; it is
    similar to the initialize function in any programming language.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we defined the `userRegister` function, which, basically, collects `userdetails`,
    and then creates the event (`UserRegisterevent(userdetails))`) as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'So, once the user is registered, he/she is authorized to update the profile
    details, which could be the email ID, password, username, and others--in our case,
    it is the password. Please refer to the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: You can write similar code for updating the email ID, username, or others.
  prefs: []
  type: TYPE_NORMAL
- en: 'Moving on, we need to add error handling, as in our `main.py` file, we call
    a custom module, `errors`, to handle operation-related errors. Let''s add the
    following code to `errors.py` to pass the errors if caught:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: As you can see in `main.py`, we call the `Aggregate` module, and you must be
    wondering why it is being used. The `Aggregate` module is very important as it
    keeps track of the changes that need to be applied. In other words, it forces
    the event to commit all its uncommented changes to the event store.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to do so, let''s add the following code to a new file called `aggregate.py`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: In `aggregate.py`, we initialize the `self` object, which is called in `main.py`,
    and then keep a track of events which are being triggered. After a period of time,
    we will make a call to apply the changes from `main.py` to update `eventstore`
    with the updated values and events.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we are left with the command handler, which is very important, as it decides
    which operation needs to be performed and the respective events that need to be
    triggered. Let''s add the file `command_handler.py` with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: In `command_handler.py`, we have written a handle function which will make the
    decision of the flow of event execution.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, we called the `@contextmanager` module, which is very important
    to understand here.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a scenario: suppose there are two people, Bob and Alice, and both
    are using the same user credentials. Let''s say they both are trying to update
    the profile details field, for example, the password, at the same time. Now, we
    need to understand how these commands get requested. In short, whose request will
    hit the event store first. Also, if both the users update the password, then it
    is highly possible that one user''s updated password will be overwritten by another.'
  prefs: []
  type: TYPE_NORMAL
- en: One way of solving the problem is to use version along with user schema, as
    we use it in the context manager. We take `user_version` as an argument, which
    will determine the state of the user data, and once it is modified, we can increment
    the version to make the data consistent.
  prefs: []
  type: TYPE_NORMAL
- en: So, in our case, if Bob's modified value is updated first (of course, with the
    new version), and if Alice's request version field doesn't match with the version
    in the database, then Alice's update request will be rejected.
  prefs: []
  type: TYPE_NORMAL
- en: Once this is updated, we should be able to register and update the password.
    Though this is an example to show how to implement CQRS, you can extend this to
    create microservices on top of it.
  prefs: []
  type: TYPE_NORMAL
- en: Kafka as an eventstore
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although we have already seen the CQRS implementation, I still feel that you
    may have a few queries related to `eventstore`, and how it works. That's why I'll
    take the use case of Kafka, which can be used as an `eventstore` for your application.
  prefs: []
  type: TYPE_NORMAL
- en: Kafka is, typically, a message broker or message queue (similar to RabbitMQ,
    JMS, and others).
  prefs: []
  type: TYPE_NORMAL
- en: As per the Kafka documentation, Event Sourcing is a style of application design
    where the state changes are logged as a time-ordered sequence of records. Kafka's
    support for very large stored log data makes it an excellent backend for an application
    built in this style.
  prefs: []
  type: TYPE_NORMAL
- en: 'For more information related to implementing Kafka, read its documentation
    at this link: [https://kafka.apache.org/documentation/](https://kafka.apache.org/documentation/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Kafka has the following basic components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Producers**: This sends messages to Kafka'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Consumers**: These subscribe to streams of messages in Kafka'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kafka works in the following manner:'
  prefs: []
  type: TYPE_NORMAL
- en: Producers write messages in Kafka topics, which could be users
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Every message that is in a Kafka topic is appended at the end of the partition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kafka only supports **write** operations.
  prefs: []
  type: TYPE_NORMAL
- en: Partitions represent streams of events, and topics can be categorized into multiple
    topics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Partitions in topics are independent of each other
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To avoid disaster, Kafka partitions are replicated across several machines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To consume Kafka messages, the client reads the message sequentially, starting
    from the offset, which is set in Kafka by the consumer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying Event Sourcing with Kafka
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s take a use case where the client tries to perform a certain operation,
    and we are using Kafka as an eventstore to capture all the messages that are being
    passed. In this case, we have the user management service, which could be a microservice
    responsible for managing all user requests. We will start with identifying the
    topics for Kafka based on user events, which could be one of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`UserCreatedEvent`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`UserUpdatedEvent`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`UserDeletionEvent`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`UserLoggedinEvent`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`UserRoleUpdatedEvent`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These events will, ideally, be published by the **User Management Service**,
    and all microservices will consume these events. The following diagram shows the
    user request flow:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00087.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: How it works
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A user makes a `POST` request to the API gateway, which is an entry point for
    the user management service to register users. The API gateway, in turn, makes
    an **RPC** Call (**Remote procedure call**) to the `createUser` method in the
    management service. The `createUser` endpoint performs a set of validations on
    the user input. If the input is invalid, it will throw an exception, and return
    the error to the API gateway. Once the user input is validated, the user is registered,
    and `UserCreatedEvent` is triggered to get published in Kafka. In Kafka, partitions
    capture the events. In our example, the users topic has three partitions, so the
    event will be published to one of the three partitions based on some defined logic;
    this logic is defined by us, which varies based on the use case.
  prefs: []
  type: TYPE_NORMAL
- en: All read operations such as listing user, and more, can be retrieved directly
    from readStore (database such as PostgreSQL).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This was a complex chapter, but if you understand it fully, it will make your
    application efficient and high performance.
  prefs: []
  type: TYPE_NORMAL
- en: We kicked off by understanding the drawbacks of the classic architecture, and
    then moved on to discuss the concept and implementation of ES and CQRS. We also
    looked at the implementation of a sample problem. We talked about why these patterns
    are useful, and how they have a particular harmony with massive-scale, cloud native
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: In the upcoming chapter, we are going to deep dive into the security of the
    application. Stay tuned!
  prefs: []
  type: TYPE_NORMAL
