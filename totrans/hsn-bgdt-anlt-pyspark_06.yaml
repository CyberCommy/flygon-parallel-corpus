- en: Putting Structure on Your Big Data with SparkSQL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we'll learn how to manipulate DataFrames with Spark SQL schemas,
    and use the Spark DSL to build queries for structured data operations. By now
    we have already learned to get big data into the Spark Environment using RDDs
    and carried out multiple operations on that big data. Let us now look that how
    to manipulate our DataFrames and build queries for structured data operations.
  prefs: []
  type: TYPE_NORMAL
- en: 'In particular, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Manipulating DataFrames with Spark SQL schemas
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Spark DSL to build queries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Manipulating DataFrames with Spark SQL schemas
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will learn more about DataFrames and learn how to use Spark
    SQL.
  prefs: []
  type: TYPE_NORMAL
- en: The Spark SQL interface is very simple. For this reason, taking away labels
    means that we are in unsupervised learning territory. Also, Spark has great support
    for clustering and dimensionality reduction algorithms. We can tackle learning
    problems effectively by using Spark SQL to give big data a structure.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at the code that we will be using in our Jupyter Notebook. To
    maintain consistency, we will be using the same KDD cup data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will first type `textFile` into a `raw_data` variable as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'What''s new here is that we are importing two new packages from `pyspark.sql`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Row`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`SQLContext`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following code shows us how to import these packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Using `SQLContext`, we create a new `sql_context` variable that holds the object
    of the `SQLContext` variable created by PySpark. As we're using `SparkContext`
    to start this `SQLContext` variable, we need to pipe in `sc` as the first parameter
    of the `SQLContext` creator. After this, we need to take our `raw_data` variable
    and map it with the `l.split` lambda function to create an object that holds our
    **comma-separated values** (**CSV**).
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll leverage our new important `Row` objects to create a new object that
    has defined labels. This is to label our datasets by what feature we are looking
    at, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we've taken our comma-separated values (`csv`), and we've
    created a `Row` object that takes the first feature, called `duration`; the second
    feature, called `protocol`; and the third feature, called `service`. This directly
    corresponds to our labels in the actual datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can create a new DataFrame by calling the `createDataFrame` function
    in our `sql_context` variable. To create this DataFrame, we need to feed in our
    row data objects, and the resulting object would be a DataFrame in `df`. After
    this, we need to register a temporary table. Here, we are just calling it `rdd`.
    By doing this, we can now use ordinary SQL syntax to query the content in this
    temporary table that is constructed by our rows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'In our example, we need to select the duration from `rdd`, which is a temporary
    table. The protocol we have selected here is equal to `''tcp''`, and the duration,
    which is our first feature in a row, is larger than `2000`, as demonstrated in
    the following code snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, when we call the `show` function, it gives us every single data point
    that matches these criteria:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We will then get the following output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Using the preceding example, we can infer that we can use the `SQLContext` variable
    from the PySpark package to package our data in a SQL friendly format.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, not only does PySpark support using SQL syntax to query the data,
    but it can also use the Spark **domain-specific language** (**DSL**) to build
    queries for structured data operations.
  prefs: []
  type: TYPE_NORMAL
- en: Using Spark DSL to build queries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will use Spark DSL to build queries for structured data
    operations:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following command, we have used the same query as used earlier; this
    time expressed in the Spark DSL to illustrate and compare how using the Spark
    DSL is different, but achieves the same goal as our SQL is shown in the previous
    section:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: In this command, we first take the `df` object that we created in the previous
    section. We then select the duration by calling the `select` function and feeding
    in the `duration` parameter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, in the preceding code snippet, we call the `filter` function twice, first
    by using `df.duration`, and the second time by using `df.protocol`. In the first
    instance, we are trying to see whether the duration is larger than `2000`, and
    in the second instance, we are trying to see whether the protocol is equal to
    `"tcp"`. We also need to append the `show` function at the very end of the command
    to get the same results, as shown in the following code block:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Here, we have the top 20 rows of the data points again that fit the description
    of the code used to get this result.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered Spark DSL and learned how to build queries. We also
    learned how to manipulate DataFrames with Spark SQL schemas, and then we used
    Spark DSL to build queries for structured data operations. Now that we have a
    good knowledge of Spark, let's look at a few tips, tricks, and techniques in Apache
    Spark in the following chapters.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will look at transformations and actions in an Apache
    Spark program.
  prefs: []
  type: TYPE_NORMAL
